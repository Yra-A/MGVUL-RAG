{
  "CVE-2012-3400": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper calculation of partition table length without proper validation against the block size of the super block.",
        "trigger_condition": "Crafted UDF filesystem input causing the calculated partition table length to exceed the block size of the super block, leading to a heap-based buffer overflow.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the calculated partition table length plus the size of the logical volume descriptor exceeds the block size of the super block, allowing a buffer overflow to occur when accessing the partition table."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure that the sum of the calculated partition table length and the size of the logical volume descriptor does not exceed the block size of the super block. This prevents the buffer overflow by ensuring that the buffer length is within the bounds of the block size before accessing the partition table.",
      "GPT_analysis": "The modification is necessary to fix a heap-based buffer overflow vulnerability in the udf_load_logicalvol function. The vulnerability allows remote attackers to cause a denial of service (system crash) or potentially have other unspecified impacts by exploiting a crafted UDF filesystem.\n\nThe vulnerability arises from the calculation of the partition table length, which can lead to a buffer overflow if the calculated length exceeds the block size of the super block. By ensuring that the buffer length is properly checked against the block size before accessing the partition table, the modification helps prevent the buffer overflow issue and enhances the security of the code.\n\nIn summary, the modification adds a check to prevent buffer overflow and addresses the vulnerability described in CVE-2012-3400 by ensuring that the partition table length is within the bounds of the block size.",
      "GPT_purpose": "Load logical volume descriptor information from a UDF filesystem super block.",
      "GPT_function": "\n1. Load logical volume descriptor from a UDF filesystem.\n2. Parse partition maps and handle different types of partitions.\n3. Extract fileset information if provided.\n4. Handle integrity sequence extension if present.\n5. Release buffer head after processing.",
      "CVE_id": "CVE-2012-3400",
      "code_before_change": "static int udf_load_logicalvol(struct super_block *sb, sector_t block,\n\t\t\t       struct kernel_lb_addr *fileset)\n{\n\tstruct logicalVolDesc *lvd;\n\tint i, j, offset;\n\tuint8_t type;\n\tstruct udf_sb_info *sbi = UDF_SB(sb);\n\tstruct genericPartitionMap *gpm;\n\tuint16_t ident;\n\tstruct buffer_head *bh;\n\tunsigned int table_len;\n\tint ret = 0;\n\n\tbh = udf_read_tagged(sb, block, block, &ident);\n\tif (!bh)\n\t\treturn 1;\n\tBUG_ON(ident != TAG_IDENT_LVD);\n\tlvd = (struct logicalVolDesc *)bh->b_data;\n\ttable_len = le32_to_cpu(lvd->mapTableLength);\n\tif (sizeof(*lvd) + table_len > sb->s_blocksize) {\n\t\tudf_err(sb, \"error loading logical volume descriptor: \"\n\t\t\t\"Partition table too long (%u > %lu)\\n\", table_len,\n\t\t\tsb->s_blocksize - sizeof(*lvd));\n\t\tgoto out_bh;\n\t}\n\n\tret = udf_sb_alloc_partition_maps(sb, le32_to_cpu(lvd->numPartitionMaps));\n\tif (ret)\n\t\tgoto out_bh;\n\n\tfor (i = 0, offset = 0;\n\t     i < sbi->s_partitions && offset < table_len;\n\t     i++, offset += gpm->partitionMapLength) {\n\t\tstruct udf_part_map *map = &sbi->s_partmaps[i];\n\t\tgpm = (struct genericPartitionMap *)\n\t\t\t\t&(lvd->partitionMaps[offset]);\n\t\ttype = gpm->partitionMapType;\n\t\tif (type == 1) {\n\t\t\tstruct genericPartitionMap1 *gpm1 =\n\t\t\t\t(struct genericPartitionMap1 *)gpm;\n\t\t\tmap->s_partition_type = UDF_TYPE1_MAP15;\n\t\t\tmap->s_volumeseqnum = le16_to_cpu(gpm1->volSeqNum);\n\t\t\tmap->s_partition_num = le16_to_cpu(gpm1->partitionNum);\n\t\t\tmap->s_partition_func = NULL;\n\t\t} else if (type == 2) {\n\t\t\tstruct udfPartitionMap2 *upm2 =\n\t\t\t\t\t\t(struct udfPartitionMap2 *)gpm;\n\t\t\tif (!strncmp(upm2->partIdent.ident, UDF_ID_VIRTUAL,\n\t\t\t\t\t\tstrlen(UDF_ID_VIRTUAL))) {\n\t\t\t\tu16 suf =\n\t\t\t\t\tle16_to_cpu(((__le16 *)upm2->partIdent.\n\t\t\t\t\t\t\tidentSuffix)[0]);\n\t\t\t\tif (suf < 0x0200) {\n\t\t\t\t\tmap->s_partition_type =\n\t\t\t\t\t\t\tUDF_VIRTUAL_MAP15;\n\t\t\t\t\tmap->s_partition_func =\n\t\t\t\t\t\t\tudf_get_pblock_virt15;\n\t\t\t\t} else {\n\t\t\t\t\tmap->s_partition_type =\n\t\t\t\t\t\t\tUDF_VIRTUAL_MAP20;\n\t\t\t\t\tmap->s_partition_func =\n\t\t\t\t\t\t\tudf_get_pblock_virt20;\n\t\t\t\t}\n\t\t\t} else if (!strncmp(upm2->partIdent.ident,\n\t\t\t\t\t\tUDF_ID_SPARABLE,\n\t\t\t\t\t\tstrlen(UDF_ID_SPARABLE))) {\n\t\t\t\tuint32_t loc;\n\t\t\t\tstruct sparingTable *st;\n\t\t\t\tstruct sparablePartitionMap *spm =\n\t\t\t\t\t(struct sparablePartitionMap *)gpm;\n\n\t\t\t\tmap->s_partition_type = UDF_SPARABLE_MAP15;\n\t\t\t\tmap->s_type_specific.s_sparing.s_packet_len =\n\t\t\t\t\t\tle16_to_cpu(spm->packetLength);\n\t\t\t\tfor (j = 0; j < spm->numSparingTables; j++) {\n\t\t\t\t\tstruct buffer_head *bh2;\n\n\t\t\t\t\tloc = le32_to_cpu(\n\t\t\t\t\t\tspm->locSparingTable[j]);\n\t\t\t\t\tbh2 = udf_read_tagged(sb, loc, loc,\n\t\t\t\t\t\t\t     &ident);\n\t\t\t\t\tmap->s_type_specific.s_sparing.\n\t\t\t\t\t\t\ts_spar_map[j] = bh2;\n\n\t\t\t\t\tif (bh2 == NULL)\n\t\t\t\t\t\tcontinue;\n\n\t\t\t\t\tst = (struct sparingTable *)bh2->b_data;\n\t\t\t\t\tif (ident != 0 || strncmp(\n\t\t\t\t\t\tst->sparingIdent.ident,\n\t\t\t\t\t\tUDF_ID_SPARING,\n\t\t\t\t\t\tstrlen(UDF_ID_SPARING))) {\n\t\t\t\t\t\tbrelse(bh2);\n\t\t\t\t\t\tmap->s_type_specific.s_sparing.\n\t\t\t\t\t\t\ts_spar_map[j] = NULL;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmap->s_partition_func = udf_get_pblock_spar15;\n\t\t\t} else if (!strncmp(upm2->partIdent.ident,\n\t\t\t\t\t\tUDF_ID_METADATA,\n\t\t\t\t\t\tstrlen(UDF_ID_METADATA))) {\n\t\t\t\tstruct udf_meta_data *mdata =\n\t\t\t\t\t&map->s_type_specific.s_metadata;\n\t\t\t\tstruct metadataPartitionMap *mdm =\n\t\t\t\t\t\t(struct metadataPartitionMap *)\n\t\t\t\t\t\t&(lvd->partitionMaps[offset]);\n\t\t\t\tudf_debug(\"Parsing Logical vol part %d type %d  id=%s\\n\",\n\t\t\t\t\t  i, type, UDF_ID_METADATA);\n\n\t\t\t\tmap->s_partition_type = UDF_METADATA_MAP25;\n\t\t\t\tmap->s_partition_func = udf_get_pblock_meta25;\n\n\t\t\t\tmdata->s_meta_file_loc   =\n\t\t\t\t\tle32_to_cpu(mdm->metadataFileLoc);\n\t\t\t\tmdata->s_mirror_file_loc =\n\t\t\t\t\tle32_to_cpu(mdm->metadataMirrorFileLoc);\n\t\t\t\tmdata->s_bitmap_file_loc =\n\t\t\t\t\tle32_to_cpu(mdm->metadataBitmapFileLoc);\n\t\t\t\tmdata->s_alloc_unit_size =\n\t\t\t\t\tle32_to_cpu(mdm->allocUnitSize);\n\t\t\t\tmdata->s_align_unit_size =\n\t\t\t\t\tle16_to_cpu(mdm->alignUnitSize);\n\t\t\t\tif (mdm->flags & 0x01)\n\t\t\t\t\tmdata->s_flags |= MF_DUPLICATE_MD;\n\n\t\t\t\tudf_debug(\"Metadata Ident suffix=0x%x\\n\",\n\t\t\t\t\t  le16_to_cpu(*(__le16 *)\n\t\t\t\t\t\t      mdm->partIdent.identSuffix));\n\t\t\t\tudf_debug(\"Metadata part num=%d\\n\",\n\t\t\t\t\t  le16_to_cpu(mdm->partitionNum));\n\t\t\t\tudf_debug(\"Metadata part alloc unit size=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->allocUnitSize));\n\t\t\t\tudf_debug(\"Metadata file loc=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->metadataFileLoc));\n\t\t\t\tudf_debug(\"Mirror file loc=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->metadataMirrorFileLoc));\n\t\t\t\tudf_debug(\"Bitmap file loc=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->metadataBitmapFileLoc));\n\t\t\t\tudf_debug(\"Flags: %d %d\\n\",\n\t\t\t\t\t  mdata->s_flags, mdm->flags);\n\t\t\t} else {\n\t\t\t\tudf_debug(\"Unknown ident: %s\\n\",\n\t\t\t\t\t  upm2->partIdent.ident);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmap->s_volumeseqnum = le16_to_cpu(upm2->volSeqNum);\n\t\t\tmap->s_partition_num = le16_to_cpu(upm2->partitionNum);\n\t\t}\n\t\tudf_debug(\"Partition (%d:%d) type %d on volume %d\\n\",\n\t\t\t  i, map->s_partition_num, type, map->s_volumeseqnum);\n\t}\n\n\tif (fileset) {\n\t\tstruct long_ad *la = (struct long_ad *)&(lvd->logicalVolContentsUse[0]);\n\n\t\t*fileset = lelb_to_cpu(la->extLocation);\n\t\tudf_debug(\"FileSet found in LogicalVolDesc at block=%d, partition=%d\\n\",\n\t\t\t  fileset->logicalBlockNum,\n\t\t\t  fileset->partitionReferenceNum);\n\t}\n\tif (lvd->integritySeqExt.extLength)\n\t\tudf_load_logicalvolint(sb, leea_to_cpu(lvd->integritySeqExt));\n\nout_bh:\n\tbrelse(bh);\n\treturn ret;\n}",
      "code_after_change": "static int udf_load_logicalvol(struct super_block *sb, sector_t block,\n\t\t\t       struct kernel_lb_addr *fileset)\n{\n\tstruct logicalVolDesc *lvd;\n\tint i, offset;\n\tuint8_t type;\n\tstruct udf_sb_info *sbi = UDF_SB(sb);\n\tstruct genericPartitionMap *gpm;\n\tuint16_t ident;\n\tstruct buffer_head *bh;\n\tunsigned int table_len;\n\tint ret = 0;\n\n\tbh = udf_read_tagged(sb, block, block, &ident);\n\tif (!bh)\n\t\treturn 1;\n\tBUG_ON(ident != TAG_IDENT_LVD);\n\tlvd = (struct logicalVolDesc *)bh->b_data;\n\ttable_len = le32_to_cpu(lvd->mapTableLength);\n\tif (sizeof(*lvd) + table_len > sb->s_blocksize) {\n\t\tudf_err(sb, \"error loading logical volume descriptor: \"\n\t\t\t\"Partition table too long (%u > %lu)\\n\", table_len,\n\t\t\tsb->s_blocksize - sizeof(*lvd));\n\t\tgoto out_bh;\n\t}\n\n\tret = udf_sb_alloc_partition_maps(sb, le32_to_cpu(lvd->numPartitionMaps));\n\tif (ret)\n\t\tgoto out_bh;\n\n\tfor (i = 0, offset = 0;\n\t     i < sbi->s_partitions && offset < table_len;\n\t     i++, offset += gpm->partitionMapLength) {\n\t\tstruct udf_part_map *map = &sbi->s_partmaps[i];\n\t\tgpm = (struct genericPartitionMap *)\n\t\t\t\t&(lvd->partitionMaps[offset]);\n\t\ttype = gpm->partitionMapType;\n\t\tif (type == 1) {\n\t\t\tstruct genericPartitionMap1 *gpm1 =\n\t\t\t\t(struct genericPartitionMap1 *)gpm;\n\t\t\tmap->s_partition_type = UDF_TYPE1_MAP15;\n\t\t\tmap->s_volumeseqnum = le16_to_cpu(gpm1->volSeqNum);\n\t\t\tmap->s_partition_num = le16_to_cpu(gpm1->partitionNum);\n\t\t\tmap->s_partition_func = NULL;\n\t\t} else if (type == 2) {\n\t\t\tstruct udfPartitionMap2 *upm2 =\n\t\t\t\t\t\t(struct udfPartitionMap2 *)gpm;\n\t\t\tif (!strncmp(upm2->partIdent.ident, UDF_ID_VIRTUAL,\n\t\t\t\t\t\tstrlen(UDF_ID_VIRTUAL))) {\n\t\t\t\tu16 suf =\n\t\t\t\t\tle16_to_cpu(((__le16 *)upm2->partIdent.\n\t\t\t\t\t\t\tidentSuffix)[0]);\n\t\t\t\tif (suf < 0x0200) {\n\t\t\t\t\tmap->s_partition_type =\n\t\t\t\t\t\t\tUDF_VIRTUAL_MAP15;\n\t\t\t\t\tmap->s_partition_func =\n\t\t\t\t\t\t\tudf_get_pblock_virt15;\n\t\t\t\t} else {\n\t\t\t\t\tmap->s_partition_type =\n\t\t\t\t\t\t\tUDF_VIRTUAL_MAP20;\n\t\t\t\t\tmap->s_partition_func =\n\t\t\t\t\t\t\tudf_get_pblock_virt20;\n\t\t\t\t}\n\t\t\t} else if (!strncmp(upm2->partIdent.ident,\n\t\t\t\t\t\tUDF_ID_SPARABLE,\n\t\t\t\t\t\tstrlen(UDF_ID_SPARABLE))) {\n\t\t\t\tif (udf_load_sparable_map(sb, map,\n\t\t\t\t    (struct sparablePartitionMap *)gpm) < 0)\n\t\t\t\t\tgoto out_bh;\n\t\t\t} else if (!strncmp(upm2->partIdent.ident,\n\t\t\t\t\t\tUDF_ID_METADATA,\n\t\t\t\t\t\tstrlen(UDF_ID_METADATA))) {\n\t\t\t\tstruct udf_meta_data *mdata =\n\t\t\t\t\t&map->s_type_specific.s_metadata;\n\t\t\t\tstruct metadataPartitionMap *mdm =\n\t\t\t\t\t\t(struct metadataPartitionMap *)\n\t\t\t\t\t\t&(lvd->partitionMaps[offset]);\n\t\t\t\tudf_debug(\"Parsing Logical vol part %d type %d  id=%s\\n\",\n\t\t\t\t\t  i, type, UDF_ID_METADATA);\n\n\t\t\t\tmap->s_partition_type = UDF_METADATA_MAP25;\n\t\t\t\tmap->s_partition_func = udf_get_pblock_meta25;\n\n\t\t\t\tmdata->s_meta_file_loc   =\n\t\t\t\t\tle32_to_cpu(mdm->metadataFileLoc);\n\t\t\t\tmdata->s_mirror_file_loc =\n\t\t\t\t\tle32_to_cpu(mdm->metadataMirrorFileLoc);\n\t\t\t\tmdata->s_bitmap_file_loc =\n\t\t\t\t\tle32_to_cpu(mdm->metadataBitmapFileLoc);\n\t\t\t\tmdata->s_alloc_unit_size =\n\t\t\t\t\tle32_to_cpu(mdm->allocUnitSize);\n\t\t\t\tmdata->s_align_unit_size =\n\t\t\t\t\tle16_to_cpu(mdm->alignUnitSize);\n\t\t\t\tif (mdm->flags & 0x01)\n\t\t\t\t\tmdata->s_flags |= MF_DUPLICATE_MD;\n\n\t\t\t\tudf_debug(\"Metadata Ident suffix=0x%x\\n\",\n\t\t\t\t\t  le16_to_cpu(*(__le16 *)\n\t\t\t\t\t\t      mdm->partIdent.identSuffix));\n\t\t\t\tudf_debug(\"Metadata part num=%d\\n\",\n\t\t\t\t\t  le16_to_cpu(mdm->partitionNum));\n\t\t\t\tudf_debug(\"Metadata part alloc unit size=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->allocUnitSize));\n\t\t\t\tudf_debug(\"Metadata file loc=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->metadataFileLoc));\n\t\t\t\tudf_debug(\"Mirror file loc=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->metadataMirrorFileLoc));\n\t\t\t\tudf_debug(\"Bitmap file loc=%d\\n\",\n\t\t\t\t\t  le32_to_cpu(mdm->metadataBitmapFileLoc));\n\t\t\t\tudf_debug(\"Flags: %d %d\\n\",\n\t\t\t\t\t  mdata->s_flags, mdm->flags);\n\t\t\t} else {\n\t\t\t\tudf_debug(\"Unknown ident: %s\\n\",\n\t\t\t\t\t  upm2->partIdent.ident);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tmap->s_volumeseqnum = le16_to_cpu(upm2->volSeqNum);\n\t\t\tmap->s_partition_num = le16_to_cpu(upm2->partitionNum);\n\t\t}\n\t\tudf_debug(\"Partition (%d:%d) type %d on volume %d\\n\",\n\t\t\t  i, map->s_partition_num, type, map->s_volumeseqnum);\n\t}\n\n\tif (fileset) {\n\t\tstruct long_ad *la = (struct long_ad *)&(lvd->logicalVolContentsUse[0]);\n\n\t\t*fileset = lelb_to_cpu(la->extLocation);\n\t\tudf_debug(\"FileSet found in LogicalVolDesc at block=%d, partition=%d\\n\",\n\t\t\t  fileset->logicalBlockNum,\n\t\t\t  fileset->partitionReferenceNum);\n\t}\n\tif (lvd->integritySeqExt.extLength)\n\t\tudf_load_logicalvolint(sb, leea_to_cpu(lvd->integritySeqExt));\n\nout_bh:\n\tbrelse(bh);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tint i, offset;",
          "\t\t\t\tif (udf_load_sparable_map(sb, map,",
          "\t\t\t\t    (struct sparablePartitionMap *)gpm) < 0)",
          "\t\t\t\t\tgoto out_bh;"
        ],
        "deleted": [
          "\tint i, j, offset;",
          "\t\t\t\tuint32_t loc;",
          "\t\t\t\tstruct sparingTable *st;",
          "\t\t\t\tstruct sparablePartitionMap *spm =",
          "\t\t\t\t\t(struct sparablePartitionMap *)gpm;",
          "",
          "\t\t\t\tmap->s_partition_type = UDF_SPARABLE_MAP15;",
          "\t\t\t\tmap->s_type_specific.s_sparing.s_packet_len =",
          "\t\t\t\t\t\tle16_to_cpu(spm->packetLength);",
          "\t\t\t\tfor (j = 0; j < spm->numSparingTables; j++) {",
          "\t\t\t\t\tstruct buffer_head *bh2;",
          "",
          "\t\t\t\t\tloc = le32_to_cpu(",
          "\t\t\t\t\t\tspm->locSparingTable[j]);",
          "\t\t\t\t\tbh2 = udf_read_tagged(sb, loc, loc,",
          "\t\t\t\t\t\t\t     &ident);",
          "\t\t\t\t\tmap->s_type_specific.s_sparing.",
          "\t\t\t\t\t\t\ts_spar_map[j] = bh2;",
          "",
          "\t\t\t\t\tif (bh2 == NULL)",
          "\t\t\t\t\t\tcontinue;",
          "",
          "\t\t\t\t\tst = (struct sparingTable *)bh2->b_data;",
          "\t\t\t\t\tif (ident != 0 || strncmp(",
          "\t\t\t\t\t\tst->sparingIdent.ident,",
          "\t\t\t\t\t\tUDF_ID_SPARING,",
          "\t\t\t\t\t\tstrlen(UDF_ID_SPARING))) {",
          "\t\t\t\t\t\tbrelse(bh2);",
          "\t\t\t\t\t\tmap->s_type_specific.s_sparing.",
          "\t\t\t\t\t\t\ts_spar_map[j] = NULL;",
          "\t\t\t\t\t}",
          "\t\t\t\t}",
          "\t\t\t\tmap->s_partition_func = udf_get_pblock_spar15;"
        ]
      },
      "preconditions_for_vulnerability": "Improper calculation of partition table length without proper validation against the block size of the super block.",
      "trigger_condition": "Crafted UDF filesystem input causing the calculated partition table length to exceed the block size of the super block, leading to a heap-based buffer overflow.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the calculated partition table length plus the size of the logical volume descriptor exceeds the block size of the super block, allowing a buffer overflow to occur when accessing the partition table.",
      "id": 0,
      "code_after_change_normalized": "static int FUN1(struct super_block *VAR1, sector_t VAR2,\nstruct kernel_lb_addr *VAR3)\n{\nstruct logicalVolDesc *VAR4;\nint VAR5, VAR6;\nuint8_t VAR7;\nstruct udf_sb_info *VAR8 = FUN2(VAR1);\nstruct genericPartitionMap *VAR9;\nuint16_t VAR10;\nstruct buffer_head *VAR11;\nunsigned int VAR12;\nint VAR13 = 0;\nVAR11 = FUN3(VAR1, VAR2, VAR2, &VAR10);\nif (!VAR11)\nreturn 1;\nFUN4(VAR10 != VAR14);\nVAR4 = (struct VAR15 *)VAR11->VAR16;\nVAR12 = FUN5(VAR4->VAR17);\nif (sizeof(*VAR4) + VAR12 > VAR1->VAR18) {\nFUN6(VAR1, \"STR\"\n\"STR\", VAR12,\nVAR1->VAR18 - sizeof(*VAR4));\ngoto VAR19;\n}\nVAR13 = FUN7(VAR1, FUN5(VAR4->VAR20));\nif (VAR13)\ngoto VAR19;\nfor (VAR5 = 0, VAR6 = 0;\nVAR5 < VAR8->VAR21 && VAR6 < VAR12;\nVAR5++, VAR6 += VAR9->VAR22) {\nstruct udf_part_map *VAR23 = &VAR8->VAR24[VAR5];\nVAR9 = (struct VAR25 *)\n&(VAR4->VAR26[VAR6]);\nVAR7 = VAR9->VAR27;\nif (VAR7 == 1) {\nstruct genericPartitionMap1 *VAR28 =\n(struct VAR29 *)VAR9;\nVAR23->VAR30 = VAR31;\nVAR23->VAR32 = FUN8(VAR28->VAR33);\nVAR23->VAR34 = FUN8(VAR28->VAR35);\nVAR23->VAR36 = NULL;\n} else if (VAR7 == 2) {\nstruct udfPartitionMap2 *VAR37 =\n(struct VAR38 *)VAR9;\nif (!FUN9(VAR37->VAR39.VAR10, VAR40,\nFUN10(VAR40))) {\nu16 VAR41 =\nFUN8(((VAR42 *)VAR37->VAR39.\nVAR43)[0]);\nif (VAR41 < VAR44) {\nVAR23->VAR30 =\nVAR45;\nVAR23->VAR36 =\nVAR46;\n} else {\nVAR23->VAR30 =\nVAR47;\nVAR23->VAR36 =\nVAR48;\n}\n} else if (!FUN9(VAR37->VAR39.VAR10,\nVAR49,\nFUN10(VAR49))) {\nif (FUN11(VAR1, VAR23,\n(struct VAR50 *)VAR9) < 0)\ngoto VAR19;\n} else if (!FUN9(VAR37->VAR39.VAR10,\nVAR51,\nFUN10(VAR51))) {\nstruct udf_meta_data *VAR52 =\n&VAR23->VAR53.VAR54;\nstruct metadataPartitionMap *VAR55 =\n(struct VAR56 *)\n&(VAR4->VAR26[VAR6]);\nFUN12(\"STR\",\nVAR5, VAR7, VAR51);\nVAR23->VAR30 = VAR57;\nVAR23->VAR36 = VAR58;\nVAR52->VAR59   =\nFUN5(VAR55->VAR60);\nVAR52->VAR61 =\nFUN5(VAR55->VAR62);\nVAR52->VAR63 =\nFUN5(VAR55->VAR64);\nVAR52->VAR65 =\nFUN5(VAR55->VAR66);\nVAR52->VAR67 =\nFUN8(VAR55->VAR68);\nif (VAR55->VAR69 & VAR44)\nVAR52->VAR70 |= VAR71;\nFUN12(\"STR\",\nFUN8(*(VAR42 *)\nVAR55->VAR39.VAR43));\nFUN12(\"STR\",\nFUN8(VAR55->VAR35));\nFUN12(\"STR\",\nFUN5(VAR55->VAR66));\nFUN12(\"STR\",\nFUN5(VAR55->VAR60));\nFUN12(\"STR\",\nFUN5(VAR55->VAR62));\nFUN12(\"STR\",\nFUN5(VAR55->VAR64));\nFUN12(\"STR\",\nVAR52->VAR70, VAR55->VAR69);\n} else {\nFUN12(\"STR\",\nVAR37->VAR39.VAR10);\ncontinue;\n}\nVAR23->VAR32 = FUN8(VAR37->VAR33);\nVAR23->VAR34 = FUN8(VAR37->VAR35);\n}\nFUN12(\"STR\",\nVAR5, VAR23->VAR34, VAR7, VAR23->VAR32);\n}\nif (VAR3) {\nstruct VAR73 *VAR72 = (struct VAR73 *)&(VAR4->VAR74[0]);\n*VAR3 = FUN13(VAR72->VAR75);\nFUN12(\"STR\",\nVAR3->VAR76,\nVAR3->VAR77);\n}\nif (VAR4->VAR78.VAR79)\nFUN14(VAR1, FUN15(VAR4->VAR78));\nVAR19:\nFUN16(VAR11);\nreturn VAR13;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct super_block *VAR1, sector_t VAR2,\nstruct kernel_lb_addr *VAR3)\n{\nstruct logicalVolDesc *VAR4;\nint VAR5, VAR6, VAR7;\nuint8_t VAR8;\nstruct udf_sb_info *VAR9 = FUN2(VAR1);\nstruct genericPartitionMap *VAR10;\nuint16_t VAR11;\nstruct buffer_head *VAR12;\nunsigned int VAR13;\nint VAR14 = 0;\nVAR12 = FUN3(VAR1, VAR2, VAR2, &VAR11);\nif (!VAR12)\nreturn 1;\nFUN4(VAR11 != VAR15);\nVAR4 = (struct VAR16 *)VAR12->VAR17;\nVAR13 = FUN5(VAR4->VAR18);\nif (sizeof(*VAR4) + VAR13 > VAR1->VAR19) {\nFUN6(VAR1, \"STR\"\n\"STR\", VAR13,\nVAR1->VAR19 - sizeof(*VAR4));\ngoto VAR20;\n}\nVAR14 = FUN7(VAR1, FUN5(VAR4->VAR21));\nif (VAR14)\ngoto VAR20;\nfor (VAR5 = 0, VAR7 = 0;\nVAR5 < VAR9->VAR22 && VAR7 < VAR13;\nVAR5++, VAR7 += VAR10->VAR23) {\nstruct udf_part_map *VAR24 = &VAR9->VAR25[VAR5];\nVAR10 = (struct VAR26 *)\n&(VAR4->VAR27[VAR7]);\nVAR8 = VAR10->VAR28;\nif (VAR8 == 1) {\nstruct genericPartitionMap1 *VAR29 =\n(struct VAR30 *)VAR10;\nVAR24->VAR31 = VAR32;\nVAR24->VAR33 = FUN8(VAR29->VAR34);\nVAR24->VAR35 = FUN8(VAR29->VAR36);\nVAR24->VAR37 = NULL;\n} else if (VAR8 == 2) {\nstruct udfPartitionMap2 *VAR38 =\n(struct VAR39 *)VAR10;\nif (!FUN9(VAR38->VAR40.VAR11, VAR41,\nFUN10(VAR41))) {\nu16 VAR42 =\nFUN8(((VAR43 *)VAR38->VAR40.\nVAR44)[0]);\nif (VAR42 < VAR45) {\nVAR24->VAR31 =\nVAR46;\nVAR24->VAR37 =\nVAR47;\n} else {\nVAR24->VAR31 =\nVAR48;\nVAR24->VAR37 =\nVAR49;\n}\n} else if (!FUN9(VAR38->VAR40.VAR11,\nVAR50,\nFUN10(VAR50))) {\nuint32_t VAR51;\nstruct sparingTable *VAR52;\nstruct sparablePartitionMap *VAR53 =\n(struct VAR54 *)VAR10;\nVAR24->VAR31 = VAR55;\nVAR24->VAR56.VAR57.VAR58 =\nFUN8(VAR53->VAR59);\nfor (VAR6 = 0; VAR6 < VAR53->VAR60; VAR6++) {\nstruct buffer_head *VAR61;\nVAR51 = FUN5(\nVAR53->VAR62[VAR6]);\nVAR61 = FUN3(VAR1, VAR51, VAR51,\n&VAR11);\nVAR24->VAR56.VAR57.\nVAR63[VAR6] = VAR61;\nif (VAR61 == NULL)\ncontinue;\nVAR52 = (struct VAR64 *)VAR61->VAR17;\nif (VAR11 != 0 || FUN9(\nVAR52->VAR65.VAR11,\nVAR66,\nFUN10(VAR66))) {\nFUN11(VAR61);\nVAR24->VAR56.VAR57.\nVAR63[VAR6] = NULL;\n}\n}\nVAR24->VAR37 = VAR67;\n} else if (!FUN9(VAR38->VAR40.VAR11,\nVAR68,\nFUN10(VAR68))) {\nstruct udf_meta_data *VAR69 =\n&VAR24->VAR56.VAR70;\nstruct metadataPartitionMap *VAR71 =\n(struct VAR72 *)\n&(VAR4->VAR27[VAR7]);\nFUN12(\"STR\",\nVAR5, VAR8, VAR68);\nVAR24->VAR31 = VAR73;\nVAR24->VAR37 = VAR74;\nVAR69->VAR75   =\nFUN5(VAR71->VAR76);\nVAR69->VAR77 =\nFUN5(VAR71->VAR78);\nVAR69->VAR79 =\nFUN5(VAR71->VAR80);\nVAR69->VAR81 =\nFUN5(VAR71->VAR82);\nVAR69->VAR83 =\nFUN8(VAR71->VAR84);\nif (VAR71->VAR85 & VAR45)\nVAR69->VAR86 |= VAR87;\nFUN12(\"STR\",\nFUN8(*(VAR43 *)\nVAR71->VAR40.VAR44));\nFUN12(\"STR\",\nFUN8(VAR71->VAR36));\nFUN12(\"STR\",\nFUN5(VAR71->VAR82));\nFUN12(\"STR\",\nFUN5(VAR71->VAR76));\nFUN12(\"STR\",\nFUN5(VAR71->VAR78));\nFUN12(\"STR\",\nFUN5(VAR71->VAR80));\nFUN12(\"STR\",\nVAR69->VAR86, VAR71->VAR85);\n} else {\nFUN12(\"STR\",\nVAR38->VAR40.VAR11);\ncontinue;\n}\nVAR24->VAR33 = FUN8(VAR38->VAR34);\nVAR24->VAR35 = FUN8(VAR38->VAR36);\n}\nFUN12(\"STR\",\nVAR5, VAR24->VAR35, VAR8, VAR24->VAR33);\n}\nif (VAR3) {\nstruct VAR89 *VAR88 = (struct VAR89 *)&(VAR4->VAR90[0]);\n*VAR3 = FUN13(VAR88->VAR91);\nFUN12(\"STR\",\nVAR3->VAR92,\nVAR3->VAR93);\n}\nif (VAR4->VAR94.VAR95)\nFUN14(VAR1, FUN15(VAR4->VAR94));\nVAR20:\nFUN11(VAR12);\nreturn VAR14;\n}\n",
      "code_after_change_raw": "static int udf_load_logicalvol(struct super_block *sb, sector_t block,\nstruct kernel_lb_addr *fileset)\n{\nstruct logicalVolDesc *lvd;\nint i, offset;\nuint8_t type;\nstruct udf_sb_info *sbi = UDF_SB(sb);\nstruct genericPartitionMap *gpm;\nuint16_t ident;\nstruct buffer_head *bh;\nunsigned int table_len;\nint ret = 0;\nbh = udf_read_tagged(sb, block, block, &ident);\nif (!bh)\nreturn 1;\nBUG_ON(ident != TAG_IDENT_LVD);\nlvd = (struct logicalVolDesc *)bh->b_data;\ntable_len = le32_to_cpu(lvd->mapTableLength);\nif (sizeof(*lvd) + table_len > sb->s_blocksize) {\nudf_err(sb, \"error loading logical volume descriptor: \"\n\"Partition table too long (%u > %lu)\\n\", table_len,\nsb->s_blocksize - sizeof(*lvd));\ngoto out_bh;\n}\nret = udf_sb_alloc_partition_maps(sb, le32_to_cpu(lvd->numPartitionMaps));\nif (ret)\ngoto out_bh;\nfor (i = 0, offset = 0;\ni < sbi->s_partitions && offset < table_len;\ni++, offset += gpm->partitionMapLength) {\nstruct udf_part_map *map = &sbi->s_partmaps[i];\ngpm = (struct genericPartitionMap *)\n&(lvd->partitionMaps[offset]);\ntype = gpm->partitionMapType;\nif (type == 1) {\nstruct genericPartitionMap1 *gpm1 =\n(struct genericPartitionMap1 *)gpm;\nmap->s_partition_type = UDF_TYPE1_MAP15;\nmap->s_volumeseqnum = le16_to_cpu(gpm1->volSeqNum);\nmap->s_partition_num = le16_to_cpu(gpm1->partitionNum);\nmap->s_partition_func = NULL;\n} else if (type == 2) {\nstruct udfPartitionMap2 *upm2 =\n(struct udfPartitionMap2 *)gpm;\nif (!strncmp(upm2->partIdent.ident, UDF_ID_VIRTUAL,\nstrlen(UDF_ID_VIRTUAL))) {\nu16 suf =\nle16_to_cpu(((__le16 *)upm2->partIdent.\nidentSuffix)[0]);\nif (suf < 0x0200) {\nmap->s_partition_type =\nUDF_VIRTUAL_MAP15;\nmap->s_partition_func =\nudf_get_pblock_virt15;\n} else {\nmap->s_partition_type =\nUDF_VIRTUAL_MAP20;\nmap->s_partition_func =\nudf_get_pblock_virt20;\n}\n} else if (!strncmp(upm2->partIdent.ident,\nUDF_ID_SPARABLE,\nstrlen(UDF_ID_SPARABLE))) {\nif (udf_load_sparable_map(sb, map,\n(struct sparablePartitionMap *)gpm) < 0)\ngoto out_bh;\n} else if (!strncmp(upm2->partIdent.ident,\nUDF_ID_METADATA,\nstrlen(UDF_ID_METADATA))) {\nstruct udf_meta_data *mdata =\n&map->s_type_specific.s_metadata;\nstruct metadataPartitionMap *mdm =\n(struct metadataPartitionMap *)\n&(lvd->partitionMaps[offset]);\nudf_debug(\"Parsing Logical vol part %d type %d  id=%s\\n\",\ni, type, UDF_ID_METADATA);\nmap->s_partition_type = UDF_METADATA_MAP25;\nmap->s_partition_func = udf_get_pblock_meta25;\nmdata->s_meta_file_loc   =\nle32_to_cpu(mdm->metadataFileLoc);\nmdata->s_mirror_file_loc =\nle32_to_cpu(mdm->metadataMirrorFileLoc);\nmdata->s_bitmap_file_loc =\nle32_to_cpu(mdm->metadataBitmapFileLoc);\nmdata->s_alloc_unit_size =\nle32_to_cpu(mdm->allocUnitSize);\nmdata->s_align_unit_size =\nle16_to_cpu(mdm->alignUnitSize);\nif (mdm->flags & 0x01)\nmdata->s_flags |= MF_DUPLICATE_MD;\nudf_debug(\"Metadata Ident suffix=0x%x\\n\",\nle16_to_cpu(*(__le16 *)\nmdm->partIdent.identSuffix));\nudf_debug(\"Metadata part num=%d\\n\",\nle16_to_cpu(mdm->partitionNum));\nudf_debug(\"Metadata part alloc unit size=%d\\n\",\nle32_to_cpu(mdm->allocUnitSize));\nudf_debug(\"Metadata file loc=%d\\n\",\nle32_to_cpu(mdm->metadataFileLoc));\nudf_debug(\"Mirror file loc=%d\\n\",\nle32_to_cpu(mdm->metadataMirrorFileLoc));\nudf_debug(\"Bitmap file loc=%d\\n\",\nle32_to_cpu(mdm->metadataBitmapFileLoc));\nudf_debug(\"Flags: %d %d\\n\",\nmdata->s_flags, mdm->flags);\n} else {\nudf_debug(\"Unknown ident: %s\\n\",\nupm2->partIdent.ident);\ncontinue;\n}\nmap->s_volumeseqnum = le16_to_cpu(upm2->volSeqNum);\nmap->s_partition_num = le16_to_cpu(upm2->partitionNum);\n}\nudf_debug(\"Partition (%d:%d) type %d on volume %d\\n\",\ni, map->s_partition_num, type, map->s_volumeseqnum);\n}\nif (fileset) {\nstruct long_ad *la = (struct long_ad *)&(lvd->logicalVolContentsUse[0]);\n*fileset = lelb_to_cpu(la->extLocation);\nudf_debug(\"FileSet found in LogicalVolDesc at block=%d, partition=%d\\n\",\nfileset->logicalBlockNum,\nfileset->partitionReferenceNum);\n}\nif (lvd->integritySeqExt.extLength)\nudf_load_logicalvolint(sb, leea_to_cpu(lvd->integritySeqExt));\nout_bh:\nbrelse(bh);\nreturn ret;\n}\n",
      "code_before_change_raw": "static int udf_load_logicalvol(struct super_block *sb, sector_t block,\nstruct kernel_lb_addr *fileset)\n{\nstruct logicalVolDesc *lvd;\nint i, j, offset;\nuint8_t type;\nstruct udf_sb_info *sbi = UDF_SB(sb);\nstruct genericPartitionMap *gpm;\nuint16_t ident;\nstruct buffer_head *bh;\nunsigned int table_len;\nint ret = 0;\nbh = udf_read_tagged(sb, block, block, &ident);\nif (!bh)\nreturn 1;\nBUG_ON(ident != TAG_IDENT_LVD);\nlvd = (struct logicalVolDesc *)bh->b_data;\ntable_len = le32_to_cpu(lvd->mapTableLength);\nif (sizeof(*lvd) + table_len > sb->s_blocksize) {\nudf_err(sb, \"error loading logical volume descriptor: \"\n\"Partition table too long (%u > %lu)\\n\", table_len,\nsb->s_blocksize - sizeof(*lvd));\ngoto out_bh;\n}\nret = udf_sb_alloc_partition_maps(sb, le32_to_cpu(lvd->numPartitionMaps));\nif (ret)\ngoto out_bh;\nfor (i = 0, offset = 0;\ni < sbi->s_partitions && offset < table_len;\ni++, offset += gpm->partitionMapLength) {\nstruct udf_part_map *map = &sbi->s_partmaps[i];\ngpm = (struct genericPartitionMap *)\n&(lvd->partitionMaps[offset]);\ntype = gpm->partitionMapType;\nif (type == 1) {\nstruct genericPartitionMap1 *gpm1 =\n(struct genericPartitionMap1 *)gpm;\nmap->s_partition_type = UDF_TYPE1_MAP15;\nmap->s_volumeseqnum = le16_to_cpu(gpm1->volSeqNum);\nmap->s_partition_num = le16_to_cpu(gpm1->partitionNum);\nmap->s_partition_func = NULL;\n} else if (type == 2) {\nstruct udfPartitionMap2 *upm2 =\n(struct udfPartitionMap2 *)gpm;\nif (!strncmp(upm2->partIdent.ident, UDF_ID_VIRTUAL,\nstrlen(UDF_ID_VIRTUAL))) {\nu16 suf =\nle16_to_cpu(((__le16 *)upm2->partIdent.\nidentSuffix)[0]);\nif (suf < 0x0200) {\nmap->s_partition_type =\nUDF_VIRTUAL_MAP15;\nmap->s_partition_func =\nudf_get_pblock_virt15;\n} else {\nmap->s_partition_type =\nUDF_VIRTUAL_MAP20;\nmap->s_partition_func =\nudf_get_pblock_virt20;\n}\n} else if (!strncmp(upm2->partIdent.ident,\nUDF_ID_SPARABLE,\nstrlen(UDF_ID_SPARABLE))) {\nuint32_t loc;\nstruct sparingTable *st;\nstruct sparablePartitionMap *spm =\n(struct sparablePartitionMap *)gpm;\nmap->s_partition_type = UDF_SPARABLE_MAP15;\nmap->s_type_specific.s_sparing.s_packet_len =\nle16_to_cpu(spm->packetLength);\nfor (j = 0; j < spm->numSparingTables; j++) {\nstruct buffer_head *bh2;\nloc = le32_to_cpu(\nspm->locSparingTable[j]);\nbh2 = udf_read_tagged(sb, loc, loc,\n&ident);\nmap->s_type_specific.s_sparing.\ns_spar_map[j] = bh2;\nif (bh2 == NULL)\ncontinue;\nst = (struct sparingTable *)bh2->b_data;\nif (ident != 0 || strncmp(\nst->sparingIdent.ident,\nUDF_ID_SPARING,\nstrlen(UDF_ID_SPARING))) {\nbrelse(bh2);\nmap->s_type_specific.s_sparing.\ns_spar_map[j] = NULL;\n}\n}\nmap->s_partition_func = udf_get_pblock_spar15;\n} else if (!strncmp(upm2->partIdent.ident,\nUDF_ID_METADATA,\nstrlen(UDF_ID_METADATA))) {\nstruct udf_meta_data *mdata =\n&map->s_type_specific.s_metadata;\nstruct metadataPartitionMap *mdm =\n(struct metadataPartitionMap *)\n&(lvd->partitionMaps[offset]);\nudf_debug(\"Parsing Logical vol part %d type %d  id=%s\\n\",\ni, type, UDF_ID_METADATA);\nmap->s_partition_type = UDF_METADATA_MAP25;\nmap->s_partition_func = udf_get_pblock_meta25;\nmdata->s_meta_file_loc   =\nle32_to_cpu(mdm->metadataFileLoc);\nmdata->s_mirror_file_loc =\nle32_to_cpu(mdm->metadataMirrorFileLoc);\nmdata->s_bitmap_file_loc =\nle32_to_cpu(mdm->metadataBitmapFileLoc);\nmdata->s_alloc_unit_size =\nle32_to_cpu(mdm->allocUnitSize);\nmdata->s_align_unit_size =\nle16_to_cpu(mdm->alignUnitSize);\nif (mdm->flags & 0x01)\nmdata->s_flags |= MF_DUPLICATE_MD;\nudf_debug(\"Metadata Ident suffix=0x%x\\n\",\nle16_to_cpu(*(__le16 *)\nmdm->partIdent.identSuffix));\nudf_debug(\"Metadata part num=%d\\n\",\nle16_to_cpu(mdm->partitionNum));\nudf_debug(\"Metadata part alloc unit size=%d\\n\",\nle32_to_cpu(mdm->allocUnitSize));\nudf_debug(\"Metadata file loc=%d\\n\",\nle32_to_cpu(mdm->metadataFileLoc));\nudf_debug(\"Mirror file loc=%d\\n\",\nle32_to_cpu(mdm->metadataMirrorFileLoc));\nudf_debug(\"Bitmap file loc=%d\\n\",\nle32_to_cpu(mdm->metadataBitmapFileLoc));\nudf_debug(\"Flags: %d %d\\n\",\nmdata->s_flags, mdm->flags);\n} else {\nudf_debug(\"Unknown ident: %s\\n\",\nupm2->partIdent.ident);\ncontinue;\n}\nmap->s_volumeseqnum = le16_to_cpu(upm2->volSeqNum);\nmap->s_partition_num = le16_to_cpu(upm2->partitionNum);\n}\nudf_debug(\"Partition (%d:%d) type %d on volume %d\\n\",\ni, map->s_partition_num, type, map->s_volumeseqnum);\n}\nif (fileset) {\nstruct long_ad *la = (struct long_ad *)&(lvd->logicalVolContentsUse[0]);\n*fileset = lelb_to_cpu(la->extLocation);\nudf_debug(\"FileSet found in LogicalVolDesc at block=%d, partition=%d\\n\",\nfileset->logicalBlockNum,\nfileset->partitionReferenceNum);\n}\nif (lvd->integritySeqExt.extLength)\nudf_load_logicalvolint(sb, leea_to_cpu(lvd->integritySeqExt));\nout_bh:\nbrelse(bh);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2014-0077": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation of packet lengths when mergeable buffers are disabled.",
        "trigger_condition": "Receiving a number of buffers that exceeds the maximum allowed value without proper handling.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the number of received buffers exceeds the maximum allowed value, leading to a potential buffer overrun situation."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that if the number of received buffers exceeds the maximum allowed value, the code should truncate and discard the excess buffers. This prevents memory corruption, denial of service, or privilege escalation vulnerabilities that could arise from a buffer overrun scenario.",
      "GPT_analysis": "The modification is necessary to address the vulnerability described in CVE-2014-0077. The vulnerability arises due to improper validation of packet lengths in the `handle_rx` function, specifically in the `get_rx_bufs` function call. When mergeable buffers are disabled, the code does not properly handle the case where the number of received buffers exceeds the maximum allowed value (`UIO_MAXIOV`).\n\nBy adding the following lines in the modified code:\n```c\nif (unlikely(headcount > UIO_MAXIOV)) {\n    msg.msg_iovlen = 1;\n    err = sock->ops->recvmsg(NULL, sock, &msg, 1, MSG_DONTWAIT | MSG_TRUNC);\n    pr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);\n    continue;\n}\n```\nwe ensure that if the number of received buffers exceeds the maximum allowed value, the code will truncate and discard the excess buffers. This prevents a potential buffer overrun situation that could lead to memory corruption, denial of service, or privilege escalation vulnerabilities.\n\nTherefore, this modification helps to improve the security and stability of the code by properly handling the case where the number of received buffers is larger than the allowed limit.",
      "GPT_purpose": "Handle receiving data packets in a vhost network driver.",
      "GPT_function": "\n1. handle_rx: Handles the receive side of the virtual network device.\n2. get_rx_bufs: Gets receive buffers for handling incoming packets.\n3. move_iovec_hdr: Moves the header part of the iovec buffer.\n4. copy_iovec_hdr: Copies the header part of the iovec buffer.\n5. vhost_discard_vq_desc: Discards virtual queue descriptors.\n6. vhost_add_used_and_signal_n: Adds used buffers and signals completion to the device.\n7. vhost_log_write: Writes to the virtual host log.\n8. vhost_poll_queue: Polls the virtual queue for new events.",
      "CVE_id": "CVE-2014-0077",
      "code_before_change": "static void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned uninitialized_var(in), log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL, /* FIXME: get and handle RX aux data. */\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr_mrg_rxbuf hdr = {\n\t\t.hdr.flags = 0,\n\t\t.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tstruct socket *sock;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\tvhost_disable_notify(&net->dev, vq);\n\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\n\tvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\n\n\twhile ((sock_len = peek_head_len(sock->sk))) {\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n\t\t\t\t\t&in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(headcount < 0))\n\t\t\tbreak;\n\t\t/* OK, now we need to know about added descriptors. */\n\t\tif (!headcount) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t/* They have slipped one in as we were\n\t\t\t\t * doing that: check again. */\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Nothing new?  Wait for eventfd to tell us\n\t\t\t * they refilled. */\n\t\t\tbreak;\n\t\t}\n\t\t/* We don't need to be notified again. */\n\t\tif (unlikely((vhost_hlen)))\n\t\t\t/* Skip header. TODO: support TSO. */\n\t\t\tmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\n\t\telse\n\t\t\t/* Copy the header for use in VIRTIO_NET_F_MRG_RXBUF:\n\t\t\t * needed because recvmsg can modify msg_iov. */\n\t\t\tcopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\n\t\tmsg.msg_iovlen = in;\n\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t/* Userspace might have consumed the packet meanwhile:\n\t\t * it's not supposed to do this usually, but might be hard\n\t\t * to prevent. Discard data we got (if any) and keep going. */\n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(vhost_hlen) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\n\t\t\t\t      vhost_hlen)) {\n\t\t\tvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\n\t\t\t       vq->iov->iov_base);\n\t\t\tbreak;\n\t\t}\n\t\t/* TODO: Should check and handle checksum. */\n\t\tif (likely(mergeable) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\n\t\t\t\t      offsetof(typeof(hdr), num_buffers),\n\t\t\t\t      sizeof hdr.num_buffers)) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tbreak;\n\t\t}\n\t\tvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\n\t\t\t\t\t    headcount);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len);\n\t\ttotal_len += vhost_len;\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}",
      "code_after_change": "static void handle_rx(struct vhost_net *net)\n{\n\tstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\n\tstruct vhost_virtqueue *vq = &nvq->vq;\n\tunsigned uninitialized_var(in), log;\n\tstruct vhost_log *vq_log;\n\tstruct msghdr msg = {\n\t\t.msg_name = NULL,\n\t\t.msg_namelen = 0,\n\t\t.msg_control = NULL, /* FIXME: get and handle RX aux data. */\n\t\t.msg_controllen = 0,\n\t\t.msg_iov = vq->iov,\n\t\t.msg_flags = MSG_DONTWAIT,\n\t};\n\tstruct virtio_net_hdr_mrg_rxbuf hdr = {\n\t\t.hdr.flags = 0,\n\t\t.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n\t};\n\tsize_t total_len = 0;\n\tint err, mergeable;\n\ts16 headcount;\n\tsize_t vhost_hlen, sock_hlen;\n\tsize_t vhost_len, sock_len;\n\tstruct socket *sock;\n\n\tmutex_lock(&vq->mutex);\n\tsock = vq->private_data;\n\tif (!sock)\n\t\tgoto out;\n\tvhost_disable_notify(&net->dev, vq);\n\n\tvhost_hlen = nvq->vhost_hlen;\n\tsock_hlen = nvq->sock_hlen;\n\n\tvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\n\t\tvq->log : NULL;\n\tmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\n\n\twhile ((sock_len = peek_head_len(sock->sk))) {\n\t\tsock_len += sock_hlen;\n\t\tvhost_len = sock_len + vhost_hlen;\n\t\theadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n\t\t\t\t\t&in, vq_log, &log,\n\t\t\t\t\tlikely(mergeable) ? UIO_MAXIOV : 1);\n\t\t/* On error, stop handling until the next kick. */\n\t\tif (unlikely(headcount < 0))\n\t\t\tbreak;\n\t\t/* On overrun, truncate and discard */\n\t\tif (unlikely(headcount > UIO_MAXIOV)) {\n\t\t\tmsg.msg_iovlen = 1;\n\t\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t\t 1, MSG_DONTWAIT | MSG_TRUNC);\n\t\t\tpr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);\n\t\t\tcontinue;\n\t\t}\n\t\t/* OK, now we need to know about added descriptors. */\n\t\tif (!headcount) {\n\t\t\tif (unlikely(vhost_enable_notify(&net->dev, vq))) {\n\t\t\t\t/* They have slipped one in as we were\n\t\t\t\t * doing that: check again. */\n\t\t\t\tvhost_disable_notify(&net->dev, vq);\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\t/* Nothing new?  Wait for eventfd to tell us\n\t\t\t * they refilled. */\n\t\t\tbreak;\n\t\t}\n\t\t/* We don't need to be notified again. */\n\t\tif (unlikely((vhost_hlen)))\n\t\t\t/* Skip header. TODO: support TSO. */\n\t\t\tmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\n\t\telse\n\t\t\t/* Copy the header for use in VIRTIO_NET_F_MRG_RXBUF:\n\t\t\t * needed because recvmsg can modify msg_iov. */\n\t\t\tcopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\n\t\tmsg.msg_iovlen = in;\n\t\terr = sock->ops->recvmsg(NULL, sock, &msg,\n\t\t\t\t\t sock_len, MSG_DONTWAIT | MSG_TRUNC);\n\t\t/* Userspace might have consumed the packet meanwhile:\n\t\t * it's not supposed to do this usually, but might be hard\n\t\t * to prevent. Discard data we got (if any) and keep going. */\n\t\tif (unlikely(err != sock_len)) {\n\t\t\tpr_debug(\"Discarded rx packet: \"\n\t\t\t\t \" len %d, expected %zd\\n\", err, sock_len);\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tcontinue;\n\t\t}\n\t\tif (unlikely(vhost_hlen) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\n\t\t\t\t      vhost_hlen)) {\n\t\t\tvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\n\t\t\t       vq->iov->iov_base);\n\t\t\tbreak;\n\t\t}\n\t\t/* TODO: Should check and handle checksum. */\n\t\tif (likely(mergeable) &&\n\t\t    memcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\n\t\t\t\t      offsetof(typeof(hdr), num_buffers),\n\t\t\t\t      sizeof hdr.num_buffers)) {\n\t\t\tvq_err(vq, \"Failed num_buffers write\");\n\t\t\tvhost_discard_vq_desc(vq, headcount);\n\t\t\tbreak;\n\t\t}\n\t\tvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\n\t\t\t\t\t    headcount);\n\t\tif (unlikely(vq_log))\n\t\t\tvhost_log_write(vq, vq_log, log, vhost_len);\n\t\ttotal_len += vhost_len;\n\t\tif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\n\t\t\tvhost_poll_queue(&vq->poll);\n\t\t\tbreak;\n\t\t}\n\t}\nout:\n\tmutex_unlock(&vq->mutex);\n}",
      "modified_lines": {
        "added": [
          "\t\t/* On overrun, truncate and discard */",
          "\t\tif (unlikely(headcount > UIO_MAXIOV)) {",
          "\t\t\tmsg.msg_iovlen = 1;",
          "\t\t\terr = sock->ops->recvmsg(NULL, sock, &msg,",
          "\t\t\t\t\t\t 1, MSG_DONTWAIT | MSG_TRUNC);",
          "\t\t\tpr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);",
          "\t\t\tcontinue;",
          "\t\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation of packet lengths when mergeable buffers are disabled.",
      "trigger_condition": "Receiving a number of buffers that exceeds the maximum allowed value without proper handling.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the number of received buffers exceeds the maximum allowed value, leading to a potential buffer overrun situation.",
      "id": 1,
      "code_after_change_normalized": "static void FUN1(struct vhost_net *VAR1)\n{\nstruct vhost_net_virtqueue *VAR2 = &VAR1->VAR3[VAR4];\nstruct vhost_virtqueue *VAR5 = &VAR2->VAR5;\nunsigned FUN2(VAR6), VAR7;\nstruct vhost_log *VAR8;\nstruct msghdr VAR9 = {\n.VAR10 = NULL,\n.VAR11 = 0,\n.VAR12 = NULL, \n.VAR13 = 0,\n.VAR14 = VAR5->VAR15,\n.VAR16 = VAR17,\n};\nstruct virtio_net_hdr_mrg_rxbuf VAR18 = {\n.VAR18.VAR19 = 0,\n.VAR18.VAR20 = VAR21\n};\nsize_t VAR22 = 0;\nint VAR23, VAR24;\ns16 VAR25;\nsize_t VAR26, VAR27;\nsize_t VAR28, VAR29;\nstruct socket *VAR30;\nFUN3(&VAR5->VAR31);\nVAR30 = VAR5->VAR32;\nif (!VAR30)\ngoto VAR33;\nFUN4(&VAR1->VAR34, VAR5);\nVAR26 = VAR2->VAR26;\nVAR27 = VAR2->VAR27;\nVAR8 = FUN5(FUN6(&VAR1->VAR34, VAR35)) ?\nVAR5->VAR7 : NULL;\nVAR24 = FUN6(&VAR1->VAR34, VAR36);\nwhile ((VAR29 = FUN7(VAR30->VAR37))) {\nVAR29 += VAR27;\nVAR28 = VAR29 + VAR26;\nVAR25 = FUN8(VAR5, VAR5->VAR38, VAR28,\n&VAR6, VAR8, &VAR7,\nFUN9(VAR24) ? VAR39 : 1);\nif (FUN5(VAR25 < 0))\nbreak;\nif (FUN5(VAR25 > VAR39)) {\nVAR9.VAR40 = 1;\nVAR23 = VAR30->VAR41->FUN10(NULL, VAR30, &VAR9,\n1, VAR17 | VAR42);\nFUN11(\"STR\", VAR29);\ncontinue;\n}\nif (!VAR25) {\nif (FUN5(FUN12(&VAR1->VAR34, VAR5))) {\nFUN4(&VAR1->VAR34, VAR5);\ncontinue;\n}\nbreak;\n}\nif (FUN5((VAR26)))\nFUN13(VAR5->VAR15, VAR2->VAR18, VAR26, VAR6);\nelse\nFUN14(VAR5->VAR15, VAR2->VAR18, VAR27, VAR6);\nVAR9.VAR40 = VAR6;\nVAR23 = VAR30->VAR41->FUN10(NULL, VAR30, &VAR9,\nVAR29, VAR17 | VAR42);\nif (FUN5(VAR23 != VAR29)) {\nFUN11(\"STR\"\n\"STR\", VAR23, VAR29);\nFUN15(VAR5, VAR25);\ncontinue;\n}\nif (FUN5(VAR26) &&\nFUN16(VAR2->VAR18, (unsigned char *)&VAR18, 0,\nVAR26)) {\nFUN17(VAR5, \"STR\",\nVAR5->VAR15->VAR43);\nbreak;\n}\nif (FUN9(VAR24) &&\nFUN16(VAR2->VAR18, (unsigned char *)&VAR25,\nFUN18(FUN19(VAR18), VAR44),\nsizeof VAR18.VAR44)) {\nFUN17(VAR5, \"STR\");\nFUN15(VAR5, VAR25);\nbreak;\n}\nFUN20(&VAR1->VAR34, VAR5, VAR5->VAR38,\nVAR25);\nif (FUN5(VAR8))\nFUN21(VAR5, VAR8, VAR7, VAR28);\nVAR22 += VAR28;\nif (FUN5(VAR22 >= VAR45)) {\nFUN22(&VAR5->VAR46);\nbreak;\n}\n}\nVAR33:\nFUN23(&VAR5->VAR31);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct vhost_net *VAR1)\n{\nstruct vhost_net_virtqueue *VAR2 = &VAR1->VAR3[VAR4];\nstruct vhost_virtqueue *VAR5 = &VAR2->VAR5;\nunsigned FUN2(VAR6), VAR7;\nstruct vhost_log *VAR8;\nstruct msghdr VAR9 = {\n.VAR10 = NULL,\n.VAR11 = 0,\n.VAR12 = NULL, \n.VAR13 = 0,\n.VAR14 = VAR5->VAR15,\n.VAR16 = VAR17,\n};\nstruct virtio_net_hdr_mrg_rxbuf VAR18 = {\n.VAR18.VAR19 = 0,\n.VAR18.VAR20 = VAR21\n};\nsize_t VAR22 = 0;\nint VAR23, VAR24;\ns16 VAR25;\nsize_t VAR26, VAR27;\nsize_t VAR28, VAR29;\nstruct socket *VAR30;\nFUN3(&VAR5->VAR31);\nVAR30 = VAR5->VAR32;\nif (!VAR30)\ngoto VAR33;\nFUN4(&VAR1->VAR34, VAR5);\nVAR26 = VAR2->VAR26;\nVAR27 = VAR2->VAR27;\nVAR8 = FUN5(FUN6(&VAR1->VAR34, VAR35)) ?\nVAR5->VAR7 : NULL;\nVAR24 = FUN6(&VAR1->VAR34, VAR36);\nwhile ((VAR29 = FUN7(VAR30->VAR37))) {\nVAR29 += VAR27;\nVAR28 = VAR29 + VAR26;\nVAR25 = FUN8(VAR5, VAR5->VAR38, VAR28,\n&VAR6, VAR8, &VAR7,\nFUN9(VAR24) ? VAR39 : 1);\nif (FUN5(VAR25 < 0))\nbreak;\nif (!VAR25) {\nif (FUN5(FUN10(&VAR1->VAR34, VAR5))) {\nFUN4(&VAR1->VAR34, VAR5);\ncontinue;\n}\nbreak;\n}\nif (FUN5((VAR26)))\nFUN11(VAR5->VAR15, VAR2->VAR18, VAR26, VAR6);\nelse\nFUN12(VAR5->VAR15, VAR2->VAR18, VAR27, VAR6);\nVAR9.VAR40 = VAR6;\nVAR23 = VAR30->VAR41->FUN13(NULL, VAR30, &VAR9,\nVAR29, VAR17 | VAR42);\nif (FUN5(VAR23 != VAR29)) {\nFUN14(\"STR\"\n\"STR\", VAR23, VAR29);\nFUN15(VAR5, VAR25);\ncontinue;\n}\nif (FUN5(VAR26) &&\nFUN16(VAR2->VAR18, (unsigned char *)&VAR18, 0,\nVAR26)) {\nFUN17(VAR5, \"STR\",\nVAR5->VAR15->VAR43);\nbreak;\n}\nif (FUN9(VAR24) &&\nFUN16(VAR2->VAR18, (unsigned char *)&VAR25,\nFUN18(FUN19(VAR18), VAR44),\nsizeof VAR18.VAR44)) {\nFUN17(VAR5, \"STR\");\nFUN15(VAR5, VAR25);\nbreak;\n}\nFUN20(&VAR1->VAR34, VAR5, VAR5->VAR38,\nVAR25);\nif (FUN5(VAR8))\nFUN21(VAR5, VAR8, VAR7, VAR28);\nVAR22 += VAR28;\nif (FUN5(VAR22 >= VAR45)) {\nFUN22(&VAR5->VAR46);\nbreak;\n}\n}\nVAR33:\nFUN23(&VAR5->VAR31);\n}\n",
      "code_after_change_raw": "static void handle_rx(struct vhost_net *net)\n{\nstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\nstruct vhost_virtqueue *vq = &nvq->vq;\nunsigned uninitialized_var(in), log;\nstruct vhost_log *vq_log;\nstruct msghdr msg = {\n.msg_name = NULL,\n.msg_namelen = 0,\n.msg_control = NULL, \n.msg_controllen = 0,\n.msg_iov = vq->iov,\n.msg_flags = MSG_DONTWAIT,\n};\nstruct virtio_net_hdr_mrg_rxbuf hdr = {\n.hdr.flags = 0,\n.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n};\nsize_t total_len = 0;\nint err, mergeable;\ns16 headcount;\nsize_t vhost_hlen, sock_hlen;\nsize_t vhost_len, sock_len;\nstruct socket *sock;\nmutex_lock(&vq->mutex);\nsock = vq->private_data;\nif (!sock)\ngoto out;\nvhost_disable_notify(&net->dev, vq);\nvhost_hlen = nvq->vhost_hlen;\nsock_hlen = nvq->sock_hlen;\nvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\nvq->log : NULL;\nmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\nwhile ((sock_len = peek_head_len(sock->sk))) {\nsock_len += sock_hlen;\nvhost_len = sock_len + vhost_hlen;\nheadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n&in, vq_log, &log,\nlikely(mergeable) ? UIO_MAXIOV : 1);\nif (unlikely(headcount < 0))\nbreak;\nif (unlikely(headcount > UIO_MAXIOV)) {\nmsg.msg_iovlen = 1;\nerr = sock->ops->recvmsg(NULL, sock, &msg,\n1, MSG_DONTWAIT | MSG_TRUNC);\npr_debug(\"Discarded rx packet: len %zd\\n\", sock_len);\ncontinue;\n}\nif (!headcount) {\nif (unlikely(vhost_enable_notify(&net->dev, vq))) {\nvhost_disable_notify(&net->dev, vq);\ncontinue;\n}\nbreak;\n}\nif (unlikely((vhost_hlen)))\nmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\nelse\ncopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\nmsg.msg_iovlen = in;\nerr = sock->ops->recvmsg(NULL, sock, &msg,\nsock_len, MSG_DONTWAIT | MSG_TRUNC);\nif (unlikely(err != sock_len)) {\npr_debug(\"Discarded rx packet: \"\n\" len %d, expected %zd\\n\", err, sock_len);\nvhost_discard_vq_desc(vq, headcount);\ncontinue;\n}\nif (unlikely(vhost_hlen) &&\nmemcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\nvhost_hlen)) {\nvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\nvq->iov->iov_base);\nbreak;\n}\nif (likely(mergeable) &&\nmemcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\noffsetof(typeof(hdr), num_buffers),\nsizeof hdr.num_buffers)) {\nvq_err(vq, \"Failed num_buffers write\");\nvhost_discard_vq_desc(vq, headcount);\nbreak;\n}\nvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\nheadcount);\nif (unlikely(vq_log))\nvhost_log_write(vq, vq_log, log, vhost_len);\ntotal_len += vhost_len;\nif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\nvhost_poll_queue(&vq->poll);\nbreak;\n}\n}\nout:\nmutex_unlock(&vq->mutex);\n}\n",
      "code_before_change_raw": "static void handle_rx(struct vhost_net *net)\n{\nstruct vhost_net_virtqueue *nvq = &net->vqs[VHOST_NET_VQ_RX];\nstruct vhost_virtqueue *vq = &nvq->vq;\nunsigned uninitialized_var(in), log;\nstruct vhost_log *vq_log;\nstruct msghdr msg = {\n.msg_name = NULL,\n.msg_namelen = 0,\n.msg_control = NULL, \n.msg_controllen = 0,\n.msg_iov = vq->iov,\n.msg_flags = MSG_DONTWAIT,\n};\nstruct virtio_net_hdr_mrg_rxbuf hdr = {\n.hdr.flags = 0,\n.hdr.gso_type = VIRTIO_NET_HDR_GSO_NONE\n};\nsize_t total_len = 0;\nint err, mergeable;\ns16 headcount;\nsize_t vhost_hlen, sock_hlen;\nsize_t vhost_len, sock_len;\nstruct socket *sock;\nmutex_lock(&vq->mutex);\nsock = vq->private_data;\nif (!sock)\ngoto out;\nvhost_disable_notify(&net->dev, vq);\nvhost_hlen = nvq->vhost_hlen;\nsock_hlen = nvq->sock_hlen;\nvq_log = unlikely(vhost_has_feature(&net->dev, VHOST_F_LOG_ALL)) ?\nvq->log : NULL;\nmergeable = vhost_has_feature(&net->dev, VIRTIO_NET_F_MRG_RXBUF);\nwhile ((sock_len = peek_head_len(sock->sk))) {\nsock_len += sock_hlen;\nvhost_len = sock_len + vhost_hlen;\nheadcount = get_rx_bufs(vq, vq->heads, vhost_len,\n&in, vq_log, &log,\nlikely(mergeable) ? UIO_MAXIOV : 1);\nif (unlikely(headcount < 0))\nbreak;\nif (!headcount) {\nif (unlikely(vhost_enable_notify(&net->dev, vq))) {\nvhost_disable_notify(&net->dev, vq);\ncontinue;\n}\nbreak;\n}\nif (unlikely((vhost_hlen)))\nmove_iovec_hdr(vq->iov, nvq->hdr, vhost_hlen, in);\nelse\ncopy_iovec_hdr(vq->iov, nvq->hdr, sock_hlen, in);\nmsg.msg_iovlen = in;\nerr = sock->ops->recvmsg(NULL, sock, &msg,\nsock_len, MSG_DONTWAIT | MSG_TRUNC);\nif (unlikely(err != sock_len)) {\npr_debug(\"Discarded rx packet: \"\n\" len %d, expected %zd\\n\", err, sock_len);\nvhost_discard_vq_desc(vq, headcount);\ncontinue;\n}\nif (unlikely(vhost_hlen) &&\nmemcpy_toiovecend(nvq->hdr, (unsigned char *)&hdr, 0,\nvhost_hlen)) {\nvq_err(vq, \"Unable to write vnet_hdr at addr %p\\n\",\nvq->iov->iov_base);\nbreak;\n}\nif (likely(mergeable) &&\nmemcpy_toiovecend(nvq->hdr, (unsigned char *)&headcount,\noffsetof(typeof(hdr), num_buffers),\nsizeof hdr.num_buffers)) {\nvq_err(vq, \"Failed num_buffers write\");\nvhost_discard_vq_desc(vq, headcount);\nbreak;\n}\nvhost_add_used_and_signal_n(&net->dev, vq, vq->heads,\nheadcount);\nif (unlikely(vq_log))\nvhost_log_write(vq, vq_log, log, vhost_len);\ntotal_len += vhost_len;\nif (unlikely(total_len >= VHOST_NET_WEIGHT)) {\nvhost_poll_queue(&vq->poll);\nbreak;\n}\n}\nout:\nmutex_unlock(&vq->mutex);\n}\n"
    }
  ],
  "CVE-2016-9755": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of IPv6 reassembly in the Linux kernel, specifically in the nf_ct_frag6_gather function.",
        "trigger_condition": "Crafted application making socket, connect, and writev system calls that interact with the nf_ct_frag6_gather function.",
        "specific_code_behavior_causing_vulnerability": "The code does not handle the cases where the payload length is 0 or when the function returns an error properly. This can lead to integer overflow, out-of-bounds write, and other issues during IPv6 reassembly."
      },
      "solution": "To mitigate the vulnerability, it is necessary to handle the cases where the payload length is 0 and when the function returns an error appropriately. This involves returning 0 in these cases to prevent potential exploitation of the vulnerability and ensure the proper functioning of the IPv6 reassembly process. By making these modifications, the code addresses the specific conditions that could lead to the vulnerability.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2016-9755 related to the mishandling of IPv6 reassembly in the Linux kernel. By making the following changes in the code snippet:\n\n1. Changing the return value to 0 when the payload length is 0:\n```c\nif (ipv6_hdr(skb)->payload_len == 0) {\n    pr_debug(\"payload len = 0\\n\");\n    return 0;\n}\n```\n\n2. Changing the return value to 0 when `find_prev_fhdr` function returns an error:\n```c\nif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\n    return 0;\n```\n\nThese modifications help prevent the vulnerability by handling the conditions that could lead to integer overflow, out-of-bounds write, and other issues. By returning 0 in these cases, the code avoids potential exploitation of the vulnerability and ensures the proper functioning of the IPv6 reassembly process.",
      "GPT_purpose": "Gather IPv6 fragments for reassembly in the netfilter subsystem.",
      "GPT_function": "\n1. Gather IPv6 fragments for reassembly.\n2. Check for jumbo payload inhibiting fragment header.\n3. Find previous fragment header in the packet.\n4. Set transport header for the packet.\n5. Find or create a fragment queue based on packet information.\n6. Queue the fragment for reassembly.\n7. Perform reassembly if all fragments are received.\n8. Unlock the fragment queue after processing.\n9. Release the fragment queue.",
      "CVE_id": "CVE-2016-9755",
      "code_before_change": "int nf_ct_frag6_gather(struct net *net, struct sk_buff *skb, u32 user)\n{\n\tstruct net_device *dev = skb->dev;\n\tint fhoff, nhoff, ret;\n\tstruct frag_hdr *fhdr;\n\tstruct frag_queue *fq;\n\tstruct ipv6hdr *hdr;\n\tu8 prevhdr;\n\n\t/* Jumbo payload inhibits frag. header */\n\tif (ipv6_hdr(skb)->payload_len == 0) {\n\t\tpr_debug(\"payload len = 0\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\n\t\treturn -EINVAL;\n\n\tif (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))\n\t\treturn -ENOMEM;\n\n\tskb_set_transport_header(skb, fhoff);\n\thdr = ipv6_hdr(skb);\n\tfhdr = (struct frag_hdr *)skb_transport_header(skb);\n\n\tfq = fq_find(net, fhdr->identification, user, &hdr->saddr, &hdr->daddr,\n\t\t     skb->dev ? skb->dev->ifindex : 0, ip6_frag_ecn(hdr));\n\tif (fq == NULL) {\n\t\tpr_debug(\"Can't find and can't create new queue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock_bh(&fq->q.lock);\n\n\tif (nf_ct_frag6_queue(fq, skb, fhdr, nhoff) < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/* after queue has assumed skb ownership, only 0 or -EINPROGRESS\n\t * must be returned.\n\t */\n\tret = -EINPROGRESS;\n\tif (fq->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    fq->q.meat == fq->q.len &&\n\t    nf_ct_frag6_reasm(fq, skb, dev))\n\t\tret = 0;\n\nout_unlock:\n\tspin_unlock_bh(&fq->q.lock);\n\tinet_frag_put(&fq->q, &nf_frags);\n\treturn ret;\n}",
      "code_after_change": "int nf_ct_frag6_gather(struct net *net, struct sk_buff *skb, u32 user)\n{\n\tstruct net_device *dev = skb->dev;\n\tint fhoff, nhoff, ret;\n\tstruct frag_hdr *fhdr;\n\tstruct frag_queue *fq;\n\tstruct ipv6hdr *hdr;\n\tu8 prevhdr;\n\n\t/* Jumbo payload inhibits frag. header */\n\tif (ipv6_hdr(skb)->payload_len == 0) {\n\t\tpr_debug(\"payload len = 0\\n\");\n\t\treturn 0;\n\t}\n\n\tif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\n\t\treturn 0;\n\n\tif (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))\n\t\treturn -ENOMEM;\n\n\tskb_set_transport_header(skb, fhoff);\n\thdr = ipv6_hdr(skb);\n\tfhdr = (struct frag_hdr *)skb_transport_header(skb);\n\n\tfq = fq_find(net, fhdr->identification, user, &hdr->saddr, &hdr->daddr,\n\t\t     skb->dev ? skb->dev->ifindex : 0, ip6_frag_ecn(hdr));\n\tif (fq == NULL) {\n\t\tpr_debug(\"Can't find and can't create new queue\\n\");\n\t\treturn -ENOMEM;\n\t}\n\n\tspin_lock_bh(&fq->q.lock);\n\n\tif (nf_ct_frag6_queue(fq, skb, fhdr, nhoff) < 0) {\n\t\tret = -EINVAL;\n\t\tgoto out_unlock;\n\t}\n\n\t/* after queue has assumed skb ownership, only 0 or -EINPROGRESS\n\t * must be returned.\n\t */\n\tret = -EINPROGRESS;\n\tif (fq->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\n\t    fq->q.meat == fq->q.len &&\n\t    nf_ct_frag6_reasm(fq, skb, dev))\n\t\tret = 0;\n\nout_unlock:\n\tspin_unlock_bh(&fq->q.lock);\n\tinet_frag_put(&fq->q, &nf_frags);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\t\treturn 0;",
          "\t\treturn 0;"
        ],
        "deleted": [
          "\t\treturn -EINVAL;",
          "\t\treturn -EINVAL;"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of IPv6 reassembly in the Linux kernel, specifically in the nf_ct_frag6_gather function.",
      "trigger_condition": "Crafted application making socket, connect, and writev system calls that interact with the nf_ct_frag6_gather function.",
      "specific_code_behavior_causing_vulnerability": "The code does not handle the cases where the payload length is 0 or when the function returns an error properly. This can lead to integer overflow, out-of-bounds write, and other issues during IPv6 reassembly.",
      "id": 2,
      "code_after_change_normalized": "int FUN1(struct VAR1 *VAR1, struct sk_buff *VAR2, u32 VAR3)\n{\nstruct net_device *VAR4 = VAR2->VAR4;\nint VAR5, VAR6, VAR7;\nstruct frag_hdr *VAR8;\nstruct frag_queue *VAR9;\nstruct ipv6hdr *VAR10;\nu8 VAR11;\nif (FUN2(VAR2)->VAR12 == 0) {\nFUN3(\"STR\");\nreturn 0;\n}\nif (FUN4(VAR2, &VAR11, &VAR6, &VAR5) < 0)\nreturn 0;\nif (!FUN5(VAR2, VAR5 + sizeof(*VAR8)))\nreturn -VAR13;\nFUN6(VAR2, VAR5);\nVAR10 = FUN2(VAR2);\nVAR8 = (struct VAR14 *)FUN7(VAR2);\nVAR9 = FUN8(VAR1, VAR8->VAR15, VAR3, &VAR10->VAR16, &VAR10->VAR17,\nVAR2->VAR4 ? VAR2->VAR4->VAR18 : 0, FUN9(VAR10));\nif (VAR9 == NULL) {\nFUN3(\"STR\");\nreturn -VAR13;\n}\nFUN10(&VAR9->VAR19.VAR20);\nif (FUN11(VAR9, VAR2, VAR8, VAR6) < 0) {\nVAR7 = -VAR21;\ngoto VAR22;\n}\nVAR7 = -VAR23;\nif (VAR9->VAR19.VAR24 == (VAR25 | VAR26) &&\nVAR9->VAR19.VAR27 == VAR9->VAR19.VAR28 &&\nFUN12(VAR9, VAR2, VAR4))\nVAR7 = 0;\nVAR22:\nFUN13(&VAR9->VAR19.VAR20);\nFUN14(&VAR9->VAR19, &VAR29);\nreturn VAR7;\n}\n",
      "code_before_change_normalized": "int FUN1(struct VAR1 *VAR1, struct sk_buff *VAR2, u32 VAR3)\n{\nstruct net_device *VAR4 = VAR2->VAR4;\nint VAR5, VAR6, VAR7;\nstruct frag_hdr *VAR8;\nstruct frag_queue *VAR9;\nstruct ipv6hdr *VAR10;\nu8 VAR11;\nif (FUN2(VAR2)->VAR12 == 0) {\nFUN3(\"STR\");\nreturn -VAR13;\n}\nif (FUN4(VAR2, &VAR11, &VAR6, &VAR5) < 0)\nreturn -VAR13;\nif (!FUN5(VAR2, VAR5 + sizeof(*VAR8)))\nreturn -VAR14;\nFUN6(VAR2, VAR5);\nVAR10 = FUN2(VAR2);\nVAR8 = (struct VAR15 *)FUN7(VAR2);\nVAR9 = FUN8(VAR1, VAR8->VAR16, VAR3, &VAR10->VAR17, &VAR10->VAR18,\nVAR2->VAR4 ? VAR2->VAR4->VAR19 : 0, FUN9(VAR10));\nif (VAR9 == NULL) {\nFUN3(\"STR\");\nreturn -VAR14;\n}\nFUN10(&VAR9->VAR20.VAR21);\nif (FUN11(VAR9, VAR2, VAR8, VAR6) < 0) {\nVAR7 = -VAR13;\ngoto VAR22;\n}\nVAR7 = -VAR23;\nif (VAR9->VAR20.VAR24 == (VAR25 | VAR26) &&\nVAR9->VAR20.VAR27 == VAR9->VAR20.VAR28 &&\nFUN12(VAR9, VAR2, VAR4))\nVAR7 = 0;\nVAR22:\nFUN13(&VAR9->VAR20.VAR21);\nFUN14(&VAR9->VAR20, &VAR29);\nreturn VAR7;\n}\n",
      "code_after_change_raw": "int nf_ct_frag6_gather(struct net *net, struct sk_buff *skb, u32 user)\n{\nstruct net_device *dev = skb->dev;\nint fhoff, nhoff, ret;\nstruct frag_hdr *fhdr;\nstruct frag_queue *fq;\nstruct ipv6hdr *hdr;\nu8 prevhdr;\nif (ipv6_hdr(skb)->payload_len == 0) {\npr_debug(\"payload len = 0\\n\");\nreturn 0;\n}\nif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\nreturn 0;\nif (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))\nreturn -ENOMEM;\nskb_set_transport_header(skb, fhoff);\nhdr = ipv6_hdr(skb);\nfhdr = (struct frag_hdr *)skb_transport_header(skb);\nfq = fq_find(net, fhdr->identification, user, &hdr->saddr, &hdr->daddr,\nskb->dev ? skb->dev->ifindex : 0, ip6_frag_ecn(hdr));\nif (fq == NULL) {\npr_debug(\"Can't find and can't create new queue\\n\");\nreturn -ENOMEM;\n}\nspin_lock_bh(&fq->q.lock);\nif (nf_ct_frag6_queue(fq, skb, fhdr, nhoff) < 0) {\nret = -EINVAL;\ngoto out_unlock;\n}\nret = -EINPROGRESS;\nif (fq->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\nfq->q.meat == fq->q.len &&\nnf_ct_frag6_reasm(fq, skb, dev))\nret = 0;\nout_unlock:\nspin_unlock_bh(&fq->q.lock);\ninet_frag_put(&fq->q, &nf_frags);\nreturn ret;\n}\n",
      "code_before_change_raw": "int nf_ct_frag6_gather(struct net *net, struct sk_buff *skb, u32 user)\n{\nstruct net_device *dev = skb->dev;\nint fhoff, nhoff, ret;\nstruct frag_hdr *fhdr;\nstruct frag_queue *fq;\nstruct ipv6hdr *hdr;\nu8 prevhdr;\nif (ipv6_hdr(skb)->payload_len == 0) {\npr_debug(\"payload len = 0\\n\");\nreturn -EINVAL;\n}\nif (find_prev_fhdr(skb, &prevhdr, &nhoff, &fhoff) < 0)\nreturn -EINVAL;\nif (!pskb_may_pull(skb, fhoff + sizeof(*fhdr)))\nreturn -ENOMEM;\nskb_set_transport_header(skb, fhoff);\nhdr = ipv6_hdr(skb);\nfhdr = (struct frag_hdr *)skb_transport_header(skb);\nfq = fq_find(net, fhdr->identification, user, &hdr->saddr, &hdr->daddr,\nskb->dev ? skb->dev->ifindex : 0, ip6_frag_ecn(hdr));\nif (fq == NULL) {\npr_debug(\"Can't find and can't create new queue\\n\");\nreturn -ENOMEM;\n}\nspin_lock_bh(&fq->q.lock);\nif (nf_ct_frag6_queue(fq, skb, fhdr, nhoff) < 0) {\nret = -EINVAL;\ngoto out_unlock;\n}\nret = -EINPROGRESS;\nif (fq->q.flags == (INET_FRAG_FIRST_IN | INET_FRAG_LAST_IN) &&\nfq->q.meat == fq->q.len &&\nnf_ct_frag6_reasm(fq, skb, dev))\nret = 0;\nout_unlock:\nspin_unlock_bh(&fq->q.lock);\ninet_frag_put(&fq->q, &nf_frags);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2017-0750": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and checks in the file system superblock verification process.",
        "trigger_condition": "An attacker exploits the lack of validation checks to manipulate the file system superblock data, leading to a privilege escalation vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not include checks for log blocks per segment, reserved inode information, and area boundaries, allowing an attacker to provide malicious data that can bypass the existing validation checks and escalate privileges."
      },
      "solution": "To mitigate the vulnerability, it is necessary to enhance the file system superblock verification process by adding checks for log blocks per segment, reserved inode information, and verifying the boundaries of CP/SIT/NAT/SSA/MAIN_AREA areas. These additional checks ensure that the superblock data is properly validated and prevent potential security risks associated with privilege escalation. By implementing these checks, the code strengthens the overall security of the file system implementation and reduces the likelihood of exploitation.",
      "GPT_analysis": "The above modification is necessary to fix the vulnerability CVE-2017-0750, which is an elevation of privilege vulnerability in the Upstream Linux file system affecting Android kernel versions. By adding the checks for log blocks per segment, reserved inode information, and verifying the boundaries of CP/SIT/NAT/SSA/MAIN_AREA areas, the code ensures that the file system superblock is properly validated and prevents potential security risks associated with the vulnerability. These additional checks enhance the overall security of the file system implementation and help mitigate the identified vulnerability.",
      "GPT_purpose": "Perform sanity checks on the raw superblock data to ensure it meets specific criteria for the F2FS file system.",
      "GPT_function": "\n1. Check for magic number mismatch in the super block.\n2. Verify the page cache size is 4KB.\n3. Ensure the block size is 4KB.\n4. Validate the log sector size falls within supported range.\n5. Check the sum of log sectors per block and log sector size.",
      "CVE_id": "CVE-2017-0750",
      "code_before_change": "static int sanity_check_raw_super(struct super_block *sb,\n\t\t\tstruct f2fs_super_block *raw_super)\n{\n\tunsigned int blocksize;\n\n\tif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\n\t\t\tF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB page cache size */\n\tif (F2FS_BLKSIZE != PAGE_CACHE_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\n\t\t\tPAGE_CACHE_SIZE);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB block size */\n\tblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\n\tif (blocksize != F2FS_BLKSIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid blocksize (%u), supports only 4KB\\n\",\n\t\t\tblocksize);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support 512/1024/2048/4096 bytes sector size */\n\tif (le32_to_cpu(raw_super->log_sectorsize) >\n\t\t\t\tF2FS_MAX_LOG_SECTOR_SIZE ||\n\t\tle32_to_cpu(raw_super->log_sectorsize) <\n\t\t\t\tF2FS_MIN_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\tif (le32_to_cpu(raw_super->log_sectors_per_block) +\n\t\tle32_to_cpu(raw_super->log_sectorsize) !=\n\t\t\tF2FS_MAX_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log sectors per block(%u) log sectorsize(%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectors_per_block),\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\treturn 0;\n}",
      "code_after_change": "static int sanity_check_raw_super(struct super_block *sb,\n\t\t\tstruct f2fs_super_block *raw_super)\n{\n\tunsigned int blocksize;\n\n\tif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\n\t\t\tF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB page cache size */\n\tif (F2FS_BLKSIZE != PAGE_CACHE_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\n\t\t\tPAGE_CACHE_SIZE);\n\t\treturn 1;\n\t}\n\n\t/* Currently, support only 4KB block size */\n\tblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\n\tif (blocksize != F2FS_BLKSIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid blocksize (%u), supports only 4KB\\n\",\n\t\t\tblocksize);\n\t\treturn 1;\n\t}\n\n\t/* check log blocks per segment */\n\tif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log blocks per segment (%u)\\n\",\n\t\t\tle32_to_cpu(raw_super->log_blocks_per_seg));\n\t\treturn 1;\n\t}\n\n\t/* Currently, support 512/1024/2048/4096 bytes sector size */\n\tif (le32_to_cpu(raw_super->log_sectorsize) >\n\t\t\t\tF2FS_MAX_LOG_SECTOR_SIZE ||\n\t\tle32_to_cpu(raw_super->log_sectorsize) <\n\t\t\t\tF2FS_MIN_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\tif (le32_to_cpu(raw_super->log_sectors_per_block) +\n\t\tle32_to_cpu(raw_super->log_sectorsize) !=\n\t\t\tF2FS_MAX_LOG_SECTOR_SIZE) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid log sectors per block(%u) log sectorsize(%u)\",\n\t\t\tle32_to_cpu(raw_super->log_sectors_per_block),\n\t\t\tle32_to_cpu(raw_super->log_sectorsize));\n\t\treturn 1;\n\t}\n\n\t/* check reserved ino info */\n\tif (le32_to_cpu(raw_super->node_ino) != 1 ||\n\t\tle32_to_cpu(raw_super->meta_ino) != 2 ||\n\t\tle32_to_cpu(raw_super->root_ino) != 3) {\n\t\tf2fs_msg(sb, KERN_INFO,\n\t\t\t\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",\n\t\t\tle32_to_cpu(raw_super->node_ino),\n\t\t\tle32_to_cpu(raw_super->meta_ino),\n\t\t\tle32_to_cpu(raw_super->root_ino));\n\t\treturn 1;\n\t}\n\n\t/* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */\n\tif (sanity_check_area_boundary(sb, raw_super))\n\t\treturn 1;\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t/* check log blocks per segment */",
          "\tif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {",
          "\t\tf2fs_msg(sb, KERN_INFO,",
          "\t\t\t\"Invalid log blocks per segment (%u)\\n\",",
          "\t\t\tle32_to_cpu(raw_super->log_blocks_per_seg));",
          "\t\treturn 1;",
          "\t}",
          "",
          "",
          "\t/* check reserved ino info */",
          "\tif (le32_to_cpu(raw_super->node_ino) != 1 ||",
          "\t\tle32_to_cpu(raw_super->meta_ino) != 2 ||",
          "\t\tle32_to_cpu(raw_super->root_ino) != 3) {",
          "\t\tf2fs_msg(sb, KERN_INFO,",
          "\t\t\t\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",",
          "\t\t\tle32_to_cpu(raw_super->node_ino),",
          "\t\t\tle32_to_cpu(raw_super->meta_ino),",
          "\t\t\tle32_to_cpu(raw_super->root_ino));",
          "\t\treturn 1;",
          "\t}",
          "",
          "\t/* check CP/SIT/NAT/SSA/MAIN_AREA area boundary */",
          "\tif (sanity_check_area_boundary(sb, raw_super))",
          "\t\treturn 1;",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation and checks in the file system superblock verification process.",
      "trigger_condition": "An attacker exploits the lack of validation checks to manipulate the file system superblock data, leading to a privilege escalation vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not include checks for log blocks per segment, reserved inode information, and area boundaries, allowing an attacker to provide malicious data that can bypass the existing validation checks and escalate privileges.",
      "id": 3,
      "code_after_change_normalized": "static int FUN1(struct super_block *VAR1,\nstruct f2fs_super_block *VAR2)\n{\nunsigned int VAR3;\nif (VAR4 != FUN2(VAR2->VAR5)) {\nFUN3(VAR1, VAR6,\n\"STR\",\nVAR4, FUN2(VAR2->VAR5));\nreturn 1;\n}\nif (VAR7 != VAR8) {\nFUN3(VAR1, VAR6,\n\"STR\",\nVAR8);\nreturn 1;\n}\nVAR3 = 1 << FUN2(VAR2->VAR9);\nif (VAR3 != VAR7) {\nFUN3(VAR1, VAR6,\n\"STR\",\nVAR3);\nreturn 1;\n}\nif (FUN2(VAR2->VAR10) != 9) {\nFUN3(VAR1, VAR6,\n\"STR\",\nFUN2(VAR2->VAR10));\nreturn 1;\n}\nif (FUN2(VAR2->VAR11) >\nVAR12 ||\nFUN2(VAR2->VAR11) <\nVAR13) {\nFUN3(VAR1, VAR6, \"STR\",\nFUN2(VAR2->VAR11));\nreturn 1;\n}\nif (FUN2(VAR2->VAR14) +\nFUN2(VAR2->VAR11) !=\nVAR12) {\nFUN3(VAR1, VAR6,\n\"STR\",\nFUN2(VAR2->VAR14),\nFUN2(VAR2->VAR11));\nreturn 1;\n}\nif (FUN2(VAR2->VAR15) != 1 ||\nFUN2(VAR2->VAR16) != 2 ||\nFUN2(VAR2->VAR17) != 3) {\nFUN3(VAR1, VAR6,\n\"STR\",\nFUN2(VAR2->VAR15),\nFUN2(VAR2->VAR16),\nFUN2(VAR2->VAR17));\nreturn 1;\n}\nif (FUN4(VAR1, VAR2))\nreturn 1;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct super_block *VAR1,\nstruct f2fs_super_block *VAR2)\n{\nunsigned int VAR3;\nif (VAR4 != FUN2(VAR2->VAR5)) {\nFUN3(VAR1, VAR6,\n\"STR\",\nVAR4, FUN2(VAR2->VAR5));\nreturn 1;\n}\nif (VAR7 != VAR8) {\nFUN3(VAR1, VAR6,\n\"STR\",\nVAR8);\nreturn 1;\n}\nVAR3 = 1 << FUN2(VAR2->VAR9);\nif (VAR3 != VAR7) {\nFUN3(VAR1, VAR6,\n\"STR\",\nVAR3);\nreturn 1;\n}\nif (FUN2(VAR2->VAR10) >\nVAR11 ||\nFUN2(VAR2->VAR10) <\nVAR12) {\nFUN3(VAR1, VAR6, \"STR\",\nFUN2(VAR2->VAR10));\nreturn 1;\n}\nif (FUN2(VAR2->VAR13) +\nFUN2(VAR2->VAR10) !=\nVAR11) {\nFUN3(VAR1, VAR6,\n\"STR\",\nFUN2(VAR2->VAR13),\nFUN2(VAR2->VAR10));\nreturn 1;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int sanity_check_raw_super(struct super_block *sb,\nstruct f2fs_super_block *raw_super)\n{\nunsigned int blocksize;\nif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\nf2fs_msg(sb, KERN_INFO,\n\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\nF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\nreturn 1;\n}\nif (F2FS_BLKSIZE != PAGE_CACHE_SIZE) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\nPAGE_CACHE_SIZE);\nreturn 1;\n}\nblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\nif (blocksize != F2FS_BLKSIZE) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid blocksize (%u), supports only 4KB\\n\",\nblocksize);\nreturn 1;\n}\nif (le32_to_cpu(raw_super->log_blocks_per_seg) != 9) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid log blocks per segment (%u)\\n\",\nle32_to_cpu(raw_super->log_blocks_per_seg));\nreturn 1;\n}\nif (le32_to_cpu(raw_super->log_sectorsize) >\nF2FS_MAX_LOG_SECTOR_SIZE ||\nle32_to_cpu(raw_super->log_sectorsize) <\nF2FS_MIN_LOG_SECTOR_SIZE) {\nf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\nle32_to_cpu(raw_super->log_sectorsize));\nreturn 1;\n}\nif (le32_to_cpu(raw_super->log_sectors_per_block) +\nle32_to_cpu(raw_super->log_sectorsize) !=\nF2FS_MAX_LOG_SECTOR_SIZE) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid log sectors per block(%u) log sectorsize(%u)\",\nle32_to_cpu(raw_super->log_sectors_per_block),\nle32_to_cpu(raw_super->log_sectorsize));\nreturn 1;\n}\nif (le32_to_cpu(raw_super->node_ino) != 1 ||\nle32_to_cpu(raw_super->meta_ino) != 2 ||\nle32_to_cpu(raw_super->root_ino) != 3) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid Fs Meta Ino: node(%u) meta(%u) root(%u)\",\nle32_to_cpu(raw_super->node_ino),\nle32_to_cpu(raw_super->meta_ino),\nle32_to_cpu(raw_super->root_ino));\nreturn 1;\n}\nif (sanity_check_area_boundary(sb, raw_super))\nreturn 1;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int sanity_check_raw_super(struct super_block *sb,\nstruct f2fs_super_block *raw_super)\n{\nunsigned int blocksize;\nif (F2FS_SUPER_MAGIC != le32_to_cpu(raw_super->magic)) {\nf2fs_msg(sb, KERN_INFO,\n\"Magic Mismatch, valid(0x%x) - read(0x%x)\",\nF2FS_SUPER_MAGIC, le32_to_cpu(raw_super->magic));\nreturn 1;\n}\nif (F2FS_BLKSIZE != PAGE_CACHE_SIZE) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid page_cache_size (%lu), supports only 4KB\\n\",\nPAGE_CACHE_SIZE);\nreturn 1;\n}\nblocksize = 1 << le32_to_cpu(raw_super->log_blocksize);\nif (blocksize != F2FS_BLKSIZE) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid blocksize (%u), supports only 4KB\\n\",\nblocksize);\nreturn 1;\n}\nif (le32_to_cpu(raw_super->log_sectorsize) >\nF2FS_MAX_LOG_SECTOR_SIZE ||\nle32_to_cpu(raw_super->log_sectorsize) <\nF2FS_MIN_LOG_SECTOR_SIZE) {\nf2fs_msg(sb, KERN_INFO, \"Invalid log sectorsize (%u)\",\nle32_to_cpu(raw_super->log_sectorsize));\nreturn 1;\n}\nif (le32_to_cpu(raw_super->log_sectors_per_block) +\nle32_to_cpu(raw_super->log_sectorsize) !=\nF2FS_MAX_LOG_SECTOR_SIZE) {\nf2fs_msg(sb, KERN_INFO,\n\"Invalid log sectors per block(%u) log sectorsize(%u)\",\nle32_to_cpu(raw_super->log_sectors_per_block),\nle32_to_cpu(raw_super->log_sectorsize));\nreturn 1;\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2017-1000111": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper synchronization mechanism to handle concurrent updates to socket state, leading to a race condition between a socket option that changes state and safety checks in the packet_set_ring function.",
        "trigger_condition": "Concurrent updates to the socket state occur while handling the PACKET_RESERVE option, potentially leading to a race condition and heap out-of-bounds vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not provide proper locking mechanism to ensure atomicity of updates to the socket state when processing the PACKET_RESERVE option. This can result in a race condition where safety checks in the packet_set_ring function may not be synchronized with changes to the socket state, leading to a vulnerability.",
        "solution": "To mitigate the vulnerability, it is necessary to add a locking mechanism to ensure atomic updates to the socket state when handling the PACKET_RESERVE option. By locking the socket before the critical section and releasing it afterwards, the code prevents race conditions and ensures that safety checks are synchronized with changes to the socket state, thereby addressing the heap out-of-bounds vulnerability."
      },
      "GPT_analysis": "The modification in the code snippet is necessary to address a vulnerability (CVE-2017-1000111) related to heap out-of-bounds in AF_PACKET sockets. The vulnerability arises due to a race condition between a socket option that changes socket state and safety checks in the `packet_set_ring` function. Specifically, the issue occurs with the `PACKET_RESERVE` option.\n\nTo mitigate this vulnerability, the correct solution involves locking the socket for the update when handling the `PACKET_RESERVE` option. By adding the following lines of code to lock and release the socket before and after the critical section, the race condition can be prevented:\n\n```c\nlock_sock(sk);\n// Critical section\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n    ret = -EBUSY;\n} else {\n    po->tp_reserve = val;\n    ret = 0;\n}\nrelease_sock(sk);\n```\n\nBy adding this locking mechanism, the code ensures that the socket state is updated atomically and prevents any potential race conditions that could lead to heap out-of-bounds issues. This modification helps to enhance the security and stability of the code when handling the `PACKET_RESERVE` option.",
      "GPT_purpose": "Set socket options for AF_PACKET sockets.",
      "GPT_function": "\n1. Set socket options for PACKET_ADD_MEMBERSHIP and PACKET_DROP_MEMBERSHIP.\n2. Set socket options for PACKET_RX_RING and PACKET_TX_RING.\n3. Set socket option for PACKET_COPY_THRESH.\n4. Set socket option for PACKET_VERSION.\n5. Set socket option for PACKET_RESERVE.\n6. Set socket option for PACKET_LOSS.\n7. Set socket option for PACKET_AUXDATA.\n8. Set socket option for PACKET_ORIGDEV.\n9. Set socket option for PACKET_VNET_HDR.\n10. Set socket option for PACKET_TIMESTAMP.\n11. Set socket option for PACKET_FANOUT.\n12. Set socket option for PACKET_FANOUT_DATA.\n13. Set socket option for PACKET_TX_HAS_OFF.\n14. Set socket option for PACKET_QDISC_BYPASS.",
      "CVE_id": "CVE-2017-1000111",
      "code_before_change": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tif (val > INT_MAX)\n\t\t\treturn -EINVAL;\n\t\tpo->tp_reserve = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
      "code_after_change": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\n\tstruct sock *sk = sock->sk;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint ret;\n\n\tif (level != SOL_PACKET)\n\t\treturn -ENOPROTOOPT;\n\n\tswitch (optname) {\n\tcase PACKET_ADD_MEMBERSHIP:\n\tcase PACKET_DROP_MEMBERSHIP:\n\t{\n\t\tstruct packet_mreq_max mreq;\n\t\tint len = optlen;\n\t\tmemset(&mreq, 0, sizeof(mreq));\n\t\tif (len < sizeof(struct packet_mreq))\n\t\t\treturn -EINVAL;\n\t\tif (len > sizeof(mreq))\n\t\t\tlen = sizeof(mreq);\n\t\tif (copy_from_user(&mreq, optval, len))\n\t\t\treturn -EFAULT;\n\t\tif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\n\t\t\treturn -EINVAL;\n\t\tif (optname == PACKET_ADD_MEMBERSHIP)\n\t\t\tret = packet_mc_add(sk, &mreq);\n\t\telse\n\t\t\tret = packet_mc_drop(sk, &mreq);\n\t\treturn ret;\n\t}\n\n\tcase PACKET_RX_RING:\n\tcase PACKET_TX_RING:\n\t{\n\t\tunion tpacket_req_u req_u;\n\t\tint len;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\t\tlen = sizeof(req_u.req);\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\tdefault:\n\t\t\tlen = sizeof(req_u.req3);\n\t\t\tbreak;\n\t\t}\n\t\tif (optlen < len)\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&req_u.req, optval, len))\n\t\t\treturn -EFAULT;\n\t\treturn packet_set_ring(sk, &req_u, 0,\n\t\t\toptname == PACKET_TX_RING);\n\t}\n\tcase PACKET_COPY_THRESH:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpkt_sk(sk)->copy_thresh = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VERSION:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tswitch (val) {\n\t\tcase TPACKET_V1:\n\t\tcase TPACKET_V2:\n\t\tcase TPACKET_V3:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_version = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_RESERVE:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tif (val > INT_MAX)\n\t\t\treturn -EINVAL;\n\t\tlock_sock(sk);\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\n\t\t\tret = -EBUSY;\n\t\t} else {\n\t\t\tpo->tp_reserve = val;\n\t\t\tret = 0;\n\t\t}\n\t\trelease_sock(sk);\n\t\treturn ret;\n\t}\n\tcase PACKET_LOSS:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_loss = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_AUXDATA:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->auxdata = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_ORIGDEV:\n\t{\n\t\tint val;\n\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->origdev = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_VNET_HDR:\n\t{\n\t\tint val;\n\n\t\tif (sock->type != SOCK_RAW)\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (optlen < sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->has_vnet_hdr = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_TIMESTAMP:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->tp_tstamp = val;\n\t\treturn 0;\n\t}\n\tcase PACKET_FANOUT:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\treturn fanout_add(sk, val & 0xffff, val >> 16);\n\t}\n\tcase PACKET_FANOUT_DATA:\n\t{\n\t\tif (!po->fanout)\n\t\t\treturn -EINVAL;\n\n\t\treturn fanout_set_data(po, optval, optlen);\n\t}\n\tcase PACKET_TX_HAS_OFF:\n\t{\n\t\tunsigned int val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\n\t\t\treturn -EBUSY;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\t\tpo->tp_tx_has_off = !!val;\n\t\treturn 0;\n\t}\n\tcase PACKET_QDISC_BYPASS:\n\t{\n\t\tint val;\n\n\t\tif (optlen != sizeof(val))\n\t\t\treturn -EINVAL;\n\t\tif (copy_from_user(&val, optval, sizeof(val)))\n\t\t\treturn -EFAULT;\n\n\t\tpo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\n\t\treturn 0;\n\t}\n\tdefault:\n\t\treturn -ENOPROTOOPT;\n\t}\n}",
      "modified_lines": {
        "added": [
          "\t\tlock_sock(sk);",
          "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {",
          "\t\t\tret = -EBUSY;",
          "\t\t} else {",
          "\t\t\tpo->tp_reserve = val;",
          "\t\t\tret = 0;",
          "\t\t}",
          "\t\trelease_sock(sk);",
          "\t\treturn ret;"
        ],
        "deleted": [
          "\t\tif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)",
          "\t\t\treturn -EBUSY;",
          "\t\tpo->tp_reserve = val;",
          "\t\treturn 0;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper synchronization mechanism to handle concurrent updates to socket state, leading to a race condition between a socket option that changes state and safety checks in the packet_set_ring function.",
      "trigger_condition": "Concurrent updates to the socket state occur while handling the PACKET_RESERVE option, potentially leading to a race condition and heap out-of-bounds vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not provide proper locking mechanism to ensure atomicity of updates to the socket state when processing the PACKET_RESERVE option. This can result in a race condition where safety checks in the packet_set_ring function may not be synchronized with changes to the socket state, leading to a vulnerability.",
      "solution": "To mitigate the vulnerability, it is necessary to add a locking mechanism to ensure atomic updates to the socket state when handling the PACKET_RESERVE option. By locking the socket before the critical section and releasing it afterwards, the code prevents race conditions and ensures that safety checks are synchronized with changes to the socket state, thereby addressing the heap out-of-bounds vulnerability.",
      "id": 4,
      "code_after_change_normalized": "static int\nFUN1(struct socket *VAR1, int VAR2, int VAR3, char __user *VAR4, unsigned int VAR5)\n{\nstruct VAR1 *VAR6 = VAR1->VAR6;\nstruct packet_sock *VAR7 = FUN2(VAR6);\nint VAR8;\nif (VAR2 != VAR9)\nreturn -VAR10;\nswitch (VAR3) {\ncase VAR11:\ncase VAR12:\n{\nstruct packet_mreq_max VAR13;\nint VAR14 = VAR5;\nFUN3(&VAR13, 0, sizeof(VAR13));\nif (VAR14 < sizeof(struct VAR15))\nreturn -VAR16;\nif (VAR14 > sizeof(VAR13))\nVAR14 = sizeof(VAR13);\nif (FUN4(&VAR13, VAR4, VAR14))\nreturn -VAR17;\nif (VAR14 < (VAR13.VAR18 + FUN5(struct VAR15, VAR19)))\nreturn -VAR16;\nif (VAR3 == VAR11)\nVAR8 = FUN6(VAR6, &VAR13);\nelse\nVAR8 = FUN7(VAR6, &VAR13);\nreturn VAR8;\n}\ncase VAR20:\ncase VAR21:\n{\nunion tpacket_req_u VAR22;\nint VAR14;\nswitch (VAR7->VAR23) {\ncase VAR24:\ncase VAR25:\nVAR14 = sizeof(VAR22.VAR26);\nbreak;\ncase VAR27:\ndefault:\nVAR14 = sizeof(VAR22.VAR28);\nbreak;\n}\nif (VAR5 < VAR14)\nreturn -VAR16;\nif (FUN4(&VAR22.VAR26, VAR4, VAR14))\nreturn -VAR17;\nreturn FUN8(VAR6, &VAR22, 0,\nVAR3 == VAR21);\n}\ncase VAR29:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nFUN2(VAR6)->VAR31 = VAR30;\nreturn 0;\n}\ncase VAR32:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nswitch (VAR30) {\ncase VAR24:\ncase VAR25:\ncase VAR27:\nbreak;\ndefault:\nreturn -VAR16;\n}\nFUN9(VAR6);\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34) {\nVAR8 = -VAR36;\n} else {\nVAR7->VAR23 = VAR30;\nVAR8 = 0;\n}\nFUN10(VAR6);\nreturn VAR8;\n}\ncase VAR37:\n{\nunsigned int VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nif (VAR30 > VAR38)\nreturn -VAR16;\nFUN9(VAR6);\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34) {\nVAR8 = -VAR36;\n} else {\nVAR7->VAR39 = VAR30;\nVAR8 = 0;\n}\nFUN10(VAR6);\nreturn VAR8;\n}\ncase VAR40:\n{\nunsigned int VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR41 = !!VAR30;\nreturn 0;\n}\ncase VAR42:\n{\nint VAR30;\nif (VAR5 < sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR43 = !!VAR30;\nreturn 0;\n}\ncase VAR44:\n{\nint VAR30;\nif (VAR5 < sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR45 = !!VAR30;\nreturn 0;\n}\ncase VAR46:\n{\nint VAR30;\nif (VAR1->VAR47 != VAR48)\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (VAR5 < sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR49 = !!VAR30;\nreturn 0;\n}\ncase VAR50:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR51 = VAR30;\nreturn 0;\n}\ncase VAR52:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nreturn FUN11(VAR6, VAR30 & VAR53, VAR30 >> 16);\n}\ncase VAR54:\n{\nif (!VAR7->VAR55)\nreturn -VAR16;\nreturn FUN12(VAR7, VAR4, VAR5);\n}\ncase VAR56:\n{\nunsigned int VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR57 = !!VAR30;\nreturn 0;\n}\ncase VAR58:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR59 = VAR30 ? VAR60 : VAR61;\nreturn 0;\n}\ndefault:\nreturn -VAR10;\n}\n}\n",
      "code_before_change_normalized": "static int\nFUN1(struct socket *VAR1, int VAR2, int VAR3, char __user *VAR4, unsigned int VAR5)\n{\nstruct VAR1 *VAR6 = VAR1->VAR6;\nstruct packet_sock *VAR7 = FUN2(VAR6);\nint VAR8;\nif (VAR2 != VAR9)\nreturn -VAR10;\nswitch (VAR3) {\ncase VAR11:\ncase VAR12:\n{\nstruct packet_mreq_max VAR13;\nint VAR14 = VAR5;\nFUN3(&VAR13, 0, sizeof(VAR13));\nif (VAR14 < sizeof(struct VAR15))\nreturn -VAR16;\nif (VAR14 > sizeof(VAR13))\nVAR14 = sizeof(VAR13);\nif (FUN4(&VAR13, VAR4, VAR14))\nreturn -VAR17;\nif (VAR14 < (VAR13.VAR18 + FUN5(struct VAR15, VAR19)))\nreturn -VAR16;\nif (VAR3 == VAR11)\nVAR8 = FUN6(VAR6, &VAR13);\nelse\nVAR8 = FUN7(VAR6, &VAR13);\nreturn VAR8;\n}\ncase VAR20:\ncase VAR21:\n{\nunion tpacket_req_u VAR22;\nint VAR14;\nswitch (VAR7->VAR23) {\ncase VAR24:\ncase VAR25:\nVAR14 = sizeof(VAR22.VAR26);\nbreak;\ncase VAR27:\ndefault:\nVAR14 = sizeof(VAR22.VAR28);\nbreak;\n}\nif (VAR5 < VAR14)\nreturn -VAR16;\nif (FUN4(&VAR22.VAR26, VAR4, VAR14))\nreturn -VAR17;\nreturn FUN8(VAR6, &VAR22, 0,\nVAR3 == VAR21);\n}\ncase VAR29:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nFUN2(VAR6)->VAR31 = VAR30;\nreturn 0;\n}\ncase VAR32:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nswitch (VAR30) {\ncase VAR24:\ncase VAR25:\ncase VAR27:\nbreak;\ndefault:\nreturn -VAR16;\n}\nFUN9(VAR6);\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34) {\nVAR8 = -VAR36;\n} else {\nVAR7->VAR23 = VAR30;\nVAR8 = 0;\n}\nFUN10(VAR6);\nreturn VAR8;\n}\ncase VAR37:\n{\nunsigned int VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nif (VAR30 > VAR38)\nreturn -VAR16;\nVAR7->VAR39 = VAR30;\nreturn 0;\n}\ncase VAR40:\n{\nunsigned int VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR41 = !!VAR30;\nreturn 0;\n}\ncase VAR42:\n{\nint VAR30;\nif (VAR5 < sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR43 = !!VAR30;\nreturn 0;\n}\ncase VAR44:\n{\nint VAR30;\nif (VAR5 < sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR45 = !!VAR30;\nreturn 0;\n}\ncase VAR46:\n{\nint VAR30;\nif (VAR1->VAR47 != VAR48)\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (VAR5 < sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR49 = !!VAR30;\nreturn 0;\n}\ncase VAR50:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR51 = VAR30;\nreturn 0;\n}\ncase VAR52:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nreturn FUN11(VAR6, VAR30 & VAR53, VAR30 >> 16);\n}\ncase VAR54:\n{\nif (!VAR7->VAR55)\nreturn -VAR16;\nreturn FUN12(VAR7, VAR4, VAR5);\n}\ncase VAR56:\n{\nunsigned int VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (VAR7->VAR33.VAR34 || VAR7->VAR35.VAR34)\nreturn -VAR36;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR57 = !!VAR30;\nreturn 0;\n}\ncase VAR58:\n{\nint VAR30;\nif (VAR5 != sizeof(VAR30))\nreturn -VAR16;\nif (FUN4(&VAR30, VAR4, sizeof(VAR30)))\nreturn -VAR17;\nVAR7->VAR59 = VAR30 ? VAR60 : VAR61;\nreturn 0;\n}\ndefault:\nreturn -VAR10;\n}\n}\n",
      "code_after_change_raw": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\nstruct sock *sk = sock->sk;\nstruct packet_sock *po = pkt_sk(sk);\nint ret;\nif (level != SOL_PACKET)\nreturn -ENOPROTOOPT;\nswitch (optname) {\ncase PACKET_ADD_MEMBERSHIP:\ncase PACKET_DROP_MEMBERSHIP:\n{\nstruct packet_mreq_max mreq;\nint len = optlen;\nmemset(&mreq, 0, sizeof(mreq));\nif (len < sizeof(struct packet_mreq))\nreturn -EINVAL;\nif (len > sizeof(mreq))\nlen = sizeof(mreq);\nif (copy_from_user(&mreq, optval, len))\nreturn -EFAULT;\nif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\nreturn -EINVAL;\nif (optname == PACKET_ADD_MEMBERSHIP)\nret = packet_mc_add(sk, &mreq);\nelse\nret = packet_mc_drop(sk, &mreq);\nreturn ret;\n}\ncase PACKET_RX_RING:\ncase PACKET_TX_RING:\n{\nunion tpacket_req_u req_u;\nint len;\nswitch (po->tp_version) {\ncase TPACKET_V1:\ncase TPACKET_V2:\nlen = sizeof(req_u.req);\nbreak;\ncase TPACKET_V3:\ndefault:\nlen = sizeof(req_u.req3);\nbreak;\n}\nif (optlen < len)\nreturn -EINVAL;\nif (copy_from_user(&req_u.req, optval, len))\nreturn -EFAULT;\nreturn packet_set_ring(sk, &req_u, 0,\noptname == PACKET_TX_RING);\n}\ncase PACKET_COPY_THRESH:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npkt_sk(sk)->copy_thresh = val;\nreturn 0;\n}\ncase PACKET_VERSION:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\nswitch (val) {\ncase TPACKET_V1:\ncase TPACKET_V2:\ncase TPACKET_V3:\nbreak;\ndefault:\nreturn -EINVAL;\n}\nlock_sock(sk);\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\nret = -EBUSY;\n} else {\npo->tp_version = val;\nret = 0;\n}\nrelease_sock(sk);\nreturn ret;\n}\ncase PACKET_RESERVE:\n{\nunsigned int val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\nif (val > INT_MAX)\nreturn -EINVAL;\nlock_sock(sk);\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\nret = -EBUSY;\n} else {\npo->tp_reserve = val;\nret = 0;\n}\nrelease_sock(sk);\nreturn ret;\n}\ncase PACKET_LOSS:\n{\nunsigned int val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->tp_loss = !!val;\nreturn 0;\n}\ncase PACKET_AUXDATA:\n{\nint val;\nif (optlen < sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->auxdata = !!val;\nreturn 0;\n}\ncase PACKET_ORIGDEV:\n{\nint val;\nif (optlen < sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->origdev = !!val;\nreturn 0;\n}\ncase PACKET_VNET_HDR:\n{\nint val;\nif (sock->type != SOCK_RAW)\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (optlen < sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->has_vnet_hdr = !!val;\nreturn 0;\n}\ncase PACKET_TIMESTAMP:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->tp_tstamp = val;\nreturn 0;\n}\ncase PACKET_FANOUT:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\nreturn fanout_add(sk, val & 0xffff, val >> 16);\n}\ncase PACKET_FANOUT_DATA:\n{\nif (!po->fanout)\nreturn -EINVAL;\nreturn fanout_set_data(po, optval, optlen);\n}\ncase PACKET_TX_HAS_OFF:\n{\nunsigned int val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->tp_tx_has_off = !!val;\nreturn 0;\n}\ncase PACKET_QDISC_BYPASS:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\nreturn 0;\n}\ndefault:\nreturn -ENOPROTOOPT;\n}\n}\n",
      "code_before_change_raw": "static int\npacket_setsockopt(struct socket *sock, int level, int optname, char __user *optval, unsigned int optlen)\n{\nstruct sock *sk = sock->sk;\nstruct packet_sock *po = pkt_sk(sk);\nint ret;\nif (level != SOL_PACKET)\nreturn -ENOPROTOOPT;\nswitch (optname) {\ncase PACKET_ADD_MEMBERSHIP:\ncase PACKET_DROP_MEMBERSHIP:\n{\nstruct packet_mreq_max mreq;\nint len = optlen;\nmemset(&mreq, 0, sizeof(mreq));\nif (len < sizeof(struct packet_mreq))\nreturn -EINVAL;\nif (len > sizeof(mreq))\nlen = sizeof(mreq);\nif (copy_from_user(&mreq, optval, len))\nreturn -EFAULT;\nif (len < (mreq.mr_alen + offsetof(struct packet_mreq, mr_address)))\nreturn -EINVAL;\nif (optname == PACKET_ADD_MEMBERSHIP)\nret = packet_mc_add(sk, &mreq);\nelse\nret = packet_mc_drop(sk, &mreq);\nreturn ret;\n}\ncase PACKET_RX_RING:\ncase PACKET_TX_RING:\n{\nunion tpacket_req_u req_u;\nint len;\nswitch (po->tp_version) {\ncase TPACKET_V1:\ncase TPACKET_V2:\nlen = sizeof(req_u.req);\nbreak;\ncase TPACKET_V3:\ndefault:\nlen = sizeof(req_u.req3);\nbreak;\n}\nif (optlen < len)\nreturn -EINVAL;\nif (copy_from_user(&req_u.req, optval, len))\nreturn -EFAULT;\nreturn packet_set_ring(sk, &req_u, 0,\noptname == PACKET_TX_RING);\n}\ncase PACKET_COPY_THRESH:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npkt_sk(sk)->copy_thresh = val;\nreturn 0;\n}\ncase PACKET_VERSION:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\nswitch (val) {\ncase TPACKET_V1:\ncase TPACKET_V2:\ncase TPACKET_V3:\nbreak;\ndefault:\nreturn -EINVAL;\n}\nlock_sock(sk);\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec) {\nret = -EBUSY;\n} else {\npo->tp_version = val;\nret = 0;\n}\nrelease_sock(sk);\nreturn ret;\n}\ncase PACKET_RESERVE:\n{\nunsigned int val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\nif (val > INT_MAX)\nreturn -EINVAL;\npo->tp_reserve = val;\nreturn 0;\n}\ncase PACKET_LOSS:\n{\nunsigned int val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->tp_loss = !!val;\nreturn 0;\n}\ncase PACKET_AUXDATA:\n{\nint val;\nif (optlen < sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->auxdata = !!val;\nreturn 0;\n}\ncase PACKET_ORIGDEV:\n{\nint val;\nif (optlen < sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->origdev = !!val;\nreturn 0;\n}\ncase PACKET_VNET_HDR:\n{\nint val;\nif (sock->type != SOCK_RAW)\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (optlen < sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->has_vnet_hdr = !!val;\nreturn 0;\n}\ncase PACKET_TIMESTAMP:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->tp_tstamp = val;\nreturn 0;\n}\ncase PACKET_FANOUT:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\nreturn fanout_add(sk, val & 0xffff, val >> 16);\n}\ncase PACKET_FANOUT_DATA:\n{\nif (!po->fanout)\nreturn -EINVAL;\nreturn fanout_set_data(po, optval, optlen);\n}\ncase PACKET_TX_HAS_OFF:\n{\nunsigned int val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (po->rx_ring.pg_vec || po->tx_ring.pg_vec)\nreturn -EBUSY;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->tp_tx_has_off = !!val;\nreturn 0;\n}\ncase PACKET_QDISC_BYPASS:\n{\nint val;\nif (optlen != sizeof(val))\nreturn -EINVAL;\nif (copy_from_user(&val, optval, sizeof(val)))\nreturn -EFAULT;\npo->xmit = val ? packet_direct_xmit : dev_queue_xmit;\nreturn 0;\n}\ndefault:\nreturn -ENOPROTOOPT;\n}\n}\n"
    }
  ],
  "CVE-2017-1000251": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when building the configuration request in the l2cap_send_cmd function.",
        "trigger_condition": "Receiving a maliciously crafted L2CAP configuration response that causes the l2cap_build_conf_req function to write beyond the bounds of the buffer, leading to a stack overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not provide the correct buffer size to the l2cap_build_conf_req function, allowing it to potentially write beyond the bounds of the buffer and trigger a stack overflow vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the correct buffer size is passed as an argument to the l2cap_build_conf_req function. This prevents the function from writing beyond the bounds of the buffer and eliminates the stack overflow vulnerability. In this case, the solution involves modifying the code to include the size of the buffer as an argument when calling the l2cap_build_conf_req function.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2017-1000251, which is a stack overflow vulnerability in the processing of L2CAP configuration responses in the native Bluetooth stack in the Linux Kernel (BlueZ). \n\nThe vulnerability occurs due to the lack of proper bounds checking when building the configuration request in the `l2cap_send_cmd` function. By adding the size of the `req` buffer as an argument to `l2cap_build_conf_req`, the function can ensure that it does not write beyond the bounds of the buffer, thus preventing a stack overflow.\n\nTherefore, the modification ensures that the `l2cap_build_conf_req` function is provided with the correct buffer size to prevent any potential buffer overflow vulnerabilities, thereby addressing the security issue.",
      "GPT_purpose": "Handle L2CAP connection creation responses in the Bluetooth stack.",
      "GPT_function": "\n1. Handle L2CAP connection creation response.\n2. Extract necessary information from the response data.\n3. Check for protocol errors and handle different result cases.\n4. Manage the state of L2CAP channels based on the response.\n5. Ensure proper locking and unlocking mechanisms for thread safety.",
      "CVE_id": "CVE-2017-1000251",
      "code_before_change": "static int l2cap_connect_create_rsp(struct l2cap_conn *conn,\n\t\t\t\t    struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t    u8 *data)\n{\n\tstruct l2cap_conn_rsp *rsp = (struct l2cap_conn_rsp *) data;\n\tu16 scid, dcid, result, status;\n\tstruct l2cap_chan *chan;\n\tu8 req[128];\n\tint err;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tdcid   = __le16_to_cpu(rsp->dcid);\n\tresult = __le16_to_cpu(rsp->result);\n\tstatus = __le16_to_cpu(rsp->status);\n\n\tBT_DBG(\"dcid 0x%4.4x scid 0x%4.4x result 0x%2.2x status 0x%2.2x\",\n\t       dcid, scid, result, status);\n\n\tmutex_lock(&conn->chan_lock);\n\n\tif (scid) {\n\t\tchan = __l2cap_get_chan_by_scid(conn, scid);\n\t\tif (!chan) {\n\t\t\terr = -EBADSLT;\n\t\t\tgoto unlock;\n\t\t}\n\t} else {\n\t\tchan = __l2cap_get_chan_by_ident(conn, cmd->ident);\n\t\tif (!chan) {\n\t\t\terr = -EBADSLT;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\terr = 0;\n\n\tl2cap_chan_lock(chan);\n\n\tswitch (result) {\n\tcase L2CAP_CR_SUCCESS:\n\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\tchan->ident = 0;\n\t\tchan->dcid = dcid;\n\t\tclear_bit(CONF_CONNECT_PEND, &chan->conf_state);\n\n\t\tif (test_and_set_bit(CONF_REQ_SENT, &chan->conf_state))\n\t\t\tbreak;\n\n\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\n\t\t\t       l2cap_build_conf_req(chan, req), req);\n\t\tchan->num_conf_req++;\n\t\tbreak;\n\n\tcase L2CAP_CR_PEND:\n\t\tset_bit(CONF_CONNECT_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tdefault:\n\t\tl2cap_chan_del(chan, ECONNREFUSED);\n\t\tbreak;\n\t}\n\n\tl2cap_chan_unlock(chan);\n\nunlock:\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn err;\n}",
      "code_after_change": "static int l2cap_connect_create_rsp(struct l2cap_conn *conn,\n\t\t\t\t    struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t    u8 *data)\n{\n\tstruct l2cap_conn_rsp *rsp = (struct l2cap_conn_rsp *) data;\n\tu16 scid, dcid, result, status;\n\tstruct l2cap_chan *chan;\n\tu8 req[128];\n\tint err;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tdcid   = __le16_to_cpu(rsp->dcid);\n\tresult = __le16_to_cpu(rsp->result);\n\tstatus = __le16_to_cpu(rsp->status);\n\n\tBT_DBG(\"dcid 0x%4.4x scid 0x%4.4x result 0x%2.2x status 0x%2.2x\",\n\t       dcid, scid, result, status);\n\n\tmutex_lock(&conn->chan_lock);\n\n\tif (scid) {\n\t\tchan = __l2cap_get_chan_by_scid(conn, scid);\n\t\tif (!chan) {\n\t\t\terr = -EBADSLT;\n\t\t\tgoto unlock;\n\t\t}\n\t} else {\n\t\tchan = __l2cap_get_chan_by_ident(conn, cmd->ident);\n\t\tif (!chan) {\n\t\t\terr = -EBADSLT;\n\t\t\tgoto unlock;\n\t\t}\n\t}\n\n\terr = 0;\n\n\tl2cap_chan_lock(chan);\n\n\tswitch (result) {\n\tcase L2CAP_CR_SUCCESS:\n\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\tchan->ident = 0;\n\t\tchan->dcid = dcid;\n\t\tclear_bit(CONF_CONNECT_PEND, &chan->conf_state);\n\n\t\tif (test_and_set_bit(CONF_REQ_SENT, &chan->conf_state))\n\t\t\tbreak;\n\n\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\n\t\t\t       l2cap_build_conf_req(chan, req, sizeof(req)), req);\n\t\tchan->num_conf_req++;\n\t\tbreak;\n\n\tcase L2CAP_CR_PEND:\n\t\tset_bit(CONF_CONNECT_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tdefault:\n\t\tl2cap_chan_del(chan, ECONNREFUSED);\n\t\tbreak;\n\t}\n\n\tl2cap_chan_unlock(chan);\n\nunlock:\n\tmutex_unlock(&conn->chan_lock);\n\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t       l2cap_build_conf_req(chan, req, sizeof(req)), req);"
        ],
        "deleted": [
          "\t\t\t       l2cap_build_conf_req(chan, req), req);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when building the configuration request in the l2cap_send_cmd function.",
      "trigger_condition": "Receiving a maliciously crafted L2CAP configuration response that causes the l2cap_build_conf_req function to write beyond the bounds of the buffer, leading to a stack overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not provide the correct buffer size to the l2cap_build_conf_req function, allowing it to potentially write beyond the bounds of the buffer and trigger a stack overflow vulnerability.",
      "id": 5,
      "code_after_change_normalized": "static int FUN1(struct l2cap_conn *VAR1,\nstruct l2cap_cmd_hdr *VAR2, u16 VAR3,\nu8 *VAR4)\n{\nstruct VAR6 *VAR5 = (struct VAR6 *) VAR4;\nu16 VAR7, VAR8, VAR9, VAR10;\nstruct l2cap_chan *VAR11;\nu8 VAR12[128];\nint VAR13;\nif (VAR3 < sizeof(*VAR5))\nreturn -VAR14;\nVAR7   = FUN2(VAR5->VAR7);\nVAR8   = FUN2(VAR5->VAR8);\nVAR9 = FUN2(VAR5->VAR9);\nVAR10 = FUN2(VAR5->VAR10);\nFUN3(\"STR\",\nVAR8, VAR7, VAR9, VAR10);\nFUN4(&VAR1->VAR15);\nif (VAR7) {\nVAR11 = FUN5(VAR1, VAR7);\nif (!VAR11) {\nVAR13 = -VAR16;\ngoto VAR17;\n}\n} else {\nVAR11 = FUN6(VAR1, VAR2->VAR18);\nif (!VAR11) {\nVAR13 = -VAR16;\ngoto VAR17;\n}\n}\nVAR13 = 0;\nFUN7(VAR11);\nswitch (VAR9) {\ncase VAR19:\nFUN8(VAR11, VAR20);\nVAR11->VAR18 = 0;\nVAR11->VAR8 = VAR8;\nFUN9(VAR21, &VAR11->VAR22);\nif (FUN10(VAR23, &VAR11->VAR22))\nbreak;\nFUN11(VAR1, FUN12(VAR1), VAR24,\nFUN13(VAR11, VAR12, sizeof(VAR12)), VAR12);\nVAR11->VAR25++;\nbreak;\ncase VAR26:\nFUN14(VAR21, &VAR11->VAR22);\nbreak;\ndefault:\nFUN15(VAR11, VAR27);\nbreak;\n}\nFUN16(VAR11);\nVAR17:\nFUN17(&VAR1->VAR15);\nreturn VAR13;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct l2cap_conn *VAR1,\nstruct l2cap_cmd_hdr *VAR2, u16 VAR3,\nu8 *VAR4)\n{\nstruct VAR6 *VAR5 = (struct VAR6 *) VAR4;\nu16 VAR7, VAR8, VAR9, VAR10;\nstruct l2cap_chan *VAR11;\nu8 VAR12[128];\nint VAR13;\nif (VAR3 < sizeof(*VAR5))\nreturn -VAR14;\nVAR7   = FUN2(VAR5->VAR7);\nVAR8   = FUN2(VAR5->VAR8);\nVAR9 = FUN2(VAR5->VAR9);\nVAR10 = FUN2(VAR5->VAR10);\nFUN3(\"STR\",\nVAR8, VAR7, VAR9, VAR10);\nFUN4(&VAR1->VAR15);\nif (VAR7) {\nVAR11 = FUN5(VAR1, VAR7);\nif (!VAR11) {\nVAR13 = -VAR16;\ngoto VAR17;\n}\n} else {\nVAR11 = FUN6(VAR1, VAR2->VAR18);\nif (!VAR11) {\nVAR13 = -VAR16;\ngoto VAR17;\n}\n}\nVAR13 = 0;\nFUN7(VAR11);\nswitch (VAR9) {\ncase VAR19:\nFUN8(VAR11, VAR20);\nVAR11->VAR18 = 0;\nVAR11->VAR8 = VAR8;\nFUN9(VAR21, &VAR11->VAR22);\nif (FUN10(VAR23, &VAR11->VAR22))\nbreak;\nFUN11(VAR1, FUN12(VAR1), VAR24,\nFUN13(VAR11, VAR12), VAR12);\nVAR11->VAR25++;\nbreak;\ncase VAR26:\nFUN14(VAR21, &VAR11->VAR22);\nbreak;\ndefault:\nFUN15(VAR11, VAR27);\nbreak;\n}\nFUN16(VAR11);\nVAR17:\nFUN17(&VAR1->VAR15);\nreturn VAR13;\n}\n",
      "code_after_change_raw": "static int l2cap_connect_create_rsp(struct l2cap_conn *conn,\nstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\nu8 *data)\n{\nstruct l2cap_conn_rsp *rsp = (struct l2cap_conn_rsp *) data;\nu16 scid, dcid, result, status;\nstruct l2cap_chan *chan;\nu8 req[128];\nint err;\nif (cmd_len < sizeof(*rsp))\nreturn -EPROTO;\nscid   = __le16_to_cpu(rsp->scid);\ndcid   = __le16_to_cpu(rsp->dcid);\nresult = __le16_to_cpu(rsp->result);\nstatus = __le16_to_cpu(rsp->status);\nBT_DBG(\"dcid 0x%4.4x scid 0x%4.4x result 0x%2.2x status 0x%2.2x\",\ndcid, scid, result, status);\nmutex_lock(&conn->chan_lock);\nif (scid) {\nchan = __l2cap_get_chan_by_scid(conn, scid);\nif (!chan) {\nerr = -EBADSLT;\ngoto unlock;\n}\n} else {\nchan = __l2cap_get_chan_by_ident(conn, cmd->ident);\nif (!chan) {\nerr = -EBADSLT;\ngoto unlock;\n}\n}\nerr = 0;\nl2cap_chan_lock(chan);\nswitch (result) {\ncase L2CAP_CR_SUCCESS:\nl2cap_state_change(chan, BT_CONFIG);\nchan->ident = 0;\nchan->dcid = dcid;\nclear_bit(CONF_CONNECT_PEND, &chan->conf_state);\nif (test_and_set_bit(CONF_REQ_SENT, &chan->conf_state))\nbreak;\nl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, req, sizeof(req)), req);\nchan->num_conf_req++;\nbreak;\ncase L2CAP_CR_PEND:\nset_bit(CONF_CONNECT_PEND, &chan->conf_state);\nbreak;\ndefault:\nl2cap_chan_del(chan, ECONNREFUSED);\nbreak;\n}\nl2cap_chan_unlock(chan);\nunlock:\nmutex_unlock(&conn->chan_lock);\nreturn err;\n}\n",
      "code_before_change_raw": "static int l2cap_connect_create_rsp(struct l2cap_conn *conn,\nstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\nu8 *data)\n{\nstruct l2cap_conn_rsp *rsp = (struct l2cap_conn_rsp *) data;\nu16 scid, dcid, result, status;\nstruct l2cap_chan *chan;\nu8 req[128];\nint err;\nif (cmd_len < sizeof(*rsp))\nreturn -EPROTO;\nscid   = __le16_to_cpu(rsp->scid);\ndcid   = __le16_to_cpu(rsp->dcid);\nresult = __le16_to_cpu(rsp->result);\nstatus = __le16_to_cpu(rsp->status);\nBT_DBG(\"dcid 0x%4.4x scid 0x%4.4x result 0x%2.2x status 0x%2.2x\",\ndcid, scid, result, status);\nmutex_lock(&conn->chan_lock);\nif (scid) {\nchan = __l2cap_get_chan_by_scid(conn, scid);\nif (!chan) {\nerr = -EBADSLT;\ngoto unlock;\n}\n} else {\nchan = __l2cap_get_chan_by_ident(conn, cmd->ident);\nif (!chan) {\nerr = -EBADSLT;\ngoto unlock;\n}\n}\nerr = 0;\nl2cap_chan_lock(chan);\nswitch (result) {\ncase L2CAP_CR_SUCCESS:\nl2cap_state_change(chan, BT_CONFIG);\nchan->ident = 0;\nchan->dcid = dcid;\nclear_bit(CONF_CONNECT_PEND, &chan->conf_state);\nif (test_and_set_bit(CONF_REQ_SENT, &chan->conf_state))\nbreak;\nl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, req), req);\nchan->num_conf_req++;\nbreak;\ncase L2CAP_CR_PEND:\nset_bit(CONF_CONNECT_PEND, &chan->conf_state);\nbreak;\ndefault:\nl2cap_chan_del(chan, ECONNREFUSED);\nbreak;\n}\nl2cap_chan_unlock(chan);\nunlock:\nmutex_unlock(&conn->chan_lock);\nreturn err;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Insufficient validation of buffer sizes in the processing of L2CAP configuration responses.",
        "trigger_condition": "Receiving a specially crafted L2CAP configuration response with a buffer size larger than expected, leading to a stack overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not ensure that the buffer size passed to the l2cap_parse_conf_req function matches the actual size of the buffer, potentially allowing an attacker to overflow the stack by providing a larger buffer size than expected."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the buffer size passed to the l2cap_parse_conf_req function matches the actual size of the buffer being used. This prevents a stack overflow by limiting the amount of data that can be written to the buffer. In this case, the solution involves modifying the code to pass the correct buffer size (sizeof(rsp)) to the l2cap_parse_conf_req function call.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-1000251, which is a stack overflow vulnerability in the processing of L2CAP configuration responses in the Linux Kernel's Bluetooth stack (BlueZ). By modifying the code as shown, the vulnerability is mitigated by ensuring that the function `l2cap_parse_conf_req` is called with the correct buffer size (`sizeof(rsp)`) to prevent a stack overflow.\n\nThe vulnerability could potentially allow an attacker to trigger a stack overflow by sending a specially crafted L2CAP configuration response, leading to remote code execution in kernel space. By correctly handling the buffer size in the `l2cap_parse_conf_req` function call, the risk of a stack overflow is reduced, thus improving the security of the code.",
      "GPT_purpose": "Handle L2CAP configuration requests and responses within the Bluetooth stack in the Linux Kernel.",
      "GPT_function": "\n1. Process L2CAP configuration requests.\n2. Check for invalid channel states and reject if necessary.\n3. Store configuration data and handle continuation flags.\n4. Send appropriate responses based on the configuration status.\n5. Initialize and finalize channel configurations.\n6. Handle compatibility checks and responses with remote devices.\n7. Unlock the L2CAP channel after processing.",
      "CVE_id": "CVE-2017-1000251",
      "code_before_change": "static inline int l2cap_config_req(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_req *req = (struct l2cap_conf_req *) data;\n\tu16 dcid, flags;\n\tu8 rsp[64];\n\tstruct l2cap_chan *chan;\n\tint len, err = 0;\n\n\tif (cmd_len < sizeof(*req))\n\t\treturn -EPROTO;\n\n\tdcid  = __le16_to_cpu(req->dcid);\n\tflags = __le16_to_cpu(req->flags);\n\n\tBT_DBG(\"dcid 0x%4.4x flags 0x%2.2x\", dcid, flags);\n\n\tchan = l2cap_get_chan_by_scid(conn, dcid);\n\tif (!chan) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, dcid, 0);\n\t\treturn 0;\n\t}\n\n\tif (chan->state != BT_CONFIG && chan->state != BT_CONNECT2) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, chan->scid,\n\t\t\t\t       chan->dcid);\n\t\tgoto unlock;\n\t}\n\n\t/* Reject if config buffer is too small. */\n\tlen = cmd_len - sizeof(*req);\n\tif (chan->conf_len + len > sizeof(chan->conf_req)) {\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_REJECT, flags), rsp);\n\t\tgoto unlock;\n\t}\n\n\t/* Store config. */\n\tmemcpy(chan->conf_req + chan->conf_len, req->data, len);\n\tchan->conf_len += len;\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION) {\n\t\t/* Incomplete config. Send empty response. */\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_SUCCESS, flags), rsp);\n\t\tgoto unlock;\n\t}\n\n\t/* Complete config. */\n\tlen = l2cap_parse_conf_req(chan, rsp);\n\tif (len < 0) {\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto unlock;\n\t}\n\n\tchan->ident = cmd->ident;\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP, len, rsp);\n\tchan->num_conf_rsp++;\n\n\t/* Reset config buffer. */\n\tchan->conf_len = 0;\n\n\tif (!test_bit(CONF_OUTPUT_DONE, &chan->conf_state))\n\t\tgoto unlock;\n\n\tif (test_bit(CONF_INPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\n\t\tgoto unlock;\n\t}\n\n\tif (!test_and_set_bit(CONF_REQ_SENT, &chan->conf_state)) {\n\t\tu8 buf[64];\n\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\n\t\t\t       l2cap_build_conf_req(chan, buf), buf);\n\t\tchan->num_conf_req++;\n\t}\n\n\t/* Got Conf Rsp PENDING from remote side and assume we sent\n\t   Conf Rsp PENDING in the code above */\n\tif (test_bit(CONF_REM_CONF_PEND, &chan->conf_state) &&\n\t    test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\n\t\t/* check compatibility */\n\n\t\t/* Send rsp for BR/EDR channel */\n\t\tif (!chan->hs_hcon)\n\t\t\tl2cap_send_efs_conf_rsp(chan, rsp, cmd->ident, flags);\n\t\telse\n\t\t\tchan->ident = cmd->ident;\n\t}\n\nunlock:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
      "code_after_change": "static inline int l2cap_config_req(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_req *req = (struct l2cap_conf_req *) data;\n\tu16 dcid, flags;\n\tu8 rsp[64];\n\tstruct l2cap_chan *chan;\n\tint len, err = 0;\n\n\tif (cmd_len < sizeof(*req))\n\t\treturn -EPROTO;\n\n\tdcid  = __le16_to_cpu(req->dcid);\n\tflags = __le16_to_cpu(req->flags);\n\n\tBT_DBG(\"dcid 0x%4.4x flags 0x%2.2x\", dcid, flags);\n\n\tchan = l2cap_get_chan_by_scid(conn, dcid);\n\tif (!chan) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, dcid, 0);\n\t\treturn 0;\n\t}\n\n\tif (chan->state != BT_CONFIG && chan->state != BT_CONNECT2) {\n\t\tcmd_reject_invalid_cid(conn, cmd->ident, chan->scid,\n\t\t\t\t       chan->dcid);\n\t\tgoto unlock;\n\t}\n\n\t/* Reject if config buffer is too small. */\n\tlen = cmd_len - sizeof(*req);\n\tif (chan->conf_len + len > sizeof(chan->conf_req)) {\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_REJECT, flags), rsp);\n\t\tgoto unlock;\n\t}\n\n\t/* Store config. */\n\tmemcpy(chan->conf_req + chan->conf_len, req->data, len);\n\tchan->conf_len += len;\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION) {\n\t\t/* Incomplete config. Send empty response. */\n\t\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\n\t\t\t       l2cap_build_conf_rsp(chan, rsp,\n\t\t\t       L2CAP_CONF_SUCCESS, flags), rsp);\n\t\tgoto unlock;\n\t}\n\n\t/* Complete config. */\n\tlen = l2cap_parse_conf_req(chan, rsp, sizeof(rsp));\n\tif (len < 0) {\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto unlock;\n\t}\n\n\tchan->ident = cmd->ident;\n\tl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP, len, rsp);\n\tchan->num_conf_rsp++;\n\n\t/* Reset config buffer. */\n\tchan->conf_len = 0;\n\n\tif (!test_bit(CONF_OUTPUT_DONE, &chan->conf_state))\n\t\tgoto unlock;\n\n\tif (test_bit(CONF_INPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\n\t\tgoto unlock;\n\t}\n\n\tif (!test_and_set_bit(CONF_REQ_SENT, &chan->conf_state)) {\n\t\tu8 buf[64];\n\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\n\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)), buf);\n\t\tchan->num_conf_req++;\n\t}\n\n\t/* Got Conf Rsp PENDING from remote side and assume we sent\n\t   Conf Rsp PENDING in the code above */\n\tif (test_bit(CONF_REM_CONF_PEND, &chan->conf_state) &&\n\t    test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\n\t\t/* check compatibility */\n\n\t\t/* Send rsp for BR/EDR channel */\n\t\tif (!chan->hs_hcon)\n\t\t\tl2cap_send_efs_conf_rsp(chan, rsp, cmd->ident, flags);\n\t\telse\n\t\t\tchan->ident = cmd->ident;\n\t}\n\nunlock:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tlen = l2cap_parse_conf_req(chan, rsp, sizeof(rsp));",
          "\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)), buf);"
        ],
        "deleted": [
          "\tlen = l2cap_parse_conf_req(chan, rsp);",
          "\t\t\t       l2cap_build_conf_req(chan, buf), buf);"
        ]
      },
      "preconditions_for_vulnerability": "Insufficient validation of buffer sizes in the processing of L2CAP configuration responses.",
      "trigger_condition": "Receiving a specially crafted L2CAP configuration response with a buffer size larger than expected, leading to a stack overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not ensure that the buffer size passed to the l2cap_parse_conf_req function matches the actual size of the buffer, potentially allowing an attacker to overflow the stack by providing a larger buffer size than expected.",
      "id": 6,
      "code_after_change_normalized": "static inline int FUN1(struct l2cap_conn *VAR1,\nstruct l2cap_cmd_hdr *VAR2, u16 VAR3,\nu8 *VAR4)\n{\nstruct VAR6 *VAR5 = (struct VAR6 *) VAR4;\nu16 VAR7, VAR8;\nu8 VAR9[64];\nstruct l2cap_chan *VAR10;\nint VAR11, VAR12 = 0;\nif (VAR3 < sizeof(*VAR5))\nreturn -VAR13;\nVAR7  = FUN2(VAR5->VAR7);\nVAR8 = FUN2(VAR5->VAR8);\nFUN3(\"STR\", VAR7, VAR8);\nVAR10 = FUN4(VAR1, VAR7);\nif (!VAR10) {\nFUN5(VAR1, VAR2->VAR14, VAR7, 0);\nreturn 0;\n}\nif (VAR10->VAR15 != VAR16 && VAR10->VAR15 != VAR17) {\nFUN5(VAR1, VAR2->VAR14, VAR10->VAR18,\nVAR10->VAR7);\ngoto VAR19;\n}\nVAR11 = VAR3 - sizeof(*VAR5);\nif (VAR10->VAR20 + VAR11 > sizeof(VAR10->VAR21)) {\nFUN6(VAR1, VAR2->VAR14, VAR22,\nFUN7(VAR10, VAR9,\nVAR23, VAR8), VAR9);\ngoto VAR19;\n}\nFUN8(VAR10->VAR21 + VAR10->VAR20, VAR5->VAR4, VAR11);\nVAR10->VAR20 += VAR11;\nif (VAR8 & VAR24) {\nFUN6(VAR1, VAR2->VAR14, VAR22,\nFUN7(VAR10, VAR9,\nVAR25, VAR8), VAR9);\ngoto VAR19;\n}\nVAR11 = FUN9(VAR10, VAR9, sizeof(VAR9));\nif (VAR11 < 0) {\nFUN10(VAR10, VAR26);\ngoto VAR19;\n}\nVAR10->VAR14 = VAR2->VAR14;\nFUN6(VAR1, VAR2->VAR14, VAR22, VAR11, VAR9);\nVAR10->VAR27++;\nVAR10->VAR20 = 0;\nif (!FUN11(VAR28, &VAR10->VAR29))\ngoto VAR19;\nif (FUN11(VAR30, &VAR10->VAR29)) {\nFUN12(VAR10);\nif (VAR10->VAR31 == VAR32 ||\nVAR10->VAR31 == VAR33)\nVAR12 = FUN13(VAR10);\nif (VAR12 < 0)\nFUN10(VAR10, -VAR12);\nelse\nFUN14(VAR10);\ngoto VAR19;\n}\nif (!FUN15(VAR34, &VAR10->VAR29)) {\nu8 VAR35[64];\nFUN6(VAR1, FUN16(VAR1), VAR36,\nFUN17(VAR10, VAR35, sizeof(VAR35)), VAR35);\nVAR10->VAR37++;\n}\nif (FUN11(VAR38, &VAR10->VAR29) &&\nFUN11(VAR39, &VAR10->VAR29)) {\nif (!VAR10->VAR40)\nFUN18(VAR10, VAR9, VAR2->VAR14, VAR8);\nelse\nVAR10->VAR14 = VAR2->VAR14;\n}\nVAR19:\nFUN19(VAR10);\nreturn VAR12;\n}\n",
      "code_before_change_normalized": "static inline int FUN1(struct l2cap_conn *VAR1,\nstruct l2cap_cmd_hdr *VAR2, u16 VAR3,\nu8 *VAR4)\n{\nstruct VAR6 *VAR5 = (struct VAR6 *) VAR4;\nu16 VAR7, VAR8;\nu8 VAR9[64];\nstruct l2cap_chan *VAR10;\nint VAR11, VAR12 = 0;\nif (VAR3 < sizeof(*VAR5))\nreturn -VAR13;\nVAR7  = FUN2(VAR5->VAR7);\nVAR8 = FUN2(VAR5->VAR8);\nFUN3(\"STR\", VAR7, VAR8);\nVAR10 = FUN4(VAR1, VAR7);\nif (!VAR10) {\nFUN5(VAR1, VAR2->VAR14, VAR7, 0);\nreturn 0;\n}\nif (VAR10->VAR15 != VAR16 && VAR10->VAR15 != VAR17) {\nFUN5(VAR1, VAR2->VAR14, VAR10->VAR18,\nVAR10->VAR7);\ngoto VAR19;\n}\nVAR11 = VAR3 - sizeof(*VAR5);\nif (VAR10->VAR20 + VAR11 > sizeof(VAR10->VAR21)) {\nFUN6(VAR1, VAR2->VAR14, VAR22,\nFUN7(VAR10, VAR9,\nVAR23, VAR8), VAR9);\ngoto VAR19;\n}\nFUN8(VAR10->VAR21 + VAR10->VAR20, VAR5->VAR4, VAR11);\nVAR10->VAR20 += VAR11;\nif (VAR8 & VAR24) {\nFUN6(VAR1, VAR2->VAR14, VAR22,\nFUN7(VAR10, VAR9,\nVAR25, VAR8), VAR9);\ngoto VAR19;\n}\nVAR11 = FUN9(VAR10, VAR9);\nif (VAR11 < 0) {\nFUN10(VAR10, VAR26);\ngoto VAR19;\n}\nVAR10->VAR14 = VAR2->VAR14;\nFUN6(VAR1, VAR2->VAR14, VAR22, VAR11, VAR9);\nVAR10->VAR27++;\nVAR10->VAR20 = 0;\nif (!FUN11(VAR28, &VAR10->VAR29))\ngoto VAR19;\nif (FUN11(VAR30, &VAR10->VAR29)) {\nFUN12(VAR10);\nif (VAR10->VAR31 == VAR32 ||\nVAR10->VAR31 == VAR33)\nVAR12 = FUN13(VAR10);\nif (VAR12 < 0)\nFUN10(VAR10, -VAR12);\nelse\nFUN14(VAR10);\ngoto VAR19;\n}\nif (!FUN15(VAR34, &VAR10->VAR29)) {\nu8 VAR35[64];\nFUN6(VAR1, FUN16(VAR1), VAR36,\nFUN17(VAR10, VAR35), VAR35);\nVAR10->VAR37++;\n}\nif (FUN11(VAR38, &VAR10->VAR29) &&\nFUN11(VAR39, &VAR10->VAR29)) {\nif (!VAR10->VAR40)\nFUN18(VAR10, VAR9, VAR2->VAR14, VAR8);\nelse\nVAR10->VAR14 = VAR2->VAR14;\n}\nVAR19:\nFUN19(VAR10);\nreturn VAR12;\n}\n",
      "code_after_change_raw": "static inline int l2cap_config_req(struct l2cap_conn *conn,\nstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\nu8 *data)\n{\nstruct l2cap_conf_req *req = (struct l2cap_conf_req *) data;\nu16 dcid, flags;\nu8 rsp[64];\nstruct l2cap_chan *chan;\nint len, err = 0;\nif (cmd_len < sizeof(*req))\nreturn -EPROTO;\ndcid  = __le16_to_cpu(req->dcid);\nflags = __le16_to_cpu(req->flags);\nBT_DBG(\"dcid 0x%4.4x flags 0x%2.2x\", dcid, flags);\nchan = l2cap_get_chan_by_scid(conn, dcid);\nif (!chan) {\ncmd_reject_invalid_cid(conn, cmd->ident, dcid, 0);\nreturn 0;\n}\nif (chan->state != BT_CONFIG && chan->state != BT_CONNECT2) {\ncmd_reject_invalid_cid(conn, cmd->ident, chan->scid,\nchan->dcid);\ngoto unlock;\n}\nlen = cmd_len - sizeof(*req);\nif (chan->conf_len + len > sizeof(chan->conf_req)) {\nl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\nl2cap_build_conf_rsp(chan, rsp,\nL2CAP_CONF_REJECT, flags), rsp);\ngoto unlock;\n}\nmemcpy(chan->conf_req + chan->conf_len, req->data, len);\nchan->conf_len += len;\nif (flags & L2CAP_CONF_FLAG_CONTINUATION) {\nl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\nl2cap_build_conf_rsp(chan, rsp,\nL2CAP_CONF_SUCCESS, flags), rsp);\ngoto unlock;\n}\nlen = l2cap_parse_conf_req(chan, rsp, sizeof(rsp));\nif (len < 0) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto unlock;\n}\nchan->ident = cmd->ident;\nl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP, len, rsp);\nchan->num_conf_rsp++;\nchan->conf_len = 0;\nif (!test_bit(CONF_OUTPUT_DONE, &chan->conf_state))\ngoto unlock;\nif (test_bit(CONF_INPUT_DONE, &chan->conf_state)) {\nset_default_fcs(chan);\nif (chan->mode == L2CAP_MODE_ERTM ||\nchan->mode == L2CAP_MODE_STREAMING)\nerr = l2cap_ertm_init(chan);\nif (err < 0)\nl2cap_send_disconn_req(chan, -err);\nelse\nl2cap_chan_ready(chan);\ngoto unlock;\n}\nif (!test_and_set_bit(CONF_REQ_SENT, &chan->conf_state)) {\nu8 buf[64];\nl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, buf, sizeof(buf)), buf);\nchan->num_conf_req++;\n}\nif (test_bit(CONF_REM_CONF_PEND, &chan->conf_state) &&\ntest_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\nif (!chan->hs_hcon)\nl2cap_send_efs_conf_rsp(chan, rsp, cmd->ident, flags);\nelse\nchan->ident = cmd->ident;\n}\nunlock:\nl2cap_chan_unlock(chan);\nreturn err;\n}\n",
      "code_before_change_raw": "static inline int l2cap_config_req(struct l2cap_conn *conn,\nstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\nu8 *data)\n{\nstruct l2cap_conf_req *req = (struct l2cap_conf_req *) data;\nu16 dcid, flags;\nu8 rsp[64];\nstruct l2cap_chan *chan;\nint len, err = 0;\nif (cmd_len < sizeof(*req))\nreturn -EPROTO;\ndcid  = __le16_to_cpu(req->dcid);\nflags = __le16_to_cpu(req->flags);\nBT_DBG(\"dcid 0x%4.4x flags 0x%2.2x\", dcid, flags);\nchan = l2cap_get_chan_by_scid(conn, dcid);\nif (!chan) {\ncmd_reject_invalid_cid(conn, cmd->ident, dcid, 0);\nreturn 0;\n}\nif (chan->state != BT_CONFIG && chan->state != BT_CONNECT2) {\ncmd_reject_invalid_cid(conn, cmd->ident, chan->scid,\nchan->dcid);\ngoto unlock;\n}\nlen = cmd_len - sizeof(*req);\nif (chan->conf_len + len > sizeof(chan->conf_req)) {\nl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\nl2cap_build_conf_rsp(chan, rsp,\nL2CAP_CONF_REJECT, flags), rsp);\ngoto unlock;\n}\nmemcpy(chan->conf_req + chan->conf_len, req->data, len);\nchan->conf_len += len;\nif (flags & L2CAP_CONF_FLAG_CONTINUATION) {\nl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP,\nl2cap_build_conf_rsp(chan, rsp,\nL2CAP_CONF_SUCCESS, flags), rsp);\ngoto unlock;\n}\nlen = l2cap_parse_conf_req(chan, rsp);\nif (len < 0) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto unlock;\n}\nchan->ident = cmd->ident;\nl2cap_send_cmd(conn, cmd->ident, L2CAP_CONF_RSP, len, rsp);\nchan->num_conf_rsp++;\nchan->conf_len = 0;\nif (!test_bit(CONF_OUTPUT_DONE, &chan->conf_state))\ngoto unlock;\nif (test_bit(CONF_INPUT_DONE, &chan->conf_state)) {\nset_default_fcs(chan);\nif (chan->mode == L2CAP_MODE_ERTM ||\nchan->mode == L2CAP_MODE_STREAMING)\nerr = l2cap_ertm_init(chan);\nif (err < 0)\nl2cap_send_disconn_req(chan, -err);\nelse\nl2cap_chan_ready(chan);\ngoto unlock;\n}\nif (!test_and_set_bit(CONF_REQ_SENT, &chan->conf_state)) {\nu8 buf[64];\nl2cap_send_cmd(conn, l2cap_get_ident(conn), L2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, buf), buf);\nchan->num_conf_req++;\n}\nif (test_bit(CONF_REM_CONF_PEND, &chan->conf_state) &&\ntest_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\nif (!chan->hs_hcon)\nl2cap_send_efs_conf_rsp(chan, rsp, cmd->ident, flags);\nelse\nchan->ident = cmd->ident;\n}\nunlock:\nl2cap_chan_unlock(chan);\nreturn err;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper input validation and buffer size checking in the processing of L2CAP configuration responses.",
        "trigger_condition": "Receiving a specially crafted input with a data size larger than the allocated buffer size, leading to a stack overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the size of the input data being processed, potentially allowing an attacker to overflow the stack by providing data larger than the allocated buffer size."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper input validation and buffer size checking in the code. Specifically, the modification involves passing the correct buffer size to the function that processes the input data to prevent buffer overflow. By validating the input size against the buffer size, the code can prevent stack overflow vulnerabilities caused by processing oversized data.",
      "GPT_analysis": "The modification is necessary to address the stack overflow vulnerability (CVE-2017-1000251) present in the original code snippet. The vulnerability allows an attacker to trigger a stack overflow by providing a specially crafted input, potentially leading to remote code execution in kernel space.\n\nThe modification involves changing the `l2cap_parse_conf_rsp` function call to include the size of the buffer `buf` in order to prevent buffer overflow. By passing the correct buffer size to the function, the code ensures that the data being processed does not exceed the allocated buffer size, thus mitigating the risk of a stack overflow vulnerability.\n\nBy making this modification, the code is more secure and less susceptible to exploitation through malicious input data that could trigger a stack overflow. It helps in preventing potential security breaches and ensures the integrity and safety of the system.",
      "GPT_purpose": "Handle L2CAP configuration responses in a Bluetooth connection.",
      "GPT_function": "\n1. Process L2CAP configuration response.\n2. Handle different cases of L2CAP configuration results.\n3. Manage continuation flags and channel states.\n4. Initialize and finalize L2CAP channel configuration.\n5. Handle errors and disconnection requests.",
      "CVE_id": "CVE-2017-1000251",
      "code_before_change": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\n\tu16 scid, flags, result;\n\tstruct l2cap_chan *chan;\n\tint len = cmd_len - sizeof(*rsp);\n\tint err = 0;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tflags  = __le16_to_cpu(rsp->flags);\n\tresult = __le16_to_cpu(rsp->result);\n\n\tBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\n\t       result, len);\n\n\tchan = l2cap_get_chan_by_scid(conn, scid);\n\tif (!chan)\n\t\treturn 0;\n\n\tswitch (result) {\n\tcase L2CAP_CONF_SUCCESS:\n\t\tl2cap_conf_rfc_get(chan, rsp->data, len);\n\t\tclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tcase L2CAP_CONF_PENDING:\n\t\tset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\n\t\tif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t\tchar buf[64];\n\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   buf, &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!chan->hs_hcon) {\n\t\t\t\tl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n\t\t\t\t\t\t\t0);\n\t\t\t} else {\n\t\t\t\tif (l2cap_check_efs(chan)) {\n\t\t\t\t\tamp_create_logical_link(chan);\n\t\t\t\t\tchan->ident = cmd->ident;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto done;\n\n\tcase L2CAP_CONF_UNACCEPT:\n\t\tif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\n\t\t\tchar req[64];\n\n\t\t\tif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* throw out any old stored conf requests */\n\t\t\tresult = L2CAP_CONF_SUCCESS;\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   req, &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t       L2CAP_CONF_REQ, len, req);\n\t\t\tchan->num_conf_req++;\n\t\t\tif (result != L2CAP_CONF_SUCCESS)\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\n\tdefault:\n\t\tl2cap_chan_set_err(chan, ECONNRESET);\n\n\t\t__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto done;\n\t}\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION)\n\t\tgoto done;\n\n\tset_bit(CONF_INPUT_DONE, &chan->conf_state);\n\n\tif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t}\n\ndone:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
      "code_after_change": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\n\t\t\t\t   struct l2cap_cmd_hdr *cmd, u16 cmd_len,\n\t\t\t\t   u8 *data)\n{\n\tstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\n\tu16 scid, flags, result;\n\tstruct l2cap_chan *chan;\n\tint len = cmd_len - sizeof(*rsp);\n\tint err = 0;\n\n\tif (cmd_len < sizeof(*rsp))\n\t\treturn -EPROTO;\n\n\tscid   = __le16_to_cpu(rsp->scid);\n\tflags  = __le16_to_cpu(rsp->flags);\n\tresult = __le16_to_cpu(rsp->result);\n\n\tBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\n\t       result, len);\n\n\tchan = l2cap_get_chan_by_scid(conn, scid);\n\tif (!chan)\n\t\treturn 0;\n\n\tswitch (result) {\n\tcase L2CAP_CONF_SUCCESS:\n\t\tl2cap_conf_rfc_get(chan, rsp->data, len);\n\t\tclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\t\tbreak;\n\n\tcase L2CAP_CONF_PENDING:\n\t\tset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\n\n\t\tif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\n\t\t\tchar buf[64];\n\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   buf, sizeof(buf), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tif (!chan->hs_hcon) {\n\t\t\t\tl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n\t\t\t\t\t\t\t0);\n\t\t\t} else {\n\t\t\t\tif (l2cap_check_efs(chan)) {\n\t\t\t\t\tamp_create_logical_link(chan);\n\t\t\t\t\tchan->ident = cmd->ident;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tgoto done;\n\n\tcase L2CAP_CONF_UNACCEPT:\n\t\tif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\n\t\t\tchar req[64];\n\n\t\t\tif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* throw out any old stored conf requests */\n\t\t\tresult = L2CAP_CONF_SUCCESS;\n\t\t\tlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\n\t\t\t\t\t\t   req, sizeof(req), &result);\n\t\t\tif (len < 0) {\n\t\t\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t       L2CAP_CONF_REQ, len, req);\n\t\t\tchan->num_conf_req++;\n\t\t\tif (result != L2CAP_CONF_SUCCESS)\n\t\t\t\tgoto done;\n\t\t\tbreak;\n\t\t}\n\n\tdefault:\n\t\tl2cap_chan_set_err(chan, ECONNRESET);\n\n\t\t__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\n\t\tl2cap_send_disconn_req(chan, ECONNRESET);\n\t\tgoto done;\n\t}\n\n\tif (flags & L2CAP_CONF_FLAG_CONTINUATION)\n\t\tgoto done;\n\n\tset_bit(CONF_INPUT_DONE, &chan->conf_state);\n\n\tif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\n\t\tset_default_fcs(chan);\n\n\t\tif (chan->mode == L2CAP_MODE_ERTM ||\n\t\t    chan->mode == L2CAP_MODE_STREAMING)\n\t\t\terr = l2cap_ertm_init(chan);\n\n\t\tif (err < 0)\n\t\t\tl2cap_send_disconn_req(chan, -err);\n\t\telse\n\t\t\tl2cap_chan_ready(chan);\n\t}\n\ndone:\n\tl2cap_chan_unlock(chan);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t\t\t   buf, sizeof(buf), &result);",
          "\t\t\t\t\t\t   req, sizeof(req), &result);"
        ],
        "deleted": [
          "\t\t\t\t\t\t   buf, &result);",
          "\t\t\t\t\t\t   req, &result);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper input validation and buffer size checking in the processing of L2CAP configuration responses.",
      "trigger_condition": "Receiving a specially crafted input with a data size larger than the allocated buffer size, leading to a stack overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the size of the input data being processed, potentially allowing an attacker to overflow the stack by providing data larger than the allocated buffer size.",
      "id": 7,
      "code_after_change_normalized": "static inline int FUN1(struct l2cap_conn *VAR1,\nstruct l2cap_cmd_hdr *VAR2, u16 VAR3,\nu8 *VAR4)\n{\nstruct VAR6 *VAR5 = (struct VAR6 *)VAR4;\nu16 VAR7, VAR8, VAR9;\nstruct l2cap_chan *VAR10;\nint VAR11 = VAR3 - sizeof(*VAR5);\nint VAR12 = 0;\nif (VAR3 < sizeof(*VAR5))\nreturn -VAR13;\nVAR7   = FUN2(VAR5->VAR7);\nVAR8  = FUN2(VAR5->VAR8);\nVAR9 = FUN2(VAR5->VAR9);\nFUN3(\"STR\", VAR7, VAR8,\nVAR9, VAR11);\nVAR10 = FUN4(VAR1, VAR7);\nif (!VAR10)\nreturn 0;\nswitch (VAR9) {\ncase VAR14:\nFUN5(VAR10, VAR5->VAR4, VAR11);\nFUN6(VAR15, &VAR10->VAR16);\nbreak;\ncase VAR17:\nFUN7(VAR15, &VAR10->VAR16);\nif (FUN8(VAR18, &VAR10->VAR16)) {\nchar VAR19[64];\nVAR11 = FUN9(VAR10, VAR5->VAR4, VAR11,\nVAR19, sizeof(VAR19), &VAR9);\nif (VAR11 < 0) {\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nif (!VAR10->VAR22) {\nFUN11(VAR10, VAR19, VAR2->VAR23,\n0);\n} else {\nif (FUN12(VAR10)) {\nFUN13(VAR10);\nVAR10->VAR23 = VAR2->VAR23;\n}\n}\n}\ngoto VAR21;\ncase VAR24:\nif (VAR10->VAR25 <= VAR26) {\nchar VAR27[64];\nif (VAR11 > sizeof(VAR27) - sizeof(struct VAR28)) {\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nVAR9 = VAR14;\nVAR11 = FUN9(VAR10, VAR5->VAR4, VAR11,\nVAR27, sizeof(VAR27), &VAR9);\nif (VAR11 < 0) {\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nFUN14(VAR1, FUN15(VAR1),\nVAR29, VAR11, VAR27);\nVAR10->VAR30++;\nif (VAR9 != VAR14)\ngoto VAR21;\nbreak;\n}\ndefault:\nFUN16(VAR10, VAR20);\nFUN17(VAR10, VAR31);\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nif (VAR8 & VAR32)\ngoto VAR21;\nFUN7(VAR33, &VAR10->VAR16);\nif (FUN8(VAR34, &VAR10->VAR16)) {\nFUN18(VAR10);\nif (VAR10->VAR35 == VAR36 ||\nVAR10->VAR35 == VAR37)\nVAR12 = FUN19(VAR10);\nif (VAR12 < 0)\nFUN10(VAR10, -VAR12);\nelse\nFUN20(VAR10);\n}\nVAR21:\nFUN21(VAR10);\nreturn VAR12;\n}\n",
      "code_before_change_normalized": "static inline int FUN1(struct l2cap_conn *VAR1,\nstruct l2cap_cmd_hdr *VAR2, u16 VAR3,\nu8 *VAR4)\n{\nstruct VAR6 *VAR5 = (struct VAR6 *)VAR4;\nu16 VAR7, VAR8, VAR9;\nstruct l2cap_chan *VAR10;\nint VAR11 = VAR3 - sizeof(*VAR5);\nint VAR12 = 0;\nif (VAR3 < sizeof(*VAR5))\nreturn -VAR13;\nVAR7   = FUN2(VAR5->VAR7);\nVAR8  = FUN2(VAR5->VAR8);\nVAR9 = FUN2(VAR5->VAR9);\nFUN3(\"STR\", VAR7, VAR8,\nVAR9, VAR11);\nVAR10 = FUN4(VAR1, VAR7);\nif (!VAR10)\nreturn 0;\nswitch (VAR9) {\ncase VAR14:\nFUN5(VAR10, VAR5->VAR4, VAR11);\nFUN6(VAR15, &VAR10->VAR16);\nbreak;\ncase VAR17:\nFUN7(VAR15, &VAR10->VAR16);\nif (FUN8(VAR18, &VAR10->VAR16)) {\nchar VAR19[64];\nVAR11 = FUN9(VAR10, VAR5->VAR4, VAR11,\nVAR19, &VAR9);\nif (VAR11 < 0) {\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nif (!VAR10->VAR22) {\nFUN11(VAR10, VAR19, VAR2->VAR23,\n0);\n} else {\nif (FUN12(VAR10)) {\nFUN13(VAR10);\nVAR10->VAR23 = VAR2->VAR23;\n}\n}\n}\ngoto VAR21;\ncase VAR24:\nif (VAR10->VAR25 <= VAR26) {\nchar VAR27[64];\nif (VAR11 > sizeof(VAR27) - sizeof(struct VAR28)) {\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nVAR9 = VAR14;\nVAR11 = FUN9(VAR10, VAR5->VAR4, VAR11,\nVAR27, &VAR9);\nif (VAR11 < 0) {\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nFUN14(VAR1, FUN15(VAR1),\nVAR29, VAR11, VAR27);\nVAR10->VAR30++;\nif (VAR9 != VAR14)\ngoto VAR21;\nbreak;\n}\ndefault:\nFUN16(VAR10, VAR20);\nFUN17(VAR10, VAR31);\nFUN10(VAR10, VAR20);\ngoto VAR21;\n}\nif (VAR8 & VAR32)\ngoto VAR21;\nFUN7(VAR33, &VAR10->VAR16);\nif (FUN8(VAR34, &VAR10->VAR16)) {\nFUN18(VAR10);\nif (VAR10->VAR35 == VAR36 ||\nVAR10->VAR35 == VAR37)\nVAR12 = FUN19(VAR10);\nif (VAR12 < 0)\nFUN10(VAR10, -VAR12);\nelse\nFUN20(VAR10);\n}\nVAR21:\nFUN21(VAR10);\nreturn VAR12;\n}\n",
      "code_after_change_raw": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\nstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\nu8 *data)\n{\nstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\nu16 scid, flags, result;\nstruct l2cap_chan *chan;\nint len = cmd_len - sizeof(*rsp);\nint err = 0;\nif (cmd_len < sizeof(*rsp))\nreturn -EPROTO;\nscid   = __le16_to_cpu(rsp->scid);\nflags  = __le16_to_cpu(rsp->flags);\nresult = __le16_to_cpu(rsp->result);\nBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\nresult, len);\nchan = l2cap_get_chan_by_scid(conn, scid);\nif (!chan)\nreturn 0;\nswitch (result) {\ncase L2CAP_CONF_SUCCESS:\nl2cap_conf_rfc_get(chan, rsp->data, len);\nclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\nbreak;\ncase L2CAP_CONF_PENDING:\nset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\nif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\nchar buf[64];\nlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\nbuf, sizeof(buf), &result);\nif (len < 0) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nif (!chan->hs_hcon) {\nl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n0);\n} else {\nif (l2cap_check_efs(chan)) {\namp_create_logical_link(chan);\nchan->ident = cmd->ident;\n}\n}\n}\ngoto done;\ncase L2CAP_CONF_UNACCEPT:\nif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\nchar req[64];\nif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nresult = L2CAP_CONF_SUCCESS;\nlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\nreq, sizeof(req), &result);\nif (len < 0) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nl2cap_send_cmd(conn, l2cap_get_ident(conn),\nL2CAP_CONF_REQ, len, req);\nchan->num_conf_req++;\nif (result != L2CAP_CONF_SUCCESS)\ngoto done;\nbreak;\n}\ndefault:\nl2cap_chan_set_err(chan, ECONNRESET);\n__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nif (flags & L2CAP_CONF_FLAG_CONTINUATION)\ngoto done;\nset_bit(CONF_INPUT_DONE, &chan->conf_state);\nif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\nset_default_fcs(chan);\nif (chan->mode == L2CAP_MODE_ERTM ||\nchan->mode == L2CAP_MODE_STREAMING)\nerr = l2cap_ertm_init(chan);\nif (err < 0)\nl2cap_send_disconn_req(chan, -err);\nelse\nl2cap_chan_ready(chan);\n}\ndone:\nl2cap_chan_unlock(chan);\nreturn err;\n}\n",
      "code_before_change_raw": "static inline int l2cap_config_rsp(struct l2cap_conn *conn,\nstruct l2cap_cmd_hdr *cmd, u16 cmd_len,\nu8 *data)\n{\nstruct l2cap_conf_rsp *rsp = (struct l2cap_conf_rsp *)data;\nu16 scid, flags, result;\nstruct l2cap_chan *chan;\nint len = cmd_len - sizeof(*rsp);\nint err = 0;\nif (cmd_len < sizeof(*rsp))\nreturn -EPROTO;\nscid   = __le16_to_cpu(rsp->scid);\nflags  = __le16_to_cpu(rsp->flags);\nresult = __le16_to_cpu(rsp->result);\nBT_DBG(\"scid 0x%4.4x flags 0x%2.2x result 0x%2.2x len %d\", scid, flags,\nresult, len);\nchan = l2cap_get_chan_by_scid(conn, scid);\nif (!chan)\nreturn 0;\nswitch (result) {\ncase L2CAP_CONF_SUCCESS:\nl2cap_conf_rfc_get(chan, rsp->data, len);\nclear_bit(CONF_REM_CONF_PEND, &chan->conf_state);\nbreak;\ncase L2CAP_CONF_PENDING:\nset_bit(CONF_REM_CONF_PEND, &chan->conf_state);\nif (test_bit(CONF_LOC_CONF_PEND, &chan->conf_state)) {\nchar buf[64];\nlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\nbuf, &result);\nif (len < 0) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nif (!chan->hs_hcon) {\nl2cap_send_efs_conf_rsp(chan, buf, cmd->ident,\n0);\n} else {\nif (l2cap_check_efs(chan)) {\namp_create_logical_link(chan);\nchan->ident = cmd->ident;\n}\n}\n}\ngoto done;\ncase L2CAP_CONF_UNACCEPT:\nif (chan->num_conf_rsp <= L2CAP_CONF_MAX_CONF_RSP) {\nchar req[64];\nif (len > sizeof(req) - sizeof(struct l2cap_conf_req)) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nresult = L2CAP_CONF_SUCCESS;\nlen = l2cap_parse_conf_rsp(chan, rsp->data, len,\nreq, &result);\nif (len < 0) {\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nl2cap_send_cmd(conn, l2cap_get_ident(conn),\nL2CAP_CONF_REQ, len, req);\nchan->num_conf_req++;\nif (result != L2CAP_CONF_SUCCESS)\ngoto done;\nbreak;\n}\ndefault:\nl2cap_chan_set_err(chan, ECONNRESET);\n__set_chan_timer(chan, L2CAP_DISC_REJ_TIMEOUT);\nl2cap_send_disconn_req(chan, ECONNRESET);\ngoto done;\n}\nif (flags & L2CAP_CONF_FLAG_CONTINUATION)\ngoto done;\nset_bit(CONF_INPUT_DONE, &chan->conf_state);\nif (test_bit(CONF_OUTPUT_DONE, &chan->conf_state)) {\nset_default_fcs(chan);\nif (chan->mode == L2CAP_MODE_ERTM ||\nchan->mode == L2CAP_MODE_STREAMING)\nerr = l2cap_ertm_init(chan);\nif (err < 0)\nl2cap_send_disconn_req(chan, -err);\nelse\nl2cap_chan_ready(chan);\n}\ndone:\nl2cap_chan_unlock(chan);\nreturn err;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper buffer size validation in the processing of L2CAP configuration responses.",
        "trigger_condition": "Receiving a maliciously crafted L2CAP configuration response that exceeds the buffer size allocated for processing.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the buffer size when constructing the L2CAP configuration request, allowing an attacker to send a response larger than the buffer size and trigger a stack overflow vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper buffer size validation when constructing the L2CAP configuration request. By adding the correct buffer size parameter to the function call, the code can prevent buffer overflows and potential stack overflow vulnerabilities. In this case, the solution involves modifying the code to include the buffer size parameter in the function call to enforce proper buffer handling.",
      "GPT_analysis": "The modification is necessary to prevent a stack overflow vulnerability in the processing of L2CAP configuration responses, which could lead to remote code execution in kernel space. By adding the parameter `sizeof(buf)` to the `l2cap_build_conf_req` function call, the code ensures that the buffer `buf` is not overflowed, thus mitigating the vulnerability. This change helps to prevent potential exploitation of the vulnerability and enhances the security of the code.",
      "GPT_purpose": "Handling the creation of L2CAP channels for Bluetooth communication, including sending requests and responses based on the channel state and result.",
      "GPT_function": "\n1. Handle the creation of an L2CAP channel.\n2. Check if the channel is outgoing or incoming.\n3. Send appropriate responses based on the result of the channel creation.\n4. Update channel state and send configuration requests if the channel creation is successful.",
      "CVE_id": "CVE-2017-1000251",
      "code_before_change": "static void l2cap_do_create(struct l2cap_chan *chan, int result,\n\t\t\t    u8 local_amp_id, u8 remote_amp_id)\n{\n\tBT_DBG(\"chan %p state %s %u -> %u\", chan, state_to_string(chan->state),\n\t       local_amp_id, remote_amp_id);\n\n\tchan->fcs = L2CAP_FCS_NONE;\n\n\t/* Outgoing channel on AMP */\n\tif (chan->state == BT_CONNECT) {\n\t\tif (result == L2CAP_CR_SUCCESS) {\n\t\t\tchan->local_amp_id = local_amp_id;\n\t\t\tl2cap_send_create_chan_req(chan, remote_amp_id);\n\t\t} else {\n\t\t\t/* Revert to BR/EDR connect */\n\t\t\tl2cap_send_conn_req(chan);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/* Incoming channel on AMP */\n\tif (__l2cap_no_conn_pending(chan)) {\n\t\tstruct l2cap_conn_rsp rsp;\n\t\tchar buf[128];\n\t\trsp.scid = cpu_to_le16(chan->dcid);\n\t\trsp.dcid = cpu_to_le16(chan->scid);\n\n\t\tif (result == L2CAP_CR_SUCCESS) {\n\t\t\t/* Send successful response */\n\t\t\trsp.result = cpu_to_le16(L2CAP_CR_SUCCESS);\n\t\t\trsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n\t\t} else {\n\t\t\t/* Send negative response */\n\t\t\trsp.result = cpu_to_le16(L2CAP_CR_NO_MEM);\n\t\t\trsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n\t\t}\n\n\t\tl2cap_send_cmd(chan->conn, chan->ident, L2CAP_CREATE_CHAN_RSP,\n\t\t\t       sizeof(rsp), &rsp);\n\n\t\tif (result == L2CAP_CR_SUCCESS) {\n\t\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\t\tset_bit(CONF_REQ_SENT, &chan->conf_state);\n\t\t\tl2cap_send_cmd(chan->conn, l2cap_get_ident(chan->conn),\n\t\t\t\t       L2CAP_CONF_REQ,\n\t\t\t\t       l2cap_build_conf_req(chan, buf), buf);\n\t\t\tchan->num_conf_req++;\n\t\t}\n\t}\n}",
      "code_after_change": "static void l2cap_do_create(struct l2cap_chan *chan, int result,\n\t\t\t    u8 local_amp_id, u8 remote_amp_id)\n{\n\tBT_DBG(\"chan %p state %s %u -> %u\", chan, state_to_string(chan->state),\n\t       local_amp_id, remote_amp_id);\n\n\tchan->fcs = L2CAP_FCS_NONE;\n\n\t/* Outgoing channel on AMP */\n\tif (chan->state == BT_CONNECT) {\n\t\tif (result == L2CAP_CR_SUCCESS) {\n\t\t\tchan->local_amp_id = local_amp_id;\n\t\t\tl2cap_send_create_chan_req(chan, remote_amp_id);\n\t\t} else {\n\t\t\t/* Revert to BR/EDR connect */\n\t\t\tl2cap_send_conn_req(chan);\n\t\t}\n\n\t\treturn;\n\t}\n\n\t/* Incoming channel on AMP */\n\tif (__l2cap_no_conn_pending(chan)) {\n\t\tstruct l2cap_conn_rsp rsp;\n\t\tchar buf[128];\n\t\trsp.scid = cpu_to_le16(chan->dcid);\n\t\trsp.dcid = cpu_to_le16(chan->scid);\n\n\t\tif (result == L2CAP_CR_SUCCESS) {\n\t\t\t/* Send successful response */\n\t\t\trsp.result = cpu_to_le16(L2CAP_CR_SUCCESS);\n\t\t\trsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n\t\t} else {\n\t\t\t/* Send negative response */\n\t\t\trsp.result = cpu_to_le16(L2CAP_CR_NO_MEM);\n\t\t\trsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n\t\t}\n\n\t\tl2cap_send_cmd(chan->conn, chan->ident, L2CAP_CREATE_CHAN_RSP,\n\t\t\t       sizeof(rsp), &rsp);\n\n\t\tif (result == L2CAP_CR_SUCCESS) {\n\t\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\t\tset_bit(CONF_REQ_SENT, &chan->conf_state);\n\t\t\tl2cap_send_cmd(chan->conn, l2cap_get_ident(chan->conn),\n\t\t\t\t       L2CAP_CONF_REQ,\n\t\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)), buf);\n\t\t\tchan->num_conf_req++;\n\t\t}\n\t}\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)), buf);"
        ],
        "deleted": [
          "\t\t\t\t       l2cap_build_conf_req(chan, buf), buf);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper buffer size validation in the processing of L2CAP configuration responses.",
      "trigger_condition": "Receiving a maliciously crafted L2CAP configuration response that exceeds the buffer size allocated for processing.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the buffer size when constructing the L2CAP configuration request, allowing an attacker to send a response larger than the buffer size and trigger a stack overflow vulnerability.",
      "id": 8,
      "code_after_change_normalized": "static void FUN1(struct l2cap_chan *VAR1, int VAR2,\nu8 VAR3, u8 VAR4)\n{\nFUN2(\"STR\", VAR1, FUN3(VAR1->VAR5),\nVAR3, VAR4);\nVAR1->VAR6 = VAR7;\nif (VAR1->VAR5 == VAR8) {\nif (VAR2 == VAR9) {\nVAR1->VAR3 = VAR3;\nFUN4(VAR1, VAR4);\n} else {\nFUN5(VAR1);\n}\nreturn;\n}\nif (FUN6(VAR1)) {\nstruct l2cap_conn_rsp VAR10;\nchar VAR11[128];\nVAR10.VAR12 = FUN7(VAR1->VAR13);\nVAR10.VAR13 = FUN7(VAR1->VAR12);\nif (VAR2 == VAR9) {\nVAR10.VAR2 = FUN7(VAR9);\nVAR10.VAR14 = FUN7(VAR15);\n} else {\nVAR10.VAR2 = FUN7(VAR16);\nVAR10.VAR14 = FUN7(VAR15);\n}\nFUN8(VAR1->VAR17, VAR1->VAR18, VAR19,\nsizeof(VAR10), &VAR10);\nif (VAR2 == VAR9) {\nFUN9(VAR1, VAR20);\nFUN10(VAR21, &VAR1->VAR22);\nFUN8(VAR1->VAR17, FUN11(VAR1->VAR17),\nVAR23,\nFUN12(VAR1, VAR11, sizeof(VAR11)), VAR11);\nVAR1->VAR24++;\n}\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct l2cap_chan *VAR1, int VAR2,\nu8 VAR3, u8 VAR4)\n{\nFUN2(\"STR\", VAR1, FUN3(VAR1->VAR5),\nVAR3, VAR4);\nVAR1->VAR6 = VAR7;\nif (VAR1->VAR5 == VAR8) {\nif (VAR2 == VAR9) {\nVAR1->VAR3 = VAR3;\nFUN4(VAR1, VAR4);\n} else {\nFUN5(VAR1);\n}\nreturn;\n}\nif (FUN6(VAR1)) {\nstruct l2cap_conn_rsp VAR10;\nchar VAR11[128];\nVAR10.VAR12 = FUN7(VAR1->VAR13);\nVAR10.VAR13 = FUN7(VAR1->VAR12);\nif (VAR2 == VAR9) {\nVAR10.VAR2 = FUN7(VAR9);\nVAR10.VAR14 = FUN7(VAR15);\n} else {\nVAR10.VAR2 = FUN7(VAR16);\nVAR10.VAR14 = FUN7(VAR15);\n}\nFUN8(VAR1->VAR17, VAR1->VAR18, VAR19,\nsizeof(VAR10), &VAR10);\nif (VAR2 == VAR9) {\nFUN9(VAR1, VAR20);\nFUN10(VAR21, &VAR1->VAR22);\nFUN8(VAR1->VAR17, FUN11(VAR1->VAR17),\nVAR23,\nFUN12(VAR1, VAR11), VAR11);\nVAR1->VAR24++;\n}\n}\n}\n",
      "code_after_change_raw": "static void l2cap_do_create(struct l2cap_chan *chan, int result,\nu8 local_amp_id, u8 remote_amp_id)\n{\nBT_DBG(\"chan %p state %s %u -> %u\", chan, state_to_string(chan->state),\nlocal_amp_id, remote_amp_id);\nchan->fcs = L2CAP_FCS_NONE;\nif (chan->state == BT_CONNECT) {\nif (result == L2CAP_CR_SUCCESS) {\nchan->local_amp_id = local_amp_id;\nl2cap_send_create_chan_req(chan, remote_amp_id);\n} else {\nl2cap_send_conn_req(chan);\n}\nreturn;\n}\nif (__l2cap_no_conn_pending(chan)) {\nstruct l2cap_conn_rsp rsp;\nchar buf[128];\nrsp.scid = cpu_to_le16(chan->dcid);\nrsp.dcid = cpu_to_le16(chan->scid);\nif (result == L2CAP_CR_SUCCESS) {\nrsp.result = cpu_to_le16(L2CAP_CR_SUCCESS);\nrsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n} else {\nrsp.result = cpu_to_le16(L2CAP_CR_NO_MEM);\nrsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n}\nl2cap_send_cmd(chan->conn, chan->ident, L2CAP_CREATE_CHAN_RSP,\nsizeof(rsp), &rsp);\nif (result == L2CAP_CR_SUCCESS) {\nl2cap_state_change(chan, BT_CONFIG);\nset_bit(CONF_REQ_SENT, &chan->conf_state);\nl2cap_send_cmd(chan->conn, l2cap_get_ident(chan->conn),\nL2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, buf, sizeof(buf)), buf);\nchan->num_conf_req++;\n}\n}\n}\n",
      "code_before_change_raw": "static void l2cap_do_create(struct l2cap_chan *chan, int result,\nu8 local_amp_id, u8 remote_amp_id)\n{\nBT_DBG(\"chan %p state %s %u -> %u\", chan, state_to_string(chan->state),\nlocal_amp_id, remote_amp_id);\nchan->fcs = L2CAP_FCS_NONE;\nif (chan->state == BT_CONNECT) {\nif (result == L2CAP_CR_SUCCESS) {\nchan->local_amp_id = local_amp_id;\nl2cap_send_create_chan_req(chan, remote_amp_id);\n} else {\nl2cap_send_conn_req(chan);\n}\nreturn;\n}\nif (__l2cap_no_conn_pending(chan)) {\nstruct l2cap_conn_rsp rsp;\nchar buf[128];\nrsp.scid = cpu_to_le16(chan->dcid);\nrsp.dcid = cpu_to_le16(chan->scid);\nif (result == L2CAP_CR_SUCCESS) {\nrsp.result = cpu_to_le16(L2CAP_CR_SUCCESS);\nrsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n} else {\nrsp.result = cpu_to_le16(L2CAP_CR_NO_MEM);\nrsp.status = cpu_to_le16(L2CAP_CS_NO_INFO);\n}\nl2cap_send_cmd(chan->conn, chan->ident, L2CAP_CREATE_CHAN_RSP,\nsizeof(rsp), &rsp);\nif (result == L2CAP_CR_SUCCESS) {\nl2cap_state_change(chan, BT_CONFIG);\nset_bit(CONF_REQ_SENT, &chan->conf_state);\nl2cap_send_cmd(chan->conn, l2cap_get_ident(chan->conn),\nL2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, buf), buf);\nchan->num_conf_req++;\n}\n}\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper buffer size handling in the processing of L2CAP configuration responses.",
        "trigger_condition": "Receiving L2CAP configuration responses with a buffer size that exceeds the allocated buffer size, leading to a stack overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not pass the correct buffer size to the l2cap_build_conf_req function, potentially allowing the function to write beyond the bounds of the buffer, resulting in a stack overflow vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper buffer size handling in the processing of L2CAP configuration responses. This involves passing the correct buffer size to the l2cap_build_conf_req function to prevent buffer overflows. In this case, the solution includes adding the parameter sizeof(buf) to the l2cap_build_conf_req function call to specify the buffer size and prevent potential stack overflow vulnerabilities.",
      "GPT_analysis": "The modification is necessary to prevent a stack overflow vulnerability in the processing of L2CAP configuration responses, which could potentially lead to remote code execution in kernel space. By adding the parameter `sizeof(buf)` to the `l2cap_build_conf_req` function call, you ensure that the function does not write beyond the bounds of the `buf` array, thus preventing a possible buffer overflow. This modification helps to mitigate the vulnerability identified as CVE-2017-1000251 by ensuring proper buffer size handling and preventing potential exploitation of the vulnerability.",
      "GPT_purpose": "Handle security confirmation for L2CAP connections in the Bluetooth stack.",
      "GPT_function": "\n1. Handle L2CAP security confirmation for a Bluetooth connection.\n2. Update security level for the L2CAP channel if encryption is enabled.\n3. Manage connection states and handle various scenarios based on the connection state.",
      "CVE_id": "CVE-2017-1000251",
      "code_before_change": "static void l2cap_security_cfm(struct hci_conn *hcon, u8 status, u8 encrypt)\n{\n\tstruct l2cap_conn *conn = hcon->l2cap_data;\n\tstruct l2cap_chan *chan;\n\n\tif (!conn)\n\t\treturn;\n\n\tBT_DBG(\"conn %p status 0x%2.2x encrypt %u\", conn, status, encrypt);\n\n\tmutex_lock(&conn->chan_lock);\n\n\tlist_for_each_entry(chan, &conn->chan_l, list) {\n\t\tl2cap_chan_lock(chan);\n\n\t\tBT_DBG(\"chan %p scid 0x%4.4x state %s\", chan, chan->scid,\n\t\t       state_to_string(chan->state));\n\n\t\tif (chan->scid == L2CAP_CID_A2MP) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!status && encrypt)\n\t\t\tchan->sec_level = hcon->sec_level;\n\n\t\tif (!__l2cap_no_conn_pending(chan)) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!status && (chan->state == BT_CONNECTED ||\n\t\t\t\tchan->state == BT_CONFIG)) {\n\t\t\tchan->ops->resume(chan);\n\t\t\tl2cap_check_encryption(chan, encrypt);\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (chan->state == BT_CONNECT) {\n\t\t\tif (!status)\n\t\t\t\tl2cap_start_connection(chan);\n\t\t\telse\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t} else if (chan->state == BT_CONNECT2 &&\n\t\t\t   chan->mode != L2CAP_MODE_LE_FLOWCTL) {\n\t\t\tstruct l2cap_conn_rsp rsp;\n\t\t\t__u16 res, stat;\n\n\t\t\tif (!status) {\n\t\t\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\t\t\tres = L2CAP_CR_PEND;\n\t\t\t\t\tstat = L2CAP_CS_AUTHOR_PEND;\n\t\t\t\t\tchan->ops->defer(chan);\n\t\t\t\t} else {\n\t\t\t\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\t\t\t\tres = L2CAP_CR_SUCCESS;\n\t\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tl2cap_state_change(chan, BT_DISCONN);\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t\t\tres = L2CAP_CR_SEC_BLOCK;\n\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t}\n\n\t\t\trsp.scid   = cpu_to_le16(chan->dcid);\n\t\t\trsp.dcid   = cpu_to_le16(chan->scid);\n\t\t\trsp.result = cpu_to_le16(res);\n\t\t\trsp.status = cpu_to_le16(stat);\n\t\t\tl2cap_send_cmd(conn, chan->ident, L2CAP_CONN_RSP,\n\t\t\t\t       sizeof(rsp), &rsp);\n\n\t\t\tif (!test_bit(CONF_REQ_SENT, &chan->conf_state) &&\n\t\t\t    res == L2CAP_CR_SUCCESS) {\n\t\t\t\tchar buf[128];\n\t\t\t\tset_bit(CONF_REQ_SENT, &chan->conf_state);\n\t\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t\t       L2CAP_CONF_REQ,\n\t\t\t\t\t       l2cap_build_conf_req(chan, buf),\n\t\t\t\t\t       buf);\n\t\t\t\tchan->num_conf_req++;\n\t\t\t}\n\t\t}\n\n\t\tl2cap_chan_unlock(chan);\n\t}\n\n\tmutex_unlock(&conn->chan_lock);\n}",
      "code_after_change": "static void l2cap_security_cfm(struct hci_conn *hcon, u8 status, u8 encrypt)\n{\n\tstruct l2cap_conn *conn = hcon->l2cap_data;\n\tstruct l2cap_chan *chan;\n\n\tif (!conn)\n\t\treturn;\n\n\tBT_DBG(\"conn %p status 0x%2.2x encrypt %u\", conn, status, encrypt);\n\n\tmutex_lock(&conn->chan_lock);\n\n\tlist_for_each_entry(chan, &conn->chan_l, list) {\n\t\tl2cap_chan_lock(chan);\n\n\t\tBT_DBG(\"chan %p scid 0x%4.4x state %s\", chan, chan->scid,\n\t\t       state_to_string(chan->state));\n\n\t\tif (chan->scid == L2CAP_CID_A2MP) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!status && encrypt)\n\t\t\tchan->sec_level = hcon->sec_level;\n\n\t\tif (!__l2cap_no_conn_pending(chan)) {\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!status && (chan->state == BT_CONNECTED ||\n\t\t\t\tchan->state == BT_CONFIG)) {\n\t\t\tchan->ops->resume(chan);\n\t\t\tl2cap_check_encryption(chan, encrypt);\n\t\t\tl2cap_chan_unlock(chan);\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (chan->state == BT_CONNECT) {\n\t\t\tif (!status)\n\t\t\t\tl2cap_start_connection(chan);\n\t\t\telse\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t} else if (chan->state == BT_CONNECT2 &&\n\t\t\t   chan->mode != L2CAP_MODE_LE_FLOWCTL) {\n\t\t\tstruct l2cap_conn_rsp rsp;\n\t\t\t__u16 res, stat;\n\n\t\t\tif (!status) {\n\t\t\t\tif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\n\t\t\t\t\tres = L2CAP_CR_PEND;\n\t\t\t\t\tstat = L2CAP_CS_AUTHOR_PEND;\n\t\t\t\t\tchan->ops->defer(chan);\n\t\t\t\t} else {\n\t\t\t\t\tl2cap_state_change(chan, BT_CONFIG);\n\t\t\t\t\tres = L2CAP_CR_SUCCESS;\n\t\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tl2cap_state_change(chan, BT_DISCONN);\n\t\t\t\t__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n\t\t\t\tres = L2CAP_CR_SEC_BLOCK;\n\t\t\t\tstat = L2CAP_CS_NO_INFO;\n\t\t\t}\n\n\t\t\trsp.scid   = cpu_to_le16(chan->dcid);\n\t\t\trsp.dcid   = cpu_to_le16(chan->scid);\n\t\t\trsp.result = cpu_to_le16(res);\n\t\t\trsp.status = cpu_to_le16(stat);\n\t\t\tl2cap_send_cmd(conn, chan->ident, L2CAP_CONN_RSP,\n\t\t\t\t       sizeof(rsp), &rsp);\n\n\t\t\tif (!test_bit(CONF_REQ_SENT, &chan->conf_state) &&\n\t\t\t    res == L2CAP_CR_SUCCESS) {\n\t\t\t\tchar buf[128];\n\t\t\t\tset_bit(CONF_REQ_SENT, &chan->conf_state);\n\t\t\t\tl2cap_send_cmd(conn, l2cap_get_ident(conn),\n\t\t\t\t\t       L2CAP_CONF_REQ,\n\t\t\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)),\n\t\t\t\t\t       buf);\n\t\t\t\tchan->num_conf_req++;\n\t\t\t}\n\t\t}\n\n\t\tl2cap_chan_unlock(chan);\n\t}\n\n\tmutex_unlock(&conn->chan_lock);\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t\t       l2cap_build_conf_req(chan, buf, sizeof(buf)),"
        ],
        "deleted": [
          "\t\t\t\t\t       l2cap_build_conf_req(chan, buf),"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper buffer size handling in the processing of L2CAP configuration responses.",
      "trigger_condition": "Receiving L2CAP configuration responses with a buffer size that exceeds the allocated buffer size, leading to a stack overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not pass the correct buffer size to the l2cap_build_conf_req function, potentially allowing the function to write beyond the bounds of the buffer, resulting in a stack overflow vulnerability.",
      "id": 9,
      "code_after_change_normalized": "static void FUN1(struct hci_conn *VAR1, u8 VAR2, u8 VAR3)\n{\nstruct l2cap_conn *VAR4 = VAR1->VAR5;\nstruct l2cap_chan *VAR6;\nif (!VAR4)\nreturn;\nFUN2(\"STR\", VAR4, VAR2, VAR3);\nFUN3(&VAR4->VAR7);\nFUN4(VAR6, &VAR4->VAR8, VAR9) {\nFUN5(VAR6);\nFUN2(\"STR\", VAR6, VAR6->VAR10,\nFUN6(VAR6->VAR11));\nif (VAR6->VAR10 == VAR12) {\nFUN7(VAR6);\ncontinue;\n}\nif (!VAR2 && VAR3)\nVAR6->VAR13 = VAR1->VAR13;\nif (!FUN8(VAR6)) {\nFUN7(VAR6);\ncontinue;\n}\nif (!VAR2 && (VAR6->VAR11 == VAR14 ||\nVAR6->VAR11 == VAR15)) {\nVAR6->VAR16->FUN9(VAR6);\nFUN10(VAR6, VAR3);\nFUN7(VAR6);\ncontinue;\n}\nif (VAR6->VAR11 == VAR17) {\nif (!VAR2)\nFUN11(VAR6);\nelse\nFUN12(VAR6, VAR18);\n} else if (VAR6->VAR11 == VAR19 &&\nVAR6->VAR20 != VAR21) {\nstruct l2cap_conn_rsp VAR22;\n__u16 VAR23, VAR24;\nif (!VAR2) {\nif (FUN13(VAR25, &VAR6->VAR26)) {\nVAR23 = VAR27;\nVAR24 = VAR28;\nVAR6->VAR16->FUN14(VAR6);\n} else {\nFUN15(VAR6, VAR15);\nVAR23 = VAR29;\nVAR24 = VAR30;\n}\n} else {\nFUN15(VAR6, VAR31);\nFUN12(VAR6, VAR18);\nVAR23 = VAR32;\nVAR24 = VAR30;\n}\nVAR22.VAR10   = FUN16(VAR6->VAR33);\nVAR22.VAR33   = FUN16(VAR6->VAR10);\nVAR22.VAR34 = FUN16(VAR23);\nVAR22.VAR2 = FUN16(VAR24);\nFUN17(VAR4, VAR6->VAR35, VAR36,\nsizeof(VAR22), &VAR22);\nif (!FUN13(VAR37, &VAR6->VAR38) &&\nVAR23 == VAR29) {\nchar VAR39[128];\nFUN18(VAR37, &VAR6->VAR38);\nFUN17(VAR4, FUN19(VAR4),\nVAR40,\nFUN20(VAR6, VAR39, sizeof(VAR39)),\nVAR39);\nVAR6->VAR41++;\n}\n}\nFUN7(VAR6);\n}\nFUN21(&VAR4->VAR7);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct hci_conn *VAR1, u8 VAR2, u8 VAR3)\n{\nstruct l2cap_conn *VAR4 = VAR1->VAR5;\nstruct l2cap_chan *VAR6;\nif (!VAR4)\nreturn;\nFUN2(\"STR\", VAR4, VAR2, VAR3);\nFUN3(&VAR4->VAR7);\nFUN4(VAR6, &VAR4->VAR8, VAR9) {\nFUN5(VAR6);\nFUN2(\"STR\", VAR6, VAR6->VAR10,\nFUN6(VAR6->VAR11));\nif (VAR6->VAR10 == VAR12) {\nFUN7(VAR6);\ncontinue;\n}\nif (!VAR2 && VAR3)\nVAR6->VAR13 = VAR1->VAR13;\nif (!FUN8(VAR6)) {\nFUN7(VAR6);\ncontinue;\n}\nif (!VAR2 && (VAR6->VAR11 == VAR14 ||\nVAR6->VAR11 == VAR15)) {\nVAR6->VAR16->FUN9(VAR6);\nFUN10(VAR6, VAR3);\nFUN7(VAR6);\ncontinue;\n}\nif (VAR6->VAR11 == VAR17) {\nif (!VAR2)\nFUN11(VAR6);\nelse\nFUN12(VAR6, VAR18);\n} else if (VAR6->VAR11 == VAR19 &&\nVAR6->VAR20 != VAR21) {\nstruct l2cap_conn_rsp VAR22;\n__u16 VAR23, VAR24;\nif (!VAR2) {\nif (FUN13(VAR25, &VAR6->VAR26)) {\nVAR23 = VAR27;\nVAR24 = VAR28;\nVAR6->VAR16->FUN14(VAR6);\n} else {\nFUN15(VAR6, VAR15);\nVAR23 = VAR29;\nVAR24 = VAR30;\n}\n} else {\nFUN15(VAR6, VAR31);\nFUN12(VAR6, VAR18);\nVAR23 = VAR32;\nVAR24 = VAR30;\n}\nVAR22.VAR10   = FUN16(VAR6->VAR33);\nVAR22.VAR33   = FUN16(VAR6->VAR10);\nVAR22.VAR34 = FUN16(VAR23);\nVAR22.VAR2 = FUN16(VAR24);\nFUN17(VAR4, VAR6->VAR35, VAR36,\nsizeof(VAR22), &VAR22);\nif (!FUN13(VAR37, &VAR6->VAR38) &&\nVAR23 == VAR29) {\nchar VAR39[128];\nFUN18(VAR37, &VAR6->VAR38);\nFUN17(VAR4, FUN19(VAR4),\nVAR40,\nFUN20(VAR6, VAR39),\nVAR39);\nVAR6->VAR41++;\n}\n}\nFUN7(VAR6);\n}\nFUN21(&VAR4->VAR7);\n}\n",
      "code_after_change_raw": "static void l2cap_security_cfm(struct hci_conn *hcon, u8 status, u8 encrypt)\n{\nstruct l2cap_conn *conn = hcon->l2cap_data;\nstruct l2cap_chan *chan;\nif (!conn)\nreturn;\nBT_DBG(\"conn %p status 0x%2.2x encrypt %u\", conn, status, encrypt);\nmutex_lock(&conn->chan_lock);\nlist_for_each_entry(chan, &conn->chan_l, list) {\nl2cap_chan_lock(chan);\nBT_DBG(\"chan %p scid 0x%4.4x state %s\", chan, chan->scid,\nstate_to_string(chan->state));\nif (chan->scid == L2CAP_CID_A2MP) {\nl2cap_chan_unlock(chan);\ncontinue;\n}\nif (!status && encrypt)\nchan->sec_level = hcon->sec_level;\nif (!__l2cap_no_conn_pending(chan)) {\nl2cap_chan_unlock(chan);\ncontinue;\n}\nif (!status && (chan->state == BT_CONNECTED ||\nchan->state == BT_CONFIG)) {\nchan->ops->resume(chan);\nl2cap_check_encryption(chan, encrypt);\nl2cap_chan_unlock(chan);\ncontinue;\n}\nif (chan->state == BT_CONNECT) {\nif (!status)\nl2cap_start_connection(chan);\nelse\n__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n} else if (chan->state == BT_CONNECT2 &&\nchan->mode != L2CAP_MODE_LE_FLOWCTL) {\nstruct l2cap_conn_rsp rsp;\n__u16 res, stat;\nif (!status) {\nif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\nres = L2CAP_CR_PEND;\nstat = L2CAP_CS_AUTHOR_PEND;\nchan->ops->defer(chan);\n} else {\nl2cap_state_change(chan, BT_CONFIG);\nres = L2CAP_CR_SUCCESS;\nstat = L2CAP_CS_NO_INFO;\n}\n} else {\nl2cap_state_change(chan, BT_DISCONN);\n__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\nres = L2CAP_CR_SEC_BLOCK;\nstat = L2CAP_CS_NO_INFO;\n}\nrsp.scid   = cpu_to_le16(chan->dcid);\nrsp.dcid   = cpu_to_le16(chan->scid);\nrsp.result = cpu_to_le16(res);\nrsp.status = cpu_to_le16(stat);\nl2cap_send_cmd(conn, chan->ident, L2CAP_CONN_RSP,\nsizeof(rsp), &rsp);\nif (!test_bit(CONF_REQ_SENT, &chan->conf_state) &&\nres == L2CAP_CR_SUCCESS) {\nchar buf[128];\nset_bit(CONF_REQ_SENT, &chan->conf_state);\nl2cap_send_cmd(conn, l2cap_get_ident(conn),\nL2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, buf, sizeof(buf)),\nbuf);\nchan->num_conf_req++;\n}\n}\nl2cap_chan_unlock(chan);\n}\nmutex_unlock(&conn->chan_lock);\n}\n",
      "code_before_change_raw": "static void l2cap_security_cfm(struct hci_conn *hcon, u8 status, u8 encrypt)\n{\nstruct l2cap_conn *conn = hcon->l2cap_data;\nstruct l2cap_chan *chan;\nif (!conn)\nreturn;\nBT_DBG(\"conn %p status 0x%2.2x encrypt %u\", conn, status, encrypt);\nmutex_lock(&conn->chan_lock);\nlist_for_each_entry(chan, &conn->chan_l, list) {\nl2cap_chan_lock(chan);\nBT_DBG(\"chan %p scid 0x%4.4x state %s\", chan, chan->scid,\nstate_to_string(chan->state));\nif (chan->scid == L2CAP_CID_A2MP) {\nl2cap_chan_unlock(chan);\ncontinue;\n}\nif (!status && encrypt)\nchan->sec_level = hcon->sec_level;\nif (!__l2cap_no_conn_pending(chan)) {\nl2cap_chan_unlock(chan);\ncontinue;\n}\nif (!status && (chan->state == BT_CONNECTED ||\nchan->state == BT_CONFIG)) {\nchan->ops->resume(chan);\nl2cap_check_encryption(chan, encrypt);\nl2cap_chan_unlock(chan);\ncontinue;\n}\nif (chan->state == BT_CONNECT) {\nif (!status)\nl2cap_start_connection(chan);\nelse\n__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\n} else if (chan->state == BT_CONNECT2 &&\nchan->mode != L2CAP_MODE_LE_FLOWCTL) {\nstruct l2cap_conn_rsp rsp;\n__u16 res, stat;\nif (!status) {\nif (test_bit(FLAG_DEFER_SETUP, &chan->flags)) {\nres = L2CAP_CR_PEND;\nstat = L2CAP_CS_AUTHOR_PEND;\nchan->ops->defer(chan);\n} else {\nl2cap_state_change(chan, BT_CONFIG);\nres = L2CAP_CR_SUCCESS;\nstat = L2CAP_CS_NO_INFO;\n}\n} else {\nl2cap_state_change(chan, BT_DISCONN);\n__set_chan_timer(chan, L2CAP_DISC_TIMEOUT);\nres = L2CAP_CR_SEC_BLOCK;\nstat = L2CAP_CS_NO_INFO;\n}\nrsp.scid   = cpu_to_le16(chan->dcid);\nrsp.dcid   = cpu_to_le16(chan->scid);\nrsp.result = cpu_to_le16(res);\nrsp.status = cpu_to_le16(stat);\nl2cap_send_cmd(conn, chan->ident, L2CAP_CONN_RSP,\nsizeof(rsp), &rsp);\nif (!test_bit(CONF_REQ_SENT, &chan->conf_state) &&\nres == L2CAP_CR_SUCCESS) {\nchar buf[128];\nset_bit(CONF_REQ_SENT, &chan->conf_state);\nl2cap_send_cmd(conn, l2cap_get_ident(conn),\nL2CAP_CONF_REQ,\nl2cap_build_conf_req(chan, buf),\nbuf);\nchan->num_conf_req++;\n}\n}\nl2cap_chan_unlock(chan);\n}\nmutex_unlock(&conn->chan_lock);\n}\n"
    }
  ],
  "CVE-2017-1000363": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of bounds checking for array access based on a static integer variable that tracks the index in the array.",
        "trigger_condition": "An attacker provides multiple 'lp=none' arguments to the kernel command line, causing the static integer variable to increment beyond the array bounds.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the bounds of the array before incrementing the static integer variable that tracks the index in the array. This allows an attacker to overflow the array by providing excessive input through the kernel command line."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a bounds check before incrementing the static integer variable that tracks the index in the array. This check ensures that the array bounds are not exceeded, preventing a potential buffer overflow. In this case, the solution involves adding a conditional check to verify that the index is within the array bounds before storing values in the array.",
      "GPT_analysis": "The modification is necessary to prevent a potential buffer overflow vulnerability in the code. In the original code snippet, there is a static integer variable `parport_ptr` that keeps track of the index in the `parport_nr` array where values are being stored. However, there is no check to ensure that `parport_ptr` does not exceed the size of the `parport_nr` array (`LP_NO`).\n\nIf an attacker were to provide multiple 'lp=none' arguments to the kernel command line, they could potentially overflow the `parport_nr` array by causing `parport_ptr` to increment beyond the array bounds. This could lead to a buffer overflow vulnerability, which is a serious security risk.\n\nBy adding the check `if (parport_ptr < LP_NO)` before attempting to store a value in the `parport_nr` array, the modified code ensures that the array bounds are not exceeded. If `parport_ptr` is already at the maximum index allowed (`LP_NO`), the code will handle the situation appropriately by printing a message and ignoring the input, thus preventing a buffer overflow.\n\nTherefore, the modification is necessary to mitigate the risk of a buffer overflow vulnerability and ensure the safe operation of the code.",
      "GPT_purpose": "Initialize the parallel port configuration based on the input parameters provided.",
      "GPT_function": "\n1. Initialize the lp_setup function with a parameter str.\n2. Check and handle different options passed in the str parameter, such as disabling the driver, setting specific parport numbers, or resetting.\n3. Handle potential vulnerabilities related to out-of-bounds write due to missing bounds check and static integer parport_ptr.",
      "CVE_id": "CVE-2017-1000363",
      "code_before_change": "static int __init lp_setup (char *str)\n{\n\tstatic int parport_ptr;\n\tint x;\n\n\tif (get_option(&str, &x)) {\n\t\tif (x == 0) {\n\t\t\t/* disable driver on \"lp=\" or \"lp=0\" */\n\t\t\tparport_nr[0] = LP_PARPORT_OFF;\n\t\t} else {\n\t\t\tprintk(KERN_WARNING \"warning: 'lp=0x%x' is deprecated, ignored\\n\", x);\n\t\t\treturn 0;\n\t\t}\n\t} else if (!strncmp(str, \"parport\", 7)) {\n\t\tint n = simple_strtoul(str+7, NULL, 10);\n\t\tif (parport_ptr < LP_NO)\n\t\t\tparport_nr[parport_ptr++] = n;\n\t\telse\n\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\n\t\t\t       str);\n\t} else if (!strcmp(str, \"auto\")) {\n\t\tparport_nr[0] = LP_PARPORT_AUTO;\n\t} else if (!strcmp(str, \"none\")) {\n\t\tparport_nr[parport_ptr++] = LP_PARPORT_NONE;\n\t} else if (!strcmp(str, \"reset\")) {\n\t\treset = 1;\n\t}\n\treturn 1;\n}",
      "code_after_change": "static int __init lp_setup (char *str)\n{\n\tstatic int parport_ptr;\n\tint x;\n\n\tif (get_option(&str, &x)) {\n\t\tif (x == 0) {\n\t\t\t/* disable driver on \"lp=\" or \"lp=0\" */\n\t\t\tparport_nr[0] = LP_PARPORT_OFF;\n\t\t} else {\n\t\t\tprintk(KERN_WARNING \"warning: 'lp=0x%x' is deprecated, ignored\\n\", x);\n\t\t\treturn 0;\n\t\t}\n\t} else if (!strncmp(str, \"parport\", 7)) {\n\t\tint n = simple_strtoul(str+7, NULL, 10);\n\t\tif (parport_ptr < LP_NO)\n\t\t\tparport_nr[parport_ptr++] = n;\n\t\telse\n\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\n\t\t\t       str);\n\t} else if (!strcmp(str, \"auto\")) {\n\t\tparport_nr[0] = LP_PARPORT_AUTO;\n\t} else if (!strcmp(str, \"none\")) {\n\t\tif (parport_ptr < LP_NO)\n\t\t\tparport_nr[parport_ptr++] = LP_PARPORT_NONE;\n\t\telse\n\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\n\t\t\t       str);\n\t} else if (!strcmp(str, \"reset\")) {\n\t\treset = 1;\n\t}\n\treturn 1;\n}",
      "modified_lines": {
        "added": [
          "\t\tif (parport_ptr < LP_NO)",
          "\t\t\tparport_nr[parport_ptr++] = LP_PARPORT_NONE;",
          "\t\telse",
          "\t\t\tprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",",
          "\t\t\t       str);"
        ],
        "deleted": [
          "\t\tparport_nr[parport_ptr++] = LP_PARPORT_NONE;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of bounds checking for array access based on a static integer variable that tracks the index in the array.",
      "trigger_condition": "An attacker provides multiple 'lp=none' arguments to the kernel command line, causing the static integer variable to increment beyond the array bounds.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the bounds of the array before incrementing the static integer variable that tracks the index in the array. This allows an attacker to overflow the array by providing excessive input through the kernel command line.",
      "id": 10,
      "code_after_change_normalized": "static int __init FUN1 (char *VAR1)\n{\nstatic int VAR2;\nint VAR3;\nif (FUN2(&VAR1, &VAR3)) {\nif (VAR3 == 0) {\nVAR4[0] = VAR5;\n} else {\nFUN3(VAR6 \"STR\", VAR3);\nreturn 0;\n}\n} else if (!FUN4(VAR1, \"STR\", 7)) {\nint VAR7 = FUN5(VAR1+7, NULL, 10);\nif (VAR2 < VAR8)\nVAR4[VAR2++] = VAR7;\nelse\nFUN3(VAR9 \"STR\",\nVAR1);\n} else if (!FUN6(VAR1, \"STR\")) {\nVAR4[0] = VAR10;\n} else if (!FUN6(VAR1, \"STR\")) {\nif (VAR2 < VAR8)\nVAR4[VAR2++] = VAR11;\nelse\nFUN3(VAR9 \"STR\",\nVAR1);\n} else if (!FUN6(VAR1, \"STR\")) {\nVAR12 = 1;\n}\nreturn 1;\n}\n",
      "code_before_change_normalized": "static int __init FUN1 (char *VAR1)\n{\nstatic int VAR2;\nint VAR3;\nif (FUN2(&VAR1, &VAR3)) {\nif (VAR3 == 0) {\nVAR4[0] = VAR5;\n} else {\nFUN3(VAR6 \"STR\", VAR3);\nreturn 0;\n}\n} else if (!FUN4(VAR1, \"STR\", 7)) {\nint VAR7 = FUN5(VAR1+7, NULL, 10);\nif (VAR2 < VAR8)\nVAR4[VAR2++] = VAR7;\nelse\nFUN3(VAR9 \"STR\",\nVAR1);\n} else if (!FUN6(VAR1, \"STR\")) {\nVAR4[0] = VAR10;\n} else if (!FUN6(VAR1, \"STR\")) {\nVAR4[VAR2++] = VAR11;\n} else if (!FUN6(VAR1, \"STR\")) {\nVAR12 = 1;\n}\nreturn 1;\n}\n",
      "code_after_change_raw": "static int __init lp_setup (char *str)\n{\nstatic int parport_ptr;\nint x;\nif (get_option(&str, &x)) {\nif (x == 0) {\nparport_nr[0] = LP_PARPORT_OFF;\n} else {\nprintk(KERN_WARNING \"warning: 'lp=0x%x' is deprecated, ignored\\n\", x);\nreturn 0;\n}\n} else if (!strncmp(str, \"parport\", 7)) {\nint n = simple_strtoul(str+7, NULL, 10);\nif (parport_ptr < LP_NO)\nparport_nr[parport_ptr++] = n;\nelse\nprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\nstr);\n} else if (!strcmp(str, \"auto\")) {\nparport_nr[0] = LP_PARPORT_AUTO;\n} else if (!strcmp(str, \"none\")) {\nif (parport_ptr < LP_NO)\nparport_nr[parport_ptr++] = LP_PARPORT_NONE;\nelse\nprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\nstr);\n} else if (!strcmp(str, \"reset\")) {\nreset = 1;\n}\nreturn 1;\n}\n",
      "code_before_change_raw": "static int __init lp_setup (char *str)\n{\nstatic int parport_ptr;\nint x;\nif (get_option(&str, &x)) {\nif (x == 0) {\nparport_nr[0] = LP_PARPORT_OFF;\n} else {\nprintk(KERN_WARNING \"warning: 'lp=0x%x' is deprecated, ignored\\n\", x);\nreturn 0;\n}\n} else if (!strncmp(str, \"parport\", 7)) {\nint n = simple_strtoul(str+7, NULL, 10);\nif (parport_ptr < LP_NO)\nparport_nr[parport_ptr++] = n;\nelse\nprintk(KERN_INFO \"lp: too many ports, %s ignored.\\n\",\nstr);\n} else if (!strcmp(str, \"auto\")) {\nparport_nr[0] = LP_PARPORT_AUTO;\n} else if (!strcmp(str, \"none\")) {\nparport_nr[parport_ptr++] = LP_PARPORT_NONE;\n} else if (!strcmp(str, \"reset\")) {\nreset = 1;\n}\nreturn 1;\n}\n"
    }
  ],
  "CVE-2017-13166": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for the format type before processing it in the switch statement.",
        "trigger_condition": "An attacker can exploit the lack of validation to manipulate the format type and trigger unauthorized access or privilege escalation.",
        "specific_code_behavior_causing_vulnerability": "The code directly processes the format type without validating it, allowing an attacker to provide a malicious format type and potentially gain elevated privileges.",
        "solution": "To mitigate the vulnerability, it is necessary to introduce a validation step for the format type before processing it in the switch statement. This ensures that only valid format types are handled, reducing the risk of unauthorized access or privilege escalation. In this case, the solution involves adding a new function to check the format type before proceeding with the format handling."
      },
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2017-13166, which is an elevation of privilege vulnerability in the kernel v4l2 video driver. The vulnerability allows an attacker to gain elevated privileges by exploiting the way the code handles video format requests.\n\nThe modification introduces a new function `check_fmt` that validates the format type before proceeding with the format handling. This helps to ensure that only valid format types are processed, reducing the risk of unauthorized access or privilege escalation.\n\nBy checking the format type before processing it in the switch statement, the code becomes more secure and less prone to exploitation. This modification helps to mitigate the vulnerability and improve the overall security of the video driver implementation.",
      "GPT_purpose": "Attempting to set and sanitize video format parameters in the v4l2 video driver.",
      "GPT_function": "\n1. Attempting to set a V4L2 format for various buffer types.\n2. Checking conditions based on video device type and direction.\n3. Calling specific functions based on the buffer type for formatting.",
      "CVE_id": "CVE-2017-13166",
      "code_before_change": "static int v4l_try_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_format *p = arg;\n\tstruct video_device *vfd = video_devdata(file);\n\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\n\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\n\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\n\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\n\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\n\tint ret;\n\n\tv4l_sanitize_format(p);\n\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_try_fmt_vid_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_try_fmt_vid_cap(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_try_fmt_vid_cap_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_vid_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_try_fmt_vid_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_CAPTURE:\n\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_try_fmt_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_try_fmt_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\n\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_try_fmt_sliced_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_try_fmt_sliced_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_try_fmt_vid_out(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_try_fmt_vid_out_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_try_fmt_vid_out_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_OUTPUT:\n\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_try_fmt_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_try_fmt_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\n\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_try_fmt_sliced_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_try_fmt_sliced_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_try_fmt_sdr_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_try_fmt_sdr_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_try_fmt_sdr_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_try_fmt_sdr_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_meta_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.meta);\n\t\treturn ops->vidioc_try_fmt_meta_cap(file, fh, arg);\n\t}\n\treturn -EINVAL;\n}",
      "code_after_change": "static int v4l_try_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_format *p = arg;\n\tint ret = check_fmt(file, p->type);\n\n\tif (ret)\n\t\treturn ret;\n\n\tv4l_sanitize_format(p);\n\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vid_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_try_fmt_vid_cap(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_try_fmt_vid_cap_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vid_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_try_fmt_vid_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_try_fmt_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_sliced_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_try_fmt_sliced_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vid_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_try_fmt_vid_out(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_try_fmt_vid_out_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vid_out_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_try_fmt_vid_out_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_try_fmt_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_try_fmt_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_try_fmt_sliced_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_try_fmt_sliced_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_sdr_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_try_fmt_sdr_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_try_fmt_sdr_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_try_fmt_sdr_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_try_fmt_meta_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.meta);\n\t\treturn ops->vidioc_try_fmt_meta_cap(file, fh, arg);\n\t}\n\treturn -EINVAL;\n}",
      "modified_lines": {
        "added": [
          "\tint ret = check_fmt(file, p->type);",
          "",
          "\tif (ret)",
          "\t\treturn ret;",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vid_cap))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vid_cap_mplane))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vid_overlay))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vbi_cap))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_sliced_vbi_cap))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vid_out))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vid_out_mplane))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vid_out_overlay))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_vbi_out))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_sliced_vbi_out))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_sdr_cap))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_sdr_out))",
          "\t\tif (unlikely(!ops->vidioc_try_fmt_meta_cap))"
        ],
        "deleted": [
          "\tstruct video_device *vfd = video_devdata(file);",
          "\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;",
          "\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;",
          "\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;",
          "\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;",
          "\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;",
          "\tint ret;",
          "\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_try_fmt_vid_cap))",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_vid_cap_mplane))",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_vid_overlay))",
          "\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_try_fmt_vbi_cap))",
          "\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_try_fmt_sliced_vbi_cap))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out_mplane))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out_overlay))",
          "\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_try_fmt_vbi_out))",
          "\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_try_fmt_sliced_vbi_out))",
          "\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_try_fmt_sdr_cap))",
          "\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_try_fmt_sdr_out))",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_meta_cap))"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for the format type before processing it in the switch statement.",
      "trigger_condition": "An attacker can exploit the lack of validation to manipulate the format type and trigger unauthorized access or privilege escalation.",
      "specific_code_behavior_causing_vulnerability": "The code directly processes the format type without validating it, allowing an attacker to provide a malicious format type and potentially gain elevated privileges.",
      "solution": "To mitigate the vulnerability, it is necessary to introduce a validation step for the format type before processing it in the switch statement. This ensures that only valid format types are handled, reducing the risk of unauthorized access or privilege escalation. In this case, the solution involves adding a new function to check the format type before proceeding with the format handling.",
      "id": 11,
      "code_after_change_normalized": "static int FUN1(const struct v4l2_ioctl_ops *VAR1,\nstruct VAR2 *VAR2, void *VAR3, void *VAR4)\n{\nstruct v4l2_format *VAR5 = VAR4;\nint VAR6 = FUN2(VAR2, VAR5->VAR7);\nif (VAR6)\nreturn VAR6;\nFUN3(VAR5);\nswitch (VAR5->VAR7) {\ncase VAR8:\nif (FUN4(!VAR1->VAR9))\nbreak;\nFUN5(VAR5, VAR10.VAR11);\nVAR6 = VAR1->FUN6(VAR2, VAR3, VAR4);\nVAR5->VAR10.VAR11.VAR12 = VAR13;\nreturn VAR6;\ncase VAR14:\nif (FUN4(!VAR1->VAR15))\nbreak;\nFUN5(VAR5, VAR10.VAR16.VAR17);\nreturn VAR1->FUN7(VAR2, VAR3, VAR4);\ncase VAR18:\nif (FUN4(!VAR1->VAR19))\nbreak;\nFUN5(VAR5, VAR10.VAR20);\nreturn VAR1->FUN8(VAR2, VAR3, VAR4);\ncase VAR21:\nif (FUN4(!VAR1->VAR22))\nbreak;\nFUN5(VAR5, VAR10.VAR23);\nreturn VAR1->FUN9(VAR2, VAR3, VAR4);\ncase VAR24:\nif (FUN4(!VAR1->VAR25))\nbreak;\nFUN5(VAR5, VAR10.VAR26);\nreturn VAR1->FUN10(VAR2, VAR3, VAR4);\ncase VAR27:\nif (FUN4(!VAR1->VAR28))\nbreak;\nFUN5(VAR5, VAR10.VAR11);\nVAR6 = VAR1->FUN11(VAR2, VAR3, VAR4);\nVAR5->VAR10.VAR11.VAR12 = VAR13;\nreturn VAR6;\ncase VAR29:\nif (FUN4(!VAR1->VAR30))\nbreak;\nFUN5(VAR5, VAR10.VAR16.VAR17);\nreturn VAR1->FUN12(VAR2, VAR3, VAR4);\ncase VAR31:\nif (FUN4(!VAR1->VAR32))\nbreak;\nFUN5(VAR5, VAR10.VAR20);\nreturn VAR1->FUN13(VAR2, VAR3, VAR4);\ncase VAR33:\nif (FUN4(!VAR1->VAR34))\nbreak;\nFUN5(VAR5, VAR10.VAR23);\nreturn VAR1->FUN14(VAR2, VAR3, VAR4);\ncase VAR35:\nif (FUN4(!VAR1->VAR36))\nbreak;\nFUN5(VAR5, VAR10.VAR26);\nreturn VAR1->FUN15(VAR2, VAR3, VAR4);\ncase VAR37:\nif (FUN4(!VAR1->VAR38))\nbreak;\nFUN5(VAR5, VAR10.VAR39);\nreturn VAR1->FUN16(VAR2, VAR3, VAR4);\ncase VAR40:\nif (FUN4(!VAR1->VAR41))\nbreak;\nFUN5(VAR5, VAR10.VAR39);\nreturn VAR1->FUN17(VAR2, VAR3, VAR4);\ncase VAR42:\nif (FUN4(!VAR1->VAR43))\nbreak;\nFUN5(VAR5, VAR10.VAR44);\nreturn VAR1->FUN18(VAR2, VAR3, VAR4);\n}\nreturn -VAR45;\n}\n",
      "code_before_change_normalized": "static int FUN1(const struct v4l2_ioctl_ops *VAR1,\nstruct VAR2 *VAR2, void *VAR3, void *VAR4)\n{\nstruct v4l2_format *VAR5 = VAR4;\nstruct video_device *VAR6 = FUN2(VAR2);\nbool VAR7 = VAR6->VAR8 == VAR9;\nbool VAR10 = VAR6->VAR8 == VAR11;\nbool VAR12 = VAR6->VAR8 == VAR13;\nbool VAR14 = VAR6->VAR15 != VAR16;\nbool VAR17 = VAR6->VAR15 != VAR18;\nint VAR19;\nFUN3(VAR5);\nswitch (VAR5->VAR20) {\ncase VAR21:\nif (FUN4(!VAR14 || (!VAR7 && !VAR12) || !VAR1->VAR22))\nbreak;\nFUN5(VAR5, VAR23.VAR24);\nVAR19 = VAR1->FUN6(VAR2, VAR3, VAR4);\nVAR5->VAR23.VAR24.VAR25 = VAR26;\nreturn VAR19;\ncase VAR27:\nif (FUN4(!VAR14 || !VAR7 || !VAR1->VAR28))\nbreak;\nFUN5(VAR5, VAR23.VAR29.VAR30);\nreturn VAR1->FUN7(VAR2, VAR3, VAR4);\ncase VAR31:\nif (FUN4(!VAR14 || !VAR7 || !VAR1->VAR32))\nbreak;\nFUN5(VAR5, VAR23.VAR33);\nreturn VAR1->FUN8(VAR2, VAR3, VAR4);\ncase VAR34:\nif (FUN4(!VAR14 || VAR7 || !VAR1->VAR35))\nbreak;\nFUN5(VAR5, VAR23.VAR36);\nreturn VAR1->FUN9(VAR2, VAR3, VAR4);\ncase VAR37:\nif (FUN4(!VAR14 || VAR7 || !VAR1->VAR38))\nbreak;\nFUN5(VAR5, VAR23.VAR39);\nreturn VAR1->FUN10(VAR2, VAR3, VAR4);\ncase VAR40:\nif (FUN4(!VAR17 || !VAR7 || !VAR1->VAR41))\nbreak;\nFUN5(VAR5, VAR23.VAR24);\nVAR19 = VAR1->FUN11(VAR2, VAR3, VAR4);\nVAR5->VAR23.VAR24.VAR25 = VAR26;\nreturn VAR19;\ncase VAR42:\nif (FUN4(!VAR17 || !VAR7 || !VAR1->VAR43))\nbreak;\nFUN5(VAR5, VAR23.VAR29.VAR30);\nreturn VAR1->FUN12(VAR2, VAR3, VAR4);\ncase VAR44:\nif (FUN4(!VAR17 || !VAR7 || !VAR1->VAR45))\nbreak;\nFUN5(VAR5, VAR23.VAR33);\nreturn VAR1->FUN13(VAR2, VAR3, VAR4);\ncase VAR46:\nif (FUN4(!VAR17 || VAR7 || !VAR1->VAR47))\nbreak;\nFUN5(VAR5, VAR23.VAR36);\nreturn VAR1->FUN14(VAR2, VAR3, VAR4);\ncase VAR48:\nif (FUN4(!VAR17 || VAR7 || !VAR1->VAR49))\nbreak;\nFUN5(VAR5, VAR23.VAR39);\nreturn VAR1->FUN15(VAR2, VAR3, VAR4);\ncase VAR50:\nif (FUN4(!VAR14 || !VAR10 || !VAR1->VAR51))\nbreak;\nFUN5(VAR5, VAR23.VAR52);\nreturn VAR1->FUN16(VAR2, VAR3, VAR4);\ncase VAR53:\nif (FUN4(!VAR17 || !VAR10 || !VAR1->VAR54))\nbreak;\nFUN5(VAR5, VAR23.VAR52);\nreturn VAR1->FUN17(VAR2, VAR3, VAR4);\ncase VAR55:\nif (FUN4(!VAR14 || !VAR7 || !VAR1->VAR56))\nbreak;\nFUN5(VAR5, VAR23.VAR57);\nreturn VAR1->FUN18(VAR2, VAR3, VAR4);\n}\nreturn -VAR58;\n}\n",
      "code_after_change_raw": "static int v4l_try_fmt(const struct v4l2_ioctl_ops *ops,\nstruct file *file, void *fh, void *arg)\n{\nstruct v4l2_format *p = arg;\nint ret = check_fmt(file, p->type);\nif (ret)\nreturn ret;\nv4l_sanitize_format(p);\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE:\nif (unlikely(!ops->vidioc_try_fmt_vid_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_try_fmt_vid_cap(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\nif (unlikely(!ops->vidioc_try_fmt_vid_cap_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_try_fmt_vid_cap_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\nif (unlikely(!ops->vidioc_try_fmt_vid_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_try_fmt_vid_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_CAPTURE:\nif (unlikely(!ops->vidioc_try_fmt_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_try_fmt_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\nif (unlikely(!ops->vidioc_try_fmt_sliced_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_try_fmt_sliced_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT:\nif (unlikely(!ops->vidioc_try_fmt_vid_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_try_fmt_vid_out(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\nif (unlikely(!ops->vidioc_try_fmt_vid_out_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_try_fmt_vid_out_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\nif (unlikely(!ops->vidioc_try_fmt_vid_out_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_try_fmt_vid_out_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_OUTPUT:\nif (unlikely(!ops->vidioc_try_fmt_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_try_fmt_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\nif (unlikely(!ops->vidioc_try_fmt_sliced_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_try_fmt_sliced_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_CAPTURE:\nif (unlikely(!ops->vidioc_try_fmt_sdr_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_try_fmt_sdr_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_OUTPUT:\nif (unlikely(!ops->vidioc_try_fmt_sdr_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_try_fmt_sdr_out(file, fh, arg);\ncase V4L2_BUF_TYPE_META_CAPTURE:\nif (unlikely(!ops->vidioc_try_fmt_meta_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.meta);\nreturn ops->vidioc_try_fmt_meta_cap(file, fh, arg);\n}\nreturn -EINVAL;\n}\n",
      "code_before_change_raw": "static int v4l_try_fmt(const struct v4l2_ioctl_ops *ops,\nstruct file *file, void *fh, void *arg)\n{\nstruct v4l2_format *p = arg;\nstruct video_device *vfd = video_devdata(file);\nbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\nbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\nbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\nbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\nbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\nint ret;\nv4l_sanitize_format(p);\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE:\nif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_try_fmt_vid_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_try_fmt_vid_cap(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_vid_cap_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_try_fmt_vid_cap_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_vid_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_try_fmt_vid_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_CAPTURE:\nif (unlikely(!is_rx || is_vid || !ops->vidioc_try_fmt_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_try_fmt_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\nif (unlikely(!is_rx || is_vid || !ops->vidioc_try_fmt_sliced_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_try_fmt_sliced_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_try_fmt_vid_out(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_try_fmt_vid_out_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_try_fmt_vid_out_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_try_fmt_vid_out_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_OUTPUT:\nif (unlikely(!is_tx || is_vid || !ops->vidioc_try_fmt_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_try_fmt_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\nif (unlikely(!is_tx || is_vid || !ops->vidioc_try_fmt_sliced_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_try_fmt_sliced_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_CAPTURE:\nif (unlikely(!is_rx || !is_sdr || !ops->vidioc_try_fmt_sdr_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_try_fmt_sdr_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_OUTPUT:\nif (unlikely(!is_tx || !is_sdr || !ops->vidioc_try_fmt_sdr_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_try_fmt_sdr_out(file, fh, arg);\ncase V4L2_BUF_TYPE_META_CAPTURE:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_try_fmt_meta_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.meta);\nreturn ops->vidioc_try_fmt_meta_cap(file, fh, arg);\n}\nreturn -EINVAL;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for the format type before processing it in the v4l2 video driver code.",
        "trigger_condition": "An attacker can exploit this vulnerability by providing a malicious format type, leading to unauthorized access or privilege escalation.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the format type before executing operations on it, allowing for potential exploitation by passing a malicious format type.",
        "solution": "To mitigate the vulnerability, it is necessary to add a validation step for the format type at the beginning of the code to ensure that only valid format types are processed. This prevents unauthorized access or privilege escalation by rejecting malicious format types."
      },
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2017-13166 in the v4l2 video driver code. The vulnerability allows for an elevation of privilege in the Android kernel. \n\nThe modification includes adding a call to a new function `check_fmt(file, p->type)` at the beginning of the `v4l_s_fmt` function. This new function is responsible for checking the validity of the format type before proceeding with the rest of the function. If the format type is not valid, the function will return an error, preventing further execution of the vulnerable code.\n\nBy adding this check, the code ensures that only valid format types are processed, reducing the risk of exploitation and potential privilege escalation. This modification helps to enhance the security of the v4l2 video driver and mitigate the identified vulnerability.",
      "GPT_purpose": "Set the format of a video device based on the provided arguments and handle different types of video buffers.",
      "GPT_function": "\n1. Set video format based on buffer type\n2. Enable media source\n3. Sanitize video format",
      "CVE_id": "CVE-2017-13166",
      "code_before_change": "static int v4l_s_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_format *p = arg;\n\tstruct video_device *vfd = video_devdata(file);\n\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\n\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\n\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\n\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\n\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\n\tint ret;\n\n\tret = v4l_enable_media_source(vfd);\n\tif (ret)\n\t\treturn ret;\n\tv4l_sanitize_format(p);\n\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_s_fmt_vid_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_s_fmt_vid_cap(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\tif (is_tch)\n\t\t\tv4l_pix_format_touch(&p->fmt.pix);\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_s_fmt_vid_cap_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_vid_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_s_fmt_vid_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_CAPTURE:\n\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_s_fmt_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_s_fmt_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\n\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_s_fmt_sliced_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_s_fmt_sliced_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_s_fmt_vid_out(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_s_fmt_vid_out_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_s_fmt_vid_out_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_OUTPUT:\n\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_s_fmt_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_s_fmt_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\n\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_s_fmt_sliced_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_s_fmt_sliced_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_s_fmt_sdr_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_s_fmt_sdr_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_s_fmt_sdr_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_s_fmt_sdr_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_meta_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.meta);\n\t\treturn ops->vidioc_s_fmt_meta_cap(file, fh, arg);\n\t}\n\treturn -EINVAL;\n}",
      "code_after_change": "static int v4l_s_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_format *p = arg;\n\tstruct video_device *vfd = video_devdata(file);\n\tint ret = check_fmt(file, p->type);\n\n\tif (ret)\n\t\treturn ret;\n\n\tret = v4l_enable_media_source(vfd);\n\tif (ret)\n\t\treturn ret;\n\tv4l_sanitize_format(p);\n\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vid_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_s_fmt_vid_cap(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\tif (vfd->vfl_type == VFL_TYPE_TOUCH)\n\t\t\tv4l_pix_format_touch(&p->fmt.pix);\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_s_fmt_vid_cap_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vid_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_s_fmt_vid_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_s_fmt_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_sliced_vbi_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_s_fmt_sliced_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vid_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix);\n\t\tret = ops->vidioc_s_fmt_vid_out(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\n\t\treturn ops->vidioc_s_fmt_vid_out_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vid_out_overlay))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.win);\n\t\treturn ops->vidioc_s_fmt_vid_out_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_s_fmt_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.vbi);\n\t\treturn ops->vidioc_s_fmt_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_s_fmt_sliced_vbi_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sliced);\n\t\treturn ops->vidioc_s_fmt_sliced_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_sdr_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_s_fmt_sdr_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_s_fmt_sdr_out))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.sdr);\n\t\treturn ops->vidioc_s_fmt_sdr_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_s_fmt_meta_cap))\n\t\t\tbreak;\n\t\tCLEAR_AFTER_FIELD(p, fmt.meta);\n\t\treturn ops->vidioc_s_fmt_meta_cap(file, fh, arg);\n\t}\n\treturn -EINVAL;\n}",
      "modified_lines": {
        "added": [
          "\tint ret = check_fmt(file, p->type);",
          "",
          "\tif (ret)",
          "\t\treturn ret;",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vid_cap))",
          "\t\tif (vfd->vfl_type == VFL_TYPE_TOUCH)",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vid_cap_mplane))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vid_overlay))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vbi_cap))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_sliced_vbi_cap))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vid_out))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vid_out_mplane))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vid_out_overlay))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_vbi_out))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_sliced_vbi_out))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_sdr_cap))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_sdr_out))",
          "\t\tif (unlikely(!ops->vidioc_s_fmt_meta_cap))"
        ],
        "deleted": [
          "\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;",
          "\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;",
          "\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;",
          "\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;",
          "\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;",
          "\tint ret;",
          "\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_s_fmt_vid_cap))",
          "\t\tif (is_tch)",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_vid_cap_mplane))",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_vid_overlay))",
          "\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_s_fmt_vbi_cap))",
          "\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_s_fmt_sliced_vbi_cap))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out_mplane))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out_overlay))",
          "\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_s_fmt_vbi_out))",
          "\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_s_fmt_sliced_vbi_out))",
          "\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_s_fmt_sdr_cap))",
          "\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_s_fmt_sdr_out))",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_meta_cap))"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for the format type before processing it in the v4l2 video driver code.",
      "trigger_condition": "An attacker can exploit this vulnerability by providing a malicious format type, leading to unauthorized access or privilege escalation.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the format type before executing operations on it, allowing for potential exploitation by passing a malicious format type.",
      "solution": "To mitigate the vulnerability, it is necessary to add a validation step for the format type at the beginning of the code to ensure that only valid format types are processed. This prevents unauthorized access or privilege escalation by rejecting malicious format types.",
      "id": 12,
      "code_after_change_normalized": "static int FUN1(const struct v4l2_ioctl_ops *VAR1,\nstruct VAR2 *VAR2, void *VAR3, void *VAR4)\n{\nstruct v4l2_format *VAR5 = VAR4;\nstruct video_device *VAR6 = FUN2(VAR2);\nint VAR7 = FUN3(VAR2, VAR5->VAR8);\nif (VAR7)\nreturn VAR7;\nVAR7 = FUN4(VAR6);\nif (VAR7)\nreturn VAR7;\nFUN5(VAR5);\nswitch (VAR5->VAR8) {\ncase VAR9:\nif (FUN6(!VAR1->VAR10))\nbreak;\nFUN7(VAR5, VAR11.VAR12);\nVAR7 = VAR1->FUN8(VAR2, VAR3, VAR4);\nVAR5->VAR11.VAR12.VAR13 = VAR14;\nif (VAR6->VAR15 == VAR16)\nFUN9(&VAR5->VAR11.VAR12);\nreturn VAR7;\ncase VAR17:\nif (FUN6(!VAR1->VAR18))\nbreak;\nFUN7(VAR5, VAR11.VAR19.VAR20);\nreturn VAR1->FUN10(VAR2, VAR3, VAR4);\ncase VAR21:\nif (FUN6(!VAR1->VAR22))\nbreak;\nFUN7(VAR5, VAR11.VAR23);\nreturn VAR1->FUN11(VAR2, VAR3, VAR4);\ncase VAR24:\nif (FUN6(!VAR1->VAR25))\nbreak;\nFUN7(VAR5, VAR11.VAR26);\nreturn VAR1->FUN12(VAR2, VAR3, VAR4);\ncase VAR27:\nif (FUN6(!VAR1->VAR28))\nbreak;\nFUN7(VAR5, VAR11.VAR29);\nreturn VAR1->FUN13(VAR2, VAR3, VAR4);\ncase VAR30:\nif (FUN6(!VAR1->VAR31))\nbreak;\nFUN7(VAR5, VAR11.VAR12);\nVAR7 = VAR1->FUN14(VAR2, VAR3, VAR4);\nVAR5->VAR11.VAR12.VAR13 = VAR14;\nreturn VAR7;\ncase VAR32:\nif (FUN6(!VAR1->VAR33))\nbreak;\nFUN7(VAR5, VAR11.VAR19.VAR20);\nreturn VAR1->FUN15(VAR2, VAR3, VAR4);\ncase VAR34:\nif (FUN6(!VAR1->VAR35))\nbreak;\nFUN7(VAR5, VAR11.VAR23);\nreturn VAR1->FUN16(VAR2, VAR3, VAR4);\ncase VAR36:\nif (FUN6(!VAR1->VAR37))\nbreak;\nFUN7(VAR5, VAR11.VAR26);\nreturn VAR1->FUN17(VAR2, VAR3, VAR4);\ncase VAR38:\nif (FUN6(!VAR1->VAR39))\nbreak;\nFUN7(VAR5, VAR11.VAR29);\nreturn VAR1->FUN18(VAR2, VAR3, VAR4);\ncase VAR40:\nif (FUN6(!VAR1->VAR41))\nbreak;\nFUN7(VAR5, VAR11.VAR42);\nreturn VAR1->FUN19(VAR2, VAR3, VAR4);\ncase VAR43:\nif (FUN6(!VAR1->VAR44))\nbreak;\nFUN7(VAR5, VAR11.VAR42);\nreturn VAR1->FUN20(VAR2, VAR3, VAR4);\ncase VAR45:\nif (FUN6(!VAR1->VAR46))\nbreak;\nFUN7(VAR5, VAR11.VAR47);\nreturn VAR1->FUN21(VAR2, VAR3, VAR4);\n}\nreturn -VAR48;\n}\n",
      "code_before_change_normalized": "static int FUN1(const struct v4l2_ioctl_ops *VAR1,\nstruct VAR2 *VAR2, void *VAR3, void *VAR4)\n{\nstruct v4l2_format *VAR5 = VAR4;\nstruct video_device *VAR6 = FUN2(VAR2);\nbool VAR7 = VAR6->VAR8 == VAR9;\nbool VAR10 = VAR6->VAR8 == VAR11;\nbool VAR12 = VAR6->VAR8 == VAR13;\nbool VAR14 = VAR6->VAR15 != VAR16;\nbool VAR17 = VAR6->VAR15 != VAR18;\nint VAR19;\nVAR19 = FUN3(VAR6);\nif (VAR19)\nreturn VAR19;\nFUN4(VAR5);\nswitch (VAR5->VAR20) {\ncase VAR21:\nif (FUN5(!VAR14 || (!VAR7 && !VAR12) || !VAR1->VAR22))\nbreak;\nFUN6(VAR5, VAR23.VAR24);\nVAR19 = VAR1->FUN7(VAR2, VAR3, VAR4);\nVAR5->VAR23.VAR24.VAR25 = VAR26;\nif (VAR12)\nFUN8(&VAR5->VAR23.VAR24);\nreturn VAR19;\ncase VAR27:\nif (FUN5(!VAR14 || !VAR7 || !VAR1->VAR28))\nbreak;\nFUN6(VAR5, VAR23.VAR29.VAR30);\nreturn VAR1->FUN9(VAR2, VAR3, VAR4);\ncase VAR31:\nif (FUN5(!VAR14 || !VAR7 || !VAR1->VAR32))\nbreak;\nFUN6(VAR5, VAR23.VAR33);\nreturn VAR1->FUN10(VAR2, VAR3, VAR4);\ncase VAR34:\nif (FUN5(!VAR14 || VAR7 || !VAR1->VAR35))\nbreak;\nFUN6(VAR5, VAR23.VAR36);\nreturn VAR1->FUN11(VAR2, VAR3, VAR4);\ncase VAR37:\nif (FUN5(!VAR14 || VAR7 || !VAR1->VAR38))\nbreak;\nFUN6(VAR5, VAR23.VAR39);\nreturn VAR1->FUN12(VAR2, VAR3, VAR4);\ncase VAR40:\nif (FUN5(!VAR17 || !VAR7 || !VAR1->VAR41))\nbreak;\nFUN6(VAR5, VAR23.VAR24);\nVAR19 = VAR1->FUN13(VAR2, VAR3, VAR4);\nVAR5->VAR23.VAR24.VAR25 = VAR26;\nreturn VAR19;\ncase VAR42:\nif (FUN5(!VAR17 || !VAR7 || !VAR1->VAR43))\nbreak;\nFUN6(VAR5, VAR23.VAR29.VAR30);\nreturn VAR1->FUN14(VAR2, VAR3, VAR4);\ncase VAR44:\nif (FUN5(!VAR17 || !VAR7 || !VAR1->VAR45))\nbreak;\nFUN6(VAR5, VAR23.VAR33);\nreturn VAR1->FUN15(VAR2, VAR3, VAR4);\ncase VAR46:\nif (FUN5(!VAR17 || VAR7 || !VAR1->VAR47))\nbreak;\nFUN6(VAR5, VAR23.VAR36);\nreturn VAR1->FUN16(VAR2, VAR3, VAR4);\ncase VAR48:\nif (FUN5(!VAR17 || VAR7 || !VAR1->VAR49))\nbreak;\nFUN6(VAR5, VAR23.VAR39);\nreturn VAR1->FUN17(VAR2, VAR3, VAR4);\ncase VAR50:\nif (FUN5(!VAR14 || !VAR10 || !VAR1->VAR51))\nbreak;\nFUN6(VAR5, VAR23.VAR52);\nreturn VAR1->FUN18(VAR2, VAR3, VAR4);\ncase VAR53:\nif (FUN5(!VAR17 || !VAR10 || !VAR1->VAR54))\nbreak;\nFUN6(VAR5, VAR23.VAR52);\nreturn VAR1->FUN19(VAR2, VAR3, VAR4);\ncase VAR55:\nif (FUN5(!VAR14 || !VAR7 || !VAR1->VAR56))\nbreak;\nFUN6(VAR5, VAR23.VAR57);\nreturn VAR1->FUN20(VAR2, VAR3, VAR4);\n}\nreturn -VAR58;\n}\n",
      "code_after_change_raw": "static int v4l_s_fmt(const struct v4l2_ioctl_ops *ops,\nstruct file *file, void *fh, void *arg)\n{\nstruct v4l2_format *p = arg;\nstruct video_device *vfd = video_devdata(file);\nint ret = check_fmt(file, p->type);\nif (ret)\nreturn ret;\nret = v4l_enable_media_source(vfd);\nif (ret)\nreturn ret;\nv4l_sanitize_format(p);\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE:\nif (unlikely(!ops->vidioc_s_fmt_vid_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_s_fmt_vid_cap(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nif (vfd->vfl_type == VFL_TYPE_TOUCH)\nv4l_pix_format_touch(&p->fmt.pix);\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\nif (unlikely(!ops->vidioc_s_fmt_vid_cap_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_s_fmt_vid_cap_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\nif (unlikely(!ops->vidioc_s_fmt_vid_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_s_fmt_vid_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_CAPTURE:\nif (unlikely(!ops->vidioc_s_fmt_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_s_fmt_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\nif (unlikely(!ops->vidioc_s_fmt_sliced_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_s_fmt_sliced_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT:\nif (unlikely(!ops->vidioc_s_fmt_vid_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_s_fmt_vid_out(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\nif (unlikely(!ops->vidioc_s_fmt_vid_out_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_s_fmt_vid_out_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\nif (unlikely(!ops->vidioc_s_fmt_vid_out_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_s_fmt_vid_out_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_OUTPUT:\nif (unlikely(!ops->vidioc_s_fmt_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_s_fmt_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\nif (unlikely(!ops->vidioc_s_fmt_sliced_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_s_fmt_sliced_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_CAPTURE:\nif (unlikely(!ops->vidioc_s_fmt_sdr_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_s_fmt_sdr_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_OUTPUT:\nif (unlikely(!ops->vidioc_s_fmt_sdr_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_s_fmt_sdr_out(file, fh, arg);\ncase V4L2_BUF_TYPE_META_CAPTURE:\nif (unlikely(!ops->vidioc_s_fmt_meta_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.meta);\nreturn ops->vidioc_s_fmt_meta_cap(file, fh, arg);\n}\nreturn -EINVAL;\n}\n",
      "code_before_change_raw": "static int v4l_s_fmt(const struct v4l2_ioctl_ops *ops,\nstruct file *file, void *fh, void *arg)\n{\nstruct v4l2_format *p = arg;\nstruct video_device *vfd = video_devdata(file);\nbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\nbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\nbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\nbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\nbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\nint ret;\nret = v4l_enable_media_source(vfd);\nif (ret)\nreturn ret;\nv4l_sanitize_format(p);\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE:\nif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_s_fmt_vid_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_s_fmt_vid_cap(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nif (is_tch)\nv4l_pix_format_touch(&p->fmt.pix);\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_vid_cap_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_s_fmt_vid_cap_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_vid_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_s_fmt_vid_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_CAPTURE:\nif (unlikely(!is_rx || is_vid || !ops->vidioc_s_fmt_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_s_fmt_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\nif (unlikely(!is_rx || is_vid || !ops->vidioc_s_fmt_sliced_vbi_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_s_fmt_sliced_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix);\nret = ops->vidioc_s_fmt_vid_out(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out_mplane))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.pix_mp.xfer_func);\nreturn ops->vidioc_s_fmt_vid_out_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_s_fmt_vid_out_overlay))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.win);\nreturn ops->vidioc_s_fmt_vid_out_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_OUTPUT:\nif (unlikely(!is_tx || is_vid || !ops->vidioc_s_fmt_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.vbi);\nreturn ops->vidioc_s_fmt_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\nif (unlikely(!is_tx || is_vid || !ops->vidioc_s_fmt_sliced_vbi_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sliced);\nreturn ops->vidioc_s_fmt_sliced_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_CAPTURE:\nif (unlikely(!is_rx || !is_sdr || !ops->vidioc_s_fmt_sdr_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_s_fmt_sdr_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_OUTPUT:\nif (unlikely(!is_tx || !is_sdr || !ops->vidioc_s_fmt_sdr_out))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.sdr);\nreturn ops->vidioc_s_fmt_sdr_out(file, fh, arg);\ncase V4L2_BUF_TYPE_META_CAPTURE:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_s_fmt_meta_cap))\nbreak;\nCLEAR_AFTER_FIELD(p, fmt.meta);\nreturn ops->vidioc_s_fmt_meta_cap(file, fh, arg);\n}\nreturn -EINVAL;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for the format type before processing operations, leading to potential exploitation of the vulnerability.",
        "trigger_condition": "The code does not validate the format type before executing operations, allowing for unauthorized access and potential privilege escalation.",
        "specific_code_behavior_causing_vulnerability": "The code directly processes the format type without verifying its validity, which can result in unauthorized access and privilege escalation. This lack of validation opens up the code to exploitation and security risks."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation step for the format type at the beginning of the function. By checking the format type before proceeding with operations, the code can prevent unauthorized access and potential privilege escalation. Additionally, simplifying the logic and removing unnecessary conditions in the switch cases helps in reducing the attack surface and making the code more secure.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2017-13166 in the v4l2 video driver code snippet. The vulnerability allows for an elevation of privilege in the Android kernel. By adding the `check_fmt` function call at the beginning of the `v4l_g_fmt` function, the code can validate the format type before proceeding with the operations. This helps prevent potential exploitation of the vulnerability by ensuring that only valid format types are processed, thereby improving the security of the code. Additionally, removing unnecessary conditions and simplifying the logic in the switch cases also helps in reducing the attack surface and making the code more robust against potential security threats.",
      "GPT_purpose": "Retrieve and format video data using V4L2 IOCTL operations, with specific handling for different video buffer types.",
      "GPT_function": "\n1. Handling V4L2 buffer format based on the type of video overlay.\n2. Checking and setting format for different types of video capture and output.\n3. Handling various V4L2 buffer types for video capture, output, VBI, sliced VBI, SDR, and meta capture.",
      "CVE_id": "CVE-2017-13166",
      "code_before_change": "static int v4l_g_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_format *p = arg;\n\tstruct video_device *vfd = video_devdata(file);\n\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\n\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\n\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\n\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\n\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\n\tint ret;\n\n\t/*\n\t * fmt can't be cleared for these overlay types due to the 'clips'\n\t * 'clipcount' and 'bitmap' pointers in struct v4l2_window.\n\t * Those are provided by the user. So handle these two overlay types\n\t * first, and then just do a simple memset for the other types.\n\t */\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY: {\n\t\tstruct v4l2_clip __user *clips = p->fmt.win.clips;\n\t\tu32 clipcount = p->fmt.win.clipcount;\n\t\tvoid __user *bitmap = p->fmt.win.bitmap;\n\n\t\tmemset(&p->fmt, 0, sizeof(p->fmt));\n\t\tp->fmt.win.clips = clips;\n\t\tp->fmt.win.clipcount = clipcount;\n\t\tp->fmt.win.bitmap = bitmap;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tmemset(&p->fmt, 0, sizeof(p->fmt));\n\t\tbreak;\n\t}\n\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_g_fmt_vid_cap))\n\t\t\tbreak;\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\tret = ops->vidioc_g_fmt_vid_cap(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_vid_cap_mplane))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_vid_cap_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_vid_overlay))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_vid_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_CAPTURE:\n\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_g_fmt_vbi_cap))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\n\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_g_fmt_sliced_vbi_cap))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_sliced_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out))\n\t\t\tbreak;\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\tret = ops->vidioc_g_fmt_vid_out(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out_mplane))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_vid_out_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\n\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out_overlay))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_vid_out_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_OUTPUT:\n\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_g_fmt_vbi_out))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\n\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_g_fmt_sliced_vbi_out))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_sliced_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_g_fmt_sdr_cap))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_sdr_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_g_fmt_sdr_out))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_sdr_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_meta_cap))\n\t\t\tbreak;\n\t\treturn ops->vidioc_g_fmt_meta_cap(file, fh, arg);\n\t}\n\treturn -EINVAL;\n}",
      "code_after_change": "static int v4l_g_fmt(const struct v4l2_ioctl_ops *ops,\n\t\t\t\tstruct file *file, void *fh, void *arg)\n{\n\tstruct v4l2_format *p = arg;\n\tint ret = check_fmt(file, p->type);\n\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * fmt can't be cleared for these overlay types due to the 'clips'\n\t * 'clipcount' and 'bitmap' pointers in struct v4l2_window.\n\t * Those are provided by the user. So handle these two overlay types\n\t * first, and then just do a simple memset for the other types.\n\t */\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY: {\n\t\tstruct v4l2_clip __user *clips = p->fmt.win.clips;\n\t\tu32 clipcount = p->fmt.win.clipcount;\n\t\tvoid __user *bitmap = p->fmt.win.bitmap;\n\n\t\tmemset(&p->fmt, 0, sizeof(p->fmt));\n\t\tp->fmt.win.clips = clips;\n\t\tp->fmt.win.clipcount = clipcount;\n\t\tp->fmt.win.bitmap = bitmap;\n\t\tbreak;\n\t}\n\tdefault:\n\t\tmemset(&p->fmt, 0, sizeof(p->fmt));\n\t\tbreak;\n\t}\n\n\tswitch (p->type) {\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE:\n\t\tif (unlikely(!ops->vidioc_g_fmt_vid_cap))\n\t\t\tbreak;\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\tret = ops->vidioc_g_fmt_vid_cap(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\n\t\treturn ops->vidioc_g_fmt_vid_cap_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OVERLAY:\n\t\treturn ops->vidioc_g_fmt_vid_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_CAPTURE:\n\t\treturn ops->vidioc_g_fmt_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\n\t\treturn ops->vidioc_g_fmt_sliced_vbi_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT:\n\t\tif (unlikely(!ops->vidioc_g_fmt_vid_out))\n\t\t\tbreak;\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\tret = ops->vidioc_g_fmt_vid_out(file, fh, arg);\n\t\t/* just in case the driver zeroed it again */\n\t\tp->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\n\t\treturn ret;\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\n\t\treturn ops->vidioc_g_fmt_vid_out_mplane(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\n\t\treturn ops->vidioc_g_fmt_vid_out_overlay(file, fh, arg);\n\tcase V4L2_BUF_TYPE_VBI_OUTPUT:\n\t\treturn ops->vidioc_g_fmt_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\n\t\treturn ops->vidioc_g_fmt_sliced_vbi_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_CAPTURE:\n\t\treturn ops->vidioc_g_fmt_sdr_cap(file, fh, arg);\n\tcase V4L2_BUF_TYPE_SDR_OUTPUT:\n\t\treturn ops->vidioc_g_fmt_sdr_out(file, fh, arg);\n\tcase V4L2_BUF_TYPE_META_CAPTURE:\n\t\treturn ops->vidioc_g_fmt_meta_cap(file, fh, arg);\n\t}\n\treturn -EINVAL;\n}",
      "modified_lines": {
        "added": [
          "\tint ret = check_fmt(file, p->type);",
          "",
          "\tif (ret)",
          "\t\treturn ret;",
          "\t\tif (unlikely(!ops->vidioc_g_fmt_vid_cap))",
          "\t\tif (unlikely(!ops->vidioc_g_fmt_vid_out))"
        ],
        "deleted": [
          "\tstruct video_device *vfd = video_devdata(file);",
          "\tbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;",
          "\tbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;",
          "\tbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;",
          "\tbool is_rx = vfd->vfl_dir != VFL_DIR_TX;",
          "\tbool is_tx = vfd->vfl_dir != VFL_DIR_RX;",
          "\tint ret;",
          "\t\tif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_g_fmt_vid_cap))",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_vid_cap_mplane))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_vid_overlay))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_g_fmt_vbi_cap))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_rx || is_vid || !ops->vidioc_g_fmt_sliced_vbi_cap))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out))",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out_mplane))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out_overlay))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_g_fmt_vbi_out))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_tx || is_vid || !ops->vidioc_g_fmt_sliced_vbi_out))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_rx || !is_sdr || !ops->vidioc_g_fmt_sdr_cap))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_tx || !is_sdr || !ops->vidioc_g_fmt_sdr_out))",
          "\t\t\tbreak;",
          "\t\tif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_meta_cap))",
          "\t\t\tbreak;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for the format type before processing operations, leading to potential exploitation of the vulnerability.",
      "trigger_condition": "The code does not validate the format type before executing operations, allowing for unauthorized access and potential privilege escalation.",
      "specific_code_behavior_causing_vulnerability": "The code directly processes the format type without verifying its validity, which can result in unauthorized access and privilege escalation. This lack of validation opens up the code to exploitation and security risks.",
      "id": 13,
      "code_after_change_normalized": "static int FUN1(const struct v4l2_ioctl_ops *VAR1,\nstruct VAR2 *VAR2, void *VAR3, void *VAR4)\n{\nstruct v4l2_format *VAR5 = VAR4;\nint VAR6 = FUN2(VAR2, VAR5->VAR7);\nif (VAR6)\nreturn VAR6;\nswitch (VAR5->VAR7) {\ncase VAR8:\ncase VAR9: {\nstruct v4l2_clip __user *VAR10 = VAR5->VAR11.VAR12.VAR10;\nu32 VAR13 = VAR5->VAR11.VAR12.VAR13;\nvoid __user *VAR14 = VAR5->VAR11.VAR12.VAR14;\nFUN3(&VAR5->VAR11, 0, sizeof(VAR5->VAR11));\nVAR5->VAR11.VAR12.VAR10 = VAR10;\nVAR5->VAR11.VAR12.VAR13 = VAR13;\nVAR5->VAR11.VAR12.VAR14 = VAR14;\nbreak;\n}\ndefault:\nFUN3(&VAR5->VAR11, 0, sizeof(VAR5->VAR11));\nbreak;\n}\nswitch (VAR5->VAR7) {\ncase VAR15:\nif (FUN4(!VAR1->VAR16))\nbreak;\nVAR5->VAR11.VAR17.VAR18 = VAR19;\nVAR6 = VAR1->FUN5(VAR2, VAR3, VAR4);\nVAR5->VAR11.VAR17.VAR18 = VAR19;\nreturn VAR6;\ncase VAR20:\nreturn VAR1->FUN6(VAR2, VAR3, VAR4);\ncase VAR8:\nreturn VAR1->FUN7(VAR2, VAR3, VAR4);\ncase VAR21:\nreturn VAR1->FUN8(VAR2, VAR3, VAR4);\ncase VAR22:\nreturn VAR1->FUN9(VAR2, VAR3, VAR4);\ncase VAR23:\nif (FUN4(!VAR1->VAR24))\nbreak;\nVAR5->VAR11.VAR17.VAR18 = VAR19;\nVAR6 = VAR1->FUN10(VAR2, VAR3, VAR4);\nVAR5->VAR11.VAR17.VAR18 = VAR19;\nreturn VAR6;\ncase VAR25:\nreturn VAR1->FUN11(VAR2, VAR3, VAR4);\ncase VAR9:\nreturn VAR1->FUN12(VAR2, VAR3, VAR4);\ncase VAR26:\nreturn VAR1->FUN13(VAR2, VAR3, VAR4);\ncase VAR27:\nreturn VAR1->FUN14(VAR2, VAR3, VAR4);\ncase VAR28:\nreturn VAR1->FUN15(VAR2, VAR3, VAR4);\ncase VAR29:\nreturn VAR1->FUN16(VAR2, VAR3, VAR4);\ncase VAR30:\nreturn VAR1->FUN17(VAR2, VAR3, VAR4);\n}\nreturn -VAR31;\n}\n",
      "code_before_change_normalized": "static int FUN1(const struct v4l2_ioctl_ops *VAR1,\nstruct VAR2 *VAR2, void *VAR3, void *VAR4)\n{\nstruct v4l2_format *VAR5 = VAR4;\nstruct video_device *VAR6 = FUN2(VAR2);\nbool VAR7 = VAR6->VAR8 == VAR9;\nbool VAR10 = VAR6->VAR8 == VAR11;\nbool VAR12 = VAR6->VAR8 == VAR13;\nbool VAR14 = VAR6->VAR15 != VAR16;\nbool VAR17 = VAR6->VAR15 != VAR18;\nint VAR19;\nswitch (VAR5->VAR20) {\ncase VAR21:\ncase VAR22: {\nstruct v4l2_clip __user *VAR23 = VAR5->VAR24.VAR25.VAR23;\nu32 VAR26 = VAR5->VAR24.VAR25.VAR26;\nvoid __user *VAR27 = VAR5->VAR24.VAR25.VAR27;\nFUN3(&VAR5->VAR24, 0, sizeof(VAR5->VAR24));\nVAR5->VAR24.VAR25.VAR23 = VAR23;\nVAR5->VAR24.VAR25.VAR26 = VAR26;\nVAR5->VAR24.VAR25.VAR27 = VAR27;\nbreak;\n}\ndefault:\nFUN3(&VAR5->VAR24, 0, sizeof(VAR5->VAR24));\nbreak;\n}\nswitch (VAR5->VAR20) {\ncase VAR28:\nif (FUN4(!VAR14 || (!VAR7 && !VAR12) || !VAR1->VAR29))\nbreak;\nVAR5->VAR24.VAR30.VAR31 = VAR32;\nVAR19 = VAR1->FUN5(VAR2, VAR3, VAR4);\nVAR5->VAR24.VAR30.VAR31 = VAR32;\nreturn VAR19;\ncase VAR33:\nif (FUN4(!VAR14 || !VAR7 || !VAR1->VAR34))\nbreak;\nreturn VAR1->FUN6(VAR2, VAR3, VAR4);\ncase VAR21:\nif (FUN4(!VAR14 || !VAR7 || !VAR1->VAR35))\nbreak;\nreturn VAR1->FUN7(VAR2, VAR3, VAR4);\ncase VAR36:\nif (FUN4(!VAR14 || VAR7 || !VAR1->VAR37))\nbreak;\nreturn VAR1->FUN8(VAR2, VAR3, VAR4);\ncase VAR38:\nif (FUN4(!VAR14 || VAR7 || !VAR1->VAR39))\nbreak;\nreturn VAR1->FUN9(VAR2, VAR3, VAR4);\ncase VAR40:\nif (FUN4(!VAR17 || !VAR7 || !VAR1->VAR41))\nbreak;\nVAR5->VAR24.VAR30.VAR31 = VAR32;\nVAR19 = VAR1->FUN10(VAR2, VAR3, VAR4);\nVAR5->VAR24.VAR30.VAR31 = VAR32;\nreturn VAR19;\ncase VAR42:\nif (FUN4(!VAR17 || !VAR7 || !VAR1->VAR43))\nbreak;\nreturn VAR1->FUN11(VAR2, VAR3, VAR4);\ncase VAR22:\nif (FUN4(!VAR17 || !VAR7 || !VAR1->VAR44))\nbreak;\nreturn VAR1->FUN12(VAR2, VAR3, VAR4);\ncase VAR45:\nif (FUN4(!VAR17 || VAR7 || !VAR1->VAR46))\nbreak;\nreturn VAR1->FUN13(VAR2, VAR3, VAR4);\ncase VAR47:\nif (FUN4(!VAR17 || VAR7 || !VAR1->VAR48))\nbreak;\nreturn VAR1->FUN14(VAR2, VAR3, VAR4);\ncase VAR49:\nif (FUN4(!VAR14 || !VAR10 || !VAR1->VAR50))\nbreak;\nreturn VAR1->FUN15(VAR2, VAR3, VAR4);\ncase VAR51:\nif (FUN4(!VAR17 || !VAR10 || !VAR1->VAR52))\nbreak;\nreturn VAR1->FUN16(VAR2, VAR3, VAR4);\ncase VAR53:\nif (FUN4(!VAR14 || !VAR7 || !VAR1->VAR54))\nbreak;\nreturn VAR1->FUN17(VAR2, VAR3, VAR4);\n}\nreturn -VAR55;\n}\n",
      "code_after_change_raw": "static int v4l_g_fmt(const struct v4l2_ioctl_ops *ops,\nstruct file *file, void *fh, void *arg)\n{\nstruct v4l2_format *p = arg;\nint ret = check_fmt(file, p->type);\nif (ret)\nreturn ret;\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY: {\nstruct v4l2_clip __user *clips = p->fmt.win.clips;\nu32 clipcount = p->fmt.win.clipcount;\nvoid __user *bitmap = p->fmt.win.bitmap;\nmemset(&p->fmt, 0, sizeof(p->fmt));\np->fmt.win.clips = clips;\np->fmt.win.clipcount = clipcount;\np->fmt.win.bitmap = bitmap;\nbreak;\n}\ndefault:\nmemset(&p->fmt, 0, sizeof(p->fmt));\nbreak;\n}\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE:\nif (unlikely(!ops->vidioc_g_fmt_vid_cap))\nbreak;\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nret = ops->vidioc_g_fmt_vid_cap(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\nreturn ops->vidioc_g_fmt_vid_cap_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\nreturn ops->vidioc_g_fmt_vid_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_CAPTURE:\nreturn ops->vidioc_g_fmt_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\nreturn ops->vidioc_g_fmt_sliced_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT:\nif (unlikely(!ops->vidioc_g_fmt_vid_out))\nbreak;\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nret = ops->vidioc_g_fmt_vid_out(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\nreturn ops->vidioc_g_fmt_vid_out_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\nreturn ops->vidioc_g_fmt_vid_out_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_OUTPUT:\nreturn ops->vidioc_g_fmt_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\nreturn ops->vidioc_g_fmt_sliced_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_CAPTURE:\nreturn ops->vidioc_g_fmt_sdr_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_OUTPUT:\nreturn ops->vidioc_g_fmt_sdr_out(file, fh, arg);\ncase V4L2_BUF_TYPE_META_CAPTURE:\nreturn ops->vidioc_g_fmt_meta_cap(file, fh, arg);\n}\nreturn -EINVAL;\n}\n",
      "code_before_change_raw": "static int v4l_g_fmt(const struct v4l2_ioctl_ops *ops,\nstruct file *file, void *fh, void *arg)\n{\nstruct v4l2_format *p = arg;\nstruct video_device *vfd = video_devdata(file);\nbool is_vid = vfd->vfl_type == VFL_TYPE_GRABBER;\nbool is_sdr = vfd->vfl_type == VFL_TYPE_SDR;\nbool is_tch = vfd->vfl_type == VFL_TYPE_TOUCH;\nbool is_rx = vfd->vfl_dir != VFL_DIR_TX;\nbool is_tx = vfd->vfl_dir != VFL_DIR_RX;\nint ret;\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY: {\nstruct v4l2_clip __user *clips = p->fmt.win.clips;\nu32 clipcount = p->fmt.win.clipcount;\nvoid __user *bitmap = p->fmt.win.bitmap;\nmemset(&p->fmt, 0, sizeof(p->fmt));\np->fmt.win.clips = clips;\np->fmt.win.clipcount = clipcount;\np->fmt.win.bitmap = bitmap;\nbreak;\n}\ndefault:\nmemset(&p->fmt, 0, sizeof(p->fmt));\nbreak;\n}\nswitch (p->type) {\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE:\nif (unlikely(!is_rx || (!is_vid && !is_tch) || !ops->vidioc_g_fmt_vid_cap))\nbreak;\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nret = ops->vidioc_g_fmt_vid_cap(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_vid_cap_mplane))\nbreak;\nreturn ops->vidioc_g_fmt_vid_cap_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OVERLAY:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_vid_overlay))\nbreak;\nreturn ops->vidioc_g_fmt_vid_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_CAPTURE:\nif (unlikely(!is_rx || is_vid || !ops->vidioc_g_fmt_vbi_cap))\nbreak;\nreturn ops->vidioc_g_fmt_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_CAPTURE:\nif (unlikely(!is_rx || is_vid || !ops->vidioc_g_fmt_sliced_vbi_cap))\nbreak;\nreturn ops->vidioc_g_fmt_sliced_vbi_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out))\nbreak;\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nret = ops->vidioc_g_fmt_vid_out(file, fh, arg);\np->fmt.pix.priv = V4L2_PIX_FMT_PRIV_MAGIC;\nreturn ret;\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_MPLANE:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out_mplane))\nbreak;\nreturn ops->vidioc_g_fmt_vid_out_mplane(file, fh, arg);\ncase V4L2_BUF_TYPE_VIDEO_OUTPUT_OVERLAY:\nif (unlikely(!is_tx || !is_vid || !ops->vidioc_g_fmt_vid_out_overlay))\nbreak;\nreturn ops->vidioc_g_fmt_vid_out_overlay(file, fh, arg);\ncase V4L2_BUF_TYPE_VBI_OUTPUT:\nif (unlikely(!is_tx || is_vid || !ops->vidioc_g_fmt_vbi_out))\nbreak;\nreturn ops->vidioc_g_fmt_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SLICED_VBI_OUTPUT:\nif (unlikely(!is_tx || is_vid || !ops->vidioc_g_fmt_sliced_vbi_out))\nbreak;\nreturn ops->vidioc_g_fmt_sliced_vbi_out(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_CAPTURE:\nif (unlikely(!is_rx || !is_sdr || !ops->vidioc_g_fmt_sdr_cap))\nbreak;\nreturn ops->vidioc_g_fmt_sdr_cap(file, fh, arg);\ncase V4L2_BUF_TYPE_SDR_OUTPUT:\nif (unlikely(!is_tx || !is_sdr || !ops->vidioc_g_fmt_sdr_out))\nbreak;\nreturn ops->vidioc_g_fmt_sdr_out(file, fh, arg);\ncase V4L2_BUF_TYPE_META_CAPTURE:\nif (unlikely(!is_rx || !is_vid || !ops->vidioc_g_fmt_meta_cap))\nbreak;\nreturn ops->vidioc_g_fmt_meta_cap(file, fh, arg);\n}\nreturn -EINVAL;\n}\n"
    }
  ],
  "CVE-2017-13216": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Insufficient locking mechanism when accessing a shared data structure, allowing for concurrent access and potential race conditions.",
        "trigger_condition": "Concurrent access to the shared data structure without proper synchronization, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code accesses a shared data structure without adequate locking in place, allowing multiple threads to modify the structure simultaneously, leading to potential out-of-bounds writes and security vulnerabilities."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add proper locking mechanisms to synchronize access to the shared data structure. In this case, adding mutex locking before and after accessing the shared data structure ensures that only one thread can modify it at a time, preventing race conditions and out-of-bounds writes. By using mutexes or other synchronization mechanisms, the code can maintain data integrity and prevent security vulnerabilities related to concurrent access.",
      "GPT_analysis": "The modification made to the code snippet is necessary to address the vulnerability CVE-2017-13216, which involves an out-of-bounds write due to insufficient locking when accessing the `asma` structure in the `ashmem_ioctl` function.\n\nBy adding the lines:\n```c\nmutex_lock(&ashmem_mutex);\n```\nand\n```c\nmutex_unlock(&ashmem_mutex);\n```\naround the block of code that accesses the `asma` structure in the `ASHMEM_SET_SIZE` case, you are ensuring that proper locking mechanisms are in place to prevent concurrent access to the `asma` structure. This helps to avoid potential race conditions and out-of-bounds writes that could lead to security vulnerabilities, such as privilege escalation.\n\nIn this case, the added mutex locking ensures that only one thread can access and modify the `asma` structure at a time, preventing the possibility of unauthorized access or modification that could be exploited for malicious purposes. By using mutexes to synchronize access to shared resources like `asma`, you can maintain data integrity and prevent security vulnerabilities like the one described in the CVE-2017-13216.",
      "GPT_purpose": "Handle various ioctl commands for ashmem shared memory regions.",
      "GPT_function": "\n1. Set the name for an ashmem area.\n2. Get the name of an ashmem area.\n3. Set the size of an ashmem area.\n4. Get the size of an ashmem area.\n5. Set the protection mask for an ashmem area.\n6. Get the protection mask of an ashmem area.\n7. Pin, unpin, or get pin status of an ashmem area.\n8. Purge all caches of ashmem if the user has CAP_SYS_ADMIN capability.",
      "CVE_id": "CVE-2017-13216",
      "code_before_change": "static long ashmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct ashmem_area *asma = file->private_data;\n\tlong ret = -ENOTTY;\n\n\tswitch (cmd) {\n\tcase ASHMEM_SET_NAME:\n\t\tret = set_name(asma, (void __user *)arg);\n\t\tbreak;\n\tcase ASHMEM_GET_NAME:\n\t\tret = get_name(asma, (void __user *)arg);\n\t\tbreak;\n\tcase ASHMEM_SET_SIZE:\n\t\tret = -EINVAL;\n\t\tif (!asma->file) {\n\t\t\tret = 0;\n\t\t\tasma->size = (size_t)arg;\n\t\t}\n\t\tbreak;\n\tcase ASHMEM_GET_SIZE:\n\t\tret = asma->size;\n\t\tbreak;\n\tcase ASHMEM_SET_PROT_MASK:\n\t\tret = set_prot_mask(asma, arg);\n\t\tbreak;\n\tcase ASHMEM_GET_PROT_MASK:\n\t\tret = asma->prot_mask;\n\t\tbreak;\n\tcase ASHMEM_PIN:\n\tcase ASHMEM_UNPIN:\n\tcase ASHMEM_GET_PIN_STATUS:\n\t\tret = ashmem_pin_unpin(asma, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase ASHMEM_PURGE_ALL_CACHES:\n\t\tret = -EPERM;\n\t\tif (capable(CAP_SYS_ADMIN)) {\n\t\t\tstruct shrink_control sc = {\n\t\t\t\t.gfp_mask = GFP_KERNEL,\n\t\t\t\t.nr_to_scan = LONG_MAX,\n\t\t\t};\n\t\t\tret = ashmem_shrink_count(&ashmem_shrinker, &sc);\n\t\t\tashmem_shrink_scan(&ashmem_shrinker, &sc);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
      "code_after_change": "static long ashmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\n\tstruct ashmem_area *asma = file->private_data;\n\tlong ret = -ENOTTY;\n\n\tswitch (cmd) {\n\tcase ASHMEM_SET_NAME:\n\t\tret = set_name(asma, (void __user *)arg);\n\t\tbreak;\n\tcase ASHMEM_GET_NAME:\n\t\tret = get_name(asma, (void __user *)arg);\n\t\tbreak;\n\tcase ASHMEM_SET_SIZE:\n\t\tret = -EINVAL;\n\t\tmutex_lock(&ashmem_mutex);\n\t\tif (!asma->file) {\n\t\t\tret = 0;\n\t\t\tasma->size = (size_t)arg;\n\t\t}\n\t\tmutex_unlock(&ashmem_mutex);\n\t\tbreak;\n\tcase ASHMEM_GET_SIZE:\n\t\tret = asma->size;\n\t\tbreak;\n\tcase ASHMEM_SET_PROT_MASK:\n\t\tret = set_prot_mask(asma, arg);\n\t\tbreak;\n\tcase ASHMEM_GET_PROT_MASK:\n\t\tret = asma->prot_mask;\n\t\tbreak;\n\tcase ASHMEM_PIN:\n\tcase ASHMEM_UNPIN:\n\tcase ASHMEM_GET_PIN_STATUS:\n\t\tret = ashmem_pin_unpin(asma, cmd, (void __user *)arg);\n\t\tbreak;\n\tcase ASHMEM_PURGE_ALL_CACHES:\n\t\tret = -EPERM;\n\t\tif (capable(CAP_SYS_ADMIN)) {\n\t\t\tstruct shrink_control sc = {\n\t\t\t\t.gfp_mask = GFP_KERNEL,\n\t\t\t\t.nr_to_scan = LONG_MAX,\n\t\t\t};\n\t\t\tret = ashmem_shrink_count(&ashmem_shrinker, &sc);\n\t\t\tashmem_shrink_scan(&ashmem_shrinker, &sc);\n\t\t}\n\t\tbreak;\n\t}\n\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\t\tmutex_lock(&ashmem_mutex);",
          "\t\tmutex_unlock(&ashmem_mutex);"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Insufficient locking mechanism when accessing a shared data structure, allowing for concurrent access and potential race conditions.",
      "trigger_condition": "Concurrent access to the shared data structure without proper synchronization, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code accesses a shared data structure without adequate locking in place, allowing multiple threads to modify the structure simultaneously, leading to potential out-of-bounds writes and security vulnerabilities.",
      "id": 14,
      "code_after_change_normalized": "static long FUN1(struct VAR1 *VAR1, unsigned int VAR2, unsigned long VAR3)\n{\nstruct ashmem_area *VAR4 = VAR1->VAR5;\nlong VAR6 = -VAR7;\nswitch (VAR2) {\ncase VAR8:\nVAR6 = FUN2(VAR4, (void VAR9 *)VAR3);\nbreak;\ncase VAR10:\nVAR6 = FUN3(VAR4, (void VAR9 *)VAR3);\nbreak;\ncase VAR11:\nVAR6 = -VAR12;\nFUN4(&VAR13);\nif (!VAR4->VAR1) {\nVAR6 = 0;\nVAR4->VAR14 = (VAR15)VAR3;\n}\nFUN5(&VAR13);\nbreak;\ncase VAR16:\nVAR6 = VAR4->VAR14;\nbreak;\ncase VAR17:\nVAR6 = FUN6(VAR4, VAR3);\nbreak;\ncase VAR18:\nVAR6 = VAR4->VAR19;\nbreak;\ncase VAR20:\ncase VAR21:\ncase VAR22:\nVAR6 = FUN7(VAR4, VAR2, (void VAR9 *)VAR3);\nbreak;\ncase VAR23:\nVAR6 = -VAR24;\nif (FUN8(VAR25)) {\nstruct shrink_control VAR26 = {\n.VAR27 = VAR28,\n.VAR29 = VAR30,\n};\nVAR6 = FUN9(&VAR31, &VAR26);\nFUN10(&VAR31, &VAR26);\n}\nbreak;\n}\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static long FUN1(struct VAR1 *VAR1, unsigned int VAR2, unsigned long VAR3)\n{\nstruct ashmem_area *VAR4 = VAR1->VAR5;\nlong VAR6 = -VAR7;\nswitch (VAR2) {\ncase VAR8:\nVAR6 = FUN2(VAR4, (void VAR9 *)VAR3);\nbreak;\ncase VAR10:\nVAR6 = FUN3(VAR4, (void VAR9 *)VAR3);\nbreak;\ncase VAR11:\nVAR6 = -VAR12;\nif (!VAR4->VAR1) {\nVAR6 = 0;\nVAR4->VAR13 = (VAR14)VAR3;\n}\nbreak;\ncase VAR15:\nVAR6 = VAR4->VAR13;\nbreak;\ncase VAR16:\nVAR6 = FUN4(VAR4, VAR3);\nbreak;\ncase VAR17:\nVAR6 = VAR4->VAR18;\nbreak;\ncase VAR19:\ncase VAR20:\ncase VAR21:\nVAR6 = FUN5(VAR4, VAR2, (void VAR9 *)VAR3);\nbreak;\ncase VAR22:\nVAR6 = -VAR23;\nif (FUN6(VAR24)) {\nstruct shrink_control VAR25 = {\n.VAR26 = VAR27,\n.VAR28 = VAR29,\n};\nVAR6 = FUN7(&VAR30, &VAR25);\nFUN8(&VAR30, &VAR25);\n}\nbreak;\n}\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static long ashmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\nstruct ashmem_area *asma = file->private_data;\nlong ret = -ENOTTY;\nswitch (cmd) {\ncase ASHMEM_SET_NAME:\nret = set_name(asma, (void __user *)arg);\nbreak;\ncase ASHMEM_GET_NAME:\nret = get_name(asma, (void __user *)arg);\nbreak;\ncase ASHMEM_SET_SIZE:\nret = -EINVAL;\nmutex_lock(&ashmem_mutex);\nif (!asma->file) {\nret = 0;\nasma->size = (size_t)arg;\n}\nmutex_unlock(&ashmem_mutex);\nbreak;\ncase ASHMEM_GET_SIZE:\nret = asma->size;\nbreak;\ncase ASHMEM_SET_PROT_MASK:\nret = set_prot_mask(asma, arg);\nbreak;\ncase ASHMEM_GET_PROT_MASK:\nret = asma->prot_mask;\nbreak;\ncase ASHMEM_PIN:\ncase ASHMEM_UNPIN:\ncase ASHMEM_GET_PIN_STATUS:\nret = ashmem_pin_unpin(asma, cmd, (void __user *)arg);\nbreak;\ncase ASHMEM_PURGE_ALL_CACHES:\nret = -EPERM;\nif (capable(CAP_SYS_ADMIN)) {\nstruct shrink_control sc = {\n.gfp_mask = GFP_KERNEL,\n.nr_to_scan = LONG_MAX,\n};\nret = ashmem_shrink_count(&ashmem_shrinker, &sc);\nashmem_shrink_scan(&ashmem_shrinker, &sc);\n}\nbreak;\n}\nreturn ret;\n}\n",
      "code_before_change_raw": "static long ashmem_ioctl(struct file *file, unsigned int cmd, unsigned long arg)\n{\nstruct ashmem_area *asma = file->private_data;\nlong ret = -ENOTTY;\nswitch (cmd) {\ncase ASHMEM_SET_NAME:\nret = set_name(asma, (void __user *)arg);\nbreak;\ncase ASHMEM_GET_NAME:\nret = get_name(asma, (void __user *)arg);\nbreak;\ncase ASHMEM_SET_SIZE:\nret = -EINVAL;\nif (!asma->file) {\nret = 0;\nasma->size = (size_t)arg;\n}\nbreak;\ncase ASHMEM_GET_SIZE:\nret = asma->size;\nbreak;\ncase ASHMEM_SET_PROT_MASK:\nret = set_prot_mask(asma, arg);\nbreak;\ncase ASHMEM_GET_PROT_MASK:\nret = asma->prot_mask;\nbreak;\ncase ASHMEM_PIN:\ncase ASHMEM_UNPIN:\ncase ASHMEM_GET_PIN_STATUS:\nret = ashmem_pin_unpin(asma, cmd, (void __user *)arg);\nbreak;\ncase ASHMEM_PURGE_ALL_CACHES:\nret = -EPERM;\nif (capable(CAP_SYS_ADMIN)) {\nstruct shrink_control sc = {\n.gfp_mask = GFP_KERNEL,\n.nr_to_scan = LONG_MAX,\n};\nret = ashmem_shrink_count(&ashmem_shrinker, &sc);\nashmem_shrink_scan(&ashmem_shrinker, &sc);\n}\nbreak;\n}\nreturn ret;\n}\n"
    }
  ],
  "CVE-2017-17558": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Failure to consider the maximum number of configurations and interfaces before releasing resources during USB device processing.",
        "trigger_condition": "A local user interacts with a crafted USB device, causing the USB core subsystem to attempt to release resources without proper validation of configuration and interface limits.",
        "specific_code_behavior_causing_vulnerability": "The code does not appropriately handle the maximum number of configurations and interfaces, leading to a denial of service (out-of-bounds write access) vulnerability when processing USB devices.",
        "solution": "To mitigate the vulnerability, it is necessary to adjust the code to properly validate and handle the maximum number of configurations and interfaces before releasing resources. This involves setting the number of interfaces to zero initially and ensuring correct counting and allocation of interfaces to prevent out-of-bounds access. The solution helps prevent denial of service and other potential impacts from a crafted USB device."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-17558, which involves the usb_destroy_configuration function in the Linux kernel's USB core subsystem. The vulnerability arises from not considering the maximum number of configurations and interfaces before attempting to release resources. By adjusting the code in the usb_parse_configuration function as shown in the modified version, specifically by setting `config->desc.bNumInterfaces = 0;` and ensuring proper handling of interface counts and allocations, we can prevent potential denial of service (out-of-bounds write access) or other unspecified impacts that could be caused by a crafted USB device exploiting this vulnerability. This modification helps to ensure that the code behaves correctly and securely when processing USB configurations and interfaces, mitigating the risk posed by the vulnerability.",
      "GPT_purpose": "Parse and process USB configuration descriptors to initialize USB device interfaces and configurations.",
      "GPT_function": "\n1. Parsing USB configuration descriptors and checking their validity.\n2. Counting the number of interfaces and their alternate settings in a USB configuration.\n3. Allocating memory for USB interface caches and altsetting arrays.\n4. Parsing USB interface and altsetting descriptors.\n5. Checking for missing altsettings in the USB configuration.",
      "CVE_id": "CVE-2017-17558",
      "code_before_change": "static int usb_parse_configuration(struct usb_device *dev, int cfgidx,\n    struct usb_host_config *config, unsigned char *buffer, int size)\n{\n\tstruct device *ddev = &dev->dev;\n\tunsigned char *buffer0 = buffer;\n\tint cfgno;\n\tint nintf, nintf_orig;\n\tint i, j, n;\n\tstruct usb_interface_cache *intfc;\n\tunsigned char *buffer2;\n\tint size2;\n\tstruct usb_descriptor_header *header;\n\tint len, retval;\n\tu8 inums[USB_MAXINTERFACES], nalts[USB_MAXINTERFACES];\n\tunsigned iad_num = 0;\n\n\tmemcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);\n\tif (config->desc.bDescriptorType != USB_DT_CONFIG ||\n\t    config->desc.bLength < USB_DT_CONFIG_SIZE ||\n\t    config->desc.bLength > size) {\n\t\tdev_err(ddev, \"invalid descriptor for config index %d: \"\n\t\t    \"type = 0x%X, length = %d\\n\", cfgidx,\n\t\t    config->desc.bDescriptorType, config->desc.bLength);\n\t\treturn -EINVAL;\n\t}\n\tcfgno = config->desc.bConfigurationValue;\n\n\tbuffer += config->desc.bLength;\n\tsize -= config->desc.bLength;\n\n\tnintf = nintf_orig = config->desc.bNumInterfaces;\n\tif (nintf > USB_MAXINTERFACES) {\n\t\tdev_warn(ddev, \"config %d has too many interfaces: %d, \"\n\t\t    \"using maximum allowed: %d\\n\",\n\t\t    cfgno, nintf, USB_MAXINTERFACES);\n\t\tnintf = USB_MAXINTERFACES;\n\t}\n\n\t/* Go through the descriptors, checking their length and counting the\n\t * number of altsettings for each interface */\n\tn = 0;\n\tfor ((buffer2 = buffer, size2 = size);\n\t      size2 > 0;\n\t     (buffer2 += header->bLength, size2 -= header->bLength)) {\n\n\t\tif (size2 < sizeof(struct usb_descriptor_header)) {\n\t\t\tdev_warn(ddev, \"config %d descriptor has %d excess \"\n\t\t\t    \"byte%s, ignoring\\n\",\n\t\t\t    cfgno, size2, plural(size2));\n\t\t\tbreak;\n\t\t}\n\n\t\theader = (struct usb_descriptor_header *) buffer2;\n\t\tif ((header->bLength > size2) || (header->bLength < 2)) {\n\t\t\tdev_warn(ddev, \"config %d has an invalid descriptor \"\n\t\t\t    \"of length %d, skipping remainder of the config\\n\",\n\t\t\t    cfgno, header->bLength);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (header->bDescriptorType == USB_DT_INTERFACE) {\n\t\t\tstruct usb_interface_descriptor *d;\n\t\t\tint inum;\n\n\t\t\td = (struct usb_interface_descriptor *) header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_SIZE) {\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface descriptor of length %d, \"\n\t\t\t\t    \"skipping\\n\", cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tinum = d->bInterfaceNumber;\n\n\t\t\tif ((dev->quirks & USB_QUIRK_HONOR_BNUMINTERFACES) &&\n\t\t\t    n >= nintf_orig) {\n\t\t\t\tdev_warn(ddev, \"config %d has more interface \"\n\t\t\t\t    \"descriptors, than it declares in \"\n\t\t\t\t    \"bNumInterfaces, ignoring interface \"\n\t\t\t\t    \"number: %d\\n\", cfgno, inum);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (inum >= nintf_orig)\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface number: %d but max is %d\\n\",\n\t\t\t\t    cfgno, inum, nintf_orig - 1);\n\n\t\t\t/* Have we already encountered this interface?\n\t\t\t * Count its altsettings */\n\t\t\tfor (i = 0; i < n; ++i) {\n\t\t\t\tif (inums[i] == inum)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (i < n) {\n\t\t\t\tif (nalts[i] < 255)\n\t\t\t\t\t++nalts[i];\n\t\t\t} else if (n < USB_MAXINTERFACES) {\n\t\t\t\tinums[n] = inum;\n\t\t\t\tnalts[n] = 1;\n\t\t\t\t++n;\n\t\t\t}\n\n\t\t} else if (header->bDescriptorType ==\n\t\t\t\tUSB_DT_INTERFACE_ASSOCIATION) {\n\t\t\tstruct usb_interface_assoc_descriptor *d;\n\n\t\t\td = (struct usb_interface_assoc_descriptor *)header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {\n\t\t\t\tdev_warn(ddev,\n\t\t\t\t\t \"config %d has an invalid interface association descriptor of length %d, skipping\\n\",\n\t\t\t\t\t cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (iad_num == USB_MAXIADS) {\n\t\t\t\tdev_warn(ddev, \"found more Interface \"\n\t\t\t\t\t       \"Association Descriptors \"\n\t\t\t\t\t       \"than allocated for in \"\n\t\t\t\t\t       \"configuration %d\\n\", cfgno);\n\t\t\t} else {\n\t\t\t\tconfig->intf_assoc[iad_num] = d;\n\t\t\t\tiad_num++;\n\t\t\t}\n\n\t\t} else if (header->bDescriptorType == USB_DT_DEVICE ||\n\t\t\t    header->bDescriptorType == USB_DT_CONFIG)\n\t\t\tdev_warn(ddev, \"config %d contains an unexpected \"\n\t\t\t    \"descriptor of type 0x%X, skipping\\n\",\n\t\t\t    cfgno, header->bDescriptorType);\n\n\t}\t/* for ((buffer2 = buffer, size2 = size); ...) */\n\tsize = buffer2 - buffer;\n\tconfig->desc.wTotalLength = cpu_to_le16(buffer2 - buffer0);\n\n\tif (n != nintf)\n\t\tdev_warn(ddev, \"config %d has %d interface%s, different from \"\n\t\t    \"the descriptor's value: %d\\n\",\n\t\t    cfgno, n, plural(n), nintf_orig);\n\telse if (n == 0)\n\t\tdev_warn(ddev, \"config %d has no interfaces?\\n\", cfgno);\n\tconfig->desc.bNumInterfaces = nintf = n;\n\n\t/* Check for missing interface numbers */\n\tfor (i = 0; i < nintf; ++i) {\n\t\tfor (j = 0; j < nintf; ++j) {\n\t\t\tif (inums[j] == i)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= nintf)\n\t\t\tdev_warn(ddev, \"config %d has no interface number \"\n\t\t\t    \"%d\\n\", cfgno, i);\n\t}\n\n\t/* Allocate the usb_interface_caches and altsetting arrays */\n\tfor (i = 0; i < nintf; ++i) {\n\t\tj = nalts[i];\n\t\tif (j > USB_MAXALTSETTING) {\n\t\t\tdev_warn(ddev, \"too many alternate settings for \"\n\t\t\t    \"config %d interface %d: %d, \"\n\t\t\t    \"using maximum allowed: %d\\n\",\n\t\t\t    cfgno, inums[i], j, USB_MAXALTSETTING);\n\t\t\tnalts[i] = j = USB_MAXALTSETTING;\n\t\t}\n\n\t\tlen = sizeof(*intfc) + sizeof(struct usb_host_interface) * j;\n\t\tconfig->intf_cache[i] = intfc = kzalloc(len, GFP_KERNEL);\n\t\tif (!intfc)\n\t\t\treturn -ENOMEM;\n\t\tkref_init(&intfc->ref);\n\t}\n\n\t/* FIXME: parse the BOS descriptor */\n\n\t/* Skip over any Class Specific or Vendor Specific descriptors;\n\t * find the first interface descriptor */\n\tconfig->extra = buffer;\n\ti = find_next_descriptor(buffer, size, USB_DT_INTERFACE,\n\t    USB_DT_INTERFACE, &n);\n\tconfig->extralen = i;\n\tif (n > 0)\n\t\tdev_dbg(ddev, \"skipped %d descriptor%s after %s\\n\",\n\t\t    n, plural(n), \"configuration\");\n\tbuffer += i;\n\tsize -= i;\n\n\t/* Parse all the interface/altsetting descriptors */\n\twhile (size > 0) {\n\t\tretval = usb_parse_interface(ddev, cfgno, config,\n\t\t    buffer, size, inums, nalts);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\n\t\tbuffer += retval;\n\t\tsize -= retval;\n\t}\n\n\t/* Check for missing altsettings */\n\tfor (i = 0; i < nintf; ++i) {\n\t\tintfc = config->intf_cache[i];\n\t\tfor (j = 0; j < intfc->num_altsetting; ++j) {\n\t\t\tfor (n = 0; n < intfc->num_altsetting; ++n) {\n\t\t\t\tif (intfc->altsetting[n].desc.\n\t\t\t\t    bAlternateSetting == j)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (n >= intfc->num_altsetting)\n\t\t\t\tdev_warn(ddev, \"config %d interface %d has no \"\n\t\t\t\t    \"altsetting %d\\n\", cfgno, inums[i], j);\n\t\t}\n\t}\n\n\treturn 0;\n}",
      "code_after_change": "static int usb_parse_configuration(struct usb_device *dev, int cfgidx,\n    struct usb_host_config *config, unsigned char *buffer, int size)\n{\n\tstruct device *ddev = &dev->dev;\n\tunsigned char *buffer0 = buffer;\n\tint cfgno;\n\tint nintf, nintf_orig;\n\tint i, j, n;\n\tstruct usb_interface_cache *intfc;\n\tunsigned char *buffer2;\n\tint size2;\n\tstruct usb_descriptor_header *header;\n\tint len, retval;\n\tu8 inums[USB_MAXINTERFACES], nalts[USB_MAXINTERFACES];\n\tunsigned iad_num = 0;\n\n\tmemcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);\n\tnintf = nintf_orig = config->desc.bNumInterfaces;\n\tconfig->desc.bNumInterfaces = 0;\t// Adjusted later\n\n\tif (config->desc.bDescriptorType != USB_DT_CONFIG ||\n\t    config->desc.bLength < USB_DT_CONFIG_SIZE ||\n\t    config->desc.bLength > size) {\n\t\tdev_err(ddev, \"invalid descriptor for config index %d: \"\n\t\t    \"type = 0x%X, length = %d\\n\", cfgidx,\n\t\t    config->desc.bDescriptorType, config->desc.bLength);\n\t\treturn -EINVAL;\n\t}\n\tcfgno = config->desc.bConfigurationValue;\n\n\tbuffer += config->desc.bLength;\n\tsize -= config->desc.bLength;\n\n\tif (nintf > USB_MAXINTERFACES) {\n\t\tdev_warn(ddev, \"config %d has too many interfaces: %d, \"\n\t\t    \"using maximum allowed: %d\\n\",\n\t\t    cfgno, nintf, USB_MAXINTERFACES);\n\t\tnintf = USB_MAXINTERFACES;\n\t}\n\n\t/* Go through the descriptors, checking their length and counting the\n\t * number of altsettings for each interface */\n\tn = 0;\n\tfor ((buffer2 = buffer, size2 = size);\n\t      size2 > 0;\n\t     (buffer2 += header->bLength, size2 -= header->bLength)) {\n\n\t\tif (size2 < sizeof(struct usb_descriptor_header)) {\n\t\t\tdev_warn(ddev, \"config %d descriptor has %d excess \"\n\t\t\t    \"byte%s, ignoring\\n\",\n\t\t\t    cfgno, size2, plural(size2));\n\t\t\tbreak;\n\t\t}\n\n\t\theader = (struct usb_descriptor_header *) buffer2;\n\t\tif ((header->bLength > size2) || (header->bLength < 2)) {\n\t\t\tdev_warn(ddev, \"config %d has an invalid descriptor \"\n\t\t\t    \"of length %d, skipping remainder of the config\\n\",\n\t\t\t    cfgno, header->bLength);\n\t\t\tbreak;\n\t\t}\n\n\t\tif (header->bDescriptorType == USB_DT_INTERFACE) {\n\t\t\tstruct usb_interface_descriptor *d;\n\t\t\tint inum;\n\n\t\t\td = (struct usb_interface_descriptor *) header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_SIZE) {\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface descriptor of length %d, \"\n\t\t\t\t    \"skipping\\n\", cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tinum = d->bInterfaceNumber;\n\n\t\t\tif ((dev->quirks & USB_QUIRK_HONOR_BNUMINTERFACES) &&\n\t\t\t    n >= nintf_orig) {\n\t\t\t\tdev_warn(ddev, \"config %d has more interface \"\n\t\t\t\t    \"descriptors, than it declares in \"\n\t\t\t\t    \"bNumInterfaces, ignoring interface \"\n\t\t\t\t    \"number: %d\\n\", cfgno, inum);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (inum >= nintf_orig)\n\t\t\t\tdev_warn(ddev, \"config %d has an invalid \"\n\t\t\t\t    \"interface number: %d but max is %d\\n\",\n\t\t\t\t    cfgno, inum, nintf_orig - 1);\n\n\t\t\t/* Have we already encountered this interface?\n\t\t\t * Count its altsettings */\n\t\t\tfor (i = 0; i < n; ++i) {\n\t\t\t\tif (inums[i] == inum)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (i < n) {\n\t\t\t\tif (nalts[i] < 255)\n\t\t\t\t\t++nalts[i];\n\t\t\t} else if (n < USB_MAXINTERFACES) {\n\t\t\t\tinums[n] = inum;\n\t\t\t\tnalts[n] = 1;\n\t\t\t\t++n;\n\t\t\t}\n\n\t\t} else if (header->bDescriptorType ==\n\t\t\t\tUSB_DT_INTERFACE_ASSOCIATION) {\n\t\t\tstruct usb_interface_assoc_descriptor *d;\n\n\t\t\td = (struct usb_interface_assoc_descriptor *)header;\n\t\t\tif (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {\n\t\t\t\tdev_warn(ddev,\n\t\t\t\t\t \"config %d has an invalid interface association descriptor of length %d, skipping\\n\",\n\t\t\t\t\t cfgno, d->bLength);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif (iad_num == USB_MAXIADS) {\n\t\t\t\tdev_warn(ddev, \"found more Interface \"\n\t\t\t\t\t       \"Association Descriptors \"\n\t\t\t\t\t       \"than allocated for in \"\n\t\t\t\t\t       \"configuration %d\\n\", cfgno);\n\t\t\t} else {\n\t\t\t\tconfig->intf_assoc[iad_num] = d;\n\t\t\t\tiad_num++;\n\t\t\t}\n\n\t\t} else if (header->bDescriptorType == USB_DT_DEVICE ||\n\t\t\t    header->bDescriptorType == USB_DT_CONFIG)\n\t\t\tdev_warn(ddev, \"config %d contains an unexpected \"\n\t\t\t    \"descriptor of type 0x%X, skipping\\n\",\n\t\t\t    cfgno, header->bDescriptorType);\n\n\t}\t/* for ((buffer2 = buffer, size2 = size); ...) */\n\tsize = buffer2 - buffer;\n\tconfig->desc.wTotalLength = cpu_to_le16(buffer2 - buffer0);\n\n\tif (n != nintf)\n\t\tdev_warn(ddev, \"config %d has %d interface%s, different from \"\n\t\t    \"the descriptor's value: %d\\n\",\n\t\t    cfgno, n, plural(n), nintf_orig);\n\telse if (n == 0)\n\t\tdev_warn(ddev, \"config %d has no interfaces?\\n\", cfgno);\n\tconfig->desc.bNumInterfaces = nintf = n;\n\n\t/* Check for missing interface numbers */\n\tfor (i = 0; i < nintf; ++i) {\n\t\tfor (j = 0; j < nintf; ++j) {\n\t\t\tif (inums[j] == i)\n\t\t\t\tbreak;\n\t\t}\n\t\tif (j >= nintf)\n\t\t\tdev_warn(ddev, \"config %d has no interface number \"\n\t\t\t    \"%d\\n\", cfgno, i);\n\t}\n\n\t/* Allocate the usb_interface_caches and altsetting arrays */\n\tfor (i = 0; i < nintf; ++i) {\n\t\tj = nalts[i];\n\t\tif (j > USB_MAXALTSETTING) {\n\t\t\tdev_warn(ddev, \"too many alternate settings for \"\n\t\t\t    \"config %d interface %d: %d, \"\n\t\t\t    \"using maximum allowed: %d\\n\",\n\t\t\t    cfgno, inums[i], j, USB_MAXALTSETTING);\n\t\t\tnalts[i] = j = USB_MAXALTSETTING;\n\t\t}\n\n\t\tlen = sizeof(*intfc) + sizeof(struct usb_host_interface) * j;\n\t\tconfig->intf_cache[i] = intfc = kzalloc(len, GFP_KERNEL);\n\t\tif (!intfc)\n\t\t\treturn -ENOMEM;\n\t\tkref_init(&intfc->ref);\n\t}\n\n\t/* FIXME: parse the BOS descriptor */\n\n\t/* Skip over any Class Specific or Vendor Specific descriptors;\n\t * find the first interface descriptor */\n\tconfig->extra = buffer;\n\ti = find_next_descriptor(buffer, size, USB_DT_INTERFACE,\n\t    USB_DT_INTERFACE, &n);\n\tconfig->extralen = i;\n\tif (n > 0)\n\t\tdev_dbg(ddev, \"skipped %d descriptor%s after %s\\n\",\n\t\t    n, plural(n), \"configuration\");\n\tbuffer += i;\n\tsize -= i;\n\n\t/* Parse all the interface/altsetting descriptors */\n\twhile (size > 0) {\n\t\tretval = usb_parse_interface(ddev, cfgno, config,\n\t\t    buffer, size, inums, nalts);\n\t\tif (retval < 0)\n\t\t\treturn retval;\n\n\t\tbuffer += retval;\n\t\tsize -= retval;\n\t}\n\n\t/* Check for missing altsettings */\n\tfor (i = 0; i < nintf; ++i) {\n\t\tintfc = config->intf_cache[i];\n\t\tfor (j = 0; j < intfc->num_altsetting; ++j) {\n\t\t\tfor (n = 0; n < intfc->num_altsetting; ++n) {\n\t\t\t\tif (intfc->altsetting[n].desc.\n\t\t\t\t    bAlternateSetting == j)\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (n >= intfc->num_altsetting)\n\t\t\t\tdev_warn(ddev, \"config %d interface %d has no \"\n\t\t\t\t    \"altsetting %d\\n\", cfgno, inums[i], j);\n\t\t}\n\t}\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tnintf = nintf_orig = config->desc.bNumInterfaces;",
          "\tconfig->desc.bNumInterfaces = 0;\t// Adjusted later",
          ""
        ],
        "deleted": [
          "\tnintf = nintf_orig = config->desc.bNumInterfaces;"
        ]
      },
      "preconditions_for_vulnerability": "Failure to consider the maximum number of configurations and interfaces before releasing resources during USB device processing.",
      "trigger_condition": "A local user interacts with a crafted USB device, causing the USB core subsystem to attempt to release resources without proper validation of configuration and interface limits.",
      "specific_code_behavior_causing_vulnerability": "The code does not appropriately handle the maximum number of configurations and interfaces, leading to a denial of service (out-of-bounds write access) vulnerability when processing USB devices.",
      "solution": "To mitigate the vulnerability, it is necessary to adjust the code to properly validate and handle the maximum number of configurations and interfaces before releasing resources. This involves setting the number of interfaces to zero initially and ensuring correct counting and allocation of interfaces to prevent out-of-bounds access. The solution helps prevent denial of service and other potential impacts from a crafted USB device.",
      "id": 15,
      "code_after_change_normalized": "static int FUN1(struct usb_device *VAR1, int VAR2,\nstruct usb_host_config *VAR3, unsigned char *VAR4, int VAR5)\n{\nstruct device *VAR6 = &VAR1->VAR1;\nunsigned char *VAR7 = VAR4;\nint VAR8;\nint VAR9, VAR10;\nint VAR11, VAR12, VAR13;\nstruct usb_interface_cache *VAR14;\nunsigned char *VAR15;\nint VAR16;\nstruct usb_descriptor_header *VAR17;\nint VAR18, VAR19;\nu8 VAR20[VAR21], VAR22[VAR21];\nunsigned VAR23 = 0;\nFUN2(&VAR3->VAR24, VAR4, VAR25);\nVAR9 = VAR10 = VAR3->VAR24.VAR26;\nVAR3->VAR24.VAR26 = 0;\t\nif (VAR3->VAR24.VAR27 != VAR28 ||\nVAR3->VAR24.VAR29 < VAR25 ||\nVAR3->VAR24.VAR29 > VAR5) {\nFUN3(VAR6, \"STR\"\n\"STR\", VAR2,\nVAR3->VAR24.VAR27, VAR3->VAR24.VAR29);\nreturn -VAR30;\n}\nVAR8 = VAR3->VAR24.VAR31;\nVAR4 += VAR3->VAR24.VAR29;\nVAR5 -= VAR3->VAR24.VAR29;\nif (VAR9 > VAR21) {\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR9, VAR21);\nVAR9 = VAR21;\n}\nVAR13 = 0;\nfor ((VAR15 = VAR4, VAR16 = VAR5);\nVAR16 > 0;\n(VAR15 += VAR17->VAR29, VAR16 -= VAR17->VAR29)) {\nif (VAR16 < sizeof(struct VAR32)) {\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR16, FUN5(VAR16));\nbreak;\n}\nVAR17 = (struct VAR32 *) VAR15;\nif ((VAR17->VAR29 > VAR16) || (VAR17->VAR29 < 2)) {\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR17->VAR29);\nbreak;\n}\nif (VAR17->VAR27 == VAR33) {\nstruct usb_interface_descriptor *VAR34;\nint VAR35;\nVAR34 = (struct VAR36 *) VAR17;\nif (VAR34->VAR29 < VAR37) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\", VAR8, VAR34->VAR29);\ncontinue;\n}\nVAR35 = VAR34->VAR38;\nif ((VAR1->VAR39 & VAR40) &&\nVAR13 >= VAR10) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\"\n\"STR\", VAR8, VAR35);\ncontinue;\n}\nif (VAR35 >= VAR10)\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR35, VAR10 - 1);\nfor (VAR11 = 0; VAR11 < VAR13; ++VAR11) {\nif (VAR20[VAR11] == VAR35)\nbreak;\n}\nif (VAR11 < VAR13) {\nif (VAR22[VAR11] < 255)\n++VAR22[VAR11];\n} else if (VAR13 < VAR21) {\nVAR20[VAR13] = VAR35;\nVAR22[VAR13] = 1;\n++VAR13;\n}\n} else if (VAR17->VAR27 ==\nVAR41) {\nstruct usb_interface_assoc_descriptor *VAR34;\nVAR34 = (struct VAR42 *)VAR17;\nif (VAR34->VAR29 < VAR43) {\nFUN4(VAR6,\n\"STR\",\nVAR8, VAR34->VAR29);\ncontinue;\n}\nif (VAR23 == VAR44) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\"\n\"STR\", VAR8);\n} else {\nVAR3->VAR45[VAR23] = VAR34;\nVAR23++;\n}\n} else if (VAR17->VAR27 == VAR46 ||\nVAR17->VAR27 == VAR28)\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR17->VAR27);\n}\t\nVAR5 = VAR15 - VAR4;\nVAR3->VAR24.VAR47 = FUN6(VAR15 - VAR7);\nif (VAR13 != VAR9)\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR13, FUN5(VAR13), VAR10);\nelse if (VAR13 == 0)\nFUN4(VAR6, \"STR\", VAR8);\nVAR3->VAR24.VAR26 = VAR9 = VAR13;\nfor (VAR11 = 0; VAR11 < VAR9; ++VAR11) {\nfor (VAR12 = 0; VAR12 < VAR9; ++VAR12) {\nif (VAR20[VAR12] == VAR11)\nbreak;\n}\nif (VAR12 >= VAR9)\nFUN4(VAR6, \"STR\"\n\"STR\", VAR8, VAR11);\n}\nfor (VAR11 = 0; VAR11 < VAR9; ++VAR11) {\nVAR12 = VAR22[VAR11];\nif (VAR12 > VAR48) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\",\nVAR8, VAR20[VAR11], VAR12, VAR48);\nVAR22[VAR11] = VAR12 = VAR48;\n}\nVAR18 = sizeof(*VAR14) + sizeof(struct VAR49) * VAR12;\nVAR3->VAR50[VAR11] = VAR14 = FUN7(VAR18, VAR51);\nif (!VAR14)\nreturn -VAR52;\nFUN8(&VAR14->VAR53);\n}\nVAR3->VAR54 = VAR4;\nVAR11 = FUN9(VAR4, VAR5, VAR33,\nVAR33, &VAR13);\nVAR3->VAR55 = VAR11;\nif (VAR13 > 0)\nFUN10(VAR6, \"STR\",\nVAR13, FUN5(VAR13), \"STR\");\nVAR4 += VAR11;\nVAR5 -= VAR11;\nwhile (VAR5 > 0) {\nVAR19 = FUN11(VAR6, VAR8, VAR3,\nVAR4, VAR5, VAR20, VAR22);\nif (VAR19 < 0)\nreturn VAR19;\nVAR4 += VAR19;\nVAR5 -= VAR19;\n}\nfor (VAR11 = 0; VAR11 < VAR9; ++VAR11) {\nVAR14 = VAR3->VAR50[VAR11];\nfor (VAR12 = 0; VAR12 < VAR14->VAR56; ++VAR12) {\nfor (VAR13 = 0; VAR13 < VAR14->VAR56; ++VAR13) {\nif (VAR14->VAR57[VAR13].VAR24.\nVAR58 == VAR12)\nbreak;\n}\nif (VAR13 >= VAR14->VAR56)\nFUN4(VAR6, \"STR\"\n\"STR\", VAR8, VAR20[VAR11], VAR12);\n}\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct usb_device *VAR1, int VAR2,\nstruct usb_host_config *VAR3, unsigned char *VAR4, int VAR5)\n{\nstruct device *VAR6 = &VAR1->VAR1;\nunsigned char *VAR7 = VAR4;\nint VAR8;\nint VAR9, VAR10;\nint VAR11, VAR12, VAR13;\nstruct usb_interface_cache *VAR14;\nunsigned char *VAR15;\nint VAR16;\nstruct usb_descriptor_header *VAR17;\nint VAR18, VAR19;\nu8 VAR20[VAR21], VAR22[VAR21];\nunsigned VAR23 = 0;\nFUN2(&VAR3->VAR24, VAR4, VAR25);\nif (VAR3->VAR24.VAR26 != VAR27 ||\nVAR3->VAR24.VAR28 < VAR25 ||\nVAR3->VAR24.VAR28 > VAR5) {\nFUN3(VAR6, \"STR\"\n\"STR\", VAR2,\nVAR3->VAR24.VAR26, VAR3->VAR24.VAR28);\nreturn -VAR29;\n}\nVAR8 = VAR3->VAR24.VAR30;\nVAR4 += VAR3->VAR24.VAR28;\nVAR5 -= VAR3->VAR24.VAR28;\nVAR9 = VAR10 = VAR3->VAR24.VAR31;\nif (VAR9 > VAR21) {\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR9, VAR21);\nVAR9 = VAR21;\n}\nVAR13 = 0;\nfor ((VAR15 = VAR4, VAR16 = VAR5);\nVAR16 > 0;\n(VAR15 += VAR17->VAR28, VAR16 -= VAR17->VAR28)) {\nif (VAR16 < sizeof(struct VAR32)) {\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR16, FUN5(VAR16));\nbreak;\n}\nVAR17 = (struct VAR32 *) VAR15;\nif ((VAR17->VAR28 > VAR16) || (VAR17->VAR28 < 2)) {\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR17->VAR28);\nbreak;\n}\nif (VAR17->VAR26 == VAR33) {\nstruct usb_interface_descriptor *VAR34;\nint VAR35;\nVAR34 = (struct VAR36 *) VAR17;\nif (VAR34->VAR28 < VAR37) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\", VAR8, VAR34->VAR28);\ncontinue;\n}\nVAR35 = VAR34->VAR38;\nif ((VAR1->VAR39 & VAR40) &&\nVAR13 >= VAR10) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\"\n\"STR\", VAR8, VAR35);\ncontinue;\n}\nif (VAR35 >= VAR10)\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR35, VAR10 - 1);\nfor (VAR11 = 0; VAR11 < VAR13; ++VAR11) {\nif (VAR20[VAR11] == VAR35)\nbreak;\n}\nif (VAR11 < VAR13) {\nif (VAR22[VAR11] < 255)\n++VAR22[VAR11];\n} else if (VAR13 < VAR21) {\nVAR20[VAR13] = VAR35;\nVAR22[VAR13] = 1;\n++VAR13;\n}\n} else if (VAR17->VAR26 ==\nVAR41) {\nstruct usb_interface_assoc_descriptor *VAR34;\nVAR34 = (struct VAR42 *)VAR17;\nif (VAR34->VAR28 < VAR43) {\nFUN4(VAR6,\n\"STR\",\nVAR8, VAR34->VAR28);\ncontinue;\n}\nif (VAR23 == VAR44) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\"\n\"STR\", VAR8);\n} else {\nVAR3->VAR45[VAR23] = VAR34;\nVAR23++;\n}\n} else if (VAR17->VAR26 == VAR46 ||\nVAR17->VAR26 == VAR27)\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR17->VAR26);\n}\t\nVAR5 = VAR15 - VAR4;\nVAR3->VAR24.VAR47 = FUN6(VAR15 - VAR7);\nif (VAR13 != VAR9)\nFUN4(VAR6, \"STR\"\n\"STR\",\nVAR8, VAR13, FUN5(VAR13), VAR10);\nelse if (VAR13 == 0)\nFUN4(VAR6, \"STR\", VAR8);\nVAR3->VAR24.VAR31 = VAR9 = VAR13;\nfor (VAR11 = 0; VAR11 < VAR9; ++VAR11) {\nfor (VAR12 = 0; VAR12 < VAR9; ++VAR12) {\nif (VAR20[VAR12] == VAR11)\nbreak;\n}\nif (VAR12 >= VAR9)\nFUN4(VAR6, \"STR\"\n\"STR\", VAR8, VAR11);\n}\nfor (VAR11 = 0; VAR11 < VAR9; ++VAR11) {\nVAR12 = VAR22[VAR11];\nif (VAR12 > VAR48) {\nFUN4(VAR6, \"STR\"\n\"STR\"\n\"STR\",\nVAR8, VAR20[VAR11], VAR12, VAR48);\nVAR22[VAR11] = VAR12 = VAR48;\n}\nVAR18 = sizeof(*VAR14) + sizeof(struct VAR49) * VAR12;\nVAR3->VAR50[VAR11] = VAR14 = FUN7(VAR18, VAR51);\nif (!VAR14)\nreturn -VAR52;\nFUN8(&VAR14->VAR53);\n}\nVAR3->VAR54 = VAR4;\nVAR11 = FUN9(VAR4, VAR5, VAR33,\nVAR33, &VAR13);\nVAR3->VAR55 = VAR11;\nif (VAR13 > 0)\nFUN10(VAR6, \"STR\",\nVAR13, FUN5(VAR13), \"STR\");\nVAR4 += VAR11;\nVAR5 -= VAR11;\nwhile (VAR5 > 0) {\nVAR19 = FUN11(VAR6, VAR8, VAR3,\nVAR4, VAR5, VAR20, VAR22);\nif (VAR19 < 0)\nreturn VAR19;\nVAR4 += VAR19;\nVAR5 -= VAR19;\n}\nfor (VAR11 = 0; VAR11 < VAR9; ++VAR11) {\nVAR14 = VAR3->VAR50[VAR11];\nfor (VAR12 = 0; VAR12 < VAR14->VAR56; ++VAR12) {\nfor (VAR13 = 0; VAR13 < VAR14->VAR56; ++VAR13) {\nif (VAR14->VAR57[VAR13].VAR24.\nVAR58 == VAR12)\nbreak;\n}\nif (VAR13 >= VAR14->VAR56)\nFUN4(VAR6, \"STR\"\n\"STR\", VAR8, VAR20[VAR11], VAR12);\n}\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int usb_parse_configuration(struct usb_device *dev, int cfgidx,\nstruct usb_host_config *config, unsigned char *buffer, int size)\n{\nstruct device *ddev = &dev->dev;\nunsigned char *buffer0 = buffer;\nint cfgno;\nint nintf, nintf_orig;\nint i, j, n;\nstruct usb_interface_cache *intfc;\nunsigned char *buffer2;\nint size2;\nstruct usb_descriptor_header *header;\nint len, retval;\nu8 inums[USB_MAXINTERFACES], nalts[USB_MAXINTERFACES];\nunsigned iad_num = 0;\nmemcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);\nnintf = nintf_orig = config->desc.bNumInterfaces;\nconfig->desc.bNumInterfaces = 0;\t\nif (config->desc.bDescriptorType != USB_DT_CONFIG ||\nconfig->desc.bLength < USB_DT_CONFIG_SIZE ||\nconfig->desc.bLength > size) {\ndev_err(ddev, \"invalid descriptor for config index %d: \"\n\"type = 0x%X, length = %d\\n\", cfgidx,\nconfig->desc.bDescriptorType, config->desc.bLength);\nreturn -EINVAL;\n}\ncfgno = config->desc.bConfigurationValue;\nbuffer += config->desc.bLength;\nsize -= config->desc.bLength;\nif (nintf > USB_MAXINTERFACES) {\ndev_warn(ddev, \"config %d has too many interfaces: %d, \"\n\"using maximum allowed: %d\\n\",\ncfgno, nintf, USB_MAXINTERFACES);\nnintf = USB_MAXINTERFACES;\n}\nn = 0;\nfor ((buffer2 = buffer, size2 = size);\nsize2 > 0;\n(buffer2 += header->bLength, size2 -= header->bLength)) {\nif (size2 < sizeof(struct usb_descriptor_header)) {\ndev_warn(ddev, \"config %d descriptor has %d excess \"\n\"byte%s, ignoring\\n\",\ncfgno, size2, plural(size2));\nbreak;\n}\nheader = (struct usb_descriptor_header *) buffer2;\nif ((header->bLength > size2) || (header->bLength < 2)) {\ndev_warn(ddev, \"config %d has an invalid descriptor \"\n\"of length %d, skipping remainder of the config\\n\",\ncfgno, header->bLength);\nbreak;\n}\nif (header->bDescriptorType == USB_DT_INTERFACE) {\nstruct usb_interface_descriptor *d;\nint inum;\nd = (struct usb_interface_descriptor *) header;\nif (d->bLength < USB_DT_INTERFACE_SIZE) {\ndev_warn(ddev, \"config %d has an invalid \"\n\"interface descriptor of length %d, \"\n\"skipping\\n\", cfgno, d->bLength);\ncontinue;\n}\ninum = d->bInterfaceNumber;\nif ((dev->quirks & USB_QUIRK_HONOR_BNUMINTERFACES) &&\nn >= nintf_orig) {\ndev_warn(ddev, \"config %d has more interface \"\n\"descriptors, than it declares in \"\n\"bNumInterfaces, ignoring interface \"\n\"number: %d\\n\", cfgno, inum);\ncontinue;\n}\nif (inum >= nintf_orig)\ndev_warn(ddev, \"config %d has an invalid \"\n\"interface number: %d but max is %d\\n\",\ncfgno, inum, nintf_orig - 1);\nfor (i = 0; i < n; ++i) {\nif (inums[i] == inum)\nbreak;\n}\nif (i < n) {\nif (nalts[i] < 255)\n++nalts[i];\n} else if (n < USB_MAXINTERFACES) {\ninums[n] = inum;\nnalts[n] = 1;\n++n;\n}\n} else if (header->bDescriptorType ==\nUSB_DT_INTERFACE_ASSOCIATION) {\nstruct usb_interface_assoc_descriptor *d;\nd = (struct usb_interface_assoc_descriptor *)header;\nif (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {\ndev_warn(ddev,\n\"config %d has an invalid interface association descriptor of length %d, skipping\\n\",\ncfgno, d->bLength);\ncontinue;\n}\nif (iad_num == USB_MAXIADS) {\ndev_warn(ddev, \"found more Interface \"\n\"Association Descriptors \"\n\"than allocated for in \"\n\"configuration %d\\n\", cfgno);\n} else {\nconfig->intf_assoc[iad_num] = d;\niad_num++;\n}\n} else if (header->bDescriptorType == USB_DT_DEVICE ||\nheader->bDescriptorType == USB_DT_CONFIG)\ndev_warn(ddev, \"config %d contains an unexpected \"\n\"descriptor of type 0x%X, skipping\\n\",\ncfgno, header->bDescriptorType);\n}\t\nsize = buffer2 - buffer;\nconfig->desc.wTotalLength = cpu_to_le16(buffer2 - buffer0);\nif (n != nintf)\ndev_warn(ddev, \"config %d has %d interface%s, different from \"\n\"the descriptor's value: %d\\n\",\ncfgno, n, plural(n), nintf_orig);\nelse if (n == 0)\ndev_warn(ddev, \"config %d has no interfaces?\\n\", cfgno);\nconfig->desc.bNumInterfaces = nintf = n;\nfor (i = 0; i < nintf; ++i) {\nfor (j = 0; j < nintf; ++j) {\nif (inums[j] == i)\nbreak;\n}\nif (j >= nintf)\ndev_warn(ddev, \"config %d has no interface number \"\n\"%d\\n\", cfgno, i);\n}\nfor (i = 0; i < nintf; ++i) {\nj = nalts[i];\nif (j > USB_MAXALTSETTING) {\ndev_warn(ddev, \"too many alternate settings for \"\n\"config %d interface %d: %d, \"\n\"using maximum allowed: %d\\n\",\ncfgno, inums[i], j, USB_MAXALTSETTING);\nnalts[i] = j = USB_MAXALTSETTING;\n}\nlen = sizeof(*intfc) + sizeof(struct usb_host_interface) * j;\nconfig->intf_cache[i] = intfc = kzalloc(len, GFP_KERNEL);\nif (!intfc)\nreturn -ENOMEM;\nkref_init(&intfc->ref);\n}\nconfig->extra = buffer;\ni = find_next_descriptor(buffer, size, USB_DT_INTERFACE,\nUSB_DT_INTERFACE, &n);\nconfig->extralen = i;\nif (n > 0)\ndev_dbg(ddev, \"skipped %d descriptor%s after %s\\n\",\nn, plural(n), \"configuration\");\nbuffer += i;\nsize -= i;\nwhile (size > 0) {\nretval = usb_parse_interface(ddev, cfgno, config,\nbuffer, size, inums, nalts);\nif (retval < 0)\nreturn retval;\nbuffer += retval;\nsize -= retval;\n}\nfor (i = 0; i < nintf; ++i) {\nintfc = config->intf_cache[i];\nfor (j = 0; j < intfc->num_altsetting; ++j) {\nfor (n = 0; n < intfc->num_altsetting; ++n) {\nif (intfc->altsetting[n].desc.\nbAlternateSetting == j)\nbreak;\n}\nif (n >= intfc->num_altsetting)\ndev_warn(ddev, \"config %d interface %d has no \"\n\"altsetting %d\\n\", cfgno, inums[i], j);\n}\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int usb_parse_configuration(struct usb_device *dev, int cfgidx,\nstruct usb_host_config *config, unsigned char *buffer, int size)\n{\nstruct device *ddev = &dev->dev;\nunsigned char *buffer0 = buffer;\nint cfgno;\nint nintf, nintf_orig;\nint i, j, n;\nstruct usb_interface_cache *intfc;\nunsigned char *buffer2;\nint size2;\nstruct usb_descriptor_header *header;\nint len, retval;\nu8 inums[USB_MAXINTERFACES], nalts[USB_MAXINTERFACES];\nunsigned iad_num = 0;\nmemcpy(&config->desc, buffer, USB_DT_CONFIG_SIZE);\nif (config->desc.bDescriptorType != USB_DT_CONFIG ||\nconfig->desc.bLength < USB_DT_CONFIG_SIZE ||\nconfig->desc.bLength > size) {\ndev_err(ddev, \"invalid descriptor for config index %d: \"\n\"type = 0x%X, length = %d\\n\", cfgidx,\nconfig->desc.bDescriptorType, config->desc.bLength);\nreturn -EINVAL;\n}\ncfgno = config->desc.bConfigurationValue;\nbuffer += config->desc.bLength;\nsize -= config->desc.bLength;\nnintf = nintf_orig = config->desc.bNumInterfaces;\nif (nintf > USB_MAXINTERFACES) {\ndev_warn(ddev, \"config %d has too many interfaces: %d, \"\n\"using maximum allowed: %d\\n\",\ncfgno, nintf, USB_MAXINTERFACES);\nnintf = USB_MAXINTERFACES;\n}\nn = 0;\nfor ((buffer2 = buffer, size2 = size);\nsize2 > 0;\n(buffer2 += header->bLength, size2 -= header->bLength)) {\nif (size2 < sizeof(struct usb_descriptor_header)) {\ndev_warn(ddev, \"config %d descriptor has %d excess \"\n\"byte%s, ignoring\\n\",\ncfgno, size2, plural(size2));\nbreak;\n}\nheader = (struct usb_descriptor_header *) buffer2;\nif ((header->bLength > size2) || (header->bLength < 2)) {\ndev_warn(ddev, \"config %d has an invalid descriptor \"\n\"of length %d, skipping remainder of the config\\n\",\ncfgno, header->bLength);\nbreak;\n}\nif (header->bDescriptorType == USB_DT_INTERFACE) {\nstruct usb_interface_descriptor *d;\nint inum;\nd = (struct usb_interface_descriptor *) header;\nif (d->bLength < USB_DT_INTERFACE_SIZE) {\ndev_warn(ddev, \"config %d has an invalid \"\n\"interface descriptor of length %d, \"\n\"skipping\\n\", cfgno, d->bLength);\ncontinue;\n}\ninum = d->bInterfaceNumber;\nif ((dev->quirks & USB_QUIRK_HONOR_BNUMINTERFACES) &&\nn >= nintf_orig) {\ndev_warn(ddev, \"config %d has more interface \"\n\"descriptors, than it declares in \"\n\"bNumInterfaces, ignoring interface \"\n\"number: %d\\n\", cfgno, inum);\ncontinue;\n}\nif (inum >= nintf_orig)\ndev_warn(ddev, \"config %d has an invalid \"\n\"interface number: %d but max is %d\\n\",\ncfgno, inum, nintf_orig - 1);\nfor (i = 0; i < n; ++i) {\nif (inums[i] == inum)\nbreak;\n}\nif (i < n) {\nif (nalts[i] < 255)\n++nalts[i];\n} else if (n < USB_MAXINTERFACES) {\ninums[n] = inum;\nnalts[n] = 1;\n++n;\n}\n} else if (header->bDescriptorType ==\nUSB_DT_INTERFACE_ASSOCIATION) {\nstruct usb_interface_assoc_descriptor *d;\nd = (struct usb_interface_assoc_descriptor *)header;\nif (d->bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE) {\ndev_warn(ddev,\n\"config %d has an invalid interface association descriptor of length %d, skipping\\n\",\ncfgno, d->bLength);\ncontinue;\n}\nif (iad_num == USB_MAXIADS) {\ndev_warn(ddev, \"found more Interface \"\n\"Association Descriptors \"\n\"than allocated for in \"\n\"configuration %d\\n\", cfgno);\n} else {\nconfig->intf_assoc[iad_num] = d;\niad_num++;\n}\n} else if (header->bDescriptorType == USB_DT_DEVICE ||\nheader->bDescriptorType == USB_DT_CONFIG)\ndev_warn(ddev, \"config %d contains an unexpected \"\n\"descriptor of type 0x%X, skipping\\n\",\ncfgno, header->bDescriptorType);\n}\t\nsize = buffer2 - buffer;\nconfig->desc.wTotalLength = cpu_to_le16(buffer2 - buffer0);\nif (n != nintf)\ndev_warn(ddev, \"config %d has %d interface%s, different from \"\n\"the descriptor's value: %d\\n\",\ncfgno, n, plural(n), nintf_orig);\nelse if (n == 0)\ndev_warn(ddev, \"config %d has no interfaces?\\n\", cfgno);\nconfig->desc.bNumInterfaces = nintf = n;\nfor (i = 0; i < nintf; ++i) {\nfor (j = 0; j < nintf; ++j) {\nif (inums[j] == i)\nbreak;\n}\nif (j >= nintf)\ndev_warn(ddev, \"config %d has no interface number \"\n\"%d\\n\", cfgno, i);\n}\nfor (i = 0; i < nintf; ++i) {\nj = nalts[i];\nif (j > USB_MAXALTSETTING) {\ndev_warn(ddev, \"too many alternate settings for \"\n\"config %d interface %d: %d, \"\n\"using maximum allowed: %d\\n\",\ncfgno, inums[i], j, USB_MAXALTSETTING);\nnalts[i] = j = USB_MAXALTSETTING;\n}\nlen = sizeof(*intfc) + sizeof(struct usb_host_interface) * j;\nconfig->intf_cache[i] = intfc = kzalloc(len, GFP_KERNEL);\nif (!intfc)\nreturn -ENOMEM;\nkref_init(&intfc->ref);\n}\nconfig->extra = buffer;\ni = find_next_descriptor(buffer, size, USB_DT_INTERFACE,\nUSB_DT_INTERFACE, &n);\nconfig->extralen = i;\nif (n > 0)\ndev_dbg(ddev, \"skipped %d descriptor%s after %s\\n\",\nn, plural(n), \"configuration\");\nbuffer += i;\nsize -= i;\nwhile (size > 0) {\nretval = usb_parse_interface(ddev, cfgno, config,\nbuffer, size, inums, nalts);\nif (retval < 0)\nreturn retval;\nbuffer += retval;\nsize -= retval;\n}\nfor (i = 0; i < nintf; ++i) {\nintfc = config->intf_cache[i];\nfor (j = 0; j < intfc->num_altsetting; ++j) {\nfor (n = 0; n < intfc->num_altsetting; ++n) {\nif (intfc->altsetting[n].desc.\nbAlternateSetting == j)\nbreak;\n}\nif (n >= intfc->num_altsetting)\ndev_warn(ddev, \"config %d interface %d has no \"\n\"altsetting %d\\n\", cfgno, inums[i], j);\n}\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2017-17806": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation to ensure that the underlying cryptographic hash algorithm used in the HMAC implementation is unkeyed.",
        "trigger_condition": "A local attacker able to use the AF_ALG-based hash interface and the SHA-3 hash algorithm can execute a crafted sequence of system calls that encounter a missing SHA-3 initialization, leading to a kernel stack buffer overflow.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the underlying hash algorithm is unkeyed, allowing the HMAC implementation to proceed with potentially vulnerable cryptographic hash algorithms."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure that the underlying hash algorithm used in the HMAC implementation is unkeyed. This involves verifying that the cryptographic hash algorithm does not support keying operations before proceeding with HMAC operations. By adding this check, the code can prevent the vulnerability that could lead to a kernel stack buffer overflow.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-17806, which involves ensuring that the underlying cryptographic hash algorithm used in the HMAC implementation is unkeyed. In the original code snippet, there is no validation to check if the underlying hash algorithm is unkeyed, which can lead to a security issue where a local attacker could exploit the system calls involving the SHA-3 hash algorithm to cause a kernel stack buffer overflow.\n\nBy adding the check `if (crypto_shash_alg_has_setkey(salg))` and the corresponding `goto out_put_alg;` statement, the modified code ensures that the HMAC implementation only proceeds if the underlying hash algorithm is unkeyed. This check helps prevent the vulnerability by ensuring that the cryptographic hash algorithm used is suitable for HMAC operations and does not allow for potential exploitation leading to a stack buffer overflow.\n\nTherefore, the modification is necessary to enhance the security of the HMAC implementation and prevent the described vulnerability from being exploited in the Linux kernel.",
      "GPT_purpose": "Create an HMAC instance using a specified cryptographic hash algorithm.",
      "GPT_function": "\n1. Check attribute type for symmetric hash algorithms.\n2. Allocate and initialize an instance for HMAC.\n3. Register the HMAC instance with the crypto template.",
      "CVE_id": "CVE-2017-17806",
      "code_before_change": "static int hmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tstruct shash_alg *salg;\n\tint err;\n\tint ds;\n\tint ss;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\n\terr = -EINVAL;\n\tds = salg->digestsize;\n\tss = salg->statesize;\n\talg = &salg->base;\n\tif (ds > alg->cra_blocksize ||\n\t    ss < alg->cra_blocksize)\n\t\tgoto out_put_alg;\n\n\tinst = shash_alloc_instance(\"hmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_shash_spawn(shash_instance_ctx(inst), salg,\n\t\t\t\t      shash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tss = ALIGN(ss, alg->cra_alignmask + 1);\n\tinst->alg.digestsize = ds;\n\tinst->alg.statesize = ss;\n\n\tinst->alg.base.cra_ctxsize = sizeof(struct hmac_ctx) +\n\t\t\t\t     ALIGN(ss * 2, crypto_tfm_ctx_alignment());\n\n\tinst->alg.base.cra_init = hmac_init_tfm;\n\tinst->alg.base.cra_exit = hmac_exit_tfm;\n\n\tinst->alg.init = hmac_init;\n\tinst->alg.update = hmac_update;\n\tinst->alg.final = hmac_final;\n\tinst->alg.finup = hmac_finup;\n\tinst->alg.export = hmac_export;\n\tinst->alg.import = hmac_import;\n\tinst->alg.setkey = hmac_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
      "code_after_change": "static int hmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\n\tstruct shash_instance *inst;\n\tstruct crypto_alg *alg;\n\tstruct shash_alg *salg;\n\tint err;\n\tint ds;\n\tint ss;\n\n\terr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\n\tif (err)\n\t\treturn err;\n\n\tsalg = shash_attr_alg(tb[1], 0, 0);\n\tif (IS_ERR(salg))\n\t\treturn PTR_ERR(salg);\n\talg = &salg->base;\n\n\t/* The underlying hash algorithm must be unkeyed */\n\terr = -EINVAL;\n\tif (crypto_shash_alg_has_setkey(salg))\n\t\tgoto out_put_alg;\n\n\tds = salg->digestsize;\n\tss = salg->statesize;\n\tif (ds > alg->cra_blocksize ||\n\t    ss < alg->cra_blocksize)\n\t\tgoto out_put_alg;\n\n\tinst = shash_alloc_instance(\"hmac\", alg);\n\terr = PTR_ERR(inst);\n\tif (IS_ERR(inst))\n\t\tgoto out_put_alg;\n\n\terr = crypto_init_shash_spawn(shash_instance_ctx(inst), salg,\n\t\t\t\t      shash_crypto_instance(inst));\n\tif (err)\n\t\tgoto out_free_inst;\n\n\tinst->alg.base.cra_priority = alg->cra_priority;\n\tinst->alg.base.cra_blocksize = alg->cra_blocksize;\n\tinst->alg.base.cra_alignmask = alg->cra_alignmask;\n\n\tss = ALIGN(ss, alg->cra_alignmask + 1);\n\tinst->alg.digestsize = ds;\n\tinst->alg.statesize = ss;\n\n\tinst->alg.base.cra_ctxsize = sizeof(struct hmac_ctx) +\n\t\t\t\t     ALIGN(ss * 2, crypto_tfm_ctx_alignment());\n\n\tinst->alg.base.cra_init = hmac_init_tfm;\n\tinst->alg.base.cra_exit = hmac_exit_tfm;\n\n\tinst->alg.init = hmac_init;\n\tinst->alg.update = hmac_update;\n\tinst->alg.final = hmac_final;\n\tinst->alg.finup = hmac_finup;\n\tinst->alg.export = hmac_export;\n\tinst->alg.import = hmac_import;\n\tinst->alg.setkey = hmac_setkey;\n\n\terr = shash_register_instance(tmpl, inst);\n\tif (err) {\nout_free_inst:\n\t\tshash_free_instance(shash_crypto_instance(inst));\n\t}\n\nout_put_alg:\n\tcrypto_mod_put(alg);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\talg = &salg->base;",
          "\t/* The underlying hash algorithm must be unkeyed */",
          "\tif (crypto_shash_alg_has_setkey(salg))",
          "\t\tgoto out_put_alg;",
          ""
        ],
        "deleted": [
          "\talg = &salg->base;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation to ensure that the underlying cryptographic hash algorithm used in the HMAC implementation is unkeyed.",
      "trigger_condition": "A local attacker able to use the AF_ALG-based hash interface and the SHA-3 hash algorithm can execute a crafted sequence of system calls that encounter a missing SHA-3 initialization, leading to a kernel stack buffer overflow.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the underlying hash algorithm is unkeyed, allowing the HMAC implementation to proceed with potentially vulnerable cryptographic hash algorithms.",
      "id": 16,
      "code_after_change_normalized": "static int FUN1(struct crypto_template *VAR1, struct rtattr **VAR2)\n{\nstruct shash_instance *VAR3;\nstruct crypto_alg *VAR4;\nstruct shash_alg *VAR5;\nint VAR6;\nint VAR7;\nint VAR8;\nVAR6 = FUN2(VAR2, VAR9);\nif (VAR6)\nreturn VAR6;\nVAR5 = FUN3(VAR2[1], 0, 0);\nif (FUN4(VAR5))\nreturn FUN5(VAR5);\nVAR4 = &VAR5->VAR10;\nVAR6 = -VAR11;\nif (FUN6(VAR5))\ngoto VAR12;\nVAR7 = VAR5->VAR13;\nVAR8 = VAR5->VAR14;\nif (VAR7 > VAR4->VAR15 ||\nVAR8 < VAR4->VAR15)\ngoto VAR12;\nVAR3 = FUN7(\"STR\", VAR4);\nVAR6 = FUN5(VAR3);\nif (FUN4(VAR3))\ngoto VAR12;\nVAR6 = FUN8(FUN9(VAR3), VAR5,\nFUN10(VAR3));\nif (VAR6)\ngoto VAR16;\nVAR3->VAR4.VAR10.VAR17 = VAR4->VAR17;\nVAR3->VAR4.VAR10.VAR15 = VAR4->VAR15;\nVAR3->VAR4.VAR10.VAR18 = VAR4->VAR18;\nVAR8 = FUN11(VAR8, VAR4->VAR18 + 1);\nVAR3->VAR4.VAR13 = VAR7;\nVAR3->VAR4.VAR14 = VAR8;\nVAR3->VAR4.VAR10.VAR19 = sizeof(struct VAR20) +\nFUN11(VAR8 * 2, FUN12());\nVAR3->VAR4.VAR10.VAR21 = VAR22;\nVAR3->VAR4.VAR10.VAR23 = VAR24;\nVAR3->VAR4.VAR25 = VAR26;\nVAR3->VAR4.VAR27 = VAR28;\nVAR3->VAR4.final = VAR29;\nVAR3->VAR4.VAR30 = VAR31;\nVAR3->VAR4.export = VAR32;\nVAR3->VAR4.VAR33 = VAR34;\nVAR3->VAR4.VAR35 = VAR36;\nVAR6 = FUN13(VAR1, VAR3);\nif (VAR6) {\nVAR16:\nFUN14(FUN10(VAR3));\n}\nVAR12:\nFUN15(VAR4);\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct crypto_template *VAR1, struct rtattr **VAR2)\n{\nstruct shash_instance *VAR3;\nstruct crypto_alg *VAR4;\nstruct shash_alg *VAR5;\nint VAR6;\nint VAR7;\nint VAR8;\nVAR6 = FUN2(VAR2, VAR9);\nif (VAR6)\nreturn VAR6;\nVAR5 = FUN3(VAR2[1], 0, 0);\nif (FUN4(VAR5))\nreturn FUN5(VAR5);\nVAR6 = -VAR10;\nVAR7 = VAR5->VAR11;\nVAR8 = VAR5->VAR12;\nVAR4 = &VAR5->VAR13;\nif (VAR7 > VAR4->VAR14 ||\nVAR8 < VAR4->VAR14)\ngoto VAR15;\nVAR3 = FUN6(\"STR\", VAR4);\nVAR6 = FUN5(VAR3);\nif (FUN4(VAR3))\ngoto VAR15;\nVAR6 = FUN7(FUN8(VAR3), VAR5,\nFUN9(VAR3));\nif (VAR6)\ngoto VAR16;\nVAR3->VAR4.VAR13.VAR17 = VAR4->VAR17;\nVAR3->VAR4.VAR13.VAR14 = VAR4->VAR14;\nVAR3->VAR4.VAR13.VAR18 = VAR4->VAR18;\nVAR8 = FUN10(VAR8, VAR4->VAR18 + 1);\nVAR3->VAR4.VAR11 = VAR7;\nVAR3->VAR4.VAR12 = VAR8;\nVAR3->VAR4.VAR13.VAR19 = sizeof(struct VAR20) +\nFUN10(VAR8 * 2, FUN11());\nVAR3->VAR4.VAR13.VAR21 = VAR22;\nVAR3->VAR4.VAR13.VAR23 = VAR24;\nVAR3->VAR4.VAR25 = VAR26;\nVAR3->VAR4.VAR27 = VAR28;\nVAR3->VAR4.final = VAR29;\nVAR3->VAR4.VAR30 = VAR31;\nVAR3->VAR4.export = VAR32;\nVAR3->VAR4.VAR33 = VAR34;\nVAR3->VAR4.VAR35 = VAR36;\nVAR6 = FUN12(VAR1, VAR3);\nif (VAR6) {\nVAR16:\nFUN13(FUN9(VAR3));\n}\nVAR15:\nFUN14(VAR4);\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static int hmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\nstruct shash_instance *inst;\nstruct crypto_alg *alg;\nstruct shash_alg *salg;\nint err;\nint ds;\nint ss;\nerr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\nif (err)\nreturn err;\nsalg = shash_attr_alg(tb[1], 0, 0);\nif (IS_ERR(salg))\nreturn PTR_ERR(salg);\nalg = &salg->base;\nerr = -EINVAL;\nif (crypto_shash_alg_has_setkey(salg))\ngoto out_put_alg;\nds = salg->digestsize;\nss = salg->statesize;\nif (ds > alg->cra_blocksize ||\nss < alg->cra_blocksize)\ngoto out_put_alg;\ninst = shash_alloc_instance(\"hmac\", alg);\nerr = PTR_ERR(inst);\nif (IS_ERR(inst))\ngoto out_put_alg;\nerr = crypto_init_shash_spawn(shash_instance_ctx(inst), salg,\nshash_crypto_instance(inst));\nif (err)\ngoto out_free_inst;\ninst->alg.base.cra_priority = alg->cra_priority;\ninst->alg.base.cra_blocksize = alg->cra_blocksize;\ninst->alg.base.cra_alignmask = alg->cra_alignmask;\nss = ALIGN(ss, alg->cra_alignmask + 1);\ninst->alg.digestsize = ds;\ninst->alg.statesize = ss;\ninst->alg.base.cra_ctxsize = sizeof(struct hmac_ctx) +\nALIGN(ss * 2, crypto_tfm_ctx_alignment());\ninst->alg.base.cra_init = hmac_init_tfm;\ninst->alg.base.cra_exit = hmac_exit_tfm;\ninst->alg.init = hmac_init;\ninst->alg.update = hmac_update;\ninst->alg.final = hmac_final;\ninst->alg.finup = hmac_finup;\ninst->alg.export = hmac_export;\ninst->alg.import = hmac_import;\ninst->alg.setkey = hmac_setkey;\nerr = shash_register_instance(tmpl, inst);\nif (err) {\nout_free_inst:\nshash_free_instance(shash_crypto_instance(inst));\n}\nout_put_alg:\ncrypto_mod_put(alg);\nreturn err;\n}\n",
      "code_before_change_raw": "static int hmac_create(struct crypto_template *tmpl, struct rtattr **tb)\n{\nstruct shash_instance *inst;\nstruct crypto_alg *alg;\nstruct shash_alg *salg;\nint err;\nint ds;\nint ss;\nerr = crypto_check_attr_type(tb, CRYPTO_ALG_TYPE_SHASH);\nif (err)\nreturn err;\nsalg = shash_attr_alg(tb[1], 0, 0);\nif (IS_ERR(salg))\nreturn PTR_ERR(salg);\nerr = -EINVAL;\nds = salg->digestsize;\nss = salg->statesize;\nalg = &salg->base;\nif (ds > alg->cra_blocksize ||\nss < alg->cra_blocksize)\ngoto out_put_alg;\ninst = shash_alloc_instance(\"hmac\", alg);\nerr = PTR_ERR(inst);\nif (IS_ERR(inst))\ngoto out_put_alg;\nerr = crypto_init_shash_spawn(shash_instance_ctx(inst), salg,\nshash_crypto_instance(inst));\nif (err)\ngoto out_free_inst;\ninst->alg.base.cra_priority = alg->cra_priority;\ninst->alg.base.cra_blocksize = alg->cra_blocksize;\ninst->alg.base.cra_alignmask = alg->cra_alignmask;\nss = ALIGN(ss, alg->cra_alignmask + 1);\ninst->alg.digestsize = ds;\ninst->alg.statesize = ss;\ninst->alg.base.cra_ctxsize = sizeof(struct hmac_ctx) +\nALIGN(ss * 2, crypto_tfm_ctx_alignment());\ninst->alg.base.cra_init = hmac_init_tfm;\ninst->alg.base.cra_exit = hmac_exit_tfm;\ninst->alg.init = hmac_init;\ninst->alg.update = hmac_update;\ninst->alg.final = hmac_final;\ninst->alg.finup = hmac_finup;\ninst->alg.export = hmac_export;\ninst->alg.import = hmac_import;\ninst->alg.setkey = hmac_setkey;\nerr = shash_register_instance(tmpl, inst);\nif (err) {\nout_free_inst:\nshash_free_instance(shash_crypto_instance(inst));\n}\nout_put_alg:\ncrypto_mod_put(alg);\nreturn err;\n}\n"
    }
  ],
  "CVE-2017-18551": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for the block size before accessing the data->block array.",
        "trigger_condition": "When the block size exceeds the defined maximum value (I2C_SMBUS_BLOCK_MAX), an out-of-bounds write vulnerability occurs.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the block size exceeds the maximum allowed value before accessing the data->block array, leading to a potential out-of-bounds write vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to validate the block size against the maximum allowed value before accessing the data->block array. By adding a check to ensure that the block size does not exceed the defined maximum value, the code can prevent the out-of-bounds write vulnerability. In this case, the solution involves adding a validation step to check if the block size is within the acceptable range before accessing the data->block array.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2017-18551, which involves an out-of-bounds write in the function `i2c_smbus_xfer_emulated`. By adding the following lines to the code snippet, the vulnerability can be mitigated:\n\n```c\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\tdev_err(&adapter->dev, \"Invalid block %s size %d\\n\",\n\t\tread_write == I2C_SMBUS_READ ? \"read\" : \"write\",\n\t\tdata->block[0]);\n\treturn -EINVAL;\n}\n```\n\nThis modification ensures that the block size is properly checked before accessing the `data->block` array to prevent writing beyond its bounds. By validating the block size against `I2C_SMBUS_BLOCK_MAX`, the code can avoid the out-of-bounds write vulnerability and improve the overall security of the function.",
      "GPT_purpose": "Emulates SMBus transactions for I2C communication.",
      "GPT_function": "\n1. Generate a series of messages for I2C communication.\n2. Handle different types of I2C transactions based on the size parameter.\n3. Compute and check PEC (Packet Error Checking) for I2C messages.\n4. Extract and store data from I2C messages based on the transaction type.",
      "CVE_id": "CVE-2017-18551",
      "code_before_change": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\n\t\t\t\t   unsigned short flags,\n\t\t\t\t   char read_write, u8 command, int size,\n\t\t\t\t   union i2c_smbus_data *data)\n{\n\t/* So we need to generate a series of msgs. In the case of writing, we\n\t  need to use only one message; when reading, we need two. We initialize\n\t  most things with sane defaults, to keep the code below somewhat\n\t  simpler. */\n\tunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\n\tunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\n\tint num = read_write == I2C_SMBUS_READ ? 2 : 1;\n\tint i;\n\tu8 partial_pec = 0;\n\tint status;\n\tstruct i2c_msg msg[2] = {\n\t\t{\n\t\t\t.addr = addr,\n\t\t\t.flags = flags,\n\t\t\t.len = 1,\n\t\t\t.buf = msgbuf0,\n\t\t}, {\n\t\t\t.addr = addr,\n\t\t\t.flags = flags | I2C_M_RD,\n\t\t\t.len = 0,\n\t\t\t.buf = msgbuf1,\n\t\t},\n\t};\n\n\tmsgbuf0[0] = command;\n\tswitch (size) {\n\tcase I2C_SMBUS_QUICK:\n\t\tmsg[0].len = 0;\n\t\t/* Special case: The read/write field is used as data */\n\t\tmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\n\t\t\t\t\tI2C_M_RD : 0);\n\t\tnum = 1;\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\t/* Special case: only a read! */\n\t\t\tmsg[0].flags = I2C_M_RD | flags;\n\t\t\tnum = 1;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 1;\n\t\telse {\n\t\t\tmsg[0].len = 2;\n\t\t\tmsgbuf0[1] = data->byte;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_WORD_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 2;\n\t\telse {\n\t\t\tmsg[0].len = 3;\n\t\t\tmsgbuf0[1] = data->word & 0xff;\n\t\t\tmsgbuf0[2] = data->word >> 8;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_PROC_CALL:\n\t\tnum = 2; /* Special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tmsg[0].len = 3;\n\t\tmsg[1].len = 2;\n\t\tmsgbuf0[1] = data->word & 0xff;\n\t\tmsgbuf0[2] = data->word >> 8;\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t\t   the underlying bus driver */\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 2;\n\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\n\t\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\t\tdata->block[0]);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\tnum = 2; /* Another special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\tdata->block[0]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmsg[0].len = data->block[0] + 2;\n\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t   the underlying bus driver */\n\t\tbreak;\n\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].len = data->block[0];\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 1;\n\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {\n\t\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\t\tdata->block[0]);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tfor (i = 1; i <= data->block[0]; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i];\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\ti = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n\t\t\t\t      && size != I2C_SMBUS_I2C_BLOCK_DATA);\n\tif (i) {\n\t\t/* Compute PEC if first message is a write */\n\t\tif (!(msg[0].flags & I2C_M_RD)) {\n\t\t\tif (num == 1) /* Write only */\n\t\t\t\ti2c_smbus_add_pec(&msg[0]);\n\t\t\telse /* Write followed by read */\n\t\t\t\tpartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n\t\t}\n\t\t/* Ask for PEC if last message is a read */\n\t\tif (msg[num-1].flags & I2C_M_RD)\n\t\t\tmsg[num-1].len++;\n\t}\n\n\tstatus = i2c_transfer(adapter, msg, num);\n\tif (status < 0)\n\t\treturn status;\n\n\t/* Check PEC if last message is a read */\n\tif (i && (msg[num-1].flags & I2C_M_RD)) {\n\t\tstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\n\t\tif (status < 0)\n\t\t\treturn status;\n\t}\n\n\tif (read_write == I2C_SMBUS_READ)\n\t\tswitch (size) {\n\t\tcase I2C_SMBUS_BYTE:\n\t\t\tdata->byte = msgbuf0[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BYTE_DATA:\n\t\t\tdata->byte = msgbuf1[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_WORD_DATA:\n\t\tcase I2C_SMBUS_PROC_CALL:\n\t\t\tdata->word = msgbuf1[0] | (msgbuf1[1] << 8);\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\t\tfor (i = 0; i < data->block[0]; i++)\n\t\t\t\tdata->block[i+1] = msgbuf1[i];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\t\tfor (i = 0; i < msgbuf1[0] + 1; i++)\n\t\t\t\tdata->block[i] = msgbuf1[i];\n\t\t\tbreak;\n\t\t}\n\treturn 0;\n}",
      "code_after_change": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\n\t\t\t\t   unsigned short flags,\n\t\t\t\t   char read_write, u8 command, int size,\n\t\t\t\t   union i2c_smbus_data *data)\n{\n\t/* So we need to generate a series of msgs. In the case of writing, we\n\t  need to use only one message; when reading, we need two. We initialize\n\t  most things with sane defaults, to keep the code below somewhat\n\t  simpler. */\n\tunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\n\tunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\n\tint num = read_write == I2C_SMBUS_READ ? 2 : 1;\n\tint i;\n\tu8 partial_pec = 0;\n\tint status;\n\tstruct i2c_msg msg[2] = {\n\t\t{\n\t\t\t.addr = addr,\n\t\t\t.flags = flags,\n\t\t\t.len = 1,\n\t\t\t.buf = msgbuf0,\n\t\t}, {\n\t\t\t.addr = addr,\n\t\t\t.flags = flags | I2C_M_RD,\n\t\t\t.len = 0,\n\t\t\t.buf = msgbuf1,\n\t\t},\n\t};\n\n\tmsgbuf0[0] = command;\n\tswitch (size) {\n\tcase I2C_SMBUS_QUICK:\n\t\tmsg[0].len = 0;\n\t\t/* Special case: The read/write field is used as data */\n\t\tmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\n\t\t\t\t\tI2C_M_RD : 0);\n\t\tnum = 1;\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\t/* Special case: only a read! */\n\t\t\tmsg[0].flags = I2C_M_RD | flags;\n\t\t\tnum = 1;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 1;\n\t\telse {\n\t\t\tmsg[0].len = 2;\n\t\t\tmsgbuf0[1] = data->byte;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_WORD_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 2;\n\t\telse {\n\t\t\tmsg[0].len = 3;\n\t\t\tmsgbuf0[1] = data->word & 0xff;\n\t\t\tmsgbuf0[2] = data->word >> 8;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_PROC_CALL:\n\t\tnum = 2; /* Special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tmsg[0].len = 3;\n\t\tmsg[1].len = 2;\n\t\tmsgbuf0[1] = data->word & 0xff;\n\t\tmsgbuf0[2] = data->word >> 8;\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t\t   the underlying bus driver */\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 2;\n\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\n\t\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\t\tdata->block[0]);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\tnum = 2; /* Another special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\tdata->block[0]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmsg[0].len = data->block[0] + 2;\n\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t   the underlying bus driver */\n\t\tbreak;\n\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\t\t\tdev_err(&adapter->dev, \"Invalid block %s size %d\\n\",\n\t\t\t\tread_write == I2C_SMBUS_READ ? \"read\" : \"write\",\n\t\t\t\tdata->block[0]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].len = data->block[0];\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 1;\n\t\t\tfor (i = 1; i <= data->block[0]; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i];\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\ti = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n\t\t\t\t      && size != I2C_SMBUS_I2C_BLOCK_DATA);\n\tif (i) {\n\t\t/* Compute PEC if first message is a write */\n\t\tif (!(msg[0].flags & I2C_M_RD)) {\n\t\t\tif (num == 1) /* Write only */\n\t\t\t\ti2c_smbus_add_pec(&msg[0]);\n\t\t\telse /* Write followed by read */\n\t\t\t\tpartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n\t\t}\n\t\t/* Ask for PEC if last message is a read */\n\t\tif (msg[num-1].flags & I2C_M_RD)\n\t\t\tmsg[num-1].len++;\n\t}\n\n\tstatus = i2c_transfer(adapter, msg, num);\n\tif (status < 0)\n\t\treturn status;\n\n\t/* Check PEC if last message is a read */\n\tif (i && (msg[num-1].flags & I2C_M_RD)) {\n\t\tstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\n\t\tif (status < 0)\n\t\t\treturn status;\n\t}\n\n\tif (read_write == I2C_SMBUS_READ)\n\t\tswitch (size) {\n\t\tcase I2C_SMBUS_BYTE:\n\t\t\tdata->byte = msgbuf0[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BYTE_DATA:\n\t\t\tdata->byte = msgbuf1[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_WORD_DATA:\n\t\tcase I2C_SMBUS_PROC_CALL:\n\t\t\tdata->word = msgbuf1[0] | (msgbuf1[1] << 8);\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\t\tfor (i = 0; i < data->block[0]; i++)\n\t\t\t\tdata->block[i+1] = msgbuf1[i];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\t\tfor (i = 0; i < msgbuf1[0] + 1; i++)\n\t\t\t\tdata->block[i] = msgbuf1[i];\n\t\t\tbreak;\n\t\t}\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {",
          "\t\t\tdev_err(&adapter->dev, \"Invalid block %s size %d\\n\",",
          "\t\t\t\tread_write == I2C_SMBUS_READ ? \"read\" : \"write\",",
          "\t\t\t\tdata->block[0]);",
          "\t\t\treturn -EINVAL;",
          "\t\t}",
          ""
        ],
        "deleted": [
          "\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {",
          "\t\t\t\tdev_err(&adapter->dev,",
          "\t\t\t\t\t\"Invalid block write size %d\\n\",",
          "\t\t\t\t\tdata->block[0]);",
          "\t\t\t\treturn -EINVAL;",
          "\t\t\t}"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for the block size before accessing the data->block array.",
      "trigger_condition": "When the block size exceeds the defined maximum value (I2C_SMBUS_BLOCK_MAX), an out-of-bounds write vulnerability occurs.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the block size exceeds the maximum allowed value before accessing the data->block array, leading to a potential out-of-bounds write vulnerability.",
      "id": 17,
      "code_after_change_normalized": "static s32 FUN1(struct i2c_adapter *VAR1, u16 VAR2,\nunsigned short VAR3,\nchar VAR4, u8 VAR5, int VAR6,\nunion i2c_smbus_data *VAR7)\n{\nunsigned char VAR8[VAR9+3];\nunsigned char VAR10[VAR9+2];\nint VAR11 = VAR4 == VAR12 ? 2 : 1;\nint VAR13;\nu8 VAR14 = 0;\nint VAR15;\nstruct i2c_msg VAR16[2] = {\n{\n.VAR2 = VAR2,\n.VAR3 = VAR3,\n.VAR17 = 1,\n.VAR18 = VAR8,\n}, {\n.VAR2 = VAR2,\n.VAR3 = VAR3 | VAR19,\n.VAR17 = 0,\n.VAR18 = VAR10,\n},\n};\nVAR8[0] = VAR5;\nswitch (VAR6) {\ncase VAR20:\nVAR16[0].VAR17 = 0;\nVAR16[0].VAR3 = VAR3 | (VAR4 == VAR12 ?\nVAR19 : 0);\nVAR11 = 1;\nbreak;\ncase VAR21:\nif (VAR4 == VAR12) {\nVAR16[0].VAR3 = VAR19 | VAR3;\nVAR11 = 1;\n}\nbreak;\ncase VAR22:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 1;\nelse {\nVAR16[0].VAR17 = 2;\nVAR8[1] = VAR7->VAR23;\n}\nbreak;\ncase VAR24:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 2;\nelse {\nVAR16[0].VAR17 = 3;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\n}\nbreak;\ncase VAR27:\nVAR11 = 2; \nVAR4 = VAR12;\nVAR16[0].VAR17 = 3;\nVAR16[1].VAR17 = 2;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\nbreak;\ncase VAR28:\nif (VAR4 == VAR12) {\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nif (VAR16[0].VAR17 > VAR9 + 2) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\n}\nbreak;\ncase VAR33:\nVAR11 = 2; \nVAR4 = VAR12;\nif (VAR7->VAR30[0] > VAR9) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \nbreak;\ncase VAR34:\nif (VAR7->VAR30[0] > VAR9) {\nFUN2(&VAR1->VAR31, \"STR\",\nVAR4 == VAR12 ? \"STR\" : \"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nif (VAR4 == VAR12) {\nVAR16[1].VAR17 = VAR7->VAR30[0];\n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 1;\nfor (VAR13 = 1; VAR13 <= VAR7->VAR30[0]; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13];\n}\nbreak;\ndefault:\nFUN2(&VAR1->VAR31, \"STR\", VAR6);\nreturn -VAR35;\n}\nVAR13 = ((VAR3 & VAR36) && VAR6 != VAR20\n&& VAR6 != VAR34);\nif (VAR13) {\nif (!(VAR16[0].VAR3 & VAR19)) {\nif (VAR11 == 1) \nFUN3(&VAR16[0]);\nelse \nVAR14 = FUN4(0, &VAR16[0]);\n}\nif (VAR16[VAR11-1].VAR3 & VAR19)\nVAR16[VAR11-1].VAR17++;\n}\nVAR15 = FUN5(VAR1, VAR16, VAR11);\nif (VAR15 < 0)\nreturn VAR15;\nif (VAR13 && (VAR16[VAR11-1].VAR3 & VAR19)) {\nVAR15 = FUN6(VAR14, &VAR16[VAR11-1]);\nif (VAR15 < 0)\nreturn VAR15;\n}\nif (VAR4 == VAR12)\nswitch (VAR6) {\ncase VAR21:\nVAR7->VAR23 = VAR8[0];\nbreak;\ncase VAR22:\nVAR7->VAR23 = VAR10[0];\nbreak;\ncase VAR24:\ncase VAR27:\nVAR7->VAR25 = VAR10[0] | (VAR10[1] << 8);\nbreak;\ncase VAR34:\nfor (VAR13 = 0; VAR13 < VAR7->VAR30[0]; VAR13++)\nVAR7->VAR30[VAR13+1] = VAR10[VAR13];\nbreak;\ncase VAR28:\ncase VAR33:\nfor (VAR13 = 0; VAR13 < VAR10[0] + 1; VAR13++)\nVAR7->VAR30[VAR13] = VAR10[VAR13];\nbreak;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static s32 FUN1(struct i2c_adapter *VAR1, u16 VAR2,\nunsigned short VAR3,\nchar VAR4, u8 VAR5, int VAR6,\nunion i2c_smbus_data *VAR7)\n{\nunsigned char VAR8[VAR9+3];\nunsigned char VAR10[VAR9+2];\nint VAR11 = VAR4 == VAR12 ? 2 : 1;\nint VAR13;\nu8 VAR14 = 0;\nint VAR15;\nstruct i2c_msg VAR16[2] = {\n{\n.VAR2 = VAR2,\n.VAR3 = VAR3,\n.VAR17 = 1,\n.VAR18 = VAR8,\n}, {\n.VAR2 = VAR2,\n.VAR3 = VAR3 | VAR19,\n.VAR17 = 0,\n.VAR18 = VAR10,\n},\n};\nVAR8[0] = VAR5;\nswitch (VAR6) {\ncase VAR20:\nVAR16[0].VAR17 = 0;\nVAR16[0].VAR3 = VAR3 | (VAR4 == VAR12 ?\nVAR19 : 0);\nVAR11 = 1;\nbreak;\ncase VAR21:\nif (VAR4 == VAR12) {\nVAR16[0].VAR3 = VAR19 | VAR3;\nVAR11 = 1;\n}\nbreak;\ncase VAR22:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 1;\nelse {\nVAR16[0].VAR17 = 2;\nVAR8[1] = VAR7->VAR23;\n}\nbreak;\ncase VAR24:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 2;\nelse {\nVAR16[0].VAR17 = 3;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\n}\nbreak;\ncase VAR27:\nVAR11 = 2; \nVAR4 = VAR12;\nVAR16[0].VAR17 = 3;\nVAR16[1].VAR17 = 2;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\nbreak;\ncase VAR28:\nif (VAR4 == VAR12) {\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nif (VAR16[0].VAR17 > VAR9 + 2) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\n}\nbreak;\ncase VAR33:\nVAR11 = 2; \nVAR4 = VAR12;\nif (VAR7->VAR30[0] > VAR9) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \nbreak;\ncase VAR34:\nif (VAR4 == VAR12) {\nVAR16[1].VAR17 = VAR7->VAR30[0];\n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 1;\nif (VAR16[0].VAR17 > VAR9 + 1) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nfor (VAR13 = 1; VAR13 <= VAR7->VAR30[0]; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13];\n}\nbreak;\ndefault:\nFUN2(&VAR1->VAR31, \"STR\", VAR6);\nreturn -VAR35;\n}\nVAR13 = ((VAR3 & VAR36) && VAR6 != VAR20\n&& VAR6 != VAR34);\nif (VAR13) {\nif (!(VAR16[0].VAR3 & VAR19)) {\nif (VAR11 == 1) \nFUN3(&VAR16[0]);\nelse \nVAR14 = FUN4(0, &VAR16[0]);\n}\nif (VAR16[VAR11-1].VAR3 & VAR19)\nVAR16[VAR11-1].VAR17++;\n}\nVAR15 = FUN5(VAR1, VAR16, VAR11);\nif (VAR15 < 0)\nreturn VAR15;\nif (VAR13 && (VAR16[VAR11-1].VAR3 & VAR19)) {\nVAR15 = FUN6(VAR14, &VAR16[VAR11-1]);\nif (VAR15 < 0)\nreturn VAR15;\n}\nif (VAR4 == VAR12)\nswitch (VAR6) {\ncase VAR21:\nVAR7->VAR23 = VAR8[0];\nbreak;\ncase VAR22:\nVAR7->VAR23 = VAR10[0];\nbreak;\ncase VAR24:\ncase VAR27:\nVAR7->VAR25 = VAR10[0] | (VAR10[1] << 8);\nbreak;\ncase VAR34:\nfor (VAR13 = 0; VAR13 < VAR7->VAR30[0]; VAR13++)\nVAR7->VAR30[VAR13+1] = VAR10[VAR13];\nbreak;\ncase VAR28:\ncase VAR33:\nfor (VAR13 = 0; VAR13 < VAR10[0] + 1; VAR13++)\nVAR7->VAR30[VAR13] = VAR10[VAR13];\nbreak;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\nunsigned short flags,\nchar read_write, u8 command, int size,\nunion i2c_smbus_data *data)\n{\nunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\nunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\nint num = read_write == I2C_SMBUS_READ ? 2 : 1;\nint i;\nu8 partial_pec = 0;\nint status;\nstruct i2c_msg msg[2] = {\n{\n.addr = addr,\n.flags = flags,\n.len = 1,\n.buf = msgbuf0,\n}, {\n.addr = addr,\n.flags = flags | I2C_M_RD,\n.len = 0,\n.buf = msgbuf1,\n},\n};\nmsgbuf0[0] = command;\nswitch (size) {\ncase I2C_SMBUS_QUICK:\nmsg[0].len = 0;\nmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\nI2C_M_RD : 0);\nnum = 1;\nbreak;\ncase I2C_SMBUS_BYTE:\nif (read_write == I2C_SMBUS_READ) {\nmsg[0].flags = I2C_M_RD | flags;\nnum = 1;\n}\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 1;\nelse {\nmsg[0].len = 2;\nmsgbuf0[1] = data->byte;\n}\nbreak;\ncase I2C_SMBUS_WORD_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 2;\nelse {\nmsg[0].len = 3;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\n}\nbreak;\ncase I2C_SMBUS_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nmsg[0].len = 3;\nmsg[1].len = 2;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \n} else {\nmsg[0].len = data->block[0] + 2;\nif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\n}\nbreak;\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nmsg[0].len = data->block[0] + 2;\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\ndev_err(&adapter->dev, \"Invalid block %s size %d\\n\",\nread_write == I2C_SMBUS_READ ? \"read\" : \"write\",\ndata->block[0]);\nreturn -EINVAL;\n}\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].len = data->block[0];\n} else {\nmsg[0].len = data->block[0] + 1;\nfor (i = 1; i <= data->block[0]; i++)\nmsgbuf0[i] = data->block[i];\n}\nbreak;\ndefault:\ndev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\nreturn -EOPNOTSUPP;\n}\ni = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n&& size != I2C_SMBUS_I2C_BLOCK_DATA);\nif (i) {\nif (!(msg[0].flags & I2C_M_RD)) {\nif (num == 1) \ni2c_smbus_add_pec(&msg[0]);\nelse \npartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n}\nif (msg[num-1].flags & I2C_M_RD)\nmsg[num-1].len++;\n}\nstatus = i2c_transfer(adapter, msg, num);\nif (status < 0)\nreturn status;\nif (i && (msg[num-1].flags & I2C_M_RD)) {\nstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\nif (status < 0)\nreturn status;\n}\nif (read_write == I2C_SMBUS_READ)\nswitch (size) {\ncase I2C_SMBUS_BYTE:\ndata->byte = msgbuf0[0];\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\ndata->byte = msgbuf1[0];\nbreak;\ncase I2C_SMBUS_WORD_DATA:\ncase I2C_SMBUS_PROC_CALL:\ndata->word = msgbuf1[0] | (msgbuf1[1] << 8);\nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nfor (i = 0; i < data->block[0]; i++)\ndata->block[i+1] = msgbuf1[i];\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nfor (i = 0; i < msgbuf1[0] + 1; i++)\ndata->block[i] = msgbuf1[i];\nbreak;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\nunsigned short flags,\nchar read_write, u8 command, int size,\nunion i2c_smbus_data *data)\n{\nunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\nunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\nint num = read_write == I2C_SMBUS_READ ? 2 : 1;\nint i;\nu8 partial_pec = 0;\nint status;\nstruct i2c_msg msg[2] = {\n{\n.addr = addr,\n.flags = flags,\n.len = 1,\n.buf = msgbuf0,\n}, {\n.addr = addr,\n.flags = flags | I2C_M_RD,\n.len = 0,\n.buf = msgbuf1,\n},\n};\nmsgbuf0[0] = command;\nswitch (size) {\ncase I2C_SMBUS_QUICK:\nmsg[0].len = 0;\nmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\nI2C_M_RD : 0);\nnum = 1;\nbreak;\ncase I2C_SMBUS_BYTE:\nif (read_write == I2C_SMBUS_READ) {\nmsg[0].flags = I2C_M_RD | flags;\nnum = 1;\n}\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 1;\nelse {\nmsg[0].len = 2;\nmsgbuf0[1] = data->byte;\n}\nbreak;\ncase I2C_SMBUS_WORD_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 2;\nelse {\nmsg[0].len = 3;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\n}\nbreak;\ncase I2C_SMBUS_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nmsg[0].len = 3;\nmsg[1].len = 2;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \n} else {\nmsg[0].len = data->block[0] + 2;\nif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\n}\nbreak;\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nmsg[0].len = data->block[0] + 2;\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].len = data->block[0];\n} else {\nmsg[0].len = data->block[0] + 1;\nif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nfor (i = 1; i <= data->block[0]; i++)\nmsgbuf0[i] = data->block[i];\n}\nbreak;\ndefault:\ndev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\nreturn -EOPNOTSUPP;\n}\ni = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n&& size != I2C_SMBUS_I2C_BLOCK_DATA);\nif (i) {\nif (!(msg[0].flags & I2C_M_RD)) {\nif (num == 1) \ni2c_smbus_add_pec(&msg[0]);\nelse \npartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n}\nif (msg[num-1].flags & I2C_M_RD)\nmsg[num-1].len++;\n}\nstatus = i2c_transfer(adapter, msg, num);\nif (status < 0)\nreturn status;\nif (i && (msg[num-1].flags & I2C_M_RD)) {\nstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\nif (status < 0)\nreturn status;\n}\nif (read_write == I2C_SMBUS_READ)\nswitch (size) {\ncase I2C_SMBUS_BYTE:\ndata->byte = msgbuf0[0];\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\ndata->byte = msgbuf1[0];\nbreak;\ncase I2C_SMBUS_WORD_DATA:\ncase I2C_SMBUS_PROC_CALL:\ndata->word = msgbuf1[0] | (msgbuf1[1] << 8);\nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nfor (i = 0; i < data->block[0]; i++)\ndata->block[i+1] = msgbuf1[i];\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nfor (i = 0; i < msgbuf1[0] + 1; i++)\ndata->block[i] = msgbuf1[i];\nbreak;\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2017-18552": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking for input data size in the function rds_recv_track_latency.",
        "trigger_condition": "Passing a user-controlled value for optlen that does not match the expected size of struct rds_rx_trace_so.",
        "specific_code_behavior_causing_vulnerability": "The code does not verify that the input optlen matches the size of struct rds_rx_trace_so, leading to a potential out-of-bounds write and read when copying data from user space to the trace structure."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that optlen matches the expected size of struct rds_rx_trace_so before proceeding with copying data from user space. Additionally, a check should be added to verify that the value of trace.rx_traces is within the bounds of RDS_MSG_RX_DGRAM_TRACE_MAX to prevent out-of-bounds access. By implementing these checks, the code can prevent the out-of-bounds write and read vulnerabilities in the function rds_recv_track_latency.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-18552, which involves an out-of-bounds write and read in the function `rds_recv_track_latency`. The original code snippet did not properly check the bounds of the `trace.rx_traces` value, which could lead to a potential out-of-bounds access when accessing `trace.rx_trace_pos[i]`.\n\nBy adding the check `if (trace.rx_traces > RDS_MSG_RX_DGRAM_TRACE_MAX)`, the modified code ensures that the `rx_traces` value is within the bounds of `RDS_MSG_RX_DGRAM_TRACE_MAX`. This prevents the possibility of writing or reading outside the allocated memory for `rx_trace_pos`, thus mitigating the vulnerability and making the code more secure.",
      "GPT_purpose": "Track latency of received RDS messages for a given RDS socket.",
      "GPT_function": "\n1. Check if the optlen parameter is equal to the size of the struct rds_rx_trace_so.\n2. Copy data from user space to kernel space using copy_from_user.\n3. Update the rs_rx_traces field in the rds_sock structure based on the received data.\n4. Validate and update the rs_rx_trace array elements based on the received data.",
      "CVE_id": "CVE-2017-18552",
      "code_before_change": "static int rds_recv_track_latency(struct rds_sock *rs, char __user *optval,\n\t\t\t\t  int optlen)\n{\n\tstruct rds_rx_trace_so trace;\n\tint i;\n\n\tif (optlen != sizeof(struct rds_rx_trace_so))\n\t\treturn -EFAULT;\n\n\tif (copy_from_user(&trace, optval, sizeof(trace)))\n\t\treturn -EFAULT;\n\n\trs->rs_rx_traces = trace.rx_traces;\n\tfor (i = 0; i < rs->rs_rx_traces; i++) {\n\t\tif (trace.rx_trace_pos[i] > RDS_MSG_RX_DGRAM_TRACE_MAX) {\n\t\t\trs->rs_rx_traces = 0;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\trs->rs_rx_trace[i] = trace.rx_trace_pos[i];\n\t}\n\n\treturn 0;\n}",
      "code_after_change": "static int rds_recv_track_latency(struct rds_sock *rs, char __user *optval,\n\t\t\t\t  int optlen)\n{\n\tstruct rds_rx_trace_so trace;\n\tint i;\n\n\tif (optlen != sizeof(struct rds_rx_trace_so))\n\t\treturn -EFAULT;\n\n\tif (copy_from_user(&trace, optval, sizeof(trace)))\n\t\treturn -EFAULT;\n\n\tif (trace.rx_traces > RDS_MSG_RX_DGRAM_TRACE_MAX)\n\t\treturn -EFAULT;\n\n\trs->rs_rx_traces = trace.rx_traces;\n\tfor (i = 0; i < rs->rs_rx_traces; i++) {\n\t\tif (trace.rx_trace_pos[i] > RDS_MSG_RX_DGRAM_TRACE_MAX) {\n\t\t\trs->rs_rx_traces = 0;\n\t\t\treturn -EFAULT;\n\t\t}\n\t\trs->rs_rx_trace[i] = trace.rx_trace_pos[i];\n\t}\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\treturn -EFAULT;",
          "",
          "\tif (trace.rx_traces > RDS_MSG_RX_DGRAM_TRACE_MAX)"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking for input data size in the function rds_recv_track_latency.",
      "trigger_condition": "Passing a user-controlled value for optlen that does not match the expected size of struct rds_rx_trace_so.",
      "specific_code_behavior_causing_vulnerability": "The code does not verify that the input optlen matches the size of struct rds_rx_trace_so, leading to a potential out-of-bounds write and read when copying data from user space to the trace structure.",
      "id": 18,
      "code_after_change_normalized": "static int FUN1(struct rds_sock *VAR1, char __user *VAR2,\nint VAR3)\n{\nstruct rds_rx_trace_so VAR4;\nint VAR5;\nif (VAR3 != sizeof(struct VAR6))\nreturn -VAR7;\nif (FUN2(&VAR4, VAR2, sizeof(VAR4)))\nreturn -VAR7;\nif (VAR4.VAR8 > VAR9)\nreturn -VAR7;\nVAR1->VAR10 = VAR4.VAR8;\nfor (VAR5 = 0; VAR5 < VAR1->VAR10; VAR5++) {\nif (VAR4.VAR11[VAR5] > VAR9) {\nVAR1->VAR10 = 0;\nreturn -VAR7;\n}\nVAR1->VAR12[VAR5] = VAR4.VAR11[VAR5];\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct rds_sock *VAR1, char __user *VAR2,\nint VAR3)\n{\nstruct rds_rx_trace_so VAR4;\nint VAR5;\nif (VAR3 != sizeof(struct VAR6))\nreturn -VAR7;\nif (FUN2(&VAR4, VAR2, sizeof(VAR4)))\nreturn -VAR7;\nVAR1->VAR8 = VAR4.VAR9;\nfor (VAR5 = 0; VAR5 < VAR1->VAR8; VAR5++) {\nif (VAR4.VAR10[VAR5] > VAR11) {\nVAR1->VAR8 = 0;\nreturn -VAR7;\n}\nVAR1->VAR12[VAR5] = VAR4.VAR10[VAR5];\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int rds_recv_track_latency(struct rds_sock *rs, char __user *optval,\nint optlen)\n{\nstruct rds_rx_trace_so trace;\nint i;\nif (optlen != sizeof(struct rds_rx_trace_so))\nreturn -EFAULT;\nif (copy_from_user(&trace, optval, sizeof(trace)))\nreturn -EFAULT;\nif (trace.rx_traces > RDS_MSG_RX_DGRAM_TRACE_MAX)\nreturn -EFAULT;\nrs->rs_rx_traces = trace.rx_traces;\nfor (i = 0; i < rs->rs_rx_traces; i++) {\nif (trace.rx_trace_pos[i] > RDS_MSG_RX_DGRAM_TRACE_MAX) {\nrs->rs_rx_traces = 0;\nreturn -EFAULT;\n}\nrs->rs_rx_trace[i] = trace.rx_trace_pos[i];\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int rds_recv_track_latency(struct rds_sock *rs, char __user *optval,\nint optlen)\n{\nstruct rds_rx_trace_so trace;\nint i;\nif (optlen != sizeof(struct rds_rx_trace_so))\nreturn -EFAULT;\nif (copy_from_user(&trace, optval, sizeof(trace)))\nreturn -EFAULT;\nrs->rs_rx_traces = trace.rx_traces;\nfor (i = 0; i < rs->rs_rx_traces; i++) {\nif (trace.rx_trace_pos[i] > RDS_MSG_RX_DGRAM_TRACE_MAX) {\nrs->rs_rx_traces = 0;\nreturn -EFAULT;\n}\nrs->rs_rx_trace[i] = trace.rx_trace_pos[i];\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2017-7294": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for addition of certain levels data in the ioctl call for a specific device.",
        "trigger_condition": "A local user triggers an integer overflow and out-of-bounds write by crafting a specific ioctl call for the device, leading to a denial of service or potential privilege escalation.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the mip level data properly, allowing for a potential integer overflow and out-of-bounds write when calculating the total number of sizes for a surface creation request."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check within the loop that calculates the total number of sizes to ensure that each mip level data does not exceed a certain limit. By validating the mip level data, the code prevents the integer overflow and out-of-bounds write issue, enhancing the security of the ioctl call for the device.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-7294, which involves an integer overflow and out-of-bounds write issue in the vmw_surface_define_ioctl function. By adding the check `if (req->mip_levels[i] > DRM_VMW_MAX_MIP_LEVELS)` within the loop that calculates the total number of sizes, we ensure that the mip level data is properly validated. This check prevents a potential integer overflow and out-of-bounds write that could be triggered by a crafted ioctl call, thus mitigating the vulnerability and enhancing the security of the code.",
      "GPT_purpose": "Define a surface for the VMware graphics device and handle surface creation requests.",
      "GPT_function": "\n1. Define ioctl function for vmw surface creation.\n2. Allocate memory and initialize surface parameters.\n3. Handle surface creation and resource management.\n4. Check for surface format validity.\n5. Handle potential integer overflow and out-of-bounds write vulnerabilities.",
      "CVE_id": "CVE-2017-7294",
      "code_before_change": "int vmw_surface_define_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_user_surface *user_srf;\n\tstruct vmw_surface *srf;\n\tstruct vmw_resource *res;\n\tstruct vmw_resource *tmp;\n\tunion drm_vmw_surface_create_arg *arg =\n\t    (union drm_vmw_surface_create_arg *)data;\n\tstruct drm_vmw_surface_create_req *req = &arg->req;\n\tstruct drm_vmw_surface_arg *rep = &arg->rep;\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tint ret;\n\tint i, j;\n\tuint32_t cur_bo_offset;\n\tstruct drm_vmw_size *cur_size;\n\tstruct vmw_surface_offset *cur_offset;\n\tuint32_t num_sizes;\n\tuint32_t size;\n\tconst struct svga3d_surface_desc *desc;\n\n\tif (unlikely(vmw_user_surface_size == 0))\n\t\tvmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +\n\t\t\t128;\n\n\tnum_sizes = 0;\n\tfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i)\n\t\tnum_sizes += req->mip_levels[i];\n\n\tif (num_sizes > DRM_VMW_MAX_SURFACE_FACES * DRM_VMW_MAX_MIP_LEVELS ||\n\t    num_sizes == 0)\n\t\treturn -EINVAL;\n\n\tsize = vmw_user_surface_size + 128 +\n\t\tttm_round_pot(num_sizes * sizeof(struct drm_vmw_size)) +\n\t\tttm_round_pot(num_sizes * sizeof(struct vmw_surface_offset));\n\n\n\tdesc = svga3dsurface_get_desc(req->format);\n\tif (unlikely(desc->block_desc == SVGA3DBLOCKDESC_NONE)) {\n\t\tDRM_ERROR(\"Invalid surface format for surface creation.\\n\");\n\t\tDRM_ERROR(\"Format requested is: %d\\n\", req->format);\n\t\treturn -EINVAL;\n\t}\n\n\tret = ttm_read_lock(&dev_priv->reservation_sem, true);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tret = ttm_mem_global_alloc(vmw_mem_glob(dev_priv),\n\t\t\t\t   size, false, true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Out of graphics memory for surface\"\n\t\t\t\t  \" creation.\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\tuser_srf = kzalloc(sizeof(*user_srf), GFP_KERNEL);\n\tif (unlikely(!user_srf)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_no_user_srf;\n\t}\n\n\tsrf = &user_srf->srf;\n\tres = &srf->res;\n\n\tsrf->flags = req->flags;\n\tsrf->format = req->format;\n\tsrf->scanout = req->scanout;\n\n\tmemcpy(srf->mip_levels, req->mip_levels, sizeof(srf->mip_levels));\n\tsrf->num_sizes = num_sizes;\n\tuser_srf->size = size;\n\tsrf->sizes = memdup_user((struct drm_vmw_size __user *)(unsigned long)\n\t\t\t\t req->size_addr,\n\t\t\t\t sizeof(*srf->sizes) * srf->num_sizes);\n\tif (IS_ERR(srf->sizes)) {\n\t\tret = PTR_ERR(srf->sizes);\n\t\tgoto out_no_sizes;\n\t}\n\tsrf->offsets = kmalloc_array(srf->num_sizes,\n\t\t\t\t     sizeof(*srf->offsets),\n\t\t\t\t     GFP_KERNEL);\n\tif (unlikely(!srf->offsets)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_no_offsets;\n\t}\n\n\tsrf->base_size = *srf->sizes;\n\tsrf->autogen_filter = SVGA3D_TEX_FILTER_NONE;\n\tsrf->multisample_count = 0;\n\n\tcur_bo_offset = 0;\n\tcur_offset = srf->offsets;\n\tcur_size = srf->sizes;\n\n\tfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {\n\t\tfor (j = 0; j < srf->mip_levels[i]; ++j) {\n\t\t\tuint32_t stride = svga3dsurface_calculate_pitch\n\t\t\t\t(desc, cur_size);\n\n\t\t\tcur_offset->face = i;\n\t\t\tcur_offset->mip = j;\n\t\t\tcur_offset->bo_offset = cur_bo_offset;\n\t\t\tcur_bo_offset += svga3dsurface_get_image_buffer_size\n\t\t\t\t(desc, cur_size, stride);\n\t\t\t++cur_offset;\n\t\t\t++cur_size;\n\t\t}\n\t}\n\tres->backup_size = cur_bo_offset;\n\tif (srf->scanout &&\n\t    srf->num_sizes == 1 &&\n\t    srf->sizes[0].width == 64 &&\n\t    srf->sizes[0].height == 64 &&\n\t    srf->format == SVGA3D_A8R8G8B8) {\n\n\t\tsrf->snooper.image = kzalloc(64 * 64 * 4, GFP_KERNEL);\n\t\tif (!srf->snooper.image) {\n\t\t\tDRM_ERROR(\"Failed to allocate cursor_image\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_no_copy;\n\t\t}\n\t} else {\n\t\tsrf->snooper.image = NULL;\n\t}\n\tsrf->snooper.crtc = NULL;\n\n\tuser_srf->prime.base.shareable = false;\n\tuser_srf->prime.base.tfile = NULL;\n\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_master_get(file_priv->master);\n\n\t/**\n\t * From this point, the generic resource management functions\n\t * destroy the object on failure.\n\t */\n\n\tret = vmw_surface_init(dev_priv, srf, vmw_user_surface_free);\n\tif (unlikely(ret != 0))\n\t\tgoto out_unlock;\n\n\t/*\n\t * A gb-aware client referencing a shared surface will\n\t * expect a backup buffer to be present.\n\t */\n\tif (dev_priv->has_mob && req->shareable) {\n\t\tuint32_t backup_handle;\n\n\t\tret = vmw_user_dmabuf_alloc(dev_priv, tfile,\n\t\t\t\t\t    res->backup_size,\n\t\t\t\t\t    true,\n\t\t\t\t\t    &backup_handle,\n\t\t\t\t\t    &res->backup,\n\t\t\t\t\t    &user_srf->backup_base);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tvmw_resource_unreference(&res);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\ttmp = vmw_resource_reference(&srf->res);\n\tret = ttm_prime_object_init(tfile, res->backup_size, &user_srf->prime,\n\t\t\t\t    req->shareable, VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release, NULL);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\trep->sid = user_srf->prime.base.hash.key;\n\tvmw_resource_unreference(&res);\n\n\tttm_read_unlock(&dev_priv->reservation_sem);\n\treturn 0;\nout_no_copy:\n\tkfree(srf->offsets);\nout_no_offsets:\n\tkfree(srf->sizes);\nout_no_sizes:\n\tttm_prime_object_kfree(user_srf, prime);\nout_no_user_srf:\n\tttm_mem_global_free(vmw_mem_glob(dev_priv), size);\nout_unlock:\n\tttm_read_unlock(&dev_priv->reservation_sem);\n\treturn ret;\n}",
      "code_after_change": "int vmw_surface_define_ioctl(struct drm_device *dev, void *data,\n\t\t\t     struct drm_file *file_priv)\n{\n\tstruct vmw_private *dev_priv = vmw_priv(dev);\n\tstruct vmw_user_surface *user_srf;\n\tstruct vmw_surface *srf;\n\tstruct vmw_resource *res;\n\tstruct vmw_resource *tmp;\n\tunion drm_vmw_surface_create_arg *arg =\n\t    (union drm_vmw_surface_create_arg *)data;\n\tstruct drm_vmw_surface_create_req *req = &arg->req;\n\tstruct drm_vmw_surface_arg *rep = &arg->rep;\n\tstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\n\tint ret;\n\tint i, j;\n\tuint32_t cur_bo_offset;\n\tstruct drm_vmw_size *cur_size;\n\tstruct vmw_surface_offset *cur_offset;\n\tuint32_t num_sizes;\n\tuint32_t size;\n\tconst struct svga3d_surface_desc *desc;\n\n\tif (unlikely(vmw_user_surface_size == 0))\n\t\tvmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +\n\t\t\t128;\n\n\tnum_sizes = 0;\n\tfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {\n\t\tif (req->mip_levels[i] > DRM_VMW_MAX_MIP_LEVELS)\n\t\t\treturn -EINVAL;\n\t\tnum_sizes += req->mip_levels[i];\n\t}\n\n\tif (num_sizes > DRM_VMW_MAX_SURFACE_FACES * DRM_VMW_MAX_MIP_LEVELS ||\n\t    num_sizes == 0)\n\t\treturn -EINVAL;\n\n\tsize = vmw_user_surface_size + 128 +\n\t\tttm_round_pot(num_sizes * sizeof(struct drm_vmw_size)) +\n\t\tttm_round_pot(num_sizes * sizeof(struct vmw_surface_offset));\n\n\n\tdesc = svga3dsurface_get_desc(req->format);\n\tif (unlikely(desc->block_desc == SVGA3DBLOCKDESC_NONE)) {\n\t\tDRM_ERROR(\"Invalid surface format for surface creation.\\n\");\n\t\tDRM_ERROR(\"Format requested is: %d\\n\", req->format);\n\t\treturn -EINVAL;\n\t}\n\n\tret = ttm_read_lock(&dev_priv->reservation_sem, true);\n\tif (unlikely(ret != 0))\n\t\treturn ret;\n\n\tret = ttm_mem_global_alloc(vmw_mem_glob(dev_priv),\n\t\t\t\t   size, false, true);\n\tif (unlikely(ret != 0)) {\n\t\tif (ret != -ERESTARTSYS)\n\t\t\tDRM_ERROR(\"Out of graphics memory for surface\"\n\t\t\t\t  \" creation.\\n\");\n\t\tgoto out_unlock;\n\t}\n\n\tuser_srf = kzalloc(sizeof(*user_srf), GFP_KERNEL);\n\tif (unlikely(!user_srf)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_no_user_srf;\n\t}\n\n\tsrf = &user_srf->srf;\n\tres = &srf->res;\n\n\tsrf->flags = req->flags;\n\tsrf->format = req->format;\n\tsrf->scanout = req->scanout;\n\n\tmemcpy(srf->mip_levels, req->mip_levels, sizeof(srf->mip_levels));\n\tsrf->num_sizes = num_sizes;\n\tuser_srf->size = size;\n\tsrf->sizes = memdup_user((struct drm_vmw_size __user *)(unsigned long)\n\t\t\t\t req->size_addr,\n\t\t\t\t sizeof(*srf->sizes) * srf->num_sizes);\n\tif (IS_ERR(srf->sizes)) {\n\t\tret = PTR_ERR(srf->sizes);\n\t\tgoto out_no_sizes;\n\t}\n\tsrf->offsets = kmalloc_array(srf->num_sizes,\n\t\t\t\t     sizeof(*srf->offsets),\n\t\t\t\t     GFP_KERNEL);\n\tif (unlikely(!srf->offsets)) {\n\t\tret = -ENOMEM;\n\t\tgoto out_no_offsets;\n\t}\n\n\tsrf->base_size = *srf->sizes;\n\tsrf->autogen_filter = SVGA3D_TEX_FILTER_NONE;\n\tsrf->multisample_count = 0;\n\n\tcur_bo_offset = 0;\n\tcur_offset = srf->offsets;\n\tcur_size = srf->sizes;\n\n\tfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {\n\t\tfor (j = 0; j < srf->mip_levels[i]; ++j) {\n\t\t\tuint32_t stride = svga3dsurface_calculate_pitch\n\t\t\t\t(desc, cur_size);\n\n\t\t\tcur_offset->face = i;\n\t\t\tcur_offset->mip = j;\n\t\t\tcur_offset->bo_offset = cur_bo_offset;\n\t\t\tcur_bo_offset += svga3dsurface_get_image_buffer_size\n\t\t\t\t(desc, cur_size, stride);\n\t\t\t++cur_offset;\n\t\t\t++cur_size;\n\t\t}\n\t}\n\tres->backup_size = cur_bo_offset;\n\tif (srf->scanout &&\n\t    srf->num_sizes == 1 &&\n\t    srf->sizes[0].width == 64 &&\n\t    srf->sizes[0].height == 64 &&\n\t    srf->format == SVGA3D_A8R8G8B8) {\n\n\t\tsrf->snooper.image = kzalloc(64 * 64 * 4, GFP_KERNEL);\n\t\tif (!srf->snooper.image) {\n\t\t\tDRM_ERROR(\"Failed to allocate cursor_image\\n\");\n\t\t\tret = -ENOMEM;\n\t\t\tgoto out_no_copy;\n\t\t}\n\t} else {\n\t\tsrf->snooper.image = NULL;\n\t}\n\tsrf->snooper.crtc = NULL;\n\n\tuser_srf->prime.base.shareable = false;\n\tuser_srf->prime.base.tfile = NULL;\n\tif (drm_is_primary_client(file_priv))\n\t\tuser_srf->master = drm_master_get(file_priv->master);\n\n\t/**\n\t * From this point, the generic resource management functions\n\t * destroy the object on failure.\n\t */\n\n\tret = vmw_surface_init(dev_priv, srf, vmw_user_surface_free);\n\tif (unlikely(ret != 0))\n\t\tgoto out_unlock;\n\n\t/*\n\t * A gb-aware client referencing a shared surface will\n\t * expect a backup buffer to be present.\n\t */\n\tif (dev_priv->has_mob && req->shareable) {\n\t\tuint32_t backup_handle;\n\n\t\tret = vmw_user_dmabuf_alloc(dev_priv, tfile,\n\t\t\t\t\t    res->backup_size,\n\t\t\t\t\t    true,\n\t\t\t\t\t    &backup_handle,\n\t\t\t\t\t    &res->backup,\n\t\t\t\t\t    &user_srf->backup_base);\n\t\tif (unlikely(ret != 0)) {\n\t\t\tvmw_resource_unreference(&res);\n\t\t\tgoto out_unlock;\n\t\t}\n\t}\n\n\ttmp = vmw_resource_reference(&srf->res);\n\tret = ttm_prime_object_init(tfile, res->backup_size, &user_srf->prime,\n\t\t\t\t    req->shareable, VMW_RES_SURFACE,\n\t\t\t\t    &vmw_user_surface_base_release, NULL);\n\n\tif (unlikely(ret != 0)) {\n\t\tvmw_resource_unreference(&tmp);\n\t\tvmw_resource_unreference(&res);\n\t\tgoto out_unlock;\n\t}\n\n\trep->sid = user_srf->prime.base.hash.key;\n\tvmw_resource_unreference(&res);\n\n\tttm_read_unlock(&dev_priv->reservation_sem);\n\treturn 0;\nout_no_copy:\n\tkfree(srf->offsets);\nout_no_offsets:\n\tkfree(srf->sizes);\nout_no_sizes:\n\tttm_prime_object_kfree(user_srf, prime);\nout_no_user_srf:\n\tttm_mem_global_free(vmw_mem_glob(dev_priv), size);\nout_unlock:\n\tttm_read_unlock(&dev_priv->reservation_sem);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {",
          "\t\tif (req->mip_levels[i] > DRM_VMW_MAX_MIP_LEVELS)",
          "\t\t\treturn -EINVAL;",
          "\t}"
        ],
        "deleted": [
          "\tfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for addition of certain levels data in the ioctl call for a specific device.",
      "trigger_condition": "A local user triggers an integer overflow and out-of-bounds write by crafting a specific ioctl call for the device, leading to a denial of service or potential privilege escalation.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the mip level data properly, allowing for a potential integer overflow and out-of-bounds write when calculating the total number of sizes for a surface creation request.",
      "id": 19,
      "code_after_change_normalized": "int FUN1(struct drm_device *VAR1, void *VAR2,\nstruct drm_file *VAR3)\n{\nstruct vmw_private *VAR4 = FUN2(VAR1);\nstruct vmw_user_surface *VAR5;\nstruct vmw_surface *VAR6;\nstruct vmw_resource *VAR7;\nstruct vmw_resource *VAR8;\nunion drm_vmw_surface_create_arg *VAR9 =\n(union VAR10 *)VAR2;\nstruct drm_vmw_surface_create_req *VAR11 = &VAR9->VAR11;\nstruct drm_vmw_surface_arg *VAR12 = &VAR9->VAR12;\nstruct ttm_object_file *VAR13 = FUN3(VAR3)->VAR13;\nint VAR14;\nint VAR15, VAR16;\nuint32_t VAR17;\nstruct drm_vmw_size *VAR18;\nstruct vmw_surface_offset *VAR19;\nuint32_t VAR20;\nuint32_t VAR21;\nconst struct svga3d_surface_desc *VAR22;\nif (FUN4(VAR23 == 0))\nVAR23 = FUN5(sizeof(*VAR5)) +\n128;\nVAR20 = 0;\nfor (VAR15 = 0; VAR15 < VAR24; ++VAR15) {\nif (VAR11->VAR25[VAR15] > VAR26)\nreturn -VAR27;\nVAR20 += VAR11->VAR25[VAR15];\n}\nif (VAR20 > VAR24 * VAR26 ||\nVAR20 == 0)\nreturn -VAR27;\nVAR21 = VAR23 + 128 +\nFUN5(VAR20 * sizeof(struct VAR28)) +\nFUN5(VAR20 * sizeof(struct VAR29));\nVAR22 = FUN6(VAR11->VAR30);\nif (FUN4(VAR22->VAR31 == VAR32)) {\nFUN7(\"STR\");\nFUN7(\"STR\", VAR11->VAR30);\nreturn -VAR27;\n}\nVAR14 = FUN8(&VAR4->VAR33, true);\nif (FUN4(VAR14 != 0))\nreturn VAR14;\nVAR14 = FUN9(FUN10(VAR4),\nVAR21, false, true);\nif (FUN4(VAR14 != 0)) {\nif (VAR14 != -VAR34)\nFUN7(\"STR\"\n\"STR\");\ngoto VAR35;\n}\nVAR5 = FUN11(sizeof(*VAR5), VAR36);\nif (FUN4(!VAR5)) {\nVAR14 = -VAR37;\ngoto VAR38;\n}\nVAR6 = &VAR5->VAR6;\nVAR7 = &VAR6->VAR7;\nVAR6->VAR39 = VAR11->VAR39;\nVAR6->VAR30 = VAR11->VAR30;\nVAR6->VAR40 = VAR11->VAR40;\nFUN12(VAR6->VAR25, VAR11->VAR25, sizeof(VAR6->VAR25));\nVAR6->VAR20 = VAR20;\nVAR5->VAR21 = VAR21;\nVAR6->VAR41 = FUN13((struct drm_vmw_size VAR42 *)(unsigned long)\nVAR11->VAR43,\nsizeof(*VAR6->VAR41) * VAR6->VAR20);\nif (FUN14(VAR6->VAR41)) {\nVAR14 = FUN15(VAR6->VAR41);\ngoto VAR44;\n}\nVAR6->VAR45 = FUN16(VAR6->VAR20,\nsizeof(*VAR6->VAR45),\nVAR36);\nif (FUN4(!VAR6->VAR45)) {\nVAR14 = -VAR37;\ngoto VAR46;\n}\nVAR6->VAR47 = *VAR6->VAR41;\nVAR6->VAR48 = VAR49;\nVAR6->VAR50 = 0;\nVAR17 = 0;\nVAR19 = VAR6->VAR45;\nVAR18 = VAR6->VAR41;\nfor (VAR15 = 0; VAR15 < VAR24; ++VAR15) {\nfor (VAR16 = 0; VAR16 < VAR6->VAR25[VAR15]; ++VAR16) {\nuint32_t VAR51 = VAR52\n(VAR22, VAR18);\nVAR19->VAR53 = VAR15;\nVAR19->VAR54 = VAR16;\nVAR19->VAR55 = VAR17;\nVAR17 += VAR56\n(VAR22, VAR18, VAR51);\n++VAR19;\n++VAR18;\n}\n}\nVAR7->VAR57 = VAR17;\nif (VAR6->VAR40 &&\nVAR6->VAR20 == 1 &&\nVAR6->VAR41[0].VAR58 == 64 &&\nVAR6->VAR41[0].VAR59 == 64 &&\nVAR6->VAR30 == VAR60) {\nVAR6->VAR61.VAR62 = FUN11(64 * 64 * 4, VAR36);\nif (!VAR6->VAR61.VAR62) {\nFUN7(\"STR\");\nVAR14 = -VAR37;\ngoto VAR63;\n}\n} else {\nVAR6->VAR61.VAR62 = NULL;\n}\nVAR6->VAR61.VAR64 = NULL;\nVAR5->VAR65.VAR66.VAR67 = false;\nVAR5->VAR65.VAR66.VAR13 = NULL;\nif (FUN17(VAR3))\nVAR5->VAR68 = FUN18(VAR3->VAR68);\nVAR14 = FUN19(VAR4, VAR6, VAR69);\nif (FUN4(VAR14 != 0))\ngoto VAR35;\nif (VAR4->VAR70 && VAR11->VAR67) {\nuint32_t VAR71;\nVAR14 = FUN20(VAR4, VAR13,\nVAR7->VAR57,\ntrue,\n&VAR71,\n&VAR7->VAR72,\n&VAR5->VAR73);\nif (FUN4(VAR14 != 0)) {\nFUN21(&VAR7);\ngoto VAR35;\n}\n}\nVAR8 = FUN22(&VAR6->VAR7);\nVAR14 = FUN23(VAR13, VAR7->VAR57, &VAR5->VAR65,\nVAR11->VAR67, VAR74,\n&VAR75, NULL);\nif (FUN4(VAR14 != 0)) {\nFUN21(&VAR8);\nFUN21(&VAR7);\ngoto VAR35;\n}\nVAR12->VAR76 = VAR5->VAR65.VAR66.VAR77.VAR78;\nFUN21(&VAR7);\nFUN24(&VAR4->VAR33);\nreturn 0;\nVAR63:\nFUN25(VAR6->VAR45);\nVAR46:\nFUN25(VAR6->VAR41);\nVAR44:\nFUN26(VAR5, VAR65);\nVAR38:\nFUN27(FUN10(VAR4), VAR21);\nVAR35:\nFUN24(&VAR4->VAR33);\nreturn VAR14;\n}\n",
      "code_before_change_normalized": "int FUN1(struct drm_device *VAR1, void *VAR2,\nstruct drm_file *VAR3)\n{\nstruct vmw_private *VAR4 = FUN2(VAR1);\nstruct vmw_user_surface *VAR5;\nstruct vmw_surface *VAR6;\nstruct vmw_resource *VAR7;\nstruct vmw_resource *VAR8;\nunion drm_vmw_surface_create_arg *VAR9 =\n(union VAR10 *)VAR2;\nstruct drm_vmw_surface_create_req *VAR11 = &VAR9->VAR11;\nstruct drm_vmw_surface_arg *VAR12 = &VAR9->VAR12;\nstruct ttm_object_file *VAR13 = FUN3(VAR3)->VAR13;\nint VAR14;\nint VAR15, VAR16;\nuint32_t VAR17;\nstruct drm_vmw_size *VAR18;\nstruct vmw_surface_offset *VAR19;\nuint32_t VAR20;\nuint32_t VAR21;\nconst struct svga3d_surface_desc *VAR22;\nif (FUN4(VAR23 == 0))\nVAR23 = FUN5(sizeof(*VAR5)) +\n128;\nVAR20 = 0;\nfor (VAR15 = 0; VAR15 < VAR24; ++VAR15)\nVAR20 += VAR11->VAR25[VAR15];\nif (VAR20 > VAR24 * VAR26 ||\nVAR20 == 0)\nreturn -VAR27;\nVAR21 = VAR23 + 128 +\nFUN5(VAR20 * sizeof(struct VAR28)) +\nFUN5(VAR20 * sizeof(struct VAR29));\nVAR22 = FUN6(VAR11->VAR30);\nif (FUN4(VAR22->VAR31 == VAR32)) {\nFUN7(\"STR\");\nFUN7(\"STR\", VAR11->VAR30);\nreturn -VAR27;\n}\nVAR14 = FUN8(&VAR4->VAR33, true);\nif (FUN4(VAR14 != 0))\nreturn VAR14;\nVAR14 = FUN9(FUN10(VAR4),\nVAR21, false, true);\nif (FUN4(VAR14 != 0)) {\nif (VAR14 != -VAR34)\nFUN7(\"STR\"\n\"STR\");\ngoto VAR35;\n}\nVAR5 = FUN11(sizeof(*VAR5), VAR36);\nif (FUN4(!VAR5)) {\nVAR14 = -VAR37;\ngoto VAR38;\n}\nVAR6 = &VAR5->VAR6;\nVAR7 = &VAR6->VAR7;\nVAR6->VAR39 = VAR11->VAR39;\nVAR6->VAR30 = VAR11->VAR30;\nVAR6->VAR40 = VAR11->VAR40;\nFUN12(VAR6->VAR25, VAR11->VAR25, sizeof(VAR6->VAR25));\nVAR6->VAR20 = VAR20;\nVAR5->VAR21 = VAR21;\nVAR6->VAR41 = FUN13((struct drm_vmw_size VAR42 *)(unsigned long)\nVAR11->VAR43,\nsizeof(*VAR6->VAR41) * VAR6->VAR20);\nif (FUN14(VAR6->VAR41)) {\nVAR14 = FUN15(VAR6->VAR41);\ngoto VAR44;\n}\nVAR6->VAR45 = FUN16(VAR6->VAR20,\nsizeof(*VAR6->VAR45),\nVAR36);\nif (FUN4(!VAR6->VAR45)) {\nVAR14 = -VAR37;\ngoto VAR46;\n}\nVAR6->VAR47 = *VAR6->VAR41;\nVAR6->VAR48 = VAR49;\nVAR6->VAR50 = 0;\nVAR17 = 0;\nVAR19 = VAR6->VAR45;\nVAR18 = VAR6->VAR41;\nfor (VAR15 = 0; VAR15 < VAR24; ++VAR15) {\nfor (VAR16 = 0; VAR16 < VAR6->VAR25[VAR15]; ++VAR16) {\nuint32_t VAR51 = VAR52\n(VAR22, VAR18);\nVAR19->VAR53 = VAR15;\nVAR19->VAR54 = VAR16;\nVAR19->VAR55 = VAR17;\nVAR17 += VAR56\n(VAR22, VAR18, VAR51);\n++VAR19;\n++VAR18;\n}\n}\nVAR7->VAR57 = VAR17;\nif (VAR6->VAR40 &&\nVAR6->VAR20 == 1 &&\nVAR6->VAR41[0].VAR58 == 64 &&\nVAR6->VAR41[0].VAR59 == 64 &&\nVAR6->VAR30 == VAR60) {\nVAR6->VAR61.VAR62 = FUN11(64 * 64 * 4, VAR36);\nif (!VAR6->VAR61.VAR62) {\nFUN7(\"STR\");\nVAR14 = -VAR37;\ngoto VAR63;\n}\n} else {\nVAR6->VAR61.VAR62 = NULL;\n}\nVAR6->VAR61.VAR64 = NULL;\nVAR5->VAR65.VAR66.VAR67 = false;\nVAR5->VAR65.VAR66.VAR13 = NULL;\nif (FUN17(VAR3))\nVAR5->VAR68 = FUN18(VAR3->VAR68);\nVAR14 = FUN19(VAR4, VAR6, VAR69);\nif (FUN4(VAR14 != 0))\ngoto VAR35;\nif (VAR4->VAR70 && VAR11->VAR67) {\nuint32_t VAR71;\nVAR14 = FUN20(VAR4, VAR13,\nVAR7->VAR57,\ntrue,\n&VAR71,\n&VAR7->VAR72,\n&VAR5->VAR73);\nif (FUN4(VAR14 != 0)) {\nFUN21(&VAR7);\ngoto VAR35;\n}\n}\nVAR8 = FUN22(&VAR6->VAR7);\nVAR14 = FUN23(VAR13, VAR7->VAR57, &VAR5->VAR65,\nVAR11->VAR67, VAR74,\n&VAR75, NULL);\nif (FUN4(VAR14 != 0)) {\nFUN21(&VAR8);\nFUN21(&VAR7);\ngoto VAR35;\n}\nVAR12->VAR76 = VAR5->VAR65.VAR66.VAR77.VAR78;\nFUN21(&VAR7);\nFUN24(&VAR4->VAR33);\nreturn 0;\nVAR63:\nFUN25(VAR6->VAR45);\nVAR46:\nFUN25(VAR6->VAR41);\nVAR44:\nFUN26(VAR5, VAR65);\nVAR38:\nFUN27(FUN10(VAR4), VAR21);\nVAR35:\nFUN24(&VAR4->VAR33);\nreturn VAR14;\n}\n",
      "code_after_change_raw": "int vmw_surface_define_ioctl(struct drm_device *dev, void *data,\nstruct drm_file *file_priv)\n{\nstruct vmw_private *dev_priv = vmw_priv(dev);\nstruct vmw_user_surface *user_srf;\nstruct vmw_surface *srf;\nstruct vmw_resource *res;\nstruct vmw_resource *tmp;\nunion drm_vmw_surface_create_arg *arg =\n(union drm_vmw_surface_create_arg *)data;\nstruct drm_vmw_surface_create_req *req = &arg->req;\nstruct drm_vmw_surface_arg *rep = &arg->rep;\nstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\nint ret;\nint i, j;\nuint32_t cur_bo_offset;\nstruct drm_vmw_size *cur_size;\nstruct vmw_surface_offset *cur_offset;\nuint32_t num_sizes;\nuint32_t size;\nconst struct svga3d_surface_desc *desc;\nif (unlikely(vmw_user_surface_size == 0))\nvmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +\n128;\nnum_sizes = 0;\nfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {\nif (req->mip_levels[i] > DRM_VMW_MAX_MIP_LEVELS)\nreturn -EINVAL;\nnum_sizes += req->mip_levels[i];\n}\nif (num_sizes > DRM_VMW_MAX_SURFACE_FACES * DRM_VMW_MAX_MIP_LEVELS ||\nnum_sizes == 0)\nreturn -EINVAL;\nsize = vmw_user_surface_size + 128 +\nttm_round_pot(num_sizes * sizeof(struct drm_vmw_size)) +\nttm_round_pot(num_sizes * sizeof(struct vmw_surface_offset));\ndesc = svga3dsurface_get_desc(req->format);\nif (unlikely(desc->block_desc == SVGA3DBLOCKDESC_NONE)) {\nDRM_ERROR(\"Invalid surface format for surface creation.\\n\");\nDRM_ERROR(\"Format requested is: %d\\n\", req->format);\nreturn -EINVAL;\n}\nret = ttm_read_lock(&dev_priv->reservation_sem, true);\nif (unlikely(ret != 0))\nreturn ret;\nret = ttm_mem_global_alloc(vmw_mem_glob(dev_priv),\nsize, false, true);\nif (unlikely(ret != 0)) {\nif (ret != -ERESTARTSYS)\nDRM_ERROR(\"Out of graphics memory for surface\"\n\" creation.\\n\");\ngoto out_unlock;\n}\nuser_srf = kzalloc(sizeof(*user_srf), GFP_KERNEL);\nif (unlikely(!user_srf)) {\nret = -ENOMEM;\ngoto out_no_user_srf;\n}\nsrf = &user_srf->srf;\nres = &srf->res;\nsrf->flags = req->flags;\nsrf->format = req->format;\nsrf->scanout = req->scanout;\nmemcpy(srf->mip_levels, req->mip_levels, sizeof(srf->mip_levels));\nsrf->num_sizes = num_sizes;\nuser_srf->size = size;\nsrf->sizes = memdup_user((struct drm_vmw_size __user *)(unsigned long)\nreq->size_addr,\nsizeof(*srf->sizes) * srf->num_sizes);\nif (IS_ERR(srf->sizes)) {\nret = PTR_ERR(srf->sizes);\ngoto out_no_sizes;\n}\nsrf->offsets = kmalloc_array(srf->num_sizes,\nsizeof(*srf->offsets),\nGFP_KERNEL);\nif (unlikely(!srf->offsets)) {\nret = -ENOMEM;\ngoto out_no_offsets;\n}\nsrf->base_size = *srf->sizes;\nsrf->autogen_filter = SVGA3D_TEX_FILTER_NONE;\nsrf->multisample_count = 0;\ncur_bo_offset = 0;\ncur_offset = srf->offsets;\ncur_size = srf->sizes;\nfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {\nfor (j = 0; j < srf->mip_levels[i]; ++j) {\nuint32_t stride = svga3dsurface_calculate_pitch\n(desc, cur_size);\ncur_offset->face = i;\ncur_offset->mip = j;\ncur_offset->bo_offset = cur_bo_offset;\ncur_bo_offset += svga3dsurface_get_image_buffer_size\n(desc, cur_size, stride);\n++cur_offset;\n++cur_size;\n}\n}\nres->backup_size = cur_bo_offset;\nif (srf->scanout &&\nsrf->num_sizes == 1 &&\nsrf->sizes[0].width == 64 &&\nsrf->sizes[0].height == 64 &&\nsrf->format == SVGA3D_A8R8G8B8) {\nsrf->snooper.image = kzalloc(64 * 64 * 4, GFP_KERNEL);\nif (!srf->snooper.image) {\nDRM_ERROR(\"Failed to allocate cursor_image\\n\");\nret = -ENOMEM;\ngoto out_no_copy;\n}\n} else {\nsrf->snooper.image = NULL;\n}\nsrf->snooper.crtc = NULL;\nuser_srf->prime.base.shareable = false;\nuser_srf->prime.base.tfile = NULL;\nif (drm_is_primary_client(file_priv))\nuser_srf->master = drm_master_get(file_priv->master);\nret = vmw_surface_init(dev_priv, srf, vmw_user_surface_free);\nif (unlikely(ret != 0))\ngoto out_unlock;\nif (dev_priv->has_mob && req->shareable) {\nuint32_t backup_handle;\nret = vmw_user_dmabuf_alloc(dev_priv, tfile,\nres->backup_size,\ntrue,\n&backup_handle,\n&res->backup,\n&user_srf->backup_base);\nif (unlikely(ret != 0)) {\nvmw_resource_unreference(&res);\ngoto out_unlock;\n}\n}\ntmp = vmw_resource_reference(&srf->res);\nret = ttm_prime_object_init(tfile, res->backup_size, &user_srf->prime,\nreq->shareable, VMW_RES_SURFACE,\n&vmw_user_surface_base_release, NULL);\nif (unlikely(ret != 0)) {\nvmw_resource_unreference(&tmp);\nvmw_resource_unreference(&res);\ngoto out_unlock;\n}\nrep->sid = user_srf->prime.base.hash.key;\nvmw_resource_unreference(&res);\nttm_read_unlock(&dev_priv->reservation_sem);\nreturn 0;\nout_no_copy:\nkfree(srf->offsets);\nout_no_offsets:\nkfree(srf->sizes);\nout_no_sizes:\nttm_prime_object_kfree(user_srf, prime);\nout_no_user_srf:\nttm_mem_global_free(vmw_mem_glob(dev_priv), size);\nout_unlock:\nttm_read_unlock(&dev_priv->reservation_sem);\nreturn ret;\n}\n",
      "code_before_change_raw": "int vmw_surface_define_ioctl(struct drm_device *dev, void *data,\nstruct drm_file *file_priv)\n{\nstruct vmw_private *dev_priv = vmw_priv(dev);\nstruct vmw_user_surface *user_srf;\nstruct vmw_surface *srf;\nstruct vmw_resource *res;\nstruct vmw_resource *tmp;\nunion drm_vmw_surface_create_arg *arg =\n(union drm_vmw_surface_create_arg *)data;\nstruct drm_vmw_surface_create_req *req = &arg->req;\nstruct drm_vmw_surface_arg *rep = &arg->rep;\nstruct ttm_object_file *tfile = vmw_fpriv(file_priv)->tfile;\nint ret;\nint i, j;\nuint32_t cur_bo_offset;\nstruct drm_vmw_size *cur_size;\nstruct vmw_surface_offset *cur_offset;\nuint32_t num_sizes;\nuint32_t size;\nconst struct svga3d_surface_desc *desc;\nif (unlikely(vmw_user_surface_size == 0))\nvmw_user_surface_size = ttm_round_pot(sizeof(*user_srf)) +\n128;\nnum_sizes = 0;\nfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i)\nnum_sizes += req->mip_levels[i];\nif (num_sizes > DRM_VMW_MAX_SURFACE_FACES * DRM_VMW_MAX_MIP_LEVELS ||\nnum_sizes == 0)\nreturn -EINVAL;\nsize = vmw_user_surface_size + 128 +\nttm_round_pot(num_sizes * sizeof(struct drm_vmw_size)) +\nttm_round_pot(num_sizes * sizeof(struct vmw_surface_offset));\ndesc = svga3dsurface_get_desc(req->format);\nif (unlikely(desc->block_desc == SVGA3DBLOCKDESC_NONE)) {\nDRM_ERROR(\"Invalid surface format for surface creation.\\n\");\nDRM_ERROR(\"Format requested is: %d\\n\", req->format);\nreturn -EINVAL;\n}\nret = ttm_read_lock(&dev_priv->reservation_sem, true);\nif (unlikely(ret != 0))\nreturn ret;\nret = ttm_mem_global_alloc(vmw_mem_glob(dev_priv),\nsize, false, true);\nif (unlikely(ret != 0)) {\nif (ret != -ERESTARTSYS)\nDRM_ERROR(\"Out of graphics memory for surface\"\n\" creation.\\n\");\ngoto out_unlock;\n}\nuser_srf = kzalloc(sizeof(*user_srf), GFP_KERNEL);\nif (unlikely(!user_srf)) {\nret = -ENOMEM;\ngoto out_no_user_srf;\n}\nsrf = &user_srf->srf;\nres = &srf->res;\nsrf->flags = req->flags;\nsrf->format = req->format;\nsrf->scanout = req->scanout;\nmemcpy(srf->mip_levels, req->mip_levels, sizeof(srf->mip_levels));\nsrf->num_sizes = num_sizes;\nuser_srf->size = size;\nsrf->sizes = memdup_user((struct drm_vmw_size __user *)(unsigned long)\nreq->size_addr,\nsizeof(*srf->sizes) * srf->num_sizes);\nif (IS_ERR(srf->sizes)) {\nret = PTR_ERR(srf->sizes);\ngoto out_no_sizes;\n}\nsrf->offsets = kmalloc_array(srf->num_sizes,\nsizeof(*srf->offsets),\nGFP_KERNEL);\nif (unlikely(!srf->offsets)) {\nret = -ENOMEM;\ngoto out_no_offsets;\n}\nsrf->base_size = *srf->sizes;\nsrf->autogen_filter = SVGA3D_TEX_FILTER_NONE;\nsrf->multisample_count = 0;\ncur_bo_offset = 0;\ncur_offset = srf->offsets;\ncur_size = srf->sizes;\nfor (i = 0; i < DRM_VMW_MAX_SURFACE_FACES; ++i) {\nfor (j = 0; j < srf->mip_levels[i]; ++j) {\nuint32_t stride = svga3dsurface_calculate_pitch\n(desc, cur_size);\ncur_offset->face = i;\ncur_offset->mip = j;\ncur_offset->bo_offset = cur_bo_offset;\ncur_bo_offset += svga3dsurface_get_image_buffer_size\n(desc, cur_size, stride);\n++cur_offset;\n++cur_size;\n}\n}\nres->backup_size = cur_bo_offset;\nif (srf->scanout &&\nsrf->num_sizes == 1 &&\nsrf->sizes[0].width == 64 &&\nsrf->sizes[0].height == 64 &&\nsrf->format == SVGA3D_A8R8G8B8) {\nsrf->snooper.image = kzalloc(64 * 64 * 4, GFP_KERNEL);\nif (!srf->snooper.image) {\nDRM_ERROR(\"Failed to allocate cursor_image\\n\");\nret = -ENOMEM;\ngoto out_no_copy;\n}\n} else {\nsrf->snooper.image = NULL;\n}\nsrf->snooper.crtc = NULL;\nuser_srf->prime.base.shareable = false;\nuser_srf->prime.base.tfile = NULL;\nif (drm_is_primary_client(file_priv))\nuser_srf->master = drm_master_get(file_priv->master);\nret = vmw_surface_init(dev_priv, srf, vmw_user_surface_free);\nif (unlikely(ret != 0))\ngoto out_unlock;\nif (dev_priv->has_mob && req->shareable) {\nuint32_t backup_handle;\nret = vmw_user_dmabuf_alloc(dev_priv, tfile,\nres->backup_size,\ntrue,\n&backup_handle,\n&res->backup,\n&user_srf->backup_base);\nif (unlikely(ret != 0)) {\nvmw_resource_unreference(&res);\ngoto out_unlock;\n}\n}\ntmp = vmw_resource_reference(&srf->res);\nret = ttm_prime_object_init(tfile, res->backup_size, &user_srf->prime,\nreq->shareable, VMW_RES_SURFACE,\n&vmw_user_surface_base_release, NULL);\nif (unlikely(ret != 0)) {\nvmw_resource_unreference(&tmp);\nvmw_resource_unreference(&res);\ngoto out_unlock;\n}\nrep->sid = user_srf->prime.base.hash.key;\nvmw_resource_unreference(&res);\nttm_read_unlock(&dev_priv->reservation_sem);\nreturn 0;\nout_no_copy:\nkfree(srf->offsets);\nout_no_offsets:\nkfree(srf->sizes);\nout_no_sizes:\nttm_prime_object_kfree(user_srf, prime);\nout_no_user_srf:\nttm_mem_global_free(vmw_mem_glob(dev_priv), size);\nout_unlock:\nttm_read_unlock(&dev_priv->reservation_sem);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2017-7308": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for block-size data in the packet_set_ring function.",
        "trigger_condition": "Local users can exploit the lack of validation to cause a denial of service or gain privileges through crafted system calls.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly validate certain block-size data, leading to an integer signedness error and out-of-bounds write vulnerability. This allows local users to manipulate system calls and potentially disrupt the system or escalate privileges.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure proper validation of block-size data in the packet_set_ring function. This involves validating block-size parameters to prevent integer signedness errors and out-of-bounds writes. By adding validation checks for block-size data, the code can prevent unauthorized manipulation of system calls and enhance the security of the system."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-7308, which involves an integer signedness error and an out-of-bounds write issue in the packet_set_ring function. By making the specified changes in the code snippet, we ensure proper validation of the block-size data, which helps prevent local users from causing a denial of service or gaining privileges through crafted system calls.\n\nSpecifically, the changes made in the modified code snippet help to correctly handle the block-size data and avoid potential integer signedness errors and out-of-bounds writes that could be exploited by malicious users. By addressing these issues, we enhance the security and stability of the code, reducing the risk of vulnerabilities being exploited.",
      "GPT_purpose": "Set up packet ring buffer for packet socket.",
      "GPT_function": "\n1. Set up packet ring buffer for transmission or reception.\n2. Perform sanity tests and calculations on block size data.\n3. Detach socket from the network and handle buffer swapping and cleanup.",
      "CVE_id": "CVE-2017-7308",
      "code_before_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    (int)(req->tp_block_size -\n\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t\t/* Block transmit is not supported yet */\n\t\t\tif (!tx_ring) {\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\t} else {\n\t\t\t\tstruct tpacket_req3 *req3 = &req_u->req3;\n\n\t\t\t\tif (req3->tp_retire_blk_tov ||\n\t\t\t\t    req3->tp_sizeof_priv ||\n\t\t\t\t    req3->tp_feature_req_word) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
      "code_after_change": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\n\t\tint closing, int tx_ring)\n{\n\tstruct pgv *pg_vec = NULL;\n\tstruct packet_sock *po = pkt_sk(sk);\n\tint was_running, order = 0;\n\tstruct packet_ring_buffer *rb;\n\tstruct sk_buff_head *rb_queue;\n\t__be16 num;\n\tint err = -EINVAL;\n\t/* Added to avoid minimal code churn */\n\tstruct tpacket_req *req = &req_u->req;\n\n\tlock_sock(sk);\n\n\trb = tx_ring ? &po->tx_ring : &po->rx_ring;\n\trb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\n\n\terr = -EBUSY;\n\tif (!closing) {\n\t\tif (atomic_read(&po->mapped))\n\t\t\tgoto out;\n\t\tif (packet_read_pending(rb))\n\t\t\tgoto out;\n\t}\n\n\tif (req->tp_block_nr) {\n\t\t/* Sanity tests and some calculations */\n\t\terr = -EBUSY;\n\t\tif (unlikely(rb->pg_vec))\n\t\t\tgoto out;\n\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V1:\n\t\t\tpo->tp_hdrlen = TPACKET_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V2:\n\t\t\tpo->tp_hdrlen = TPACKET2_HDRLEN;\n\t\t\tbreak;\n\t\tcase TPACKET_V3:\n\t\t\tpo->tp_hdrlen = TPACKET3_HDRLEN;\n\t\t\tbreak;\n\t\t}\n\n\t\terr = -EINVAL;\n\t\tif (unlikely((int)req->tp_block_size <= 0))\n\t\t\tgoto out;\n\t\tif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\n\t\t\tgoto out;\n\t\tif (po->tp_version >= TPACKET_V3 &&\n\t\t    req->tp_block_size <=\n\t\t\t  BLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size < po->tp_hdrlen +\n\t\t\t\t\tpo->tp_reserve))\n\t\t\tgoto out;\n\t\tif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\n\t\t\tgoto out;\n\n\t\trb->frames_per_block = req->tp_block_size / req->tp_frame_size;\n\t\tif (unlikely(rb->frames_per_block == 0))\n\t\t\tgoto out;\n\t\tif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\n\t\t\t\t\treq->tp_frame_nr))\n\t\t\tgoto out;\n\n\t\terr = -ENOMEM;\n\t\torder = get_order(req->tp_block_size);\n\t\tpg_vec = alloc_pg_vec(req, order);\n\t\tif (unlikely(!pg_vec))\n\t\t\tgoto out;\n\t\tswitch (po->tp_version) {\n\t\tcase TPACKET_V3:\n\t\t\t/* Block transmit is not supported yet */\n\t\t\tif (!tx_ring) {\n\t\t\t\tinit_prb_bdqc(po, rb, pg_vec, req_u);\n\t\t\t} else {\n\t\t\t\tstruct tpacket_req3 *req3 = &req_u->req3;\n\n\t\t\t\tif (req3->tp_retire_blk_tov ||\n\t\t\t\t    req3->tp_sizeof_priv ||\n\t\t\t\t    req3->tp_feature_req_word) {\n\t\t\t\t\terr = -EINVAL;\n\t\t\t\t\tgoto out;\n\t\t\t\t}\n\t\t\t}\n\t\t\tbreak;\n\t\tdefault:\n\t\t\tbreak;\n\t\t}\n\t}\n\t/* Done */\n\telse {\n\t\terr = -EINVAL;\n\t\tif (unlikely(req->tp_frame_nr))\n\t\t\tgoto out;\n\t}\n\n\n\t/* Detach socket from network */\n\tspin_lock(&po->bind_lock);\n\twas_running = po->running;\n\tnum = po->num;\n\tif (was_running) {\n\t\tpo->num = 0;\n\t\t__unregister_prot_hook(sk, false);\n\t}\n\tspin_unlock(&po->bind_lock);\n\n\tsynchronize_net();\n\n\terr = -EBUSY;\n\tmutex_lock(&po->pg_vec_lock);\n\tif (closing || atomic_read(&po->mapped) == 0) {\n\t\terr = 0;\n\t\tspin_lock_bh(&rb_queue->lock);\n\t\tswap(rb->pg_vec, pg_vec);\n\t\trb->frame_max = (req->tp_frame_nr - 1);\n\t\trb->head = 0;\n\t\trb->frame_size = req->tp_frame_size;\n\t\tspin_unlock_bh(&rb_queue->lock);\n\n\t\tswap(rb->pg_vec_order, order);\n\t\tswap(rb->pg_vec_len, req->tp_block_nr);\n\n\t\trb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\n\t\tpo->prot_hook.func = (po->rx_ring.pg_vec) ?\n\t\t\t\t\t\ttpacket_rcv : packet_rcv;\n\t\tskb_queue_purge(rb_queue);\n\t\tif (atomic_read(&po->mapped))\n\t\t\tpr_err(\"packet_mmap: vma is busy: %d\\n\",\n\t\t\t       atomic_read(&po->mapped));\n\t}\n\tmutex_unlock(&po->pg_vec_lock);\n\n\tspin_lock(&po->bind_lock);\n\tif (was_running) {\n\t\tpo->num = num;\n\t\tregister_prot_hook(sk);\n\t}\n\tspin_unlock(&po->bind_lock);\n\tif (closing && (po->tp_version > TPACKET_V2)) {\n\t\t/* Because we don't support block-based V3 on tx-ring */\n\t\tif (!tx_ring)\n\t\t\tprb_shutdown_retire_blk_timer(po, rb_queue);\n\t}\n\n\tif (pg_vec)\n\t\tfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\n\trelease_sock(sk);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\t\t    req->tp_block_size <=",
          "\t\t\t  BLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv))"
        ],
        "deleted": [
          "\t\t    (int)(req->tp_block_size -",
          "\t\t\t  BLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for block-size data in the packet_set_ring function.",
      "trigger_condition": "Local users can exploit the lack of validation to cause a denial of service or gain privileges through crafted system calls.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly validate certain block-size data, leading to an integer signedness error and out-of-bounds write vulnerability. This allows local users to manipulate system calls and potentially disrupt the system or escalate privileges.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper validation of block-size data in the packet_set_ring function. This involves validating block-size parameters to prevent integer signedness errors and out-of-bounds writes. By adding validation checks for block-size data, the code can prevent unauthorized manipulation of system calls and enhance the security of the system.",
      "id": 20,
      "code_after_change_normalized": "static int FUN1(struct sock *VAR1, union tpacket_req_u *VAR2,\nint VAR3, int VAR4)\n{\nstruct pgv *VAR5 = NULL;\nstruct packet_sock *VAR6 = FUN2(VAR1);\nint VAR7, VAR8 = 0;\nstruct packet_ring_buffer *VAR9;\nstruct sk_buff_head *VAR10;\n__be16 VAR11;\nint VAR12 = -VAR13;\nstruct tpacket_req *VAR14 = &VAR2->VAR14;\nFUN3(VAR1);\nVAR9 = VAR4 ? &VAR6->VAR4 : &VAR6->VAR15;\nVAR10 = VAR4 ? &VAR1->VAR16 : &VAR1->VAR17;\nVAR12 = -VAR18;\nif (!VAR3) {\nif (FUN4(&VAR6->VAR19))\ngoto VAR20;\nif (FUN5(VAR9))\ngoto VAR20;\n}\nif (VAR14->VAR21) {\nVAR12 = -VAR18;\nif (FUN6(VAR9->VAR5))\ngoto VAR20;\nswitch (VAR6->VAR22) {\ncase VAR23:\nVAR6->VAR24 = VAR25;\nbreak;\ncase VAR26:\nVAR6->VAR24 = VAR27;\nbreak;\ncase VAR28:\nVAR6->VAR24 = VAR29;\nbreak;\n}\nVAR12 = -VAR13;\nif (FUN6((int)VAR14->VAR30 <= 0))\ngoto VAR20;\nif (FUN6(!FUN7(VAR14->VAR30)))\ngoto VAR20;\nif (VAR6->VAR22 >= VAR28 &&\nVAR14->VAR30 <=\nFUN8((VAR31)VAR2->VAR32.VAR33))\ngoto VAR20;\nif (FUN6(VAR14->VAR34 < VAR6->VAR24 +\nVAR6->VAR35))\ngoto VAR20;\nif (FUN6(VAR14->VAR34 & (VAR36 - 1)))\ngoto VAR20;\nVAR9->VAR37 = VAR14->VAR30 / VAR14->VAR34;\nif (FUN6(VAR9->VAR37 == 0))\ngoto VAR20;\nif (FUN6((VAR9->VAR37 * VAR14->VAR21) !=\nVAR14->VAR38))\ngoto VAR20;\nVAR12 = -VAR39;\nVAR8 = FUN9(VAR14->VAR30);\nVAR5 = FUN10(VAR14, VAR8);\nif (FUN6(!VAR5))\ngoto VAR20;\nswitch (VAR6->VAR22) {\ncase VAR28:\nif (!VAR4) {\nFUN11(VAR6, VAR9, VAR5, VAR2);\n} else {\nstruct tpacket_req3 *VAR32 = &VAR2->VAR32;\nif (VAR32->VAR40 ||\nVAR32->VAR33 ||\nVAR32->VAR41) {\nVAR12 = -VAR13;\ngoto VAR20;\n}\n}\nbreak;\ndefault:\nbreak;\n}\n}\nelse {\nVAR12 = -VAR13;\nif (FUN6(VAR14->VAR38))\ngoto VAR20;\n}\nFUN12(&VAR6->VAR42);\nVAR7 = VAR6->VAR43;\nVAR11 = VAR6->VAR11;\nif (VAR7) {\nVAR6->VAR11 = 0;\nFUN13(VAR1, false);\n}\nFUN14(&VAR6->VAR42);\nFUN15();\nVAR12 = -VAR18;\nFUN16(&VAR6->VAR44);\nif (VAR3 || FUN4(&VAR6->VAR19) == 0) {\nVAR12 = 0;\nFUN17(&VAR10->VAR45);\nFUN18(VAR9->VAR5, VAR5);\nVAR9->VAR46 = (VAR14->VAR38 - 1);\nVAR9->VAR47 = 0;\nVAR9->VAR48 = VAR14->VAR34;\nFUN19(&VAR10->VAR45);\nFUN18(VAR9->VAR49, VAR8);\nFUN18(VAR9->VAR50, VAR14->VAR21);\nVAR9->VAR51 = VAR14->VAR30/VAR52;\nVAR6->VAR53.VAR54 = (VAR6->VAR15.VAR5) ?\nVAR55 : VAR56;\nFUN20(VAR10);\nif (FUN4(&VAR6->VAR19))\nFUN21(\"STR\",\nFUN4(&VAR6->VAR19));\n}\nFUN22(&VAR6->VAR44);\nFUN12(&VAR6->VAR42);\nif (VAR7) {\nVAR6->VAR11 = VAR11;\nFUN23(VAR1);\n}\nFUN14(&VAR6->VAR42);\nif (VAR3 && (VAR6->VAR22 > VAR26)) {\nif (!VAR4)\nFUN24(VAR6, VAR10);\n}\nif (VAR5)\nFUN25(VAR5, VAR8, VAR14->VAR21);\nVAR20:\nFUN26(VAR1);\nreturn VAR12;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sock *VAR1, union tpacket_req_u *VAR2,\nint VAR3, int VAR4)\n{\nstruct pgv *VAR5 = NULL;\nstruct packet_sock *VAR6 = FUN2(VAR1);\nint VAR7, VAR8 = 0;\nstruct packet_ring_buffer *VAR9;\nstruct sk_buff_head *VAR10;\n__be16 VAR11;\nint VAR12 = -VAR13;\nstruct tpacket_req *VAR14 = &VAR2->VAR14;\nFUN3(VAR1);\nVAR9 = VAR4 ? &VAR6->VAR4 : &VAR6->VAR15;\nVAR10 = VAR4 ? &VAR1->VAR16 : &VAR1->VAR17;\nVAR12 = -VAR18;\nif (!VAR3) {\nif (FUN4(&VAR6->VAR19))\ngoto VAR20;\nif (FUN5(VAR9))\ngoto VAR20;\n}\nif (VAR14->VAR21) {\nVAR12 = -VAR18;\nif (FUN6(VAR9->VAR5))\ngoto VAR20;\nswitch (VAR6->VAR22) {\ncase VAR23:\nVAR6->VAR24 = VAR25;\nbreak;\ncase VAR26:\nVAR6->VAR24 = VAR27;\nbreak;\ncase VAR28:\nVAR6->VAR24 = VAR29;\nbreak;\n}\nVAR12 = -VAR13;\nif (FUN6((int)VAR14->VAR30 <= 0))\ngoto VAR20;\nif (FUN6(!FUN7(VAR14->VAR30)))\ngoto VAR20;\nif (VAR6->VAR22 >= VAR28 &&\n(int)(VAR14->VAR30 -\nFUN8(VAR2->VAR31.VAR32)) <= 0)\ngoto VAR20;\nif (FUN6(VAR14->VAR33 < VAR6->VAR24 +\nVAR6->VAR34))\ngoto VAR20;\nif (FUN6(VAR14->VAR33 & (VAR35 - 1)))\ngoto VAR20;\nVAR9->VAR36 = VAR14->VAR30 / VAR14->VAR33;\nif (FUN6(VAR9->VAR36 == 0))\ngoto VAR20;\nif (FUN6((VAR9->VAR36 * VAR14->VAR21) !=\nVAR14->VAR37))\ngoto VAR20;\nVAR12 = -VAR38;\nVAR8 = FUN9(VAR14->VAR30);\nVAR5 = FUN10(VAR14, VAR8);\nif (FUN6(!VAR5))\ngoto VAR20;\nswitch (VAR6->VAR22) {\ncase VAR28:\nif (!VAR4) {\nFUN11(VAR6, VAR9, VAR5, VAR2);\n} else {\nstruct tpacket_req3 *VAR31 = &VAR2->VAR31;\nif (VAR31->VAR39 ||\nVAR31->VAR32 ||\nVAR31->VAR40) {\nVAR12 = -VAR13;\ngoto VAR20;\n}\n}\nbreak;\ndefault:\nbreak;\n}\n}\nelse {\nVAR12 = -VAR13;\nif (FUN6(VAR14->VAR37))\ngoto VAR20;\n}\nFUN12(&VAR6->VAR41);\nVAR7 = VAR6->VAR42;\nVAR11 = VAR6->VAR11;\nif (VAR7) {\nVAR6->VAR11 = 0;\nFUN13(VAR1, false);\n}\nFUN14(&VAR6->VAR41);\nFUN15();\nVAR12 = -VAR18;\nFUN16(&VAR6->VAR43);\nif (VAR3 || FUN4(&VAR6->VAR19) == 0) {\nVAR12 = 0;\nFUN17(&VAR10->VAR44);\nFUN18(VAR9->VAR5, VAR5);\nVAR9->VAR45 = (VAR14->VAR37 - 1);\nVAR9->VAR46 = 0;\nVAR9->VAR47 = VAR14->VAR33;\nFUN19(&VAR10->VAR44);\nFUN18(VAR9->VAR48, VAR8);\nFUN18(VAR9->VAR49, VAR14->VAR21);\nVAR9->VAR50 = VAR14->VAR30/VAR51;\nVAR6->VAR52.VAR53 = (VAR6->VAR15.VAR5) ?\nVAR54 : VAR55;\nFUN20(VAR10);\nif (FUN4(&VAR6->VAR19))\nFUN21(\"STR\",\nFUN4(&VAR6->VAR19));\n}\nFUN22(&VAR6->VAR43);\nFUN12(&VAR6->VAR41);\nif (VAR7) {\nVAR6->VAR11 = VAR11;\nFUN23(VAR1);\n}\nFUN14(&VAR6->VAR41);\nif (VAR3 && (VAR6->VAR22 > VAR26)) {\nif (!VAR4)\nFUN24(VAR6, VAR10);\n}\nif (VAR5)\nFUN25(VAR5, VAR8, VAR14->VAR21);\nVAR20:\nFUN26(VAR1);\nreturn VAR12;\n}\n",
      "code_after_change_raw": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\nint closing, int tx_ring)\n{\nstruct pgv *pg_vec = NULL;\nstruct packet_sock *po = pkt_sk(sk);\nint was_running, order = 0;\nstruct packet_ring_buffer *rb;\nstruct sk_buff_head *rb_queue;\n__be16 num;\nint err = -EINVAL;\nstruct tpacket_req *req = &req_u->req;\nlock_sock(sk);\nrb = tx_ring ? &po->tx_ring : &po->rx_ring;\nrb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\nerr = -EBUSY;\nif (!closing) {\nif (atomic_read(&po->mapped))\ngoto out;\nif (packet_read_pending(rb))\ngoto out;\n}\nif (req->tp_block_nr) {\nerr = -EBUSY;\nif (unlikely(rb->pg_vec))\ngoto out;\nswitch (po->tp_version) {\ncase TPACKET_V1:\npo->tp_hdrlen = TPACKET_HDRLEN;\nbreak;\ncase TPACKET_V2:\npo->tp_hdrlen = TPACKET2_HDRLEN;\nbreak;\ncase TPACKET_V3:\npo->tp_hdrlen = TPACKET3_HDRLEN;\nbreak;\n}\nerr = -EINVAL;\nif (unlikely((int)req->tp_block_size <= 0))\ngoto out;\nif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\ngoto out;\nif (po->tp_version >= TPACKET_V3 &&\nreq->tp_block_size <=\nBLK_PLUS_PRIV((u64)req_u->req3.tp_sizeof_priv))\ngoto out;\nif (unlikely(req->tp_frame_size < po->tp_hdrlen +\npo->tp_reserve))\ngoto out;\nif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\ngoto out;\nrb->frames_per_block = req->tp_block_size / req->tp_frame_size;\nif (unlikely(rb->frames_per_block == 0))\ngoto out;\nif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\nreq->tp_frame_nr))\ngoto out;\nerr = -ENOMEM;\norder = get_order(req->tp_block_size);\npg_vec = alloc_pg_vec(req, order);\nif (unlikely(!pg_vec))\ngoto out;\nswitch (po->tp_version) {\ncase TPACKET_V3:\nif (!tx_ring) {\ninit_prb_bdqc(po, rb, pg_vec, req_u);\n} else {\nstruct tpacket_req3 *req3 = &req_u->req3;\nif (req3->tp_retire_blk_tov ||\nreq3->tp_sizeof_priv ||\nreq3->tp_feature_req_word) {\nerr = -EINVAL;\ngoto out;\n}\n}\nbreak;\ndefault:\nbreak;\n}\n}\nelse {\nerr = -EINVAL;\nif (unlikely(req->tp_frame_nr))\ngoto out;\n}\nspin_lock(&po->bind_lock);\nwas_running = po->running;\nnum = po->num;\nif (was_running) {\npo->num = 0;\n__unregister_prot_hook(sk, false);\n}\nspin_unlock(&po->bind_lock);\nsynchronize_net();\nerr = -EBUSY;\nmutex_lock(&po->pg_vec_lock);\nif (closing || atomic_read(&po->mapped) == 0) {\nerr = 0;\nspin_lock_bh(&rb_queue->lock);\nswap(rb->pg_vec, pg_vec);\nrb->frame_max = (req->tp_frame_nr - 1);\nrb->head = 0;\nrb->frame_size = req->tp_frame_size;\nspin_unlock_bh(&rb_queue->lock);\nswap(rb->pg_vec_order, order);\nswap(rb->pg_vec_len, req->tp_block_nr);\nrb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\npo->prot_hook.func = (po->rx_ring.pg_vec) ?\ntpacket_rcv : packet_rcv;\nskb_queue_purge(rb_queue);\nif (atomic_read(&po->mapped))\npr_err(\"packet_mmap: vma is busy: %d\\n\",\natomic_read(&po->mapped));\n}\nmutex_unlock(&po->pg_vec_lock);\nspin_lock(&po->bind_lock);\nif (was_running) {\npo->num = num;\nregister_prot_hook(sk);\n}\nspin_unlock(&po->bind_lock);\nif (closing && (po->tp_version > TPACKET_V2)) {\nif (!tx_ring)\nprb_shutdown_retire_blk_timer(po, rb_queue);\n}\nif (pg_vec)\nfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\nrelease_sock(sk);\nreturn err;\n}\n",
      "code_before_change_raw": "static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u,\nint closing, int tx_ring)\n{\nstruct pgv *pg_vec = NULL;\nstruct packet_sock *po = pkt_sk(sk);\nint was_running, order = 0;\nstruct packet_ring_buffer *rb;\nstruct sk_buff_head *rb_queue;\n__be16 num;\nint err = -EINVAL;\nstruct tpacket_req *req = &req_u->req;\nlock_sock(sk);\nrb = tx_ring ? &po->tx_ring : &po->rx_ring;\nrb_queue = tx_ring ? &sk->sk_write_queue : &sk->sk_receive_queue;\nerr = -EBUSY;\nif (!closing) {\nif (atomic_read(&po->mapped))\ngoto out;\nif (packet_read_pending(rb))\ngoto out;\n}\nif (req->tp_block_nr) {\nerr = -EBUSY;\nif (unlikely(rb->pg_vec))\ngoto out;\nswitch (po->tp_version) {\ncase TPACKET_V1:\npo->tp_hdrlen = TPACKET_HDRLEN;\nbreak;\ncase TPACKET_V2:\npo->tp_hdrlen = TPACKET2_HDRLEN;\nbreak;\ncase TPACKET_V3:\npo->tp_hdrlen = TPACKET3_HDRLEN;\nbreak;\n}\nerr = -EINVAL;\nif (unlikely((int)req->tp_block_size <= 0))\ngoto out;\nif (unlikely(!PAGE_ALIGNED(req->tp_block_size)))\ngoto out;\nif (po->tp_version >= TPACKET_V3 &&\n(int)(req->tp_block_size -\nBLK_PLUS_PRIV(req_u->req3.tp_sizeof_priv)) <= 0)\ngoto out;\nif (unlikely(req->tp_frame_size < po->tp_hdrlen +\npo->tp_reserve))\ngoto out;\nif (unlikely(req->tp_frame_size & (TPACKET_ALIGNMENT - 1)))\ngoto out;\nrb->frames_per_block = req->tp_block_size / req->tp_frame_size;\nif (unlikely(rb->frames_per_block == 0))\ngoto out;\nif (unlikely((rb->frames_per_block * req->tp_block_nr) !=\nreq->tp_frame_nr))\ngoto out;\nerr = -ENOMEM;\norder = get_order(req->tp_block_size);\npg_vec = alloc_pg_vec(req, order);\nif (unlikely(!pg_vec))\ngoto out;\nswitch (po->tp_version) {\ncase TPACKET_V3:\nif (!tx_ring) {\ninit_prb_bdqc(po, rb, pg_vec, req_u);\n} else {\nstruct tpacket_req3 *req3 = &req_u->req3;\nif (req3->tp_retire_blk_tov ||\nreq3->tp_sizeof_priv ||\nreq3->tp_feature_req_word) {\nerr = -EINVAL;\ngoto out;\n}\n}\nbreak;\ndefault:\nbreak;\n}\n}\nelse {\nerr = -EINVAL;\nif (unlikely(req->tp_frame_nr))\ngoto out;\n}\nspin_lock(&po->bind_lock);\nwas_running = po->running;\nnum = po->num;\nif (was_running) {\npo->num = 0;\n__unregister_prot_hook(sk, false);\n}\nspin_unlock(&po->bind_lock);\nsynchronize_net();\nerr = -EBUSY;\nmutex_lock(&po->pg_vec_lock);\nif (closing || atomic_read(&po->mapped) == 0) {\nerr = 0;\nspin_lock_bh(&rb_queue->lock);\nswap(rb->pg_vec, pg_vec);\nrb->frame_max = (req->tp_frame_nr - 1);\nrb->head = 0;\nrb->frame_size = req->tp_frame_size;\nspin_unlock_bh(&rb_queue->lock);\nswap(rb->pg_vec_order, order);\nswap(rb->pg_vec_len, req->tp_block_nr);\nrb->pg_vec_pages = req->tp_block_size/PAGE_SIZE;\npo->prot_hook.func = (po->rx_ring.pg_vec) ?\ntpacket_rcv : packet_rcv;\nskb_queue_purge(rb_queue);\nif (atomic_read(&po->mapped))\npr_err(\"packet_mmap: vma is busy: %d\\n\",\natomic_read(&po->mapped));\n}\nmutex_unlock(&po->pg_vec_lock);\nspin_lock(&po->bind_lock);\nif (was_running) {\npo->num = num;\nregister_prot_hook(sk);\n}\nspin_unlock(&po->bind_lock);\nif (closing && (po->tp_version > TPACKET_V2)) {\nif (!tx_ring)\nprb_shutdown_retire_blk_timer(po, rb_queue);\n}\nif (pg_vec)\nfree_pg_vec(pg_vec, order, req->tp_block_nr);\nout:\nrelease_sock(sk);\nreturn err;\n}\n"
    }
  ],
  "CVE-2017-8067": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper memory allocation and handling for scatterlist data buffer, leading to potential memory corruption or system crash when using more than one virtual page for DMA operations.",
        "trigger_condition": "Interacting incorrectly with a specific kernel configuration option that allows scatterlist to use more than one virtual page for DMA operations.",
        "specific_code_behavior_causing_vulnerability": "The code directly uses the input buffer 'buf' in the scatterlist without proper memory allocation or copying, which can lead to issues when the scatterlist uses more than one virtual page for DMA operations, potentially causing memory corruption or system crash."
      },
      "solution": "To mitigate the vulnerability, it is necessary to allocate a new buffer, copy the contents of the input buffer 'buf' into it, and use the new buffer in the scatterlist. Additionally, the allocated buffer should be properly freed after its use to prevent memory leaks. This ensures that the scatterlist operates on a properly allocated buffer within a single virtual page, addressing the memory corruption or system crash risk associated with using more than one virtual page for DMA operations.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2017-8067 by ensuring that the data buffer used in the scatterlist is properly allocated and freed. \n\nIn the original code snippet, the buffer 'buf' is directly used in the scatterlist without any memory allocation or copying. This can lead to issues when the CONFIG_VMAP_STACK option is enabled, as it may cause the scatterlist to use more than one virtual page for DMA operations. This can result in a denial of service (system crash or memory corruption) or other unspecified impacts.\n\nThe modified code snippet addresses this vulnerability by using 'kmemdup' to allocate a new buffer 'data' and copying the contents of 'buf' into it. This ensures that the scatterlist operates on a properly allocated buffer that is guaranteed to be within a single virtual page. Additionally, the allocated buffer is freed using 'kfree' after its use, preventing memory leaks.\n\nBy making these changes, the code mitigates the risk associated with the vulnerability and ensures the proper handling of data buffers in the scatterlist, thereby improving the overall security and stability of the system.",
      "GPT_purpose": "To send characters to a specified port for a virtual terminal.",
      "GPT_function": "\n1. put_chars - Writes characters to a virtual terminal.\n2. find_port_by_vtermno - Finds a port by virtual terminal number.\n3. __send_to_port - Sends data to a port.",
      "CVE_id": "CVE-2017-8067",
      "code_before_change": "static int put_chars(u32 vtermno, const char *buf, int count)\n{\n\tstruct port *port;\n\tstruct scatterlist sg[1];\n\n\tif (unlikely(early_put_chars))\n\t\treturn early_put_chars(vtermno, buf, count);\n\n\tport = find_port_by_vtermno(vtermno);\n\tif (!port)\n\t\treturn -EPIPE;\n\n\tsg_init_one(sg, buf, count);\n\treturn __send_to_port(port, sg, 1, count, (void *)buf, false);\n}",
      "code_after_change": "static int put_chars(u32 vtermno, const char *buf, int count)\n{\n\tstruct port *port;\n\tstruct scatterlist sg[1];\n\tvoid *data;\n\tint ret;\n\n\tif (unlikely(early_put_chars))\n\t\treturn early_put_chars(vtermno, buf, count);\n\n\tport = find_port_by_vtermno(vtermno);\n\tif (!port)\n\t\treturn -EPIPE;\n\n\tdata = kmemdup(buf, count, GFP_ATOMIC);\n\tif (!data)\n\t\treturn -ENOMEM;\n\n\tsg_init_one(sg, data, count);\n\tret = __send_to_port(port, sg, 1, count, data, false);\n\tkfree(data);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tvoid *data;",
          "\tint ret;",
          "\tdata = kmemdup(buf, count, GFP_ATOMIC);",
          "\tif (!data)",
          "\t\treturn -ENOMEM;",
          "",
          "\tsg_init_one(sg, data, count);",
          "\tret = __send_to_port(port, sg, 1, count, data, false);",
          "\tkfree(data);",
          "\treturn ret;"
        ],
        "deleted": [
          "\tsg_init_one(sg, buf, count);",
          "\treturn __send_to_port(port, sg, 1, count, (void *)buf, false);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper memory allocation and handling for scatterlist data buffer, leading to potential memory corruption or system crash when using more than one virtual page for DMA operations.",
      "trigger_condition": "Interacting incorrectly with a specific kernel configuration option that allows scatterlist to use more than one virtual page for DMA operations.",
      "specific_code_behavior_causing_vulnerability": "The code directly uses the input buffer 'buf' in the scatterlist without proper memory allocation or copying, which can lead to issues when the scatterlist uses more than one virtual page for DMA operations, potentially causing memory corruption or system crash.",
      "id": 21,
      "code_after_change_normalized": "static int FUN1(u32 VAR1, const char *VAR2, int VAR3)\n{\nstruct VAR4 *VAR4;\nstruct scatterlist VAR5[1];\nvoid *VAR6;\nint VAR7;\nif (FUN2(VAR8))\nreturn FUN3(VAR1, VAR2, VAR3);\nVAR4 = FUN4(VAR1);\nif (!VAR4)\nreturn -VAR9;\nVAR6 = FUN5(VAR2, VAR3, VAR10);\nif (!VAR6)\nreturn -VAR11;\nFUN6(VAR5, VAR6, VAR3);\nVAR7 = FUN7(VAR4, VAR5, 1, VAR3, VAR6, false);\nFUN8(VAR6);\nreturn VAR7;\n}\n",
      "code_before_change_normalized": "static int FUN1(u32 VAR1, const char *VAR2, int VAR3)\n{\nstruct VAR4 *VAR4;\nstruct scatterlist VAR5[1];\nif (FUN2(VAR6))\nreturn FUN3(VAR1, VAR2, VAR3);\nVAR4 = FUN4(VAR1);\nif (!VAR4)\nreturn -VAR7;\nFUN5(VAR5, VAR2, VAR3);\nreturn FUN6(VAR4, VAR5, 1, VAR3, (void *)VAR2, false);\n}\n",
      "code_after_change_raw": "static int put_chars(u32 vtermno, const char *buf, int count)\n{\nstruct port *port;\nstruct scatterlist sg[1];\nvoid *data;\nint ret;\nif (unlikely(early_put_chars))\nreturn early_put_chars(vtermno, buf, count);\nport = find_port_by_vtermno(vtermno);\nif (!port)\nreturn -EPIPE;\ndata = kmemdup(buf, count, GFP_ATOMIC);\nif (!data)\nreturn -ENOMEM;\nsg_init_one(sg, data, count);\nret = __send_to_port(port, sg, 1, count, data, false);\nkfree(data);\nreturn ret;\n}\n",
      "code_before_change_raw": "static int put_chars(u32 vtermno, const char *buf, int count)\n{\nstruct port *port;\nstruct scatterlist sg[1];\nif (unlikely(early_put_chars))\nreturn early_put_chars(vtermno, buf, count);\nport = find_port_by_vtermno(vtermno);\nif (!port)\nreturn -EPIPE;\nsg_init_one(sg, buf, count);\nreturn __send_to_port(port, sg, 1, count, (void *)buf, false);\n}\n"
    }
  ],
  "CVE-2018-1068": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper validation of offsets in the code.",
        "trigger_condition": "A privileged user can arbitrarily write to a limited range of kernel memory due to the vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly validate the offsets used in processing data, allowing for potential out-of-bounds memory access and unauthorized writes to kernel memory."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a loop to validate the offsets before processing them. This loop should check that each offset is within the bounds of the total data size and ensure that the offsets are in ascending order to prevent out-of-bounds memory access. By adding this validation step, the code becomes more secure and prevents exploitation of the vulnerability by restricting processing to only valid offsets.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-1068 in the code snippet. The vulnerability allowed a privileged user to arbitrarily write to a limited range of kernel memory due to improper validation of offsets in the code.\n\nThe modification includes adding a loop to check the validity of the offsets before processing them in the subsequent loop. This loop ensures that each offset is within the bounds of the total size of the data being processed. Additionally, it checks that the offsets are in ascending order to prevent any out-of-bounds memory access.\n\nBy adding this validation step, the code becomes more secure and prevents potential exploitation of the vulnerability by ensuring that only valid offsets are processed, thus mitigating the risk of unauthorized memory writes.",
      "GPT_purpose": "Calculate the size of an entry in a bridge table and update offsets for match, watchers, target, and next entry based on the entry's structure.",
      "GPT_function": "\n1. Calculate the size of an entry in the ebt_entries buffer.\n2. Handle offsets for matches, watchers, targets, and the next ebt_entry.\n3. Update offsets in the buffer if needed.\n4. Check and adjust offsets to prevent writing outside the buffer.\n5. Add compatibility offset for the bridge protocol.",
      "CVE_id": "CVE-2018-1068",
      "code_before_change": "static int size_entry_mwt(struct ebt_entry *entry, const unsigned char *base,\n\t\t\t  unsigned int *total,\n\t\t\t  struct ebt_entries_buf_state *state)\n{\n\tunsigned int i, j, startoff, new_offset = 0;\n\t/* stores match/watchers/targets & offset of next struct ebt_entry: */\n\tunsigned int offsets[4];\n\tunsigned int *offsets_update = NULL;\n\tint ret;\n\tchar *buf_start;\n\n\tif (*total < sizeof(struct ebt_entries))\n\t\treturn -EINVAL;\n\n\tif (!entry->bitmask) {\n\t\t*total -= sizeof(struct ebt_entries);\n\t\treturn ebt_buf_add(state, entry, sizeof(struct ebt_entries));\n\t}\n\tif (*total < sizeof(*entry) || entry->next_offset < sizeof(*entry))\n\t\treturn -EINVAL;\n\n\tstartoff = state->buf_user_offset;\n\t/* pull in most part of ebt_entry, it does not need to be changed. */\n\tret = ebt_buf_add(state, entry,\n\t\t\toffsetof(struct ebt_entry, watchers_offset));\n\tif (ret < 0)\n\t\treturn ret;\n\n\toffsets[0] = sizeof(struct ebt_entry); /* matches come first */\n\tmemcpy(&offsets[1], &entry->watchers_offset,\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\n\tif (state->buf_kern_start) {\n\t\tbuf_start = state->buf_kern_start + state->buf_kern_offset;\n\t\toffsets_update = (unsigned int *) buf_start;\n\t}\n\tret = ebt_buf_add(state, &offsets[1],\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\tif (ret < 0)\n\t\treturn ret;\n\tbuf_start = (char *) entry;\n\t/* 0: matches offset, always follows ebt_entry.\n\t * 1: watchers offset, from ebt_entry structure\n\t * 2: target offset, from ebt_entry structure\n\t * 3: next ebt_entry offset, from ebt_entry structure\n\t *\n\t * offsets are relative to beginning of struct ebt_entry (i.e., 0).\n\t */\n\tfor (i = 0, j = 1 ; j < 4 ; j++, i++) {\n\t\tstruct compat_ebt_entry_mwt *match32;\n\t\tunsigned int size;\n\t\tchar *buf = buf_start + offsets[i];\n\n\t\tif (offsets[i] > offsets[j])\n\t\t\treturn -EINVAL;\n\n\t\tmatch32 = (struct compat_ebt_entry_mwt *) buf;\n\t\tsize = offsets[j] - offsets[i];\n\t\tret = ebt_size_mwt(match32, size, i, state, base);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tnew_offset += ret;\n\t\tif (offsets_update && new_offset) {\n\t\t\tpr_debug(\"change offset %d to %d\\n\",\n\t\t\t\toffsets_update[i], offsets[j] + new_offset);\n\t\t\toffsets_update[i] = offsets[j] + new_offset;\n\t\t}\n\t}\n\n\tif (state->buf_kern_start == NULL) {\n\t\tunsigned int offset = buf_start - (char *) base;\n\n\t\tret = xt_compat_add_offset(NFPROTO_BRIDGE, offset, new_offset);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tstartoff = state->buf_user_offset - startoff;\n\n\tif (WARN_ON(*total < startoff))\n\t\treturn -EINVAL;\n\t*total -= startoff;\n\treturn 0;\n}",
      "code_after_change": "static int size_entry_mwt(struct ebt_entry *entry, const unsigned char *base,\n\t\t\t  unsigned int *total,\n\t\t\t  struct ebt_entries_buf_state *state)\n{\n\tunsigned int i, j, startoff, new_offset = 0;\n\t/* stores match/watchers/targets & offset of next struct ebt_entry: */\n\tunsigned int offsets[4];\n\tunsigned int *offsets_update = NULL;\n\tint ret;\n\tchar *buf_start;\n\n\tif (*total < sizeof(struct ebt_entries))\n\t\treturn -EINVAL;\n\n\tif (!entry->bitmask) {\n\t\t*total -= sizeof(struct ebt_entries);\n\t\treturn ebt_buf_add(state, entry, sizeof(struct ebt_entries));\n\t}\n\tif (*total < sizeof(*entry) || entry->next_offset < sizeof(*entry))\n\t\treturn -EINVAL;\n\n\tstartoff = state->buf_user_offset;\n\t/* pull in most part of ebt_entry, it does not need to be changed. */\n\tret = ebt_buf_add(state, entry,\n\t\t\toffsetof(struct ebt_entry, watchers_offset));\n\tif (ret < 0)\n\t\treturn ret;\n\n\toffsets[0] = sizeof(struct ebt_entry); /* matches come first */\n\tmemcpy(&offsets[1], &entry->watchers_offset,\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\n\tif (state->buf_kern_start) {\n\t\tbuf_start = state->buf_kern_start + state->buf_kern_offset;\n\t\toffsets_update = (unsigned int *) buf_start;\n\t}\n\tret = ebt_buf_add(state, &offsets[1],\n\t\t\tsizeof(offsets) - sizeof(offsets[0]));\n\tif (ret < 0)\n\t\treturn ret;\n\tbuf_start = (char *) entry;\n\t/* 0: matches offset, always follows ebt_entry.\n\t * 1: watchers offset, from ebt_entry structure\n\t * 2: target offset, from ebt_entry structure\n\t * 3: next ebt_entry offset, from ebt_entry structure\n\t *\n\t * offsets are relative to beginning of struct ebt_entry (i.e., 0).\n\t */\n\tfor (i = 0; i < 4 ; ++i) {\n\t\tif (offsets[i] >= *total)\n\t\t\treturn -EINVAL;\n\t\tif (i == 0)\n\t\t\tcontinue;\n\t\tif (offsets[i-1] > offsets[i])\n\t\t\treturn -EINVAL;\n\t}\n\n\tfor (i = 0, j = 1 ; j < 4 ; j++, i++) {\n\t\tstruct compat_ebt_entry_mwt *match32;\n\t\tunsigned int size;\n\t\tchar *buf = buf_start + offsets[i];\n\n\t\tif (offsets[i] > offsets[j])\n\t\t\treturn -EINVAL;\n\n\t\tmatch32 = (struct compat_ebt_entry_mwt *) buf;\n\t\tsize = offsets[j] - offsets[i];\n\t\tret = ebt_size_mwt(match32, size, i, state, base);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t\tnew_offset += ret;\n\t\tif (offsets_update && new_offset) {\n\t\t\tpr_debug(\"change offset %d to %d\\n\",\n\t\t\t\toffsets_update[i], offsets[j] + new_offset);\n\t\t\toffsets_update[i] = offsets[j] + new_offset;\n\t\t}\n\t}\n\n\tif (state->buf_kern_start == NULL) {\n\t\tunsigned int offset = buf_start - (char *) base;\n\n\t\tret = xt_compat_add_offset(NFPROTO_BRIDGE, offset, new_offset);\n\t\tif (ret < 0)\n\t\t\treturn ret;\n\t}\n\n\tstartoff = state->buf_user_offset - startoff;\n\n\tif (WARN_ON(*total < startoff))\n\t\treturn -EINVAL;\n\t*total -= startoff;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tfor (i = 0; i < 4 ; ++i) {",
          "\t\tif (offsets[i] >= *total)",
          "\t\t\treturn -EINVAL;",
          "\t\tif (i == 0)",
          "\t\t\tcontinue;",
          "\t\tif (offsets[i-1] > offsets[i])",
          "\t\t\treturn -EINVAL;",
          "\t}",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Improper validation of offsets in the code.",
      "trigger_condition": "A privileged user can arbitrarily write to a limited range of kernel memory due to the vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly validate the offsets used in processing data, allowing for potential out-of-bounds memory access and unauthorized writes to kernel memory.",
      "id": 22,
      "code_after_change_normalized": "static int FUN1(struct ebt_entry *VAR1, const unsigned char *VAR2,\nunsigned int *VAR3,\nstruct ebt_entries_buf_state *VAR4)\n{\nunsigned int VAR5, VAR6, VAR7, VAR8 = 0;\nunsigned int VAR9[4];\nunsigned int *VAR10 = NULL;\nint VAR11;\nchar *VAR12;\nif (*VAR3 < sizeof(struct VAR13))\nreturn -VAR14;\nif (!VAR1->VAR15) {\n*VAR3 -= sizeof(struct VAR13);\nreturn FUN2(VAR4, VAR1, sizeof(struct VAR13));\n}\nif (*VAR3 < sizeof(*VAR1) || VAR1->VAR16 < sizeof(*VAR1))\nreturn -VAR14;\nVAR7 = VAR4->VAR17;\nVAR11 = FUN2(VAR4, VAR1,\nFUN3(struct VAR18, VAR19));\nif (VAR11 < 0)\nreturn VAR11;\nVAR9[0] = sizeof(struct VAR18); \nFUN4(&VAR9[1], &VAR1->VAR19,\nsizeof(VAR9) - sizeof(VAR9[0]));\nif (VAR4->VAR20) {\nVAR12 = VAR4->VAR20 + VAR4->VAR21;\nVAR10 = (unsigned int *) VAR12;\n}\nVAR11 = FUN2(VAR4, &VAR9[1],\nsizeof(VAR9) - sizeof(VAR9[0]));\nif (VAR11 < 0)\nreturn VAR11;\nVAR12 = (char *) VAR1;\nfor (VAR5 = 0; VAR5 < 4 ; ++VAR5) {\nif (VAR9[VAR5] >= *VAR3)\nreturn -VAR14;\nif (VAR5 == 0)\ncontinue;\nif (VAR9[VAR5-1] > VAR9[VAR5])\nreturn -VAR14;\n}\nfor (VAR5 = 0, VAR6 = 1 ; VAR6 < 4 ; VAR6++, VAR5++) {\nstruct compat_ebt_entry_mwt *VAR22;\nunsigned int VAR23;\nchar *VAR24 = VAR12 + VAR9[VAR5];\nif (VAR9[VAR5] > VAR9[VAR6])\nreturn -VAR14;\nVAR22 = (struct VAR25 *) VAR24;\nVAR23 = VAR9[VAR6] - VAR9[VAR5];\nVAR11 = FUN5(VAR22, VAR23, VAR5, VAR4, VAR2);\nif (VAR11 < 0)\nreturn VAR11;\nVAR8 += VAR11;\nif (VAR10 && VAR8) {\nFUN6(\"STR\",\nVAR10[VAR5], VAR9[VAR6] + VAR8);\nVAR10[VAR5] = VAR9[VAR6] + VAR8;\n}\n}\nif (VAR4->VAR20 == NULL) {\nunsigned int VAR26 = VAR12 - (char *) VAR2;\nVAR11 = FUN7(VAR27, VAR26, VAR8);\nif (VAR11 < 0)\nreturn VAR11;\n}\nVAR7 = VAR4->VAR17 - VAR7;\nif (FUN8(*VAR3 < VAR7))\nreturn -VAR14;\n*VAR3 -= VAR7;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct ebt_entry *VAR1, const unsigned char *VAR2,\nunsigned int *VAR3,\nstruct ebt_entries_buf_state *VAR4)\n{\nunsigned int VAR5, VAR6, VAR7, VAR8 = 0;\nunsigned int VAR9[4];\nunsigned int *VAR10 = NULL;\nint VAR11;\nchar *VAR12;\nif (*VAR3 < sizeof(struct VAR13))\nreturn -VAR14;\nif (!VAR1->VAR15) {\n*VAR3 -= sizeof(struct VAR13);\nreturn FUN2(VAR4, VAR1, sizeof(struct VAR13));\n}\nif (*VAR3 < sizeof(*VAR1) || VAR1->VAR16 < sizeof(*VAR1))\nreturn -VAR14;\nVAR7 = VAR4->VAR17;\nVAR11 = FUN2(VAR4, VAR1,\nFUN3(struct VAR18, VAR19));\nif (VAR11 < 0)\nreturn VAR11;\nVAR9[0] = sizeof(struct VAR18); \nFUN4(&VAR9[1], &VAR1->VAR19,\nsizeof(VAR9) - sizeof(VAR9[0]));\nif (VAR4->VAR20) {\nVAR12 = VAR4->VAR20 + VAR4->VAR21;\nVAR10 = (unsigned int *) VAR12;\n}\nVAR11 = FUN2(VAR4, &VAR9[1],\nsizeof(VAR9) - sizeof(VAR9[0]));\nif (VAR11 < 0)\nreturn VAR11;\nVAR12 = (char *) VAR1;\nfor (VAR5 = 0, VAR6 = 1 ; VAR6 < 4 ; VAR6++, VAR5++) {\nstruct compat_ebt_entry_mwt *VAR22;\nunsigned int VAR23;\nchar *VAR24 = VAR12 + VAR9[VAR5];\nif (VAR9[VAR5] > VAR9[VAR6])\nreturn -VAR14;\nVAR22 = (struct VAR25 *) VAR24;\nVAR23 = VAR9[VAR6] - VAR9[VAR5];\nVAR11 = FUN5(VAR22, VAR23, VAR5, VAR4, VAR2);\nif (VAR11 < 0)\nreturn VAR11;\nVAR8 += VAR11;\nif (VAR10 && VAR8) {\nFUN6(\"STR\",\nVAR10[VAR5], VAR9[VAR6] + VAR8);\nVAR10[VAR5] = VAR9[VAR6] + VAR8;\n}\n}\nif (VAR4->VAR20 == NULL) {\nunsigned int VAR26 = VAR12 - (char *) VAR2;\nVAR11 = FUN7(VAR27, VAR26, VAR8);\nif (VAR11 < 0)\nreturn VAR11;\n}\nVAR7 = VAR4->VAR17 - VAR7;\nif (FUN8(*VAR3 < VAR7))\nreturn -VAR14;\n*VAR3 -= VAR7;\nreturn 0;\n}\n",
      "code_after_change_raw": "static int size_entry_mwt(struct ebt_entry *entry, const unsigned char *base,\nunsigned int *total,\nstruct ebt_entries_buf_state *state)\n{\nunsigned int i, j, startoff, new_offset = 0;\nunsigned int offsets[4];\nunsigned int *offsets_update = NULL;\nint ret;\nchar *buf_start;\nif (*total < sizeof(struct ebt_entries))\nreturn -EINVAL;\nif (!entry->bitmask) {\n*total -= sizeof(struct ebt_entries);\nreturn ebt_buf_add(state, entry, sizeof(struct ebt_entries));\n}\nif (*total < sizeof(*entry) || entry->next_offset < sizeof(*entry))\nreturn -EINVAL;\nstartoff = state->buf_user_offset;\nret = ebt_buf_add(state, entry,\noffsetof(struct ebt_entry, watchers_offset));\nif (ret < 0)\nreturn ret;\noffsets[0] = sizeof(struct ebt_entry); \nmemcpy(&offsets[1], &entry->watchers_offset,\nsizeof(offsets) - sizeof(offsets[0]));\nif (state->buf_kern_start) {\nbuf_start = state->buf_kern_start + state->buf_kern_offset;\noffsets_update = (unsigned int *) buf_start;\n}\nret = ebt_buf_add(state, &offsets[1],\nsizeof(offsets) - sizeof(offsets[0]));\nif (ret < 0)\nreturn ret;\nbuf_start = (char *) entry;\nfor (i = 0; i < 4 ; ++i) {\nif (offsets[i] >= *total)\nreturn -EINVAL;\nif (i == 0)\ncontinue;\nif (offsets[i-1] > offsets[i])\nreturn -EINVAL;\n}\nfor (i = 0, j = 1 ; j < 4 ; j++, i++) {\nstruct compat_ebt_entry_mwt *match32;\nunsigned int size;\nchar *buf = buf_start + offsets[i];\nif (offsets[i] > offsets[j])\nreturn -EINVAL;\nmatch32 = (struct compat_ebt_entry_mwt *) buf;\nsize = offsets[j] - offsets[i];\nret = ebt_size_mwt(match32, size, i, state, base);\nif (ret < 0)\nreturn ret;\nnew_offset += ret;\nif (offsets_update && new_offset) {\npr_debug(\"change offset %d to %d\\n\",\noffsets_update[i], offsets[j] + new_offset);\noffsets_update[i] = offsets[j] + new_offset;\n}\n}\nif (state->buf_kern_start == NULL) {\nunsigned int offset = buf_start - (char *) base;\nret = xt_compat_add_offset(NFPROTO_BRIDGE, offset, new_offset);\nif (ret < 0)\nreturn ret;\n}\nstartoff = state->buf_user_offset - startoff;\nif (WARN_ON(*total < startoff))\nreturn -EINVAL;\n*total -= startoff;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int size_entry_mwt(struct ebt_entry *entry, const unsigned char *base,\nunsigned int *total,\nstruct ebt_entries_buf_state *state)\n{\nunsigned int i, j, startoff, new_offset = 0;\nunsigned int offsets[4];\nunsigned int *offsets_update = NULL;\nint ret;\nchar *buf_start;\nif (*total < sizeof(struct ebt_entries))\nreturn -EINVAL;\nif (!entry->bitmask) {\n*total -= sizeof(struct ebt_entries);\nreturn ebt_buf_add(state, entry, sizeof(struct ebt_entries));\n}\nif (*total < sizeof(*entry) || entry->next_offset < sizeof(*entry))\nreturn -EINVAL;\nstartoff = state->buf_user_offset;\nret = ebt_buf_add(state, entry,\noffsetof(struct ebt_entry, watchers_offset));\nif (ret < 0)\nreturn ret;\noffsets[0] = sizeof(struct ebt_entry); \nmemcpy(&offsets[1], &entry->watchers_offset,\nsizeof(offsets) - sizeof(offsets[0]));\nif (state->buf_kern_start) {\nbuf_start = state->buf_kern_start + state->buf_kern_offset;\noffsets_update = (unsigned int *) buf_start;\n}\nret = ebt_buf_add(state, &offsets[1],\nsizeof(offsets) - sizeof(offsets[0]));\nif (ret < 0)\nreturn ret;\nbuf_start = (char *) entry;\nfor (i = 0, j = 1 ; j < 4 ; j++, i++) {\nstruct compat_ebt_entry_mwt *match32;\nunsigned int size;\nchar *buf = buf_start + offsets[i];\nif (offsets[i] > offsets[j])\nreturn -EINVAL;\nmatch32 = (struct compat_ebt_entry_mwt *) buf;\nsize = offsets[j] - offsets[i];\nret = ebt_size_mwt(match32, size, i, state, base);\nif (ret < 0)\nreturn ret;\nnew_offset += ret;\nif (offsets_update && new_offset) {\npr_debug(\"change offset %d to %d\\n\",\noffsets_update[i], offsets[j] + new_offset);\noffsets_update[i] = offsets[j] + new_offset;\n}\n}\nif (state->buf_kern_start == NULL) {\nunsigned int offset = buf_start - (char *) base;\nret = xt_compat_add_offset(NFPROTO_BRIDGE, offset, new_offset);\nif (ret < 0)\nreturn ret;\n}\nstartoff = state->buf_user_offset - startoff;\nif (WARN_ON(*total < startoff))\nreturn -EINVAL;\n*total -= startoff;\nreturn 0;\n}\n"
    }
  ],
  "CVE-2018-10878": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for block group boundaries in the ext4 filesystem initialization code.",
        "trigger_condition": "A local user mounts and operates a crafted ext4 filesystem image, causing an out-of-bounds write due to incorrect handling of block group boundaries.",
        "specific_code_behavior_causing_vulnerability": "The code fails to properly check block group boundaries when setting bits in the block and inode bitmaps, leading to potential out-of-bounds writes and a denial of service or other unspecified impact.",
        "solution": "To mitigate the vulnerability, it is necessary to remove the condition that allows setting bits outside the block group boundaries. By ensuring that the write operations stay within the bounds of the buffer, the code prevents the out-of-bounds write issue and the associated denial of service risk. In this case, the solution involves modifying the code to remove the condition that checks for flex_bg or block group boundaries before setting the bitmap bits."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-10878, which involves an out-of-bounds write issue in the ext4 filesystem code. By removing the condition `!flex_bg ||` from the `if` statements that check whether to set a bit in the block and inode bitmaps, the code ensures that the write operation stays within the bounds of the buffer. This change prevents a local user from exploiting the vulnerability to cause a denial of service or other unspecified impacts by operating a crafted ext4 filesystem image.",
      "GPT_purpose": "Initialize the block bitmap for an ext4 filesystem group, setting bits for block and inode bitmaps, and inode table while handling potential corruption.",
      "GPT_function": "\n1. Initialize the block bitmap for an ext4 filesystem.\n2. Verify checksum for the group descriptor.\n3. Set bits for block and inode bitmaps, and inode table.\n4. Mark the end of the block bitmap if the number of blocks within the group is less than a certain threshold.",
      "CVE_id": "CVE-2018-10878",
      "code_before_change": "static int ext4_init_block_bitmap(struct super_block *sb,\n\t\t\t\t   struct buffer_head *bh,\n\t\t\t\t   ext4_group_t block_group,\n\t\t\t\t   struct ext4_group_desc *gdp)\n{\n\tunsigned int bit, bit_max;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t start, tmp;\n\tint flex_bg = 0;\n\n\tJ_ASSERT_BH(bh, buffer_locked(bh));\n\n\t/* If checksum is bad mark all blocks used to prevent allocation\n\t * essentially implementing a per-group read-only flag. */\n\tif (!ext4_group_desc_csum_verify(sb, block_group, gdp)) {\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT |\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn -EFSBADCRC;\n\t}\n\tmemset(bh->b_data, 0, sb->s_blocksize);\n\n\tbit_max = ext4_num_base_meta_clusters(sb, block_group);\n\tif ((bit_max >> 3) >= bh->b_size)\n\t\treturn -EFSCORRUPTED;\n\n\tfor (bit = 0; bit < bit_max; bit++)\n\t\text4_set_bit(bit, bh->b_data);\n\n\tstart = ext4_group_first_block_no(sb, block_group);\n\n\tif (ext4_has_feature_flex_bg(sb))\n\t\tflex_bg = 1;\n\n\t/* Set bits for block and inode bitmaps, and inode table */\n\ttmp = ext4_block_bitmap(sb, gdp);\n\tif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))\n\t\text4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n\n\ttmp = ext4_inode_bitmap(sb, gdp);\n\tif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))\n\t\text4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n\n\ttmp = ext4_inode_table(sb, gdp);\n\tfor (; tmp < ext4_inode_table(sb, gdp) +\n\t\t     sbi->s_itb_per_group; tmp++) {\n\t\tif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))\n\t\t\text4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n\t}\n\n\t/*\n\t * Also if the number of blocks within the group is less than\n\t * the blocksize * 8 ( which is the size of bitmap ), set rest\n\t * of the block bitmap to 1\n\t */\n\text4_mark_bitmap_end(num_clusters_in_group(sb, block_group),\n\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\treturn 0;\n}",
      "code_after_change": "static int ext4_init_block_bitmap(struct super_block *sb,\n\t\t\t\t   struct buffer_head *bh,\n\t\t\t\t   ext4_group_t block_group,\n\t\t\t\t   struct ext4_group_desc *gdp)\n{\n\tunsigned int bit, bit_max;\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\text4_fsblk_t start, tmp;\n\n\tJ_ASSERT_BH(bh, buffer_locked(bh));\n\n\t/* If checksum is bad mark all blocks used to prevent allocation\n\t * essentially implementing a per-group read-only flag. */\n\tif (!ext4_group_desc_csum_verify(sb, block_group, gdp)) {\n\t\text4_mark_group_bitmap_corrupted(sb, block_group,\n\t\t\t\t\tEXT4_GROUP_INFO_BBITMAP_CORRUPT |\n\t\t\t\t\tEXT4_GROUP_INFO_IBITMAP_CORRUPT);\n\t\treturn -EFSBADCRC;\n\t}\n\tmemset(bh->b_data, 0, sb->s_blocksize);\n\n\tbit_max = ext4_num_base_meta_clusters(sb, block_group);\n\tif ((bit_max >> 3) >= bh->b_size)\n\t\treturn -EFSCORRUPTED;\n\n\tfor (bit = 0; bit < bit_max; bit++)\n\t\text4_set_bit(bit, bh->b_data);\n\n\tstart = ext4_group_first_block_no(sb, block_group);\n\n\t/* Set bits for block and inode bitmaps, and inode table */\n\ttmp = ext4_block_bitmap(sb, gdp);\n\tif (ext4_block_in_group(sb, tmp, block_group))\n\t\text4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n\n\ttmp = ext4_inode_bitmap(sb, gdp);\n\tif (ext4_block_in_group(sb, tmp, block_group))\n\t\text4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n\n\ttmp = ext4_inode_table(sb, gdp);\n\tfor (; tmp < ext4_inode_table(sb, gdp) +\n\t\t     sbi->s_itb_per_group; tmp++) {\n\t\tif (ext4_block_in_group(sb, tmp, block_group))\n\t\t\text4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n\t}\n\n\t/*\n\t * Also if the number of blocks within the group is less than\n\t * the blocksize * 8 ( which is the size of bitmap ), set rest\n\t * of the block bitmap to 1\n\t */\n\text4_mark_bitmap_end(num_clusters_in_group(sb, block_group),\n\t\t\t     sb->s_blocksize * 8, bh->b_data);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tif (ext4_block_in_group(sb, tmp, block_group))",
          "\tif (ext4_block_in_group(sb, tmp, block_group))",
          "\t\tif (ext4_block_in_group(sb, tmp, block_group))"
        ],
        "deleted": [
          "\tint flex_bg = 0;",
          "\tif (ext4_has_feature_flex_bg(sb))",
          "\t\tflex_bg = 1;",
          "",
          "\tif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))",
          "\tif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))",
          "\t\tif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for block group boundaries in the ext4 filesystem initialization code.",
      "trigger_condition": "A local user mounts and operates a crafted ext4 filesystem image, causing an out-of-bounds write due to incorrect handling of block group boundaries.",
      "specific_code_behavior_causing_vulnerability": "The code fails to properly check block group boundaries when setting bits in the block and inode bitmaps, leading to potential out-of-bounds writes and a denial of service or other unspecified impact.",
      "solution": "To mitigate the vulnerability, it is necessary to remove the condition that allows setting bits outside the block group boundaries. By ensuring that the write operations stay within the bounds of the buffer, the code prevents the out-of-bounds write issue and the associated denial of service risk. In this case, the solution involves modifying the code to remove the condition that checks for flex_bg or block group boundaries before setting the bitmap bits.",
      "id": 23,
      "code_after_change_normalized": "static int FUN1(struct super_block *VAR1,\nstruct buffer_head *VAR2,\next4_group_t VAR3,\nstruct ext4_group_desc *VAR4)\n{\nunsigned int VAR5, VAR6;\nstruct ext4_sb_info *VAR7 = FUN2(VAR1);\next4_fsblk_t VAR8, VAR9;\nFUN3(VAR2, FUN4(VAR2));\nif (!FUN5(VAR1, VAR3, VAR4)) {\nFUN6(VAR1, VAR3,\nVAR10 |\nVAR11);\nreturn -VAR12;\n}\nFUN7(VAR2->VAR13, 0, VAR1->VAR14);\nVAR6 = FUN8(VAR1, VAR3);\nif ((VAR6 >> 3) >= VAR2->VAR15)\nreturn -VAR16;\nfor (VAR5 = 0; VAR5 < VAR6; VAR5++)\nFUN9(VAR5, VAR2->VAR13);\nVAR8 = FUN10(VAR1, VAR3);\nVAR9 = FUN11(VAR1, VAR4);\nif (FUN12(VAR1, VAR9, VAR3))\nFUN9(FUN13(VAR7, VAR9 - VAR8), VAR2->VAR13);\nVAR9 = FUN14(VAR1, VAR4);\nif (FUN12(VAR1, VAR9, VAR3))\nFUN9(FUN13(VAR7, VAR9 - VAR8), VAR2->VAR13);\nVAR9 = FUN15(VAR1, VAR4);\nfor (; VAR9 < FUN15(VAR1, VAR4) +\nVAR7->VAR17; VAR9++) {\nif (FUN12(VAR1, VAR9, VAR3))\nFUN9(FUN13(VAR7, VAR9 - VAR8), VAR2->VAR13);\n}\nFUN16(FUN17(VAR1, VAR3),\nVAR1->VAR14 * 8, VAR2->VAR13);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct super_block *VAR1,\nstruct buffer_head *VAR2,\next4_group_t VAR3,\nstruct ext4_group_desc *VAR4)\n{\nunsigned int VAR5, VAR6;\nstruct ext4_sb_info *VAR7 = FUN2(VAR1);\next4_fsblk_t VAR8, VAR9;\nint VAR10 = 0;\nFUN3(VAR2, FUN4(VAR2));\nif (!FUN5(VAR1, VAR3, VAR4)) {\nFUN6(VAR1, VAR3,\nVAR11 |\nVAR12);\nreturn -VAR13;\n}\nFUN7(VAR2->VAR14, 0, VAR1->VAR15);\nVAR6 = FUN8(VAR1, VAR3);\nif ((VAR6 >> 3) >= VAR2->VAR16)\nreturn -VAR17;\nfor (VAR5 = 0; VAR5 < VAR6; VAR5++)\nFUN9(VAR5, VAR2->VAR14);\nVAR8 = FUN10(VAR1, VAR3);\nif (FUN11(VAR1))\nVAR10 = 1;\nVAR9 = FUN12(VAR1, VAR4);\nif (!VAR10 || FUN13(VAR1, VAR9, VAR3))\nFUN9(FUN14(VAR7, VAR9 - VAR8), VAR2->VAR14);\nVAR9 = FUN15(VAR1, VAR4);\nif (!VAR10 || FUN13(VAR1, VAR9, VAR3))\nFUN9(FUN14(VAR7, VAR9 - VAR8), VAR2->VAR14);\nVAR9 = FUN16(VAR1, VAR4);\nfor (; VAR9 < FUN16(VAR1, VAR4) +\nVAR7->VAR18; VAR9++) {\nif (!VAR10 || FUN13(VAR1, VAR9, VAR3))\nFUN9(FUN14(VAR7, VAR9 - VAR8), VAR2->VAR14);\n}\nFUN17(FUN18(VAR1, VAR3),\nVAR1->VAR15 * 8, VAR2->VAR14);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int ext4_init_block_bitmap(struct super_block *sb,\nstruct buffer_head *bh,\next4_group_t block_group,\nstruct ext4_group_desc *gdp)\n{\nunsigned int bit, bit_max;\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\next4_fsblk_t start, tmp;\nJ_ASSERT_BH(bh, buffer_locked(bh));\nif (!ext4_group_desc_csum_verify(sb, block_group, gdp)) {\next4_mark_group_bitmap_corrupted(sb, block_group,\nEXT4_GROUP_INFO_BBITMAP_CORRUPT |\nEXT4_GROUP_INFO_IBITMAP_CORRUPT);\nreturn -EFSBADCRC;\n}\nmemset(bh->b_data, 0, sb->s_blocksize);\nbit_max = ext4_num_base_meta_clusters(sb, block_group);\nif ((bit_max >> 3) >= bh->b_size)\nreturn -EFSCORRUPTED;\nfor (bit = 0; bit < bit_max; bit++)\next4_set_bit(bit, bh->b_data);\nstart = ext4_group_first_block_no(sb, block_group);\ntmp = ext4_block_bitmap(sb, gdp);\nif (ext4_block_in_group(sb, tmp, block_group))\next4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\ntmp = ext4_inode_bitmap(sb, gdp);\nif (ext4_block_in_group(sb, tmp, block_group))\next4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\ntmp = ext4_inode_table(sb, gdp);\nfor (; tmp < ext4_inode_table(sb, gdp) +\nsbi->s_itb_per_group; tmp++) {\nif (ext4_block_in_group(sb, tmp, block_group))\next4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n}\next4_mark_bitmap_end(num_clusters_in_group(sb, block_group),\nsb->s_blocksize * 8, bh->b_data);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int ext4_init_block_bitmap(struct super_block *sb,\nstruct buffer_head *bh,\next4_group_t block_group,\nstruct ext4_group_desc *gdp)\n{\nunsigned int bit, bit_max;\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\next4_fsblk_t start, tmp;\nint flex_bg = 0;\nJ_ASSERT_BH(bh, buffer_locked(bh));\nif (!ext4_group_desc_csum_verify(sb, block_group, gdp)) {\next4_mark_group_bitmap_corrupted(sb, block_group,\nEXT4_GROUP_INFO_BBITMAP_CORRUPT |\nEXT4_GROUP_INFO_IBITMAP_CORRUPT);\nreturn -EFSBADCRC;\n}\nmemset(bh->b_data, 0, sb->s_blocksize);\nbit_max = ext4_num_base_meta_clusters(sb, block_group);\nif ((bit_max >> 3) >= bh->b_size)\nreturn -EFSCORRUPTED;\nfor (bit = 0; bit < bit_max; bit++)\next4_set_bit(bit, bh->b_data);\nstart = ext4_group_first_block_no(sb, block_group);\nif (ext4_has_feature_flex_bg(sb))\nflex_bg = 1;\ntmp = ext4_block_bitmap(sb, gdp);\nif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))\next4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\ntmp = ext4_inode_bitmap(sb, gdp);\nif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))\next4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\ntmp = ext4_inode_table(sb, gdp);\nfor (; tmp < ext4_inode_table(sb, gdp) +\nsbi->s_itb_per_group; tmp++) {\nif (!flex_bg || ext4_block_in_group(sb, tmp, block_group))\next4_set_bit(EXT4_B2C(sbi, tmp - start), bh->b_data);\n}\next4_mark_bitmap_end(num_clusters_in_group(sb, block_group),\nsb->s_blocksize * 8, bh->b_data);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2018-10880": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and handling of specific attributes during inode space management in the ext4 filesystem code.",
        "trigger_condition": "Crafting a malicious ext4 image with a specific attribute that can trigger a stack-out-of-bounds write vulnerability in the ext4_update_inline_data() function.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly check and skip moving a specific attribute ('system.data') out of the inode, leading to a stack-out-of-bounds write vulnerability when processing this attribute.",
        "solution": "To mitigate the vulnerability, it is necessary to add a condition to skip moving the 'system.data' attribute out of the inode during inode space management. This prevents the vulnerable behavior that could be triggered by processing this specific attribute and avoids the stack-out-of-bounds write vulnerability."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-10880, which involves a stack-out-of-bounds write in the ext4 filesystem code. By adding the condition to skip moving the \"system.data\" attribute out of the inode, the code ensures that this specific attribute is not processed in a way that could lead to the vulnerability being exploited. This modification helps prevent the attacker from crafting a malicious ext4 image that could trigger a system crash and denial of service by exploiting the vulnerability in the ext4_update_inline_data() function.",
      "GPT_purpose": "This function is responsible for managing inode space allocation for extended attributes in the ext4 filesystem.",
      "GPT_function": "\n1. Allocate space for extended attributes in an inode.\n2. Identify the best entry to be pushed into an extended attribute block.\n3. Move the identified entry to the extended attribute block.",
      "CVE_id": "CVE-2018-10880",
      "code_before_change": "static int ext4_xattr_make_inode_space(handle_t *handle, struct inode *inode,\n\t\t\t\t       struct ext4_inode *raw_inode,\n\t\t\t\t       int isize_diff, size_t ifree,\n\t\t\t\t       size_t bfree, int *total_ino)\n{\n\tstruct ext4_xattr_ibody_header *header = IHDR(inode, raw_inode);\n\tstruct ext4_xattr_entry *small_entry;\n\tstruct ext4_xattr_entry *entry;\n\tstruct ext4_xattr_entry *last;\n\tunsigned int entry_size;\t/* EA entry size */\n\tunsigned int total_size;\t/* EA entry size + value size */\n\tunsigned int min_total_size;\n\tint error;\n\n\twhile (isize_diff > ifree) {\n\t\tentry = NULL;\n\t\tsmall_entry = NULL;\n\t\tmin_total_size = ~0U;\n\t\tlast = IFIRST(header);\n\t\t/* Find the entry best suited to be pushed into EA block */\n\t\tfor (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) {\n\t\t\ttotal_size = EXT4_XATTR_LEN(last->e_name_len);\n\t\t\tif (!last->e_value_inum)\n\t\t\t\ttotal_size += EXT4_XATTR_SIZE(\n\t\t\t\t\t       le32_to_cpu(last->e_value_size));\n\t\t\tif (total_size <= bfree &&\n\t\t\t    total_size < min_total_size) {\n\t\t\t\tif (total_size + ifree < isize_diff) {\n\t\t\t\t\tsmall_entry = last;\n\t\t\t\t} else {\n\t\t\t\t\tentry = last;\n\t\t\t\t\tmin_total_size = total_size;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (entry == NULL) {\n\t\t\tif (small_entry == NULL)\n\t\t\t\treturn -ENOSPC;\n\t\t\tentry = small_entry;\n\t\t}\n\n\t\tentry_size = EXT4_XATTR_LEN(entry->e_name_len);\n\t\ttotal_size = entry_size;\n\t\tif (!entry->e_value_inum)\n\t\t\ttotal_size += EXT4_XATTR_SIZE(\n\t\t\t\t\t      le32_to_cpu(entry->e_value_size));\n\t\terror = ext4_xattr_move_to_block(handle, inode, raw_inode,\n\t\t\t\t\t\t entry);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\t*total_ino -= entry_size;\n\t\tifree += total_size;\n\t\tbfree -= total_size;\n\t}\n\n\treturn 0;\n}",
      "code_after_change": "static int ext4_xattr_make_inode_space(handle_t *handle, struct inode *inode,\n\t\t\t\t       struct ext4_inode *raw_inode,\n\t\t\t\t       int isize_diff, size_t ifree,\n\t\t\t\t       size_t bfree, int *total_ino)\n{\n\tstruct ext4_xattr_ibody_header *header = IHDR(inode, raw_inode);\n\tstruct ext4_xattr_entry *small_entry;\n\tstruct ext4_xattr_entry *entry;\n\tstruct ext4_xattr_entry *last;\n\tunsigned int entry_size;\t/* EA entry size */\n\tunsigned int total_size;\t/* EA entry size + value size */\n\tunsigned int min_total_size;\n\tint error;\n\n\twhile (isize_diff > ifree) {\n\t\tentry = NULL;\n\t\tsmall_entry = NULL;\n\t\tmin_total_size = ~0U;\n\t\tlast = IFIRST(header);\n\t\t/* Find the entry best suited to be pushed into EA block */\n\t\tfor (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) {\n\t\t\t/* never move system.data out of the inode */\n\t\t\tif ((last->e_name_len == 4) &&\n\t\t\t    (last->e_name_index == EXT4_XATTR_INDEX_SYSTEM) &&\n\t\t\t    !memcmp(last->e_name, \"data\", 4))\n\t\t\t\tcontinue;\n\t\t\ttotal_size = EXT4_XATTR_LEN(last->e_name_len);\n\t\t\tif (!last->e_value_inum)\n\t\t\t\ttotal_size += EXT4_XATTR_SIZE(\n\t\t\t\t\t       le32_to_cpu(last->e_value_size));\n\t\t\tif (total_size <= bfree &&\n\t\t\t    total_size < min_total_size) {\n\t\t\t\tif (total_size + ifree < isize_diff) {\n\t\t\t\t\tsmall_entry = last;\n\t\t\t\t} else {\n\t\t\t\t\tentry = last;\n\t\t\t\t\tmin_total_size = total_size;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tif (entry == NULL) {\n\t\t\tif (small_entry == NULL)\n\t\t\t\treturn -ENOSPC;\n\t\t\tentry = small_entry;\n\t\t}\n\n\t\tentry_size = EXT4_XATTR_LEN(entry->e_name_len);\n\t\ttotal_size = entry_size;\n\t\tif (!entry->e_value_inum)\n\t\t\ttotal_size += EXT4_XATTR_SIZE(\n\t\t\t\t\t      le32_to_cpu(entry->e_value_size));\n\t\terror = ext4_xattr_move_to_block(handle, inode, raw_inode,\n\t\t\t\t\t\t entry);\n\t\tif (error)\n\t\t\treturn error;\n\n\t\t*total_ino -= entry_size;\n\t\tifree += total_size;\n\t\tbfree -= total_size;\n\t}\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t/* never move system.data out of the inode */",
          "\t\t\tif ((last->e_name_len == 4) &&",
          "\t\t\t    (last->e_name_index == EXT4_XATTR_INDEX_SYSTEM) &&",
          "\t\t\t    !memcmp(last->e_name, \"data\", 4))",
          "\t\t\t\tcontinue;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation and handling of specific attributes during inode space management in the ext4 filesystem code.",
      "trigger_condition": "Crafting a malicious ext4 image with a specific attribute that can trigger a stack-out-of-bounds write vulnerability in the ext4_update_inline_data() function.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly check and skip moving a specific attribute ('system.data') out of the inode, leading to a stack-out-of-bounds write vulnerability when processing this attribute.",
      "solution": "To mitigate the vulnerability, it is necessary to add a condition to skip moving the 'system.data' attribute out of the inode during inode space management. This prevents the vulnerable behavior that could be triggered by processing this specific attribute and avoids the stack-out-of-bounds write vulnerability.",
      "id": 24,
      "code_after_change_normalized": "static int FUN1(handle_t *VAR1, struct VAR2 *VAR2,\nstruct ext4_inode *VAR3,\nint VAR4, size_t VAR5,\nsize_t VAR6, int *VAR7)\n{\nstruct ext4_xattr_ibody_header *VAR8 = FUN2(VAR2, VAR3);\nstruct ext4_xattr_entry *VAR9;\nstruct ext4_xattr_entry *VAR10;\nstruct ext4_xattr_entry *VAR11;\nunsigned int VAR12;\t\nunsigned int VAR13;\t\nunsigned int VAR14;\nint VAR15;\nwhile (VAR4 > VAR5) {\nVAR10 = NULL;\nVAR9 = NULL;\nVAR14 = ~0U;\nVAR11 = FUN3(VAR8);\nfor (; !FUN4(VAR11); VAR11 = FUN5(VAR11)) {\nif ((VAR11->VAR16 == 4) &&\n(VAR11->VAR17 == VAR18) &&\n!FUN6(VAR11->VAR19, \"STR\", 4))\ncontinue;\nVAR13 = FUN7(VAR11->VAR16);\nif (!VAR11->VAR20)\nVAR13 += FUN8(\nFUN9(VAR11->VAR21));\nif (VAR13 <= VAR6 &&\nVAR13 < VAR14) {\nif (VAR13 + VAR5 < VAR4) {\nVAR9 = VAR11;\n} else {\nVAR10 = VAR11;\nVAR14 = VAR13;\n}\n}\n}\nif (VAR10 == NULL) {\nif (VAR9 == NULL)\nreturn -VAR22;\nVAR10 = VAR9;\n}\nVAR12 = FUN7(VAR10->VAR16);\nVAR13 = VAR12;\nif (!VAR10->VAR20)\nVAR13 += FUN8(\nFUN9(VAR10->VAR21));\nVAR15 = FUN10(VAR1, VAR2, VAR3,\nVAR10);\nif (VAR15)\nreturn VAR15;\n*VAR7 -= VAR12;\nVAR5 += VAR13;\nVAR6 -= VAR13;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(handle_t *VAR1, struct VAR2 *VAR2,\nstruct ext4_inode *VAR3,\nint VAR4, size_t VAR5,\nsize_t VAR6, int *VAR7)\n{\nstruct ext4_xattr_ibody_header *VAR8 = FUN2(VAR2, VAR3);\nstruct ext4_xattr_entry *VAR9;\nstruct ext4_xattr_entry *VAR10;\nstruct ext4_xattr_entry *VAR11;\nunsigned int VAR12;\t\nunsigned int VAR13;\t\nunsigned int VAR14;\nint VAR15;\nwhile (VAR4 > VAR5) {\nVAR10 = NULL;\nVAR9 = NULL;\nVAR14 = ~0U;\nVAR11 = FUN3(VAR8);\nfor (; !FUN4(VAR11); VAR11 = FUN5(VAR11)) {\nVAR13 = FUN6(VAR11->VAR16);\nif (!VAR11->VAR17)\nVAR13 += FUN7(\nFUN8(VAR11->VAR18));\nif (VAR13 <= VAR6 &&\nVAR13 < VAR14) {\nif (VAR13 + VAR5 < VAR4) {\nVAR9 = VAR11;\n} else {\nVAR10 = VAR11;\nVAR14 = VAR13;\n}\n}\n}\nif (VAR10 == NULL) {\nif (VAR9 == NULL)\nreturn -VAR19;\nVAR10 = VAR9;\n}\nVAR12 = FUN6(VAR10->VAR16);\nVAR13 = VAR12;\nif (!VAR10->VAR17)\nVAR13 += FUN7(\nFUN8(VAR10->VAR18));\nVAR15 = FUN9(VAR1, VAR2, VAR3,\nVAR10);\nif (VAR15)\nreturn VAR15;\n*VAR7 -= VAR12;\nVAR5 += VAR13;\nVAR6 -= VAR13;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int ext4_xattr_make_inode_space(handle_t *handle, struct inode *inode,\nstruct ext4_inode *raw_inode,\nint isize_diff, size_t ifree,\nsize_t bfree, int *total_ino)\n{\nstruct ext4_xattr_ibody_header *header = IHDR(inode, raw_inode);\nstruct ext4_xattr_entry *small_entry;\nstruct ext4_xattr_entry *entry;\nstruct ext4_xattr_entry *last;\nunsigned int entry_size;\t\nunsigned int total_size;\t\nunsigned int min_total_size;\nint error;\nwhile (isize_diff > ifree) {\nentry = NULL;\nsmall_entry = NULL;\nmin_total_size = ~0U;\nlast = IFIRST(header);\nfor (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) {\nif ((last->e_name_len == 4) &&\n(last->e_name_index == EXT4_XATTR_INDEX_SYSTEM) &&\n!memcmp(last->e_name, \"data\", 4))\ncontinue;\ntotal_size = EXT4_XATTR_LEN(last->e_name_len);\nif (!last->e_value_inum)\ntotal_size += EXT4_XATTR_SIZE(\nle32_to_cpu(last->e_value_size));\nif (total_size <= bfree &&\ntotal_size < min_total_size) {\nif (total_size + ifree < isize_diff) {\nsmall_entry = last;\n} else {\nentry = last;\nmin_total_size = total_size;\n}\n}\n}\nif (entry == NULL) {\nif (small_entry == NULL)\nreturn -ENOSPC;\nentry = small_entry;\n}\nentry_size = EXT4_XATTR_LEN(entry->e_name_len);\ntotal_size = entry_size;\nif (!entry->e_value_inum)\ntotal_size += EXT4_XATTR_SIZE(\nle32_to_cpu(entry->e_value_size));\nerror = ext4_xattr_move_to_block(handle, inode, raw_inode,\nentry);\nif (error)\nreturn error;\n*total_ino -= entry_size;\nifree += total_size;\nbfree -= total_size;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int ext4_xattr_make_inode_space(handle_t *handle, struct inode *inode,\nstruct ext4_inode *raw_inode,\nint isize_diff, size_t ifree,\nsize_t bfree, int *total_ino)\n{\nstruct ext4_xattr_ibody_header *header = IHDR(inode, raw_inode);\nstruct ext4_xattr_entry *small_entry;\nstruct ext4_xattr_entry *entry;\nstruct ext4_xattr_entry *last;\nunsigned int entry_size;\t\nunsigned int total_size;\t\nunsigned int min_total_size;\nint error;\nwhile (isize_diff > ifree) {\nentry = NULL;\nsmall_entry = NULL;\nmin_total_size = ~0U;\nlast = IFIRST(header);\nfor (; !IS_LAST_ENTRY(last); last = EXT4_XATTR_NEXT(last)) {\ntotal_size = EXT4_XATTR_LEN(last->e_name_len);\nif (!last->e_value_inum)\ntotal_size += EXT4_XATTR_SIZE(\nle32_to_cpu(last->e_value_size));\nif (total_size <= bfree &&\ntotal_size < min_total_size) {\nif (total_size + ifree < isize_diff) {\nsmall_entry = last;\n} else {\nentry = last;\nmin_total_size = total_size;\n}\n}\n}\nif (entry == NULL) {\nif (small_entry == NULL)\nreturn -ENOSPC;\nentry = small_entry;\n}\nentry_size = EXT4_XATTR_LEN(entry->e_name_len);\ntotal_size = entry_size;\nif (!entry->e_value_inum)\ntotal_size += EXT4_XATTR_SIZE(\nle32_to_cpu(entry->e_value_size));\nerror = ext4_xattr_move_to_block(handle, inode, raw_inode,\nentry);\nif (error)\nreturn error;\n*total_ino -= entry_size;\nifree += total_size;\nbfree -= total_size;\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2018-10881": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for inline data in the ext4 filesystem.",
        "trigger_condition": "A local user mounts and operates on a crafted ext4 filesystem image, causing an out-of-bound access in the ext4_get_group_info function, leading to a denial of service and potential system crash.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly clear the i_data field in the ext4_inode_info structure, leaving it susceptible to out-of-bound access and potential exploitation when handling inline data.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure proper initialization and handling of the i_data field in the ext4_inode_info structure. This involves adding a line to clear the i_data field in addition to the existing memset call, thereby reducing the risk of out-of-bound access and improving the security of the ext4 filesystem implementation."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-10881 in the ext4 filesystem. The vulnerability allows a local user to cause an out-of-bound access in the ext4_get_group_info function, leading to a denial of service and potentially crashing the system by mounting and operating on a crafted ext4 filesystem image.\n\nThe modification includes adding the line `memset(ei->i_data, 0, EXT4_MIN_INLINE_DATA_SIZE);` after the existing `memset` call in the code snippet. This addition ensures that the `i_data` field in the `ext4_inode_info` structure is properly cleared to prevent any potential out-of-bound access or other issues related to inline data handling.\n\nBy clearing the `i_data` field along with the existing `memset` call, the modified code helps mitigate the vulnerability by ensuring that the inline data area is properly handled and initialized, reducing the risk of exploitation and improving the overall security of the ext4 filesystem implementation.",
      "GPT_purpose": "Destroy inline data in an ext4 filesystem inode.",
      "GPT_function": "\n1. Destroy inline data for an ext4 filesystem inode.\n2. Find and set extended attributes for the inode.\n3. Clear flags and states related to inline data in the inode.",
      "CVE_id": "CVE-2018-10881",
      "code_before_change": "static int ext4_destroy_inline_data_nolock(handle_t *handle,\n\t\t\t\t\t   struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_xattr_ibody_find is = {\n\t\t.s = { .not_found = 0, },\n\t};\n\tstruct ext4_xattr_info i = {\n\t\t.name_index = EXT4_XATTR_INDEX_SYSTEM,\n\t\t.name = EXT4_XATTR_SYSTEM_DATA,\n\t\t.value = NULL,\n\t\t.value_len = 0,\n\t};\n\tint error;\n\n\tif (!ei->i_inline_off)\n\t\treturn 0;\n\n\terror = ext4_get_inode_loc(inode, &is.iloc);\n\tif (error)\n\t\treturn error;\n\n\terror = ext4_xattr_ibody_find(inode, &i, &is);\n\tif (error)\n\t\tgoto out;\n\n\tBUFFER_TRACE(is.iloc.bh, \"get_write_access\");\n\terror = ext4_journal_get_write_access(handle, is.iloc.bh);\n\tif (error)\n\t\tgoto out;\n\n\terror = ext4_xattr_ibody_inline_set(handle, inode, &i, &is);\n\tif (error)\n\t\tgoto out;\n\n\tmemset((void *)ext4_raw_inode(&is.iloc)->i_block,\n\t\t0, EXT4_MIN_INLINE_DATA_SIZE);\n\n\tif (ext4_has_feature_extents(inode->i_sb)) {\n\t\tif (S_ISDIR(inode->i_mode) ||\n\t\t    S_ISREG(inode->i_mode) || S_ISLNK(inode->i_mode)) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\n\t\t\text4_ext_tree_init(handle, inode);\n\t\t}\n\t}\n\text4_clear_inode_flag(inode, EXT4_INODE_INLINE_DATA);\n\n\tget_bh(is.iloc.bh);\n\terror = ext4_mark_iloc_dirty(handle, inode, &is.iloc);\n\n\tEXT4_I(inode)->i_inline_off = 0;\n\tEXT4_I(inode)->i_inline_size = 0;\n\text4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\nout:\n\tbrelse(is.iloc.bh);\n\tif (error == -ENODATA)\n\t\terror = 0;\n\treturn error;\n}",
      "code_after_change": "static int ext4_destroy_inline_data_nolock(handle_t *handle,\n\t\t\t\t\t   struct inode *inode)\n{\n\tstruct ext4_inode_info *ei = EXT4_I(inode);\n\tstruct ext4_xattr_ibody_find is = {\n\t\t.s = { .not_found = 0, },\n\t};\n\tstruct ext4_xattr_info i = {\n\t\t.name_index = EXT4_XATTR_INDEX_SYSTEM,\n\t\t.name = EXT4_XATTR_SYSTEM_DATA,\n\t\t.value = NULL,\n\t\t.value_len = 0,\n\t};\n\tint error;\n\n\tif (!ei->i_inline_off)\n\t\treturn 0;\n\n\terror = ext4_get_inode_loc(inode, &is.iloc);\n\tif (error)\n\t\treturn error;\n\n\terror = ext4_xattr_ibody_find(inode, &i, &is);\n\tif (error)\n\t\tgoto out;\n\n\tBUFFER_TRACE(is.iloc.bh, \"get_write_access\");\n\terror = ext4_journal_get_write_access(handle, is.iloc.bh);\n\tif (error)\n\t\tgoto out;\n\n\terror = ext4_xattr_ibody_inline_set(handle, inode, &i, &is);\n\tif (error)\n\t\tgoto out;\n\n\tmemset((void *)ext4_raw_inode(&is.iloc)->i_block,\n\t\t0, EXT4_MIN_INLINE_DATA_SIZE);\n\tmemset(ei->i_data, 0, EXT4_MIN_INLINE_DATA_SIZE);\n\n\tif (ext4_has_feature_extents(inode->i_sb)) {\n\t\tif (S_ISDIR(inode->i_mode) ||\n\t\t    S_ISREG(inode->i_mode) || S_ISLNK(inode->i_mode)) {\n\t\t\text4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\n\t\t\text4_ext_tree_init(handle, inode);\n\t\t}\n\t}\n\text4_clear_inode_flag(inode, EXT4_INODE_INLINE_DATA);\n\n\tget_bh(is.iloc.bh);\n\terror = ext4_mark_iloc_dirty(handle, inode, &is.iloc);\n\n\tEXT4_I(inode)->i_inline_off = 0;\n\tEXT4_I(inode)->i_inline_size = 0;\n\text4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\nout:\n\tbrelse(is.iloc.bh);\n\tif (error == -ENODATA)\n\t\terror = 0;\n\treturn error;\n}",
      "modified_lines": {
        "added": [
          "\tmemset(ei->i_data, 0, EXT4_MIN_INLINE_DATA_SIZE);"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper handling for inline data in the ext4 filesystem.",
      "trigger_condition": "A local user mounts and operates on a crafted ext4 filesystem image, causing an out-of-bound access in the ext4_get_group_info function, leading to a denial of service and potential system crash.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly clear the i_data field in the ext4_inode_info structure, leaving it susceptible to out-of-bound access and potential exploitation when handling inline data.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper initialization and handling of the i_data field in the ext4_inode_info structure. This involves adding a line to clear the i_data field in addition to the existing memset call, thereby reducing the risk of out-of-bound access and improving the security of the ext4 filesystem implementation.",
      "id": 25,
      "code_after_change_normalized": "static int FUN1(handle_t *VAR1,\nstruct VAR2 *VAR2)\n{\nstruct ext4_inode_info *VAR3 = FUN2(VAR2);\nstruct ext4_xattr_ibody_find VAR4 = {\n.VAR5 = { .VAR6 = 0, },\n};\nstruct ext4_xattr_info VAR7 = {\n.VAR8 = VAR9,\n.VAR10 = VAR11,\n.VAR12 = NULL,\n.VAR13 = 0,\n};\nint VAR14;\nif (!VAR3->VAR15)\nreturn 0;\nVAR14 = FUN3(VAR2, &VAR4.VAR16);\nif (VAR14)\nreturn VAR14;\nVAR14 = FUN4(VAR2, &VAR7, &VAR4);\nif (VAR14)\ngoto VAR17;\nFUN5(VAR4.VAR16.VAR18, \"STR\");\nVAR14 = FUN6(VAR1, VAR4.VAR16.VAR18);\nif (VAR14)\ngoto VAR17;\nVAR14 = FUN7(VAR1, VAR2, &VAR7, &VAR4);\nif (VAR14)\ngoto VAR17;\nFUN8((void *)FUN9(&VAR4.VAR16)->VAR19,\n0, VAR20);\nFUN8(VAR3->VAR21, 0, VAR20);\nif (FUN10(VAR2->VAR22)) {\nif (FUN11(VAR2->VAR23) ||\nFUN12(VAR2->VAR23) || FUN13(VAR2->VAR23)) {\nFUN14(VAR2, VAR24);\nFUN15(VAR1, VAR2);\n}\n}\nFUN16(VAR2, VAR25);\nFUN17(VAR4.VAR16.VAR18);\nVAR14 = FUN18(VAR1, VAR2, &VAR4.VAR16);\nFUN2(VAR2)->VAR15 = 0;\nFUN2(VAR2)->VAR26 = 0;\nFUN19(VAR2, VAR27);\nVAR17:\nFUN20(VAR4.VAR16.VAR18);\nif (VAR14 == -VAR28)\nVAR14 = 0;\nreturn VAR14;\n}\n",
      "code_before_change_normalized": "static int FUN1(handle_t *VAR1,\nstruct VAR2 *VAR2)\n{\nstruct ext4_inode_info *VAR3 = FUN2(VAR2);\nstruct ext4_xattr_ibody_find VAR4 = {\n.VAR5 = { .VAR6 = 0, },\n};\nstruct ext4_xattr_info VAR7 = {\n.VAR8 = VAR9,\n.VAR10 = VAR11,\n.VAR12 = NULL,\n.VAR13 = 0,\n};\nint VAR14;\nif (!VAR3->VAR15)\nreturn 0;\nVAR14 = FUN3(VAR2, &VAR4.VAR16);\nif (VAR14)\nreturn VAR14;\nVAR14 = FUN4(VAR2, &VAR7, &VAR4);\nif (VAR14)\ngoto VAR17;\nFUN5(VAR4.VAR16.VAR18, \"STR\");\nVAR14 = FUN6(VAR1, VAR4.VAR16.VAR18);\nif (VAR14)\ngoto VAR17;\nVAR14 = FUN7(VAR1, VAR2, &VAR7, &VAR4);\nif (VAR14)\ngoto VAR17;\nFUN8((void *)FUN9(&VAR4.VAR16)->VAR19,\n0, VAR20);\nif (FUN10(VAR2->VAR21)) {\nif (FUN11(VAR2->VAR22) ||\nFUN12(VAR2->VAR22) || FUN13(VAR2->VAR22)) {\nFUN14(VAR2, VAR23);\nFUN15(VAR1, VAR2);\n}\n}\nFUN16(VAR2, VAR24);\nFUN17(VAR4.VAR16.VAR18);\nVAR14 = FUN18(VAR1, VAR2, &VAR4.VAR16);\nFUN2(VAR2)->VAR15 = 0;\nFUN2(VAR2)->VAR25 = 0;\nFUN19(VAR2, VAR26);\nVAR17:\nFUN20(VAR4.VAR16.VAR18);\nif (VAR14 == -VAR27)\nVAR14 = 0;\nreturn VAR14;\n}\n",
      "code_after_change_raw": "static int ext4_destroy_inline_data_nolock(handle_t *handle,\nstruct inode *inode)\n{\nstruct ext4_inode_info *ei = EXT4_I(inode);\nstruct ext4_xattr_ibody_find is = {\n.s = { .not_found = 0, },\n};\nstruct ext4_xattr_info i = {\n.name_index = EXT4_XATTR_INDEX_SYSTEM,\n.name = EXT4_XATTR_SYSTEM_DATA,\n.value = NULL,\n.value_len = 0,\n};\nint error;\nif (!ei->i_inline_off)\nreturn 0;\nerror = ext4_get_inode_loc(inode, &is.iloc);\nif (error)\nreturn error;\nerror = ext4_xattr_ibody_find(inode, &i, &is);\nif (error)\ngoto out;\nBUFFER_TRACE(is.iloc.bh, \"get_write_access\");\nerror = ext4_journal_get_write_access(handle, is.iloc.bh);\nif (error)\ngoto out;\nerror = ext4_xattr_ibody_inline_set(handle, inode, &i, &is);\nif (error)\ngoto out;\nmemset((void *)ext4_raw_inode(&is.iloc)->i_block,\n0, EXT4_MIN_INLINE_DATA_SIZE);\nmemset(ei->i_data, 0, EXT4_MIN_INLINE_DATA_SIZE);\nif (ext4_has_feature_extents(inode->i_sb)) {\nif (S_ISDIR(inode->i_mode) ||\nS_ISREG(inode->i_mode) || S_ISLNK(inode->i_mode)) {\next4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\next4_ext_tree_init(handle, inode);\n}\n}\next4_clear_inode_flag(inode, EXT4_INODE_INLINE_DATA);\nget_bh(is.iloc.bh);\nerror = ext4_mark_iloc_dirty(handle, inode, &is.iloc);\nEXT4_I(inode)->i_inline_off = 0;\nEXT4_I(inode)->i_inline_size = 0;\next4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\nout:\nbrelse(is.iloc.bh);\nif (error == -ENODATA)\nerror = 0;\nreturn error;\n}\n",
      "code_before_change_raw": "static int ext4_destroy_inline_data_nolock(handle_t *handle,\nstruct inode *inode)\n{\nstruct ext4_inode_info *ei = EXT4_I(inode);\nstruct ext4_xattr_ibody_find is = {\n.s = { .not_found = 0, },\n};\nstruct ext4_xattr_info i = {\n.name_index = EXT4_XATTR_INDEX_SYSTEM,\n.name = EXT4_XATTR_SYSTEM_DATA,\n.value = NULL,\n.value_len = 0,\n};\nint error;\nif (!ei->i_inline_off)\nreturn 0;\nerror = ext4_get_inode_loc(inode, &is.iloc);\nif (error)\nreturn error;\nerror = ext4_xattr_ibody_find(inode, &i, &is);\nif (error)\ngoto out;\nBUFFER_TRACE(is.iloc.bh, \"get_write_access\");\nerror = ext4_journal_get_write_access(handle, is.iloc.bh);\nif (error)\ngoto out;\nerror = ext4_xattr_ibody_inline_set(handle, inode, &i, &is);\nif (error)\ngoto out;\nmemset((void *)ext4_raw_inode(&is.iloc)->i_block,\n0, EXT4_MIN_INLINE_DATA_SIZE);\nif (ext4_has_feature_extents(inode->i_sb)) {\nif (S_ISDIR(inode->i_mode) ||\nS_ISREG(inode->i_mode) || S_ISLNK(inode->i_mode)) {\next4_set_inode_flag(inode, EXT4_INODE_EXTENTS);\next4_ext_tree_init(handle, inode);\n}\n}\next4_clear_inode_flag(inode, EXT4_INODE_INLINE_DATA);\nget_bh(is.iloc.bh);\nerror = ext4_mark_iloc_dirty(handle, inode, &is.iloc);\nEXT4_I(inode)->i_inline_off = 0;\nEXT4_I(inode)->i_inline_size = 0;\next4_clear_inode_state(inode, EXT4_STATE_MAY_INLINE_DATA);\nout:\nbrelse(is.iloc.bh);\nif (error == -ENODATA)\nerror = 0;\nreturn error;\n}\n"
    }
  ],
  "CVE-2018-10882": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for inode numbers in the ext4 filesystem function.",
        "trigger_condition": "A local user unmounts a crafted ext4 filesystem image, causing an out-of-bound write in the filesystem code.",
        "specific_code_behavior_causing_vulnerability": "The code snippet does not check if the inode number is within the valid range of inodes for the ext4 filesystem, allowing for a potential out-of-bound write vulnerability when handling inode numbers.",
        "impact": "The vulnerability can lead to a denial of service, system crash, and potentially be exploited by a local user to cause harm to the system."
      },
      "solution": {
        "description": "To mitigate the vulnerability, it is necessary to validate the inode number to ensure it falls within the valid range of inodes for the ext4 filesystem.",
        "specific_fix": "Add a check to verify that the inode number is not greater than the total number of inodes in the filesystem. This prevents the out-of-bound write vulnerability by ensuring that the inode number is within the valid range.",
        "code_fix": "Add the following code snippet to the function: if (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count)) { return 0; }"
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-10882, which involves an out-of-bound write issue in the ext4 filesystem. By adding the following lines to the code snippet, you can mitigate the vulnerability:\n\n```c\nif (ino > le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count)) {\n    return 0;\n}\n```\n\nThis modification ensures that the `ino` value is within the valid range of inodes for the ext4 filesystem. By checking if `ino` is greater than the total number of inodes in the filesystem (`s_inodes_count`), you prevent the out-of-bound write vulnerability that could be exploited by unmounting a crafted ext4 filesystem image.",
      "GPT_purpose": "Check if the given inode number is within valid range for ext4 filesystem operations.",
      "GPT_function": "\n1. Checks if the given inode number is the root inode.\n2. Checks if the given inode number is a user quota inode.\n3. Checks if the given inode number is a group quota inode.\n4. Checks if the given inode number is a boot loader inode.\n5. Checks if the given inode number is a journal inode.\n6. Checks if the given inode number is a resize inode.\n7. Validates if the given inode number falls within the valid range of inodes for the filesystem.",
      "CVE_id": "CVE-2018-10882",
      "code_before_change": "static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\tino == EXT4_USR_QUOTA_INO ||\n\t\tino == EXT4_GRP_QUOTA_INO ||\n\t\tino == EXT4_BOOT_LOADER_INO ||\n\t\tino == EXT4_JOURNAL_INO ||\n\t\tino == EXT4_RESIZE_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}",
      "code_after_change": "static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\n\treturn ino == EXT4_ROOT_INO ||\n\t\t(ino >= EXT4_FIRST_INO(sb) &&\n\t\t ino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\t\tino == EXT4_USR_QUOTA_INO ||",
          "\t\tino == EXT4_GRP_QUOTA_INO ||",
          "\t\tino == EXT4_BOOT_LOADER_INO ||",
          "\t\tino == EXT4_JOURNAL_INO ||",
          "\t\tino == EXT4_RESIZE_INO ||"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for inode numbers in the ext4 filesystem function.",
      "trigger_condition": "A local user unmounts a crafted ext4 filesystem image, causing an out-of-bound write in the filesystem code.",
      "specific_code_behavior_causing_vulnerability": "The code snippet does not check if the inode number is within the valid range of inodes for the ext4 filesystem, allowing for a potential out-of-bound write vulnerability when handling inode numbers.",
      "id": 26,
      "code_after_change_normalized": "static inline int FUN1(struct super_block *VAR1, unsigned long VAR2)\n{\nreturn VAR2 == VAR3 ||\n(VAR2 >= FUN2(VAR1) &&\nVAR2 <= FUN3(FUN4(VAR1)->VAR4->VAR5));\n}\n",
      "code_before_change_normalized": "static inline int FUN1(struct super_block *VAR1, unsigned long VAR2)\n{\nreturn VAR2 == VAR3 ||\nVAR2 == VAR4 ||\nVAR2 == VAR5 ||\nVAR2 == VAR6 ||\nVAR2 == VAR7 ||\nVAR2 == VAR8 ||\n(VAR2 >= FUN2(VAR1) &&\nVAR2 <= FUN3(FUN4(VAR1)->VAR9->VAR10));\n}\n",
      "code_after_change_raw": "static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\nreturn ino == EXT4_ROOT_INO ||\n(ino >= EXT4_FIRST_INO(sb) &&\nino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}\n",
      "code_before_change_raw": "static inline int ext4_valid_inum(struct super_block *sb, unsigned long ino)\n{\nreturn ino == EXT4_ROOT_INO ||\nino == EXT4_USR_QUOTA_INO ||\nino == EXT4_GRP_QUOTA_INO ||\nino == EXT4_BOOT_LOADER_INO ||\nino == EXT4_JOURNAL_INO ||\nino == EXT4_RESIZE_INO ||\n(ino >= EXT4_FIRST_INO(sb) &&\nino <= le32_to_cpu(EXT4_SB(sb)->s_es->s_inodes_count));\n}\n"
    }
  ],
  "CVE-2018-10883": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and error handling in the jbd2_journal_dirty_metadata function.",
        "trigger_condition": "A local user mounts and operates on a crafted ext4 filesystem image, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not adequately validate and handle certain conditions related to buffer metadata, allowing for an out-of-bounds write vulnerability to be triggered when operating on a crafted ext4 filesystem image."
      },
      "solution": "To mitigate the vulnerability, it is necessary to enhance the validation and error handling within the jbd2_journal_dirty_metadata function. Specifically, the modification involves adding error checking and logging mechanisms to properly handle the conditions that could lead to the out-of-bounds write vulnerability. By implementing these enhancements, the code can detect and respond to potential issues, reducing the risk of exploitation and system crashes.",
      "GPT_analysis": "The modification in the code snippet is necessary to address a vulnerability identified as CVE-2018-10883. The vulnerability allows a local user to trigger an out-of-bounds write in the jbd2_journal_dirty_metadata function within the Linux kernel's ext4 filesystem. This vulnerability can lead to a denial of service and potentially crash the system by mounting and manipulating a specially crafted ext4 filesystem image.\n\nThe specific modification made in the code snippet involves adding error handling and logging mechanisms to better handle the conditions that could lead to the vulnerability being exploited. By adding the error handling code, the system can detect and respond appropriately to the problematic scenarios, preventing the out-of-bounds write and potential system crash.\n\nIn summary, the modification is necessary to enhance the security and stability of the system by addressing the identified vulnerability and improving the error handling within the jbd2_journal_dirty_metadata function.",
      "GPT_purpose": "This function is responsible for marking metadata buffers as dirty in the journaling layer of the ext4 filesystem.",
      "GPT_function": "\n1. Marking metadata buffers as dirty in the journal.\n2. Checking and handling buffer modifications within a transaction.\n3. Filing metadata buffers in the journal for committing.",
      "CVE_id": "CVE-2018-10883",
      "code_before_change": "int jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal;\n\tstruct journal_head *jh;\n\tint ret = 0;\n\n\tif (is_handle_aborted(handle))\n\t\treturn -EROFS;\n\tif (!buffer_jbd(bh)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't grab jh reference here since the buffer must be part\n\t * of the running transaction.\n\t */\n\tjh = bh2jh(bh);\n\t/*\n\t * This and the following assertions are unreliable since we may see jh\n\t * in inconsistent state unless we grab bh_state lock. But this is\n\t * crucial to catch bugs so let's do a reliable check until the\n\t * lockless handling is fully proven.\n\t */\n\tif (jh->b_transaction != transaction &&\n\t    jh->b_next_transaction != transaction) {\n\t\tjbd_lock_bh_state(bh);\n\t\tJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\n\t\t\t\tjh->b_next_transaction == transaction);\n\t\tjbd_unlock_bh_state(bh);\n\t}\n\tif (jh->b_modified == 1) {\n\t\t/* If it's in our transaction it must be in BJ_Metadata list. */\n\t\tif (jh->b_transaction == transaction &&\n\t\t    jh->b_jlist != BJ_Metadata) {\n\t\t\tjbd_lock_bh_state(bh);\n\t\t\tJ_ASSERT_JH(jh, jh->b_transaction != transaction ||\n\t\t\t\t\tjh->b_jlist == BJ_Metadata);\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tjournal = transaction->t_journal;\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\tJBUFFER_TRACE(jh, \"entry\");\n\n\tjbd_lock_bh_state(bh);\n\n\tif (jh->b_modified == 0) {\n\t\t/*\n\t\t * This buffer's got modified and becoming part\n\t\t * of the transaction. This needs to be done\n\t\t * once a transaction -bzzz\n\t\t */\n\t\tjh->b_modified = 1;\n\t\tif (handle->h_buffer_credits <= 0) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out_unlock_bh;\n\t\t}\n\t\thandle->h_buffer_credits--;\n\t}\n\n\t/*\n\t * fastpath, to avoid expensive locking.  If this buffer is already\n\t * on the running transaction's metadata list there is nothing to do.\n\t * Nobody can take it off again because there is a handle open.\n\t * I _think_ we're OK here with SMP barriers - a mistaken decision will\n\t * result in this test being false, so we go in and take the locks.\n\t */\n\tif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\n\t\tJBUFFER_TRACE(jh, \"fastpath\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_running_transaction)) {\n\t\t\tprintk(KERN_ERR \"JBD2: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_running_transaction (%p, %u)\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_running_transaction,\n\t\t\t       journal->j_running_transaction ?\n\t\t\t       journal->j_running_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto out_unlock_bh;\n\t}\n\n\tset_buffer_jbddirty(bh);\n\n\t/*\n\t * Metadata already on the current transaction list doesn't\n\t * need to be filed.  Metadata on another transaction's list must\n\t * be committing, and will be refiled once the commit completes:\n\t * leave it alone for now.\n\t */\n\tif (jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"already on other transaction\");\n\t\tif (unlikely(((jh->b_transaction !=\n\t\t\t       journal->j_committing_transaction)) ||\n\t\t\t     (jh->b_next_transaction != transaction))) {\n\t\t\tprintk(KERN_ERR \"jbd2_journal_dirty_metadata: %s: \"\n\t\t\t       \"bad jh for block %llu: \"\n\t\t\t       \"transaction (%p, %u), \"\n\t\t\t       \"jh->b_transaction (%p, %u), \"\n\t\t\t       \"jh->b_next_transaction (%p, %u), jlist %u\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       transaction, transaction->t_tid,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ?\n\t\t\t       jh->b_transaction->t_tid : 0,\n\t\t\t       jh->b_next_transaction,\n\t\t\t       jh->b_next_transaction ?\n\t\t\t       jh->b_next_transaction->t_tid : 0,\n\t\t\t       jh->b_jlist);\n\t\t\tWARN_ON(1);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\t/* And this case is illegal: we can't reuse another\n\t\t * transaction's data buffer, ever. */\n\t\tgoto out_unlock_bh;\n\t}\n\n\t/* That test should have eliminated the following case: */\n\tJ_ASSERT_JH(jh, jh->b_frozen_data == NULL);\n\n\tJBUFFER_TRACE(jh, \"file as BJ_Metadata\");\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_file_buffer(jh, transaction, BJ_Metadata);\n\tspin_unlock(&journal->j_list_lock);\nout_unlock_bh:\n\tjbd_unlock_bh_state(bh);\nout:\n\tJBUFFER_TRACE(jh, \"exit\");\n\treturn ret;\n}",
      "code_after_change": "int jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\n\ttransaction_t *transaction = handle->h_transaction;\n\tjournal_t *journal;\n\tstruct journal_head *jh;\n\tint ret = 0;\n\n\tif (is_handle_aborted(handle))\n\t\treturn -EROFS;\n\tif (!buffer_jbd(bh)) {\n\t\tret = -EUCLEAN;\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't grab jh reference here since the buffer must be part\n\t * of the running transaction.\n\t */\n\tjh = bh2jh(bh);\n\t/*\n\t * This and the following assertions are unreliable since we may see jh\n\t * in inconsistent state unless we grab bh_state lock. But this is\n\t * crucial to catch bugs so let's do a reliable check until the\n\t * lockless handling is fully proven.\n\t */\n\tif (jh->b_transaction != transaction &&\n\t    jh->b_next_transaction != transaction) {\n\t\tjbd_lock_bh_state(bh);\n\t\tJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\n\t\t\t\tjh->b_next_transaction == transaction);\n\t\tjbd_unlock_bh_state(bh);\n\t}\n\tif (jh->b_modified == 1) {\n\t\t/* If it's in our transaction it must be in BJ_Metadata list. */\n\t\tif (jh->b_transaction == transaction &&\n\t\t    jh->b_jlist != BJ_Metadata) {\n\t\t\tjbd_lock_bh_state(bh);\n\t\t\tif (jh->b_transaction == transaction &&\n\t\t\t    jh->b_jlist != BJ_Metadata)\n\t\t\t\tpr_err(\"JBD2: assertion failure: h_type=%u \"\n\t\t\t\t       \"h_line_no=%u block_no=%llu jlist=%u\\n\",\n\t\t\t\t       handle->h_type, handle->h_line_no,\n\t\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t\t       jh->b_jlist);\n\t\t\tJ_ASSERT_JH(jh, jh->b_transaction != transaction ||\n\t\t\t\t\tjh->b_jlist == BJ_Metadata);\n\t\t\tjbd_unlock_bh_state(bh);\n\t\t}\n\t\tgoto out;\n\t}\n\n\tjournal = transaction->t_journal;\n\tjbd_debug(5, \"journal_head %p\\n\", jh);\n\tJBUFFER_TRACE(jh, \"entry\");\n\n\tjbd_lock_bh_state(bh);\n\n\tif (jh->b_modified == 0) {\n\t\t/*\n\t\t * This buffer's got modified and becoming part\n\t\t * of the transaction. This needs to be done\n\t\t * once a transaction -bzzz\n\t\t */\n\t\tif (handle->h_buffer_credits <= 0) {\n\t\t\tret = -ENOSPC;\n\t\t\tgoto out_unlock_bh;\n\t\t}\n\t\tjh->b_modified = 1;\n\t\thandle->h_buffer_credits--;\n\t}\n\n\t/*\n\t * fastpath, to avoid expensive locking.  If this buffer is already\n\t * on the running transaction's metadata list there is nothing to do.\n\t * Nobody can take it off again because there is a handle open.\n\t * I _think_ we're OK here with SMP barriers - a mistaken decision will\n\t * result in this test being false, so we go in and take the locks.\n\t */\n\tif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\n\t\tJBUFFER_TRACE(jh, \"fastpath\");\n\t\tif (unlikely(jh->b_transaction !=\n\t\t\t     journal->j_running_transaction)) {\n\t\t\tprintk(KERN_ERR \"JBD2: %s: \"\n\t\t\t       \"jh->b_transaction (%llu, %p, %u) != \"\n\t\t\t       \"journal->j_running_transaction (%p, %u)\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ? jh->b_transaction->t_tid : 0,\n\t\t\t       journal->j_running_transaction,\n\t\t\t       journal->j_running_transaction ?\n\t\t\t       journal->j_running_transaction->t_tid : 0);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\tgoto out_unlock_bh;\n\t}\n\n\tset_buffer_jbddirty(bh);\n\n\t/*\n\t * Metadata already on the current transaction list doesn't\n\t * need to be filed.  Metadata on another transaction's list must\n\t * be committing, and will be refiled once the commit completes:\n\t * leave it alone for now.\n\t */\n\tif (jh->b_transaction != transaction) {\n\t\tJBUFFER_TRACE(jh, \"already on other transaction\");\n\t\tif (unlikely(((jh->b_transaction !=\n\t\t\t       journal->j_committing_transaction)) ||\n\t\t\t     (jh->b_next_transaction != transaction))) {\n\t\t\tprintk(KERN_ERR \"jbd2_journal_dirty_metadata: %s: \"\n\t\t\t       \"bad jh for block %llu: \"\n\t\t\t       \"transaction (%p, %u), \"\n\t\t\t       \"jh->b_transaction (%p, %u), \"\n\t\t\t       \"jh->b_next_transaction (%p, %u), jlist %u\\n\",\n\t\t\t       journal->j_devname,\n\t\t\t       (unsigned long long) bh->b_blocknr,\n\t\t\t       transaction, transaction->t_tid,\n\t\t\t       jh->b_transaction,\n\t\t\t       jh->b_transaction ?\n\t\t\t       jh->b_transaction->t_tid : 0,\n\t\t\t       jh->b_next_transaction,\n\t\t\t       jh->b_next_transaction ?\n\t\t\t       jh->b_next_transaction->t_tid : 0,\n\t\t\t       jh->b_jlist);\n\t\t\tWARN_ON(1);\n\t\t\tret = -EINVAL;\n\t\t}\n\t\t/* And this case is illegal: we can't reuse another\n\t\t * transaction's data buffer, ever. */\n\t\tgoto out_unlock_bh;\n\t}\n\n\t/* That test should have eliminated the following case: */\n\tJ_ASSERT_JH(jh, jh->b_frozen_data == NULL);\n\n\tJBUFFER_TRACE(jh, \"file as BJ_Metadata\");\n\tspin_lock(&journal->j_list_lock);\n\t__jbd2_journal_file_buffer(jh, transaction, BJ_Metadata);\n\tspin_unlock(&journal->j_list_lock);\nout_unlock_bh:\n\tjbd_unlock_bh_state(bh);\nout:\n\tJBUFFER_TRACE(jh, \"exit\");\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tif (jh->b_transaction == transaction &&",
          "\t\t\t    jh->b_jlist != BJ_Metadata)",
          "\t\t\t\tpr_err(\"JBD2: assertion failure: h_type=%u \"",
          "\t\t\t\t       \"h_line_no=%u block_no=%llu jlist=%u\\n\",",
          "\t\t\t\t       handle->h_type, handle->h_line_no,",
          "\t\t\t\t       (unsigned long long) bh->b_blocknr,",
          "\t\t\t\t       jh->b_jlist);",
          "\t\tjh->b_modified = 1;"
        ],
        "deleted": [
          "\t\tjh->b_modified = 1;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and error handling in the jbd2_journal_dirty_metadata function.",
      "trigger_condition": "A local user mounts and operates on a crafted ext4 filesystem image, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not adequately validate and handle certain conditions related to buffer metadata, allowing for an out-of-bounds write vulnerability to be triggered when operating on a crafted ext4 filesystem image.",
      "id": 27,
      "code_after_change_normalized": "int FUN1(handle_t *VAR1, struct buffer_head *VAR2)\n{\ntransaction_t *VAR3 = VAR1->VAR4;\njournal_t *VAR5;\nstruct journal_head *VAR6;\nint VAR7 = 0;\nif (FUN2(VAR1))\nreturn -VAR8;\nif (!FUN3(VAR2)) {\nVAR7 = -VAR9;\ngoto VAR10;\n}\nVAR6 = FUN4(VAR2);\nif (VAR6->VAR11 != VAR3 &&\nVAR6->VAR12 != VAR3) {\nFUN5(VAR2);\nFUN6(VAR6, VAR6->VAR11 == VAR3 ||\nVAR6->VAR12 == VAR3);\nFUN7(VAR2);\n}\nif (VAR6->VAR13 == 1) {\nif (VAR6->VAR11 == VAR3 &&\nVAR6->VAR14 != VAR15) {\nFUN5(VAR2);\nif (VAR6->VAR11 == VAR3 &&\nVAR6->VAR14 != VAR15)\nFUN8(\"STR\"\n\"STR\",\nVAR1->VAR16, VAR1->VAR17,\n(unsigned long long) VAR2->VAR18,\nVAR6->VAR14);\nFUN6(VAR6, VAR6->VAR11 != VAR3 ||\nVAR6->VAR14 == VAR15);\nFUN7(VAR2);\n}\ngoto VAR10;\n}\nVAR5 = VAR3->VAR19;\nFUN9(5, \"STR\", VAR6);\nFUN10(VAR6, \"STR\");\nFUN5(VAR2);\nif (VAR6->VAR13 == 0) {\nif (VAR1->VAR20 <= 0) {\nVAR7 = -VAR21;\ngoto VAR22;\n}\nVAR6->VAR13 = 1;\nVAR1->VAR20--;\n}\nif (VAR6->VAR11 == VAR3 && VAR6->VAR14 == VAR15) {\nFUN10(VAR6, \"STR\");\nif (FUN11(VAR6->VAR11 !=\nVAR5->VAR23)) {\nFUN12(VAR24 \"STR\"\n\"STR\"\n\"STR\",\nVAR5->VAR25,\n(unsigned long long) VAR2->VAR18,\nVAR6->VAR11,\nVAR6->VAR11 ? VAR6->VAR11->VAR26 : 0,\nVAR5->VAR23,\nVAR5->VAR23 ?\nVAR5->VAR23->VAR26 : 0);\nVAR7 = -VAR27;\n}\ngoto VAR22;\n}\nFUN13(VAR2);\nif (VAR6->VAR11 != VAR3) {\nFUN10(VAR6, \"STR\");\nif (FUN11(((VAR6->VAR11 !=\nVAR5->VAR28)) ||\n(VAR6->VAR12 != VAR3))) {\nFUN12(VAR24 \"STR\"\n\"STR\"\n\"STR\"\n\"STR\"\n\"STR\",\nVAR5->VAR25,\n(unsigned long long) VAR2->VAR18,\nVAR3, VAR3->VAR26,\nVAR6->VAR11,\nVAR6->VAR11 ?\nVAR6->VAR11->VAR26 : 0,\nVAR6->VAR12,\nVAR6->VAR12 ?\nVAR6->VAR12->VAR26 : 0,\nVAR6->VAR14);\nFUN14(1);\nVAR7 = -VAR27;\n}\ngoto VAR22;\n}\nFUN6(VAR6, VAR6->VAR29 == NULL);\nFUN10(VAR6, \"STR\");\nFUN15(&VAR5->VAR30);\nFUN16(VAR6, VAR3, VAR15);\nFUN17(&VAR5->VAR30);\nVAR22:\nFUN7(VAR2);\nVAR10:\nFUN10(VAR6, \"STR\");\nreturn VAR7;\n}\n",
      "code_before_change_normalized": "int FUN1(handle_t *VAR1, struct buffer_head *VAR2)\n{\ntransaction_t *VAR3 = VAR1->VAR4;\njournal_t *VAR5;\nstruct journal_head *VAR6;\nint VAR7 = 0;\nif (FUN2(VAR1))\nreturn -VAR8;\nif (!FUN3(VAR2)) {\nVAR7 = -VAR9;\ngoto VAR10;\n}\nVAR6 = FUN4(VAR2);\nif (VAR6->VAR11 != VAR3 &&\nVAR6->VAR12 != VAR3) {\nFUN5(VAR2);\nFUN6(VAR6, VAR6->VAR11 == VAR3 ||\nVAR6->VAR12 == VAR3);\nFUN7(VAR2);\n}\nif (VAR6->VAR13 == 1) {\nif (VAR6->VAR11 == VAR3 &&\nVAR6->VAR14 != VAR15) {\nFUN5(VAR2);\nFUN6(VAR6, VAR6->VAR11 != VAR3 ||\nVAR6->VAR14 == VAR15);\nFUN7(VAR2);\n}\ngoto VAR10;\n}\nVAR5 = VAR3->VAR16;\nFUN8(5, \"STR\", VAR6);\nFUN9(VAR6, \"STR\");\nFUN5(VAR2);\nif (VAR6->VAR13 == 0) {\nVAR6->VAR13 = 1;\nif (VAR1->VAR17 <= 0) {\nVAR7 = -VAR18;\ngoto VAR19;\n}\nVAR1->VAR17--;\n}\nif (VAR6->VAR11 == VAR3 && VAR6->VAR14 == VAR15) {\nFUN9(VAR6, \"STR\");\nif (FUN10(VAR6->VAR11 !=\nVAR5->VAR20)) {\nFUN11(VAR21 \"STR\"\n\"STR\"\n\"STR\",\nVAR5->VAR22,\n(unsigned long long) VAR2->VAR23,\nVAR6->VAR11,\nVAR6->VAR11 ? VAR6->VAR11->VAR24 : 0,\nVAR5->VAR20,\nVAR5->VAR20 ?\nVAR5->VAR20->VAR24 : 0);\nVAR7 = -VAR25;\n}\ngoto VAR19;\n}\nFUN12(VAR2);\nif (VAR6->VAR11 != VAR3) {\nFUN9(VAR6, \"STR\");\nif (FUN10(((VAR6->VAR11 !=\nVAR5->VAR26)) ||\n(VAR6->VAR12 != VAR3))) {\nFUN11(VAR21 \"STR\"\n\"STR\"\n\"STR\"\n\"STR\"\n\"STR\",\nVAR5->VAR22,\n(unsigned long long) VAR2->VAR23,\nVAR3, VAR3->VAR24,\nVAR6->VAR11,\nVAR6->VAR11 ?\nVAR6->VAR11->VAR24 : 0,\nVAR6->VAR12,\nVAR6->VAR12 ?\nVAR6->VAR12->VAR24 : 0,\nVAR6->VAR14);\nFUN13(1);\nVAR7 = -VAR25;\n}\ngoto VAR19;\n}\nFUN6(VAR6, VAR6->VAR27 == NULL);\nFUN9(VAR6, \"STR\");\nFUN14(&VAR5->VAR28);\nFUN15(VAR6, VAR3, VAR15);\nFUN16(&VAR5->VAR28);\nVAR19:\nFUN7(VAR2);\nVAR10:\nFUN9(VAR6, \"STR\");\nreturn VAR7;\n}\n",
      "code_after_change_raw": "int jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\ntransaction_t *transaction = handle->h_transaction;\njournal_t *journal;\nstruct journal_head *jh;\nint ret = 0;\nif (is_handle_aborted(handle))\nreturn -EROFS;\nif (!buffer_jbd(bh)) {\nret = -EUCLEAN;\ngoto out;\n}\njh = bh2jh(bh);\nif (jh->b_transaction != transaction &&\njh->b_next_transaction != transaction) {\njbd_lock_bh_state(bh);\nJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\njh->b_next_transaction == transaction);\njbd_unlock_bh_state(bh);\n}\nif (jh->b_modified == 1) {\nif (jh->b_transaction == transaction &&\njh->b_jlist != BJ_Metadata) {\njbd_lock_bh_state(bh);\nif (jh->b_transaction == transaction &&\njh->b_jlist != BJ_Metadata)\npr_err(\"JBD2: assertion failure: h_type=%u \"\n\"h_line_no=%u block_no=%llu jlist=%u\\n\",\nhandle->h_type, handle->h_line_no,\n(unsigned long long) bh->b_blocknr,\njh->b_jlist);\nJ_ASSERT_JH(jh, jh->b_transaction != transaction ||\njh->b_jlist == BJ_Metadata);\njbd_unlock_bh_state(bh);\n}\ngoto out;\n}\njournal = transaction->t_journal;\njbd_debug(5, \"journal_head %p\\n\", jh);\nJBUFFER_TRACE(jh, \"entry\");\njbd_lock_bh_state(bh);\nif (jh->b_modified == 0) {\nif (handle->h_buffer_credits <= 0) {\nret = -ENOSPC;\ngoto out_unlock_bh;\n}\njh->b_modified = 1;\nhandle->h_buffer_credits--;\n}\nif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\nJBUFFER_TRACE(jh, \"fastpath\");\nif (unlikely(jh->b_transaction !=\njournal->j_running_transaction)) {\nprintk(KERN_ERR \"JBD2: %s: \"\n\"jh->b_transaction (%llu, %p, %u) != \"\n\"journal->j_running_transaction (%p, %u)\\n\",\njournal->j_devname,\n(unsigned long long) bh->b_blocknr,\njh->b_transaction,\njh->b_transaction ? jh->b_transaction->t_tid : 0,\njournal->j_running_transaction,\njournal->j_running_transaction ?\njournal->j_running_transaction->t_tid : 0);\nret = -EINVAL;\n}\ngoto out_unlock_bh;\n}\nset_buffer_jbddirty(bh);\nif (jh->b_transaction != transaction) {\nJBUFFER_TRACE(jh, \"already on other transaction\");\nif (unlikely(((jh->b_transaction !=\njournal->j_committing_transaction)) ||\n(jh->b_next_transaction != transaction))) {\nprintk(KERN_ERR \"jbd2_journal_dirty_metadata: %s: \"\n\"bad jh for block %llu: \"\n\"transaction (%p, %u), \"\n\"jh->b_transaction (%p, %u), \"\n\"jh->b_next_transaction (%p, %u), jlist %u\\n\",\njournal->j_devname,\n(unsigned long long) bh->b_blocknr,\ntransaction, transaction->t_tid,\njh->b_transaction,\njh->b_transaction ?\njh->b_transaction->t_tid : 0,\njh->b_next_transaction,\njh->b_next_transaction ?\njh->b_next_transaction->t_tid : 0,\njh->b_jlist);\nWARN_ON(1);\nret = -EINVAL;\n}\ngoto out_unlock_bh;\n}\nJ_ASSERT_JH(jh, jh->b_frozen_data == NULL);\nJBUFFER_TRACE(jh, \"file as BJ_Metadata\");\nspin_lock(&journal->j_list_lock);\n__jbd2_journal_file_buffer(jh, transaction, BJ_Metadata);\nspin_unlock(&journal->j_list_lock);\nout_unlock_bh:\njbd_unlock_bh_state(bh);\nout:\nJBUFFER_TRACE(jh, \"exit\");\nreturn ret;\n}\n",
      "code_before_change_raw": "int jbd2_journal_dirty_metadata(handle_t *handle, struct buffer_head *bh)\n{\ntransaction_t *transaction = handle->h_transaction;\njournal_t *journal;\nstruct journal_head *jh;\nint ret = 0;\nif (is_handle_aborted(handle))\nreturn -EROFS;\nif (!buffer_jbd(bh)) {\nret = -EUCLEAN;\ngoto out;\n}\njh = bh2jh(bh);\nif (jh->b_transaction != transaction &&\njh->b_next_transaction != transaction) {\njbd_lock_bh_state(bh);\nJ_ASSERT_JH(jh, jh->b_transaction == transaction ||\njh->b_next_transaction == transaction);\njbd_unlock_bh_state(bh);\n}\nif (jh->b_modified == 1) {\nif (jh->b_transaction == transaction &&\njh->b_jlist != BJ_Metadata) {\njbd_lock_bh_state(bh);\nJ_ASSERT_JH(jh, jh->b_transaction != transaction ||\njh->b_jlist == BJ_Metadata);\njbd_unlock_bh_state(bh);\n}\ngoto out;\n}\njournal = transaction->t_journal;\njbd_debug(5, \"journal_head %p\\n\", jh);\nJBUFFER_TRACE(jh, \"entry\");\njbd_lock_bh_state(bh);\nif (jh->b_modified == 0) {\njh->b_modified = 1;\nif (handle->h_buffer_credits <= 0) {\nret = -ENOSPC;\ngoto out_unlock_bh;\n}\nhandle->h_buffer_credits--;\n}\nif (jh->b_transaction == transaction && jh->b_jlist == BJ_Metadata) {\nJBUFFER_TRACE(jh, \"fastpath\");\nif (unlikely(jh->b_transaction !=\njournal->j_running_transaction)) {\nprintk(KERN_ERR \"JBD2: %s: \"\n\"jh->b_transaction (%llu, %p, %u) != \"\n\"journal->j_running_transaction (%p, %u)\\n\",\njournal->j_devname,\n(unsigned long long) bh->b_blocknr,\njh->b_transaction,\njh->b_transaction ? jh->b_transaction->t_tid : 0,\njournal->j_running_transaction,\njournal->j_running_transaction ?\njournal->j_running_transaction->t_tid : 0);\nret = -EINVAL;\n}\ngoto out_unlock_bh;\n}\nset_buffer_jbddirty(bh);\nif (jh->b_transaction != transaction) {\nJBUFFER_TRACE(jh, \"already on other transaction\");\nif (unlikely(((jh->b_transaction !=\njournal->j_committing_transaction)) ||\n(jh->b_next_transaction != transaction))) {\nprintk(KERN_ERR \"jbd2_journal_dirty_metadata: %s: \"\n\"bad jh for block %llu: \"\n\"transaction (%p, %u), \"\n\"jh->b_transaction (%p, %u), \"\n\"jh->b_next_transaction (%p, %u), jlist %u\\n\",\njournal->j_devname,\n(unsigned long long) bh->b_blocknr,\ntransaction, transaction->t_tid,\njh->b_transaction,\njh->b_transaction ?\njh->b_transaction->t_tid : 0,\njh->b_next_transaction,\njh->b_next_transaction ?\njh->b_next_transaction->t_tid : 0,\njh->b_jlist);\nWARN_ON(1);\nret = -EINVAL;\n}\ngoto out_unlock_bh;\n}\nJ_ASSERT_JH(jh, jh->b_frozen_data == NULL);\nJBUFFER_TRACE(jh, \"file as BJ_Metadata\");\nspin_lock(&journal->j_list_lock);\n__jbd2_journal_file_buffer(jh, transaction, BJ_Metadata);\nspin_unlock(&journal->j_list_lock);\nout_unlock_bh:\njbd_unlock_bh_state(bh);\nout:\nJBUFFER_TRACE(jh, \"exit\");\nreturn ret;\n}\n"
    }
  ],
  "CVE-2018-11506": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inconsistent sizes of sense buffers at different layers (CDROM and SCSI) lead to potential stack-based buffer overflow vulnerabilities.",
        "trigger_condition": "The code does not ensure consistent sizes of sense buffers used in the scsi_execute function call, allowing for potential buffer overflow issues.",
        "specific_code_behavior_causing_vulnerability": "The code does not allocate a fixed-size buffer for sense data, leading to potential buffer overflow vulnerabilities when handling sense buffers at different layers."
      },
      "solution": "To mitigate the vulnerability, it is necessary to allocate a fixed-size buffer for sense data and ensure consistent sizes of sense buffers used in the scsi_execute function call. This prevents potential buffer overflow issues and ensures secure handling of sense data. In this case, the solution involves introducing a new sense_buffer array with a fixed size and a senseptr pointer to it, along with copying the sense data to cgc->sense if cgc->sense is not NULL.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2018-11506, which involves a stack-based buffer overflow in the sr_do_ioctl function. The vulnerability arises from the fact that sense buffers have different sizes at the CDROM layer and the SCSI layer, leading to potential buffer overflow issues.\n\nThe modification introduces a new sense_buffer array with a fixed size of SCSI_SENSE_BUFFERSIZE and a senseptr pointer to it. This ensures that the sense buffer used in the scsi_execute function call has a consistent size and prevents potential buffer overflow vulnerabilities.\n\nAdditionally, the modification includes a check to copy the sense data from the sense_buffer to cgc->sense if cgc->sense is not NULL. This ensures that the sense data is properly handled and avoids any potential memory corruption issues.\n\nBy making these changes, the code is more robust and secure, mitigating the risk of a denial of service or other impacts related to the buffer overflow vulnerability described in CVE-2018-11506.",
      "GPT_purpose": "Perform SCSI command execution and error handling for CD-ROM devices.",
      "GPT_function": "\n1. Execute SCSI command on a SCSI device.\n2. Handle error cases related to CDROM operations.\n3. Set error status and return it.",
      "CVE_id": "CVE-2018-11506",
      "code_before_change": "int sr_do_ioctl(Scsi_CD *cd, struct packet_command *cgc)\n{\n\tstruct scsi_device *SDev;\n\tstruct scsi_sense_hdr sshdr;\n\tint result, err = 0, retries = 0;\n\n\tSDev = cd->device;\n\n      retry:\n\tif (!scsi_block_when_processing_errors(SDev)) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tresult = scsi_execute(SDev, cgc->cmd, cgc->data_direction,\n\t\t\t      cgc->buffer, cgc->buflen,\n\t\t\t      (unsigned char *)cgc->sense, &sshdr,\n\t\t\t      cgc->timeout, IOCTL_RETRIES, 0, 0, NULL);\n\n\t/* Minimal error checking.  Ignore cases we know about, and report the rest. */\n\tif (driver_byte(result) != 0) {\n\t\tswitch (sshdr.sense_key) {\n\t\tcase UNIT_ATTENTION:\n\t\t\tSDev->changed = 1;\n\t\t\tif (!cgc->quiet)\n\t\t\t\tsr_printk(KERN_INFO, cd,\n\t\t\t\t\t  \"disc change detected.\\n\");\n\t\t\tif (retries++ < 10)\n\t\t\t\tgoto retry;\n\t\t\terr = -ENOMEDIUM;\n\t\t\tbreak;\n\t\tcase NOT_READY:\t/* This happens if there is no disc in drive */\n\t\t\tif (sshdr.asc == 0x04 &&\n\t\t\t    sshdr.ascq == 0x01) {\n\t\t\t\t/* sense: Logical unit is in process of becoming ready */\n\t\t\t\tif (!cgc->quiet)\n\t\t\t\t\tsr_printk(KERN_INFO, cd,\n\t\t\t\t\t\t  \"CDROM not ready yet.\\n\");\n\t\t\t\tif (retries++ < 10) {\n\t\t\t\t\t/* sleep 2 sec and try again */\n\t\t\t\t\tssleep(2);\n\t\t\t\t\tgoto retry;\n\t\t\t\t} else {\n\t\t\t\t\t/* 20 secs are enough? */\n\t\t\t\t\terr = -ENOMEDIUM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!cgc->quiet)\n\t\t\t\tsr_printk(KERN_INFO, cd,\n\t\t\t\t\t  \"CDROM not ready.  Make sure there \"\n\t\t\t\t\t  \"is a disc in the drive.\\n\");\n\t\t\terr = -ENOMEDIUM;\n\t\t\tbreak;\n\t\tcase ILLEGAL_REQUEST:\n\t\t\terr = -EIO;\n\t\t\tif (sshdr.asc == 0x20 &&\n\t\t\t    sshdr.ascq == 0x00)\n\t\t\t\t/* sense: Invalid command operation code */\n\t\t\t\terr = -EDRIVE_CANT_DO_THIS;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EIO;\n\t\t}\n\t}\n\n\t/* Wake up a process waiting for device */\n      out:\n\tcgc->stat = err;\n\treturn err;\n}",
      "code_after_change": "int sr_do_ioctl(Scsi_CD *cd, struct packet_command *cgc)\n{\n\tstruct scsi_device *SDev;\n\tstruct scsi_sense_hdr sshdr;\n\tint result, err = 0, retries = 0;\n\tunsigned char sense_buffer[SCSI_SENSE_BUFFERSIZE], *senseptr = NULL;\n\n\tSDev = cd->device;\n\n\tif (cgc->sense)\n\t\tsenseptr = sense_buffer;\n\n      retry:\n\tif (!scsi_block_when_processing_errors(SDev)) {\n\t\terr = -ENODEV;\n\t\tgoto out;\n\t}\n\n\tresult = scsi_execute(SDev, cgc->cmd, cgc->data_direction,\n\t\t\t      cgc->buffer, cgc->buflen, senseptr, &sshdr,\n\t\t\t      cgc->timeout, IOCTL_RETRIES, 0, 0, NULL);\n\n\tif (cgc->sense)\n\t\tmemcpy(cgc->sense, sense_buffer, sizeof(*cgc->sense));\n\n\t/* Minimal error checking.  Ignore cases we know about, and report the rest. */\n\tif (driver_byte(result) != 0) {\n\t\tswitch (sshdr.sense_key) {\n\t\tcase UNIT_ATTENTION:\n\t\t\tSDev->changed = 1;\n\t\t\tif (!cgc->quiet)\n\t\t\t\tsr_printk(KERN_INFO, cd,\n\t\t\t\t\t  \"disc change detected.\\n\");\n\t\t\tif (retries++ < 10)\n\t\t\t\tgoto retry;\n\t\t\terr = -ENOMEDIUM;\n\t\t\tbreak;\n\t\tcase NOT_READY:\t/* This happens if there is no disc in drive */\n\t\t\tif (sshdr.asc == 0x04 &&\n\t\t\t    sshdr.ascq == 0x01) {\n\t\t\t\t/* sense: Logical unit is in process of becoming ready */\n\t\t\t\tif (!cgc->quiet)\n\t\t\t\t\tsr_printk(KERN_INFO, cd,\n\t\t\t\t\t\t  \"CDROM not ready yet.\\n\");\n\t\t\t\tif (retries++ < 10) {\n\t\t\t\t\t/* sleep 2 sec and try again */\n\t\t\t\t\tssleep(2);\n\t\t\t\t\tgoto retry;\n\t\t\t\t} else {\n\t\t\t\t\t/* 20 secs are enough? */\n\t\t\t\t\terr = -ENOMEDIUM;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (!cgc->quiet)\n\t\t\t\tsr_printk(KERN_INFO, cd,\n\t\t\t\t\t  \"CDROM not ready.  Make sure there \"\n\t\t\t\t\t  \"is a disc in the drive.\\n\");\n\t\t\terr = -ENOMEDIUM;\n\t\t\tbreak;\n\t\tcase ILLEGAL_REQUEST:\n\t\t\terr = -EIO;\n\t\t\tif (sshdr.asc == 0x20 &&\n\t\t\t    sshdr.ascq == 0x00)\n\t\t\t\t/* sense: Invalid command operation code */\n\t\t\t\terr = -EDRIVE_CANT_DO_THIS;\n\t\t\tbreak;\n\t\tdefault:\n\t\t\terr = -EIO;\n\t\t}\n\t}\n\n\t/* Wake up a process waiting for device */\n      out:\n\tcgc->stat = err;\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tunsigned char sense_buffer[SCSI_SENSE_BUFFERSIZE], *senseptr = NULL;",
          "",
          "\tif (cgc->sense)",
          "\t\tsenseptr = sense_buffer;",
          "\t\t\t      cgc->buffer, cgc->buflen, senseptr, &sshdr,",
          "",
          "\tif (cgc->sense)",
          "\t\tmemcpy(cgc->sense, sense_buffer, sizeof(*cgc->sense));"
        ],
        "deleted": [
          "\t\t\t      cgc->buffer, cgc->buflen,",
          "\t\t\t      (unsigned char *)cgc->sense, &sshdr,"
        ]
      },
      "preconditions_for_vulnerability": "Inconsistent sizes of sense buffers at different layers (CDROM and SCSI) lead to potential stack-based buffer overflow vulnerabilities.",
      "trigger_condition": "The code does not ensure consistent sizes of sense buffers used in the scsi_execute function call, allowing for potential buffer overflow issues.",
      "specific_code_behavior_causing_vulnerability": "The code does not allocate a fixed-size buffer for sense data, leading to potential buffer overflow vulnerabilities when handling sense buffers at different layers.",
      "id": 28,
      "code_after_change_normalized": "int FUN1(Scsi_CD *VAR1, struct packet_command *VAR2)\n{\nstruct scsi_device *VAR3;\nstruct scsi_sense_hdr VAR4;\nint VAR5, VAR6 = 0, VAR7 = 0;\nunsigned char VAR8[VAR9], *VAR10 = NULL;\nVAR3 = VAR1->VAR11;\nif (VAR2->VAR12)\nVAR10 = VAR8;\nVAR13:\nif (!FUN2(VAR3)) {\nVAR6 = -VAR14;\ngoto VAR15;\n}\nVAR5 = FUN3(VAR3, VAR2->VAR16, VAR2->VAR17,\nVAR2->VAR18, VAR2->VAR19, VAR10, &VAR4,\nVAR2->VAR20, VAR21, 0, 0, NULL);\nif (VAR2->VAR12)\nFUN4(VAR2->VAR12, VAR8, sizeof(*VAR2->VAR12));\nif (FUN5(VAR5) != 0) {\nswitch (VAR4.VAR22) {\ncase VAR23:\nVAR3->VAR24 = 1;\nif (!VAR2->VAR25)\nFUN6(VAR26, VAR1,\n\"STR\");\nif (VAR7++ < 10)\ngoto VAR13;\nVAR6 = -VAR27;\nbreak;\ncase VAR28:\t\nif (VAR4.VAR29 == VAR30 &&\nVAR4.VAR31 == VAR30) {\nif (!VAR2->VAR25)\nFUN6(VAR26, VAR1,\n\"STR\");\nif (VAR7++ < 10) {\nFUN7(2);\ngoto VAR13;\n} else {\nVAR6 = -VAR27;\nbreak;\n}\n}\nif (!VAR2->VAR25)\nFUN6(VAR26, VAR1,\n\"STR\"\n\"STR\");\nVAR6 = -VAR27;\nbreak;\ncase VAR32:\nVAR6 = -VAR33;\nif (VAR4.VAR29 == VAR30 &&\nVAR4.VAR31 == VAR30)\nVAR6 = -VAR34;\nbreak;\ndefault:\nVAR6 = -VAR33;\n}\n}\nVAR15:\nVAR2->VAR35 = VAR6;\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "int FUN1(Scsi_CD *VAR1, struct packet_command *VAR2)\n{\nstruct scsi_device *VAR3;\nstruct scsi_sense_hdr VAR4;\nint VAR5, VAR6 = 0, VAR7 = 0;\nVAR3 = VAR1->VAR8;\nVAR9:\nif (!FUN2(VAR3)) {\nVAR6 = -VAR10;\ngoto VAR11;\n}\nVAR5 = FUN3(VAR3, VAR2->VAR12, VAR2->VAR13,\nVAR2->VAR14, VAR2->VAR15,\n(unsigned char *)VAR2->VAR16, &VAR4,\nVAR2->VAR17, VAR18, 0, 0, NULL);\nif (FUN4(VAR5) != 0) {\nswitch (VAR4.VAR19) {\ncase VAR20:\nVAR3->VAR21 = 1;\nif (!VAR2->VAR22)\nFUN5(VAR23, VAR1,\n\"STR\");\nif (VAR7++ < 10)\ngoto VAR9;\nVAR6 = -VAR24;\nbreak;\ncase VAR25:\t\nif (VAR4.VAR26 == VAR27 &&\nVAR4.VAR28 == VAR27) {\nif (!VAR2->VAR22)\nFUN5(VAR23, VAR1,\n\"STR\");\nif (VAR7++ < 10) {\nFUN6(2);\ngoto VAR9;\n} else {\nVAR6 = -VAR24;\nbreak;\n}\n}\nif (!VAR2->VAR22)\nFUN5(VAR23, VAR1,\n\"STR\"\n\"STR\");\nVAR6 = -VAR24;\nbreak;\ncase VAR29:\nVAR6 = -VAR30;\nif (VAR4.VAR26 == VAR27 &&\nVAR4.VAR28 == VAR27)\nVAR6 = -VAR31;\nbreak;\ndefault:\nVAR6 = -VAR30;\n}\n}\nVAR11:\nVAR2->VAR32 = VAR6;\nreturn VAR6;\n}\n",
      "code_after_change_raw": "int sr_do_ioctl(Scsi_CD *cd, struct packet_command *cgc)\n{\nstruct scsi_device *SDev;\nstruct scsi_sense_hdr sshdr;\nint result, err = 0, retries = 0;\nunsigned char sense_buffer[SCSI_SENSE_BUFFERSIZE], *senseptr = NULL;\nSDev = cd->device;\nif (cgc->sense)\nsenseptr = sense_buffer;\nretry:\nif (!scsi_block_when_processing_errors(SDev)) {\nerr = -ENODEV;\ngoto out;\n}\nresult = scsi_execute(SDev, cgc->cmd, cgc->data_direction,\ncgc->buffer, cgc->buflen, senseptr, &sshdr,\ncgc->timeout, IOCTL_RETRIES, 0, 0, NULL);\nif (cgc->sense)\nmemcpy(cgc->sense, sense_buffer, sizeof(*cgc->sense));\nif (driver_byte(result) != 0) {\nswitch (sshdr.sense_key) {\ncase UNIT_ATTENTION:\nSDev->changed = 1;\nif (!cgc->quiet)\nsr_printk(KERN_INFO, cd,\n\"disc change detected.\\n\");\nif (retries++ < 10)\ngoto retry;\nerr = -ENOMEDIUM;\nbreak;\ncase NOT_READY:\t\nif (sshdr.asc == 0x04 &&\nsshdr.ascq == 0x01) {\nif (!cgc->quiet)\nsr_printk(KERN_INFO, cd,\n\"CDROM not ready yet.\\n\");\nif (retries++ < 10) {\nssleep(2);\ngoto retry;\n} else {\nerr = -ENOMEDIUM;\nbreak;\n}\n}\nif (!cgc->quiet)\nsr_printk(KERN_INFO, cd,\n\"CDROM not ready.  Make sure there \"\n\"is a disc in the drive.\\n\");\nerr = -ENOMEDIUM;\nbreak;\ncase ILLEGAL_REQUEST:\nerr = -EIO;\nif (sshdr.asc == 0x20 &&\nsshdr.ascq == 0x00)\nerr = -EDRIVE_CANT_DO_THIS;\nbreak;\ndefault:\nerr = -EIO;\n}\n}\nout:\ncgc->stat = err;\nreturn err;\n}\n",
      "code_before_change_raw": "int sr_do_ioctl(Scsi_CD *cd, struct packet_command *cgc)\n{\nstruct scsi_device *SDev;\nstruct scsi_sense_hdr sshdr;\nint result, err = 0, retries = 0;\nSDev = cd->device;\nretry:\nif (!scsi_block_when_processing_errors(SDev)) {\nerr = -ENODEV;\ngoto out;\n}\nresult = scsi_execute(SDev, cgc->cmd, cgc->data_direction,\ncgc->buffer, cgc->buflen,\n(unsigned char *)cgc->sense, &sshdr,\ncgc->timeout, IOCTL_RETRIES, 0, 0, NULL);\nif (driver_byte(result) != 0) {\nswitch (sshdr.sense_key) {\ncase UNIT_ATTENTION:\nSDev->changed = 1;\nif (!cgc->quiet)\nsr_printk(KERN_INFO, cd,\n\"disc change detected.\\n\");\nif (retries++ < 10)\ngoto retry;\nerr = -ENOMEDIUM;\nbreak;\ncase NOT_READY:\t\nif (sshdr.asc == 0x04 &&\nsshdr.ascq == 0x01) {\nif (!cgc->quiet)\nsr_printk(KERN_INFO, cd,\n\"CDROM not ready yet.\\n\");\nif (retries++ < 10) {\nssleep(2);\ngoto retry;\n} else {\nerr = -ENOMEDIUM;\nbreak;\n}\n}\nif (!cgc->quiet)\nsr_printk(KERN_INFO, cd,\n\"CDROM not ready.  Make sure there \"\n\"is a disc in the drive.\\n\");\nerr = -ENOMEDIUM;\nbreak;\ncase ILLEGAL_REQUEST:\nerr = -EIO;\nif (sshdr.asc == 0x20 &&\nsshdr.ascq == 0x00)\nerr = -EDRIVE_CANT_DO_THIS;\nbreak;\ndefault:\nerr = -EIO;\n}\n}\nout:\ncgc->stat = err;\nreturn err;\n}\n"
    }
  ],
  "CVE-2018-12714": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Calling the filter parsing function with no filter, resulting in an N=0 case where it expects at least one line to have been read.",
        "trigger_condition": "Crafted perf_event_open and mmap system calls are used, causing the filter parsing function to operate on an empty set of program entries, leading to an invalid N-1 index and potential denial of service or other unspecified impacts.",
        "specific_code_behavior_causing_vulnerability": "The code does not check for the case where no program entries are created after the first pass, leading to the program operating on an empty set of entries and potentially causing an out-of-bounds write and other issues.",
        "solution": "To mitigate the vulnerability, it is necessary to add a check after the first pass to ensure that at least one program entry is created before proceeding to the second and third passes. This check prevents the program from operating on an empty set of entries, thus avoiding the potential out-of-bounds write and other related issues."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability described in CVE-2018-12714. The vulnerability occurs when the filter parsing function `predicate_parse` is called with no filter, resulting in an N=0 case where it expects at least one line to have been read. This leads to an invalid N-1 index, which can cause a denial of service (slab out-of-bounds write) or other unspecified impacts when crafted `perf_event_open` and `mmap` system calls are used.\n\nBy adding the check `if (!N)` after the first pass in the code, we ensure that there is at least one program entry created before proceeding to the second and third passes. This check prevents the vulnerability by ensuring that the program does not operate on an empty set of program entries, thus avoiding the potential out-of-bounds write and other related issues.",
      "GPT_purpose": "Parse a predicate string to create a program entry structure for filtering trace events.",
      "GPT_function": "\n1. Parse a predicate string to create a program entry.\n2. Handle filter parsing with stack operations.\n3. Perform multiple passes to process the parsed predicates and update program entries accordingly.",
      "CVE_id": "CVE-2018-12714",
      "code_before_change": "static struct prog_entry *\npredicate_parse(const char *str, int nr_parens, int nr_preds,\n\t\tparse_pred_fn parse_pred, void *data,\n\t\tstruct filter_parse_error *pe)\n{\n\tstruct prog_entry *prog_stack;\n\tstruct prog_entry *prog;\n\tconst char *ptr = str;\n\tchar *inverts = NULL;\n\tint *op_stack;\n\tint *top;\n\tint invert = 0;\n\tint ret = -ENOMEM;\n\tint len;\n\tint N = 0;\n\tint i;\n\n\tnr_preds += 2; /* For TRUE and FALSE */\n\n\top_stack = kmalloc_array(nr_parens, sizeof(*op_stack), GFP_KERNEL);\n\tif (!op_stack)\n\t\treturn ERR_PTR(-ENOMEM);\n\tprog_stack = kmalloc_array(nr_preds, sizeof(*prog_stack), GFP_KERNEL);\n\tif (!prog_stack) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\tinverts = kmalloc_array(nr_preds, sizeof(*inverts), GFP_KERNEL);\n\tif (!inverts) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\n\ttop = op_stack;\n\tprog = prog_stack;\n\t*top = 0;\n\n\t/* First pass */\n\twhile (*ptr) {\t\t\t\t\t\t/* #1 */\n\t\tconst char *next = ptr++;\n\n\t\tif (isspace(*next))\n\t\t\tcontinue;\n\n\t\tswitch (*next) {\n\t\tcase '(':\t\t\t\t\t/* #2 */\n\t\t\tif (top - op_stack > nr_parens)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t*(++top) = invert;\n\t\t\tcontinue;\n\t\tcase '!':\t\t\t\t\t/* #3 */\n\t\t\tif (!is_not(next))\n\t\t\t\tbreak;\n\t\t\tinvert = !invert;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (N >= nr_preds) {\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS, next - str);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tinverts[N] = invert;\t\t\t\t/* #4 */\n\t\tprog[N].target = N-1;\n\n\t\tlen = parse_pred(next, data, ptr - str, pe, &prog[N].pred);\n\t\tif (len < 0) {\n\t\t\tret = len;\n\t\t\tgoto out_free;\n\t\t}\n\t\tptr = next + len;\n\n\t\tN++;\n\n\t\tret = -1;\n\t\twhile (1) {\t\t\t\t\t/* #5 */\n\t\t\tnext = ptr++;\n\t\t\tif (isspace(*next))\n\t\t\t\tcontinue;\n\n\t\t\tswitch (*next) {\n\t\t\tcase ')':\n\t\t\tcase '\\0':\n\t\t\t\tbreak;\n\t\t\tcase '&':\n\t\t\tcase '|':\n\t\t\t\tif (next[1] == next[0]) {\n\t\t\t\t\tptr++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS,\n\t\t\t\t\t    next - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tinvert = *top & INVERT;\n\n\t\t\tif (*top & PROCESS_AND) {\t\t/* #7 */\n\t\t\t\tupdate_preds(prog, N - 1, invert);\n\t\t\t\t*top &= ~PROCESS_AND;\n\t\t\t}\n\t\t\tif (*next == '&') {\t\t\t/* #8 */\n\t\t\t\t*top |= PROCESS_AND;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (*top & PROCESS_OR) {\t\t/* #9 */\n\t\t\t\tupdate_preds(prog, N - 1, !invert);\n\t\t\t\t*top &= ~PROCESS_OR;\n\t\t\t}\n\t\t\tif (*next == '|') {\t\t\t/* #10 */\n\t\t\t\t*top |= PROCESS_OR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!*next)\t\t\t\t/* #11 */\n\t\t\t\tgoto out;\n\n\t\t\tif (top == op_stack) {\n\t\t\t\tret = -1;\n\t\t\t\t/* Too few '(' */\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, ptr - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\ttop--;\t\t\t\t\t/* #12 */\n\t\t}\n\t}\n out:\n\tif (top != op_stack) {\n\t\t/* Too many '(' */\n\t\tparse_error(pe, FILT_ERR_TOO_MANY_OPEN, ptr - str);\n\t\tgoto out_free;\n\t}\n\n\tprog[N].pred = NULL;\t\t\t\t\t/* #13 */\n\tprog[N].target = 1;\t\t/* TRUE */\n\tprog[N+1].pred = NULL;\n\tprog[N+1].target = 0;\t\t/* FALSE */\n\tprog[N-1].target = N;\n\tprog[N-1].when_to_branch = false;\n\n\t/* Second Pass */\n\tfor (i = N-1 ; i--; ) {\n\t\tint target = prog[i].target;\n\t\tif (prog[i].when_to_branch == prog[target].when_to_branch)\n\t\t\tprog[i].target = prog[target].target;\n\t}\n\n\t/* Third Pass */\n\tfor (i = 0; i < N; i++) {\n\t\tinvert = inverts[i] ^ prog[i].when_to_branch;\n\t\tprog[i].when_to_branch = invert;\n\t\t/* Make sure the program always moves forward */\n\t\tif (WARN_ON(prog[i].target <= i)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\treturn prog;\nout_free:\n\tkfree(op_stack);\n\tkfree(prog_stack);\n\tkfree(inverts);\n\treturn ERR_PTR(ret);\n}",
      "code_after_change": "static struct prog_entry *\npredicate_parse(const char *str, int nr_parens, int nr_preds,\n\t\tparse_pred_fn parse_pred, void *data,\n\t\tstruct filter_parse_error *pe)\n{\n\tstruct prog_entry *prog_stack;\n\tstruct prog_entry *prog;\n\tconst char *ptr = str;\n\tchar *inverts = NULL;\n\tint *op_stack;\n\tint *top;\n\tint invert = 0;\n\tint ret = -ENOMEM;\n\tint len;\n\tint N = 0;\n\tint i;\n\n\tnr_preds += 2; /* For TRUE and FALSE */\n\n\top_stack = kmalloc_array(nr_parens, sizeof(*op_stack), GFP_KERNEL);\n\tif (!op_stack)\n\t\treturn ERR_PTR(-ENOMEM);\n\tprog_stack = kmalloc_array(nr_preds, sizeof(*prog_stack), GFP_KERNEL);\n\tif (!prog_stack) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\tinverts = kmalloc_array(nr_preds, sizeof(*inverts), GFP_KERNEL);\n\tif (!inverts) {\n\t\tparse_error(pe, -ENOMEM, 0);\n\t\tgoto out_free;\n\t}\n\n\ttop = op_stack;\n\tprog = prog_stack;\n\t*top = 0;\n\n\t/* First pass */\n\twhile (*ptr) {\t\t\t\t\t\t/* #1 */\n\t\tconst char *next = ptr++;\n\n\t\tif (isspace(*next))\n\t\t\tcontinue;\n\n\t\tswitch (*next) {\n\t\tcase '(':\t\t\t\t\t/* #2 */\n\t\t\tif (top - op_stack > nr_parens)\n\t\t\t\treturn ERR_PTR(-EINVAL);\n\t\t\t*(++top) = invert;\n\t\t\tcontinue;\n\t\tcase '!':\t\t\t\t\t/* #3 */\n\t\t\tif (!is_not(next))\n\t\t\t\tbreak;\n\t\t\tinvert = !invert;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (N >= nr_preds) {\n\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS, next - str);\n\t\t\tgoto out_free;\n\t\t}\n\n\t\tinverts[N] = invert;\t\t\t\t/* #4 */\n\t\tprog[N].target = N-1;\n\n\t\tlen = parse_pred(next, data, ptr - str, pe, &prog[N].pred);\n\t\tif (len < 0) {\n\t\t\tret = len;\n\t\t\tgoto out_free;\n\t\t}\n\t\tptr = next + len;\n\n\t\tN++;\n\n\t\tret = -1;\n\t\twhile (1) {\t\t\t\t\t/* #5 */\n\t\t\tnext = ptr++;\n\t\t\tif (isspace(*next))\n\t\t\t\tcontinue;\n\n\t\t\tswitch (*next) {\n\t\t\tcase ')':\n\t\t\tcase '\\0':\n\t\t\t\tbreak;\n\t\t\tcase '&':\n\t\t\tcase '|':\n\t\t\t\tif (next[1] == next[0]) {\n\t\t\t\t\tptr++;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_PREDS,\n\t\t\t\t\t    next - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\n\t\t\tinvert = *top & INVERT;\n\n\t\t\tif (*top & PROCESS_AND) {\t\t/* #7 */\n\t\t\t\tupdate_preds(prog, N - 1, invert);\n\t\t\t\t*top &= ~PROCESS_AND;\n\t\t\t}\n\t\t\tif (*next == '&') {\t\t\t/* #8 */\n\t\t\t\t*top |= PROCESS_AND;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (*top & PROCESS_OR) {\t\t/* #9 */\n\t\t\t\tupdate_preds(prog, N - 1, !invert);\n\t\t\t\t*top &= ~PROCESS_OR;\n\t\t\t}\n\t\t\tif (*next == '|') {\t\t\t/* #10 */\n\t\t\t\t*top |= PROCESS_OR;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (!*next)\t\t\t\t/* #11 */\n\t\t\t\tgoto out;\n\n\t\t\tif (top == op_stack) {\n\t\t\t\tret = -1;\n\t\t\t\t/* Too few '(' */\n\t\t\t\tparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, ptr - str);\n\t\t\t\tgoto out_free;\n\t\t\t}\n\t\t\ttop--;\t\t\t\t\t/* #12 */\n\t\t}\n\t}\n out:\n\tif (top != op_stack) {\n\t\t/* Too many '(' */\n\t\tparse_error(pe, FILT_ERR_TOO_MANY_OPEN, ptr - str);\n\t\tgoto out_free;\n\t}\n\n\tif (!N) {\n\t\t/* No program? */\n\t\tret = -EINVAL;\n\t\tparse_error(pe, FILT_ERR_NO_FILTER, ptr - str);\n\t\tgoto out_free;\n\t}\n\n\tprog[N].pred = NULL;\t\t\t\t\t/* #13 */\n\tprog[N].target = 1;\t\t/* TRUE */\n\tprog[N+1].pred = NULL;\n\tprog[N+1].target = 0;\t\t/* FALSE */\n\tprog[N-1].target = N;\n\tprog[N-1].when_to_branch = false;\n\n\t/* Second Pass */\n\tfor (i = N-1 ; i--; ) {\n\t\tint target = prog[i].target;\n\t\tif (prog[i].when_to_branch == prog[target].when_to_branch)\n\t\t\tprog[i].target = prog[target].target;\n\t}\n\n\t/* Third Pass */\n\tfor (i = 0; i < N; i++) {\n\t\tinvert = inverts[i] ^ prog[i].when_to_branch;\n\t\tprog[i].when_to_branch = invert;\n\t\t/* Make sure the program always moves forward */\n\t\tif (WARN_ON(prog[i].target <= i)) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out_free;\n\t\t}\n\t}\n\n\treturn prog;\nout_free:\n\tkfree(op_stack);\n\tkfree(prog_stack);\n\tkfree(inverts);\n\treturn ERR_PTR(ret);\n}",
      "modified_lines": {
        "added": [
          "\tif (!N) {",
          "\t\t/* No program? */",
          "\t\tret = -EINVAL;",
          "\t\tparse_error(pe, FILT_ERR_NO_FILTER, ptr - str);",
          "\t\tgoto out_free;",
          "\t}",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Calling the filter parsing function with no filter, resulting in an N=0 case where it expects at least one line to have been read.",
      "trigger_condition": "Crafted perf_event_open and mmap system calls are used, causing the filter parsing function to operate on an empty set of program entries, leading to an invalid N-1 index and potential denial of service or other unspecified impacts.",
      "specific_code_behavior_causing_vulnerability": "The code does not check for the case where no program entries are created after the first pass, leading to the program operating on an empty set of entries and potentially causing an out-of-bounds write and other issues.",
      "solution": "To mitigate the vulnerability, it is necessary to add a check after the first pass to ensure that at least one program entry is created before proceeding to the second and third passes. This check prevents the program from operating on an empty set of entries, thus avoiding the potential out-of-bounds write and other related issues.",
      "id": 29,
      "code_after_change_normalized": "static struct VAR1 *\nFUN1(const char *VAR2, int VAR3, int VAR4,\nparse_pred_fn VAR5, void *VAR6,\nstruct filter_parse_error *VAR7)\n{\nstruct prog_entry *VAR8;\nstruct prog_entry *VAR9;\nconst char *VAR10 = VAR2;\nchar *VAR11 = NULL;\nint *VAR12;\nint *VAR13;\nint VAR14 = 0;\nint VAR15 = -VAR16;\nint VAR17;\nint VAR18 = 0;\nint VAR19;\nVAR4 += 2; \nVAR12 = FUN2(VAR3, sizeof(*VAR12), VAR20);\nif (!VAR12)\nreturn FUN3(-VAR16);\nVAR8 = FUN2(VAR4, sizeof(*VAR8), VAR20);\nif (!VAR8) {\nFUN4(VAR7, -VAR16, 0);\ngoto VAR21;\n}\nVAR11 = FUN2(VAR4, sizeof(*VAR11), VAR20);\nif (!VAR11) {\nFUN4(VAR7, -VAR16, 0);\ngoto VAR21;\n}\nVAR13 = VAR12;\nVAR9 = VAR8;\n*VAR13 = 0;\nwhile (*VAR10) {\t\t\t\t\t\t\nconst char *VAR22 = VAR10++;\nif (FUN5(*VAR22))\ncontinue;\nswitch (*VAR22) {\ncase :\t\t\t\t\t\nif (VAR13 - VAR12 > VAR3)\nreturn FUN3(-VAR23);\n*(++VAR13) = VAR14;\ncontinue;\ncase :\t\t\t\t\t\nif (!FUN6(VAR22))\nbreak;\nVAR14 = !VAR14;\ncontinue;\n}\nif (VAR18 >= VAR4) {\nFUN4(VAR7, VAR24, VAR22 - VAR2);\ngoto VAR21;\n}\nVAR11[VAR18] = VAR14;\t\t\t\t\nVAR9[VAR18].VAR25 = VAR18-1;\nVAR17 = FUN7(VAR22, VAR6, VAR10 - VAR2, VAR7, &VAR9[VAR18].VAR26);\nif (VAR17 < 0) {\nVAR15 = VAR17;\ngoto VAR21;\n}\nVAR10 = VAR22 + VAR17;\nVAR18++;\nVAR15 = -1;\nwhile (1) {\t\t\t\t\t\nVAR22 = VAR10++;\nif (FUN5(*VAR22))\ncontinue;\nswitch (*VAR22) {\ncase :\ncase :\nbreak;\ncase :\ncase :\nif (VAR22[1] == VAR22[0]) {\nVAR10++;\nbreak;\n}\ndefault:\nFUN4(VAR7, VAR24,\nVAR22 - VAR2);\ngoto VAR21;\n}\nVAR14 = *VAR13 & VAR27;\nif (*VAR13 & VAR28) {\t\t\nFUN8(VAR9, VAR18 - 1, VAR14);\n*VAR13 &= ~VAR28;\n}\nif (*VAR22 == ) {\t\t\t\n*VAR13 |= VAR28;\nbreak;\n}\nif (*VAR13 & VAR29) {\t\t\nFUN8(VAR9, VAR18 - 1, !VAR14);\n*VAR13 &= ~VAR29;\n}\nif (*VAR22 == ) {\t\t\t\n*VAR13 |= VAR29;\nbreak;\n}\nif (!*VAR22)\t\t\t\t\ngoto VAR30;\nif (VAR13 == VAR12) {\nVAR15 = -1;\nFUN4(VAR7, VAR31, VAR10 - VAR2);\ngoto VAR21;\n}\nVAR13--;\t\t\t\t\t\n}\n}\nVAR30:\nif (VAR13 != VAR12) {\nFUN4(VAR7, VAR32, VAR10 - VAR2);\ngoto VAR21;\n}\nif (!VAR18) {\nVAR15 = -VAR23;\nFUN4(VAR7, VAR33, VAR10 - VAR2);\ngoto VAR21;\n}\nVAR9[VAR18].VAR26 = NULL;\t\t\t\t\t\nVAR9[VAR18].VAR25 = 1;\t\t\nVAR9[VAR18+1].VAR26 = NULL;\nVAR9[VAR18+1].VAR25 = 0;\t\t\nVAR9[VAR18-1].VAR25 = VAR18;\nVAR9[VAR18-1].VAR34 = false;\nfor (VAR19 = VAR18-1 ; VAR19--; ) {\nint VAR25 = VAR9[VAR19].VAR25;\nif (VAR9[VAR19].VAR34 == VAR9[VAR25].VAR34)\nVAR9[VAR19].VAR25 = VAR9[VAR25].VAR25;\n}\nfor (VAR19 = 0; VAR19 < VAR18; VAR19++) {\nVAR14 = VAR11[VAR19] ^ VAR9[VAR19].VAR34;\nVAR9[VAR19].VAR34 = VAR14;\nif (FUN9(VAR9[VAR19].VAR25 <= VAR19)) {\nVAR15 = -VAR23;\ngoto VAR21;\n}\n}\nreturn VAR9;\nVAR21:\nFUN10(VAR12);\nFUN10(VAR8);\nFUN10(VAR11);\nreturn FUN3(VAR15);\n}\n",
      "code_before_change_normalized": "static struct VAR1 *\nFUN1(const char *VAR2, int VAR3, int VAR4,\nparse_pred_fn VAR5, void *VAR6,\nstruct filter_parse_error *VAR7)\n{\nstruct prog_entry *VAR8;\nstruct prog_entry *VAR9;\nconst char *VAR10 = VAR2;\nchar *VAR11 = NULL;\nint *VAR12;\nint *VAR13;\nint VAR14 = 0;\nint VAR15 = -VAR16;\nint VAR17;\nint VAR18 = 0;\nint VAR19;\nVAR4 += 2; \nVAR12 = FUN2(VAR3, sizeof(*VAR12), VAR20);\nif (!VAR12)\nreturn FUN3(-VAR16);\nVAR8 = FUN2(VAR4, sizeof(*VAR8), VAR20);\nif (!VAR8) {\nFUN4(VAR7, -VAR16, 0);\ngoto VAR21;\n}\nVAR11 = FUN2(VAR4, sizeof(*VAR11), VAR20);\nif (!VAR11) {\nFUN4(VAR7, -VAR16, 0);\ngoto VAR21;\n}\nVAR13 = VAR12;\nVAR9 = VAR8;\n*VAR13 = 0;\nwhile (*VAR10) {\t\t\t\t\t\t\nconst char *VAR22 = VAR10++;\nif (FUN5(*VAR22))\ncontinue;\nswitch (*VAR22) {\ncase :\t\t\t\t\t\nif (VAR13 - VAR12 > VAR3)\nreturn FUN3(-VAR23);\n*(++VAR13) = VAR14;\ncontinue;\ncase :\t\t\t\t\t\nif (!FUN6(VAR22))\nbreak;\nVAR14 = !VAR14;\ncontinue;\n}\nif (VAR18 >= VAR4) {\nFUN4(VAR7, VAR24, VAR22 - VAR2);\ngoto VAR21;\n}\nVAR11[VAR18] = VAR14;\t\t\t\t\nVAR9[VAR18].VAR25 = VAR18-1;\nVAR17 = FUN7(VAR22, VAR6, VAR10 - VAR2, VAR7, &VAR9[VAR18].VAR26);\nif (VAR17 < 0) {\nVAR15 = VAR17;\ngoto VAR21;\n}\nVAR10 = VAR22 + VAR17;\nVAR18++;\nVAR15 = -1;\nwhile (1) {\t\t\t\t\t\nVAR22 = VAR10++;\nif (FUN5(*VAR22))\ncontinue;\nswitch (*VAR22) {\ncase :\ncase :\nbreak;\ncase :\ncase :\nif (VAR22[1] == VAR22[0]) {\nVAR10++;\nbreak;\n}\ndefault:\nFUN4(VAR7, VAR24,\nVAR22 - VAR2);\ngoto VAR21;\n}\nVAR14 = *VAR13 & VAR27;\nif (*VAR13 & VAR28) {\t\t\nFUN8(VAR9, VAR18 - 1, VAR14);\n*VAR13 &= ~VAR28;\n}\nif (*VAR22 == ) {\t\t\t\n*VAR13 |= VAR28;\nbreak;\n}\nif (*VAR13 & VAR29) {\t\t\nFUN8(VAR9, VAR18 - 1, !VAR14);\n*VAR13 &= ~VAR29;\n}\nif (*VAR22 == ) {\t\t\t\n*VAR13 |= VAR29;\nbreak;\n}\nif (!*VAR22)\t\t\t\t\ngoto VAR30;\nif (VAR13 == VAR12) {\nVAR15 = -1;\nFUN4(VAR7, VAR31, VAR10 - VAR2);\ngoto VAR21;\n}\nVAR13--;\t\t\t\t\t\n}\n}\nVAR30:\nif (VAR13 != VAR12) {\nFUN4(VAR7, VAR32, VAR10 - VAR2);\ngoto VAR21;\n}\nVAR9[VAR18].VAR26 = NULL;\t\t\t\t\t\nVAR9[VAR18].VAR25 = 1;\t\t\nVAR9[VAR18+1].VAR26 = NULL;\nVAR9[VAR18+1].VAR25 = 0;\t\t\nVAR9[VAR18-1].VAR25 = VAR18;\nVAR9[VAR18-1].VAR33 = false;\nfor (VAR19 = VAR18-1 ; VAR19--; ) {\nint VAR25 = VAR9[VAR19].VAR25;\nif (VAR9[VAR19].VAR33 == VAR9[VAR25].VAR33)\nVAR9[VAR19].VAR25 = VAR9[VAR25].VAR25;\n}\nfor (VAR19 = 0; VAR19 < VAR18; VAR19++) {\nVAR14 = VAR11[VAR19] ^ VAR9[VAR19].VAR33;\nVAR9[VAR19].VAR33 = VAR14;\nif (FUN9(VAR9[VAR19].VAR25 <= VAR19)) {\nVAR15 = -VAR23;\ngoto VAR21;\n}\n}\nreturn VAR9;\nVAR21:\nFUN10(VAR12);\nFUN10(VAR8);\nFUN10(VAR11);\nreturn FUN3(VAR15);\n}\n",
      "code_after_change_raw": "static struct prog_entry *\npredicate_parse(const char *str, int nr_parens, int nr_preds,\nparse_pred_fn parse_pred, void *data,\nstruct filter_parse_error *pe)\n{\nstruct prog_entry *prog_stack;\nstruct prog_entry *prog;\nconst char *ptr = str;\nchar *inverts = NULL;\nint *op_stack;\nint *top;\nint invert = 0;\nint ret = -ENOMEM;\nint len;\nint N = 0;\nint i;\nnr_preds += 2; \nop_stack = kmalloc_array(nr_parens, sizeof(*op_stack), GFP_KERNEL);\nif (!op_stack)\nreturn ERR_PTR(-ENOMEM);\nprog_stack = kmalloc_array(nr_preds, sizeof(*prog_stack), GFP_KERNEL);\nif (!prog_stack) {\nparse_error(pe, -ENOMEM, 0);\ngoto out_free;\n}\ninverts = kmalloc_array(nr_preds, sizeof(*inverts), GFP_KERNEL);\nif (!inverts) {\nparse_error(pe, -ENOMEM, 0);\ngoto out_free;\n}\ntop = op_stack;\nprog = prog_stack;\n*top = 0;\nwhile (*ptr) {\t\t\t\t\t\t\nconst char *next = ptr++;\nif (isspace(*next))\ncontinue;\nswitch (*next) {\ncase '(':\t\t\t\t\t\nif (top - op_stack > nr_parens)\nreturn ERR_PTR(-EINVAL);\n*(++top) = invert;\ncontinue;\ncase '!':\t\t\t\t\t\nif (!is_not(next))\nbreak;\ninvert = !invert;\ncontinue;\n}\nif (N >= nr_preds) {\nparse_error(pe, FILT_ERR_TOO_MANY_PREDS, next - str);\ngoto out_free;\n}\ninverts[N] = invert;\t\t\t\t\nprog[N].target = N-1;\nlen = parse_pred(next, data, ptr - str, pe, &prog[N].pred);\nif (len < 0) {\nret = len;\ngoto out_free;\n}\nptr = next + len;\nN++;\nret = -1;\nwhile (1) {\t\t\t\t\t\nnext = ptr++;\nif (isspace(*next))\ncontinue;\nswitch (*next) {\ncase ')':\ncase '\\0':\nbreak;\ncase '&':\ncase '|':\nif (next[1] == next[0]) {\nptr++;\nbreak;\n}\ndefault:\nparse_error(pe, FILT_ERR_TOO_MANY_PREDS,\nnext - str);\ngoto out_free;\n}\ninvert = *top & INVERT;\nif (*top & PROCESS_AND) {\t\t\nupdate_preds(prog, N - 1, invert);\n*top &= ~PROCESS_AND;\n}\nif (*next == '&') {\t\t\t\n*top |= PROCESS_AND;\nbreak;\n}\nif (*top & PROCESS_OR) {\t\t\nupdate_preds(prog, N - 1, !invert);\n*top &= ~PROCESS_OR;\n}\nif (*next == '|') {\t\t\t\n*top |= PROCESS_OR;\nbreak;\n}\nif (!*next)\t\t\t\t\ngoto out;\nif (top == op_stack) {\nret = -1;\nparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, ptr - str);\ngoto out_free;\n}\ntop--;\t\t\t\t\t\n}\n}\nout:\nif (top != op_stack) {\nparse_error(pe, FILT_ERR_TOO_MANY_OPEN, ptr - str);\ngoto out_free;\n}\nif (!N) {\nret = -EINVAL;\nparse_error(pe, FILT_ERR_NO_FILTER, ptr - str);\ngoto out_free;\n}\nprog[N].pred = NULL;\t\t\t\t\t\nprog[N].target = 1;\t\t\nprog[N+1].pred = NULL;\nprog[N+1].target = 0;\t\t\nprog[N-1].target = N;\nprog[N-1].when_to_branch = false;\nfor (i = N-1 ; i--; ) {\nint target = prog[i].target;\nif (prog[i].when_to_branch == prog[target].when_to_branch)\nprog[i].target = prog[target].target;\n}\nfor (i = 0; i < N; i++) {\ninvert = inverts[i] ^ prog[i].when_to_branch;\nprog[i].when_to_branch = invert;\nif (WARN_ON(prog[i].target <= i)) {\nret = -EINVAL;\ngoto out_free;\n}\n}\nreturn prog;\nout_free:\nkfree(op_stack);\nkfree(prog_stack);\nkfree(inverts);\nreturn ERR_PTR(ret);\n}\n",
      "code_before_change_raw": "static struct prog_entry *\npredicate_parse(const char *str, int nr_parens, int nr_preds,\nparse_pred_fn parse_pred, void *data,\nstruct filter_parse_error *pe)\n{\nstruct prog_entry *prog_stack;\nstruct prog_entry *prog;\nconst char *ptr = str;\nchar *inverts = NULL;\nint *op_stack;\nint *top;\nint invert = 0;\nint ret = -ENOMEM;\nint len;\nint N = 0;\nint i;\nnr_preds += 2; \nop_stack = kmalloc_array(nr_parens, sizeof(*op_stack), GFP_KERNEL);\nif (!op_stack)\nreturn ERR_PTR(-ENOMEM);\nprog_stack = kmalloc_array(nr_preds, sizeof(*prog_stack), GFP_KERNEL);\nif (!prog_stack) {\nparse_error(pe, -ENOMEM, 0);\ngoto out_free;\n}\ninverts = kmalloc_array(nr_preds, sizeof(*inverts), GFP_KERNEL);\nif (!inverts) {\nparse_error(pe, -ENOMEM, 0);\ngoto out_free;\n}\ntop = op_stack;\nprog = prog_stack;\n*top = 0;\nwhile (*ptr) {\t\t\t\t\t\t\nconst char *next = ptr++;\nif (isspace(*next))\ncontinue;\nswitch (*next) {\ncase '(':\t\t\t\t\t\nif (top - op_stack > nr_parens)\nreturn ERR_PTR(-EINVAL);\n*(++top) = invert;\ncontinue;\ncase '!':\t\t\t\t\t\nif (!is_not(next))\nbreak;\ninvert = !invert;\ncontinue;\n}\nif (N >= nr_preds) {\nparse_error(pe, FILT_ERR_TOO_MANY_PREDS, next - str);\ngoto out_free;\n}\ninverts[N] = invert;\t\t\t\t\nprog[N].target = N-1;\nlen = parse_pred(next, data, ptr - str, pe, &prog[N].pred);\nif (len < 0) {\nret = len;\ngoto out_free;\n}\nptr = next + len;\nN++;\nret = -1;\nwhile (1) {\t\t\t\t\t\nnext = ptr++;\nif (isspace(*next))\ncontinue;\nswitch (*next) {\ncase ')':\ncase '\\0':\nbreak;\ncase '&':\ncase '|':\nif (next[1] == next[0]) {\nptr++;\nbreak;\n}\ndefault:\nparse_error(pe, FILT_ERR_TOO_MANY_PREDS,\nnext - str);\ngoto out_free;\n}\ninvert = *top & INVERT;\nif (*top & PROCESS_AND) {\t\t\nupdate_preds(prog, N - 1, invert);\n*top &= ~PROCESS_AND;\n}\nif (*next == '&') {\t\t\t\n*top |= PROCESS_AND;\nbreak;\n}\nif (*top & PROCESS_OR) {\t\t\nupdate_preds(prog, N - 1, !invert);\n*top &= ~PROCESS_OR;\n}\nif (*next == '|') {\t\t\t\n*top |= PROCESS_OR;\nbreak;\n}\nif (!*next)\t\t\t\t\ngoto out;\nif (top == op_stack) {\nret = -1;\nparse_error(pe, FILT_ERR_TOO_MANY_CLOSE, ptr - str);\ngoto out_free;\n}\ntop--;\t\t\t\t\t\n}\n}\nout:\nif (top != op_stack) {\nparse_error(pe, FILT_ERR_TOO_MANY_OPEN, ptr - str);\ngoto out_free;\n}\nprog[N].pred = NULL;\t\t\t\t\t\nprog[N].target = 1;\t\t\nprog[N+1].pred = NULL;\nprog[N+1].target = 0;\t\t\nprog[N-1].target = N;\nprog[N-1].when_to_branch = false;\nfor (i = N-1 ; i--; ) {\nint target = prog[i].target;\nif (prog[i].when_to_branch == prog[target].when_to_branch)\nprog[i].target = prog[target].target;\n}\nfor (i = 0; i < N; i++) {\ninvert = inverts[i] ^ prog[i].when_to_branch;\nprog[i].when_to_branch = invert;\nif (WARN_ON(prog[i].target <= i)) {\nret = -EINVAL;\ngoto out_free;\n}\n}\nreturn prog;\nout_free:\nkfree(op_stack);\nkfree(prog_stack);\nkfree(inverts);\nreturn ERR_PTR(ret);\n}\n"
    }
  ],
  "CVE-2018-13095": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper validation of the number of extents in the inode fork during the verification process.",
        "trigger_condition": "Encountering a corrupted xfs image with an inode in extent format but having more extents than fit in the inode fork.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly validate the number of extents in the inode fork, allowing a denial of service due to memory corruption and BUG when encountering the specific condition described."
      },
      "solution": "To mitigate the vulnerability, it is necessary to enhance the verification process for inode forks by adding calls to a function that separately validates the number of extents in the data and attribute forks. This ensures that the code handles the extent format correctly and prevents memory corruption issues when encountering an inode with more extents than fit in the inode fork.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-13095, which can lead to a denial of service (memory corruption and BUG) in the Linux kernel when encountering a corrupted xfs image with an inode that is in extent format but has more extents than fit in the inode fork. By adding the lines that call the function `xfs_dinode_verify_fork` for data and attribute forks separately, the code ensures that the number of extents in the inode fork is validated correctly, preventing the memory corruption issue. This modification enhances the verification process for inode forks and helps mitigate the vulnerability by handling the extent format properly.",
      "GPT_purpose": "Verify the integrity and validity of an XFS inode structure.",
      "GPT_function": "\n1. Verify the integrity information of a v3 XFS inode.\n2. Perform various checks on the XFS inode attributes and flags.\n3. Validate extent size hint and other flags for version 3 or greater inodes.",
      "CVE_id": "CVE-2018-13095",
      "code_before_change": "xfs_failaddr_t\nxfs_dinode_verify(\n\tstruct xfs_mount\t*mp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_dinode\t*dip)\n{\n\txfs_failaddr_t\t\tfa;\n\tuint16_t\t\tmode;\n\tuint16_t\t\tflags;\n\tuint64_t\t\tflags2;\n\tuint64_t\t\tdi_size;\n\n\tif (dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC))\n\t\treturn __this_address;\n\n\t/* Verify v3 integrity information first */\n\tif (dip->di_version >= 3) {\n\t\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\t\treturn __this_address;\n\t\tif (!xfs_verify_cksum((char *)dip, mp->m_sb.sb_inodesize,\n\t\t\t\t      XFS_DINODE_CRC_OFF))\n\t\t\treturn __this_address;\n\t\tif (be64_to_cpu(dip->di_ino) != ino)\n\t\t\treturn __this_address;\n\t\tif (!uuid_equal(&dip->di_uuid, &mp->m_sb.sb_meta_uuid))\n\t\t\treturn __this_address;\n\t}\n\n\t/* don't allow invalid i_size */\n\tdi_size = be64_to_cpu(dip->di_size);\n\tif (di_size & (1ULL << 63))\n\t\treturn __this_address;\n\n\tmode = be16_to_cpu(dip->di_mode);\n\tif (mode && xfs_mode_to_ftype(mode) == XFS_DIR3_FT_UNKNOWN)\n\t\treturn __this_address;\n\n\t/* No zero-length symlinks/dirs. */\n\tif ((S_ISLNK(mode) || S_ISDIR(mode)) && di_size == 0)\n\t\treturn __this_address;\n\n\t/* Fork checks carried over from xfs_iformat_fork */\n\tif (mode &&\n\t    be32_to_cpu(dip->di_nextents) + be16_to_cpu(dip->di_anextents) >\n\t\t\tbe64_to_cpu(dip->di_nblocks))\n\t\treturn __this_address;\n\n\tif (mode && XFS_DFORK_BOFF(dip) > mp->m_sb.sb_inodesize)\n\t\treturn __this_address;\n\n\tflags = be16_to_cpu(dip->di_flags);\n\n\tif (mode && (flags & XFS_DIFLAG_REALTIME) && !mp->m_rtdev_targp)\n\t\treturn __this_address;\n\n\t/* Do we have appropriate data fork formats for the mode? */\n\tswitch (mode & S_IFMT) {\n\tcase S_IFIFO:\n\tcase S_IFCHR:\n\tcase S_IFBLK:\n\tcase S_IFSOCK:\n\t\tif (dip->di_format != XFS_DINODE_FMT_DEV)\n\t\t\treturn __this_address;\n\t\tbreak;\n\tcase S_IFREG:\n\tcase S_IFLNK:\n\tcase S_IFDIR:\n\t\tswitch (dip->di_format) {\n\t\tcase XFS_DINODE_FMT_LOCAL:\n\t\t\t/*\n\t\t\t * no local regular files yet\n\t\t\t */\n\t\t\tif (S_ISREG(mode))\n\t\t\t\treturn __this_address;\n\t\t\tif (di_size > XFS_DFORK_DSIZE(dip, mp))\n\t\t\t\treturn __this_address;\n\t\t\tif (dip->di_nextents)\n\t\t\t\treturn __this_address;\n\t\t\t/* fall through */\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\tcase XFS_DINODE_FMT_BTREE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t\tbreak;\n\tcase 0:\n\t\t/* Uninitialized inode ok. */\n\t\tbreak;\n\tdefault:\n\t\treturn __this_address;\n\t}\n\n\tif (XFS_DFORK_Q(dip)) {\n\t\tswitch (dip->di_aformat) {\n\t\tcase XFS_DINODE_FMT_LOCAL:\n\t\t\tif (dip->di_anextents)\n\t\t\t\treturn __this_address;\n\t\t/* fall through */\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\tcase XFS_DINODE_FMT_BTREE:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t} else {\n\t\t/*\n\t\t * If there is no fork offset, this may be a freshly-made inode\n\t\t * in a new disk cluster, in which case di_aformat is zeroed.\n\t\t * Otherwise, such an inode must be in EXTENTS format; this goes\n\t\t * for freed inodes as well.\n\t\t */\n\t\tswitch (dip->di_aformat) {\n\t\tcase 0:\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t\tif (dip->di_anextents)\n\t\t\treturn __this_address;\n\t}\n\n\t/* extent size hint validation */\n\tfa = xfs_inode_validate_extsize(mp, be32_to_cpu(dip->di_extsize),\n\t\t\tmode, flags);\n\tif (fa)\n\t\treturn fa;\n\n\t/* only version 3 or greater inodes are extensively verified here */\n\tif (dip->di_version < 3)\n\t\treturn NULL;\n\n\tflags2 = be64_to_cpu(dip->di_flags2);\n\n\t/* don't allow reflink/cowextsize if we don't have reflink */\n\tif ((flags2 & (XFS_DIFLAG2_REFLINK | XFS_DIFLAG2_COWEXTSIZE)) &&\n\t     !xfs_sb_version_hasreflink(&mp->m_sb))\n\t\treturn __this_address;\n\n\t/* only regular files get reflink */\n\tif ((flags2 & XFS_DIFLAG2_REFLINK) && (mode & S_IFMT) != S_IFREG)\n\t\treturn __this_address;\n\n\t/* don't let reflink and realtime mix */\n\tif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags & XFS_DIFLAG_REALTIME))\n\t\treturn __this_address;\n\n\t/* don't let reflink and dax mix */\n\tif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags2 & XFS_DIFLAG2_DAX))\n\t\treturn __this_address;\n\n\t/* COW extent size hint validation */\n\tfa = xfs_inode_validate_cowextsize(mp, be32_to_cpu(dip->di_cowextsize),\n\t\t\tmode, flags, flags2);\n\tif (fa)\n\t\treturn fa;\n\n\treturn NULL;\n}",
      "code_after_change": "xfs_failaddr_t\nxfs_dinode_verify(\n\tstruct xfs_mount\t*mp,\n\txfs_ino_t\t\tino,\n\tstruct xfs_dinode\t*dip)\n{\n\txfs_failaddr_t\t\tfa;\n\tuint16_t\t\tmode;\n\tuint16_t\t\tflags;\n\tuint64_t\t\tflags2;\n\tuint64_t\t\tdi_size;\n\n\tif (dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC))\n\t\treturn __this_address;\n\n\t/* Verify v3 integrity information first */\n\tif (dip->di_version >= 3) {\n\t\tif (!xfs_sb_version_hascrc(&mp->m_sb))\n\t\t\treturn __this_address;\n\t\tif (!xfs_verify_cksum((char *)dip, mp->m_sb.sb_inodesize,\n\t\t\t\t      XFS_DINODE_CRC_OFF))\n\t\t\treturn __this_address;\n\t\tif (be64_to_cpu(dip->di_ino) != ino)\n\t\t\treturn __this_address;\n\t\tif (!uuid_equal(&dip->di_uuid, &mp->m_sb.sb_meta_uuid))\n\t\t\treturn __this_address;\n\t}\n\n\t/* don't allow invalid i_size */\n\tdi_size = be64_to_cpu(dip->di_size);\n\tif (di_size & (1ULL << 63))\n\t\treturn __this_address;\n\n\tmode = be16_to_cpu(dip->di_mode);\n\tif (mode && xfs_mode_to_ftype(mode) == XFS_DIR3_FT_UNKNOWN)\n\t\treturn __this_address;\n\n\t/* No zero-length symlinks/dirs. */\n\tif ((S_ISLNK(mode) || S_ISDIR(mode)) && di_size == 0)\n\t\treturn __this_address;\n\n\t/* Fork checks carried over from xfs_iformat_fork */\n\tif (mode &&\n\t    be32_to_cpu(dip->di_nextents) + be16_to_cpu(dip->di_anextents) >\n\t\t\tbe64_to_cpu(dip->di_nblocks))\n\t\treturn __this_address;\n\n\tif (mode && XFS_DFORK_BOFF(dip) > mp->m_sb.sb_inodesize)\n\t\treturn __this_address;\n\n\tflags = be16_to_cpu(dip->di_flags);\n\n\tif (mode && (flags & XFS_DIFLAG_REALTIME) && !mp->m_rtdev_targp)\n\t\treturn __this_address;\n\n\t/* Do we have appropriate data fork formats for the mode? */\n\tswitch (mode & S_IFMT) {\n\tcase S_IFIFO:\n\tcase S_IFCHR:\n\tcase S_IFBLK:\n\tcase S_IFSOCK:\n\t\tif (dip->di_format != XFS_DINODE_FMT_DEV)\n\t\t\treturn __this_address;\n\t\tbreak;\n\tcase S_IFREG:\n\tcase S_IFLNK:\n\tcase S_IFDIR:\n\t\tfa = xfs_dinode_verify_fork(dip, mp, XFS_DATA_FORK);\n\t\tif (fa)\n\t\t\treturn fa;\n\t\tbreak;\n\tcase 0:\n\t\t/* Uninitialized inode ok. */\n\t\tbreak;\n\tdefault:\n\t\treturn __this_address;\n\t}\n\n\tif (XFS_DFORK_Q(dip)) {\n\t\tfa = xfs_dinode_verify_fork(dip, mp, XFS_ATTR_FORK);\n\t\tif (fa)\n\t\t\treturn fa;\n\t} else {\n\t\t/*\n\t\t * If there is no fork offset, this may be a freshly-made inode\n\t\t * in a new disk cluster, in which case di_aformat is zeroed.\n\t\t * Otherwise, such an inode must be in EXTENTS format; this goes\n\t\t * for freed inodes as well.\n\t\t */\n\t\tswitch (dip->di_aformat) {\n\t\tcase 0:\n\t\tcase XFS_DINODE_FMT_EXTENTS:\n\t\t\tbreak;\n\t\tdefault:\n\t\t\treturn __this_address;\n\t\t}\n\t\tif (dip->di_anextents)\n\t\t\treturn __this_address;\n\t}\n\n\t/* extent size hint validation */\n\tfa = xfs_inode_validate_extsize(mp, be32_to_cpu(dip->di_extsize),\n\t\t\tmode, flags);\n\tif (fa)\n\t\treturn fa;\n\n\t/* only version 3 or greater inodes are extensively verified here */\n\tif (dip->di_version < 3)\n\t\treturn NULL;\n\n\tflags2 = be64_to_cpu(dip->di_flags2);\n\n\t/* don't allow reflink/cowextsize if we don't have reflink */\n\tif ((flags2 & (XFS_DIFLAG2_REFLINK | XFS_DIFLAG2_COWEXTSIZE)) &&\n\t     !xfs_sb_version_hasreflink(&mp->m_sb))\n\t\treturn __this_address;\n\n\t/* only regular files get reflink */\n\tif ((flags2 & XFS_DIFLAG2_REFLINK) && (mode & S_IFMT) != S_IFREG)\n\t\treturn __this_address;\n\n\t/* don't let reflink and realtime mix */\n\tif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags & XFS_DIFLAG_REALTIME))\n\t\treturn __this_address;\n\n\t/* don't let reflink and dax mix */\n\tif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags2 & XFS_DIFLAG2_DAX))\n\t\treturn __this_address;\n\n\t/* COW extent size hint validation */\n\tfa = xfs_inode_validate_cowextsize(mp, be32_to_cpu(dip->di_cowextsize),\n\t\t\tmode, flags, flags2);\n\tif (fa)\n\t\treturn fa;\n\n\treturn NULL;\n}",
      "modified_lines": {
        "added": [
          "\t\tfa = xfs_dinode_verify_fork(dip, mp, XFS_DATA_FORK);",
          "\t\tif (fa)",
          "\t\t\treturn fa;",
          "\t\tfa = xfs_dinode_verify_fork(dip, mp, XFS_ATTR_FORK);",
          "\t\tif (fa)",
          "\t\t\treturn fa;"
        ],
        "deleted": [
          "\t\tswitch (dip->di_format) {",
          "\t\tcase XFS_DINODE_FMT_LOCAL:",
          "\t\t\t/*",
          "\t\t\t * no local regular files yet",
          "\t\t\t */",
          "\t\t\tif (S_ISREG(mode))",
          "\t\t\t\treturn __this_address;",
          "\t\t\tif (di_size > XFS_DFORK_DSIZE(dip, mp))",
          "\t\t\t\treturn __this_address;",
          "\t\t\tif (dip->di_nextents)",
          "\t\t\t\treturn __this_address;",
          "\t\t\t/* fall through */",
          "\t\tcase XFS_DINODE_FMT_EXTENTS:",
          "\t\tcase XFS_DINODE_FMT_BTREE:",
          "\t\t\tbreak;",
          "\t\tdefault:",
          "\t\t\treturn __this_address;",
          "\t\t}",
          "\t\tswitch (dip->di_aformat) {",
          "\t\tcase XFS_DINODE_FMT_LOCAL:",
          "\t\t\tif (dip->di_anextents)",
          "\t\t\t\treturn __this_address;",
          "\t\t/* fall through */",
          "\t\tcase XFS_DINODE_FMT_EXTENTS:",
          "\t\tcase XFS_DINODE_FMT_BTREE:",
          "\t\t\tbreak;",
          "\t\tdefault:",
          "\t\t\treturn __this_address;",
          "\t\t}"
        ]
      },
      "preconditions_for_vulnerability": "Improper validation of the number of extents in the inode fork during the verification process.",
      "trigger_condition": "Encountering a corrupted xfs image with an inode in extent format but having more extents than fit in the inode fork.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly validate the number of extents in the inode fork, allowing a denial of service due to memory corruption and BUG when encountering the specific condition described.",
      "id": 30,
      "code_after_change_normalized": "VAR1\nFUN1(\nstruct xfs_mount\t*VAR2,\nxfs_ino_t\t\tVAR3,\nstruct xfs_dinode\t*VAR4)\n{\nxfs_failaddr_t\t\tVAR5;\nuint16_t\t\tVAR6;\nuint16_t\t\tVAR7;\nuint64_t\t\tVAR8;\nuint64_t\t\tVAR9;\nif (VAR4->VAR10 != FUN2(VAR11))\nreturn VAR12;\nif (VAR4->VAR13 >= 3) {\nif (!FUN3(&VAR2->VAR14))\nreturn VAR12;\nif (!FUN4((char *)VAR4, VAR2->VAR14.VAR15,\nVAR16))\nreturn VAR12;\nif (FUN5(VAR4->VAR17) != VAR3)\nreturn VAR12;\nif (!FUN6(&VAR4->VAR18, &VAR2->VAR14.VAR19))\nreturn VAR12;\n}\nVAR9 = FUN5(VAR4->VAR9);\nif (VAR9 & (1ULL << 63))\nreturn VAR12;\nVAR6 = FUN7(VAR4->VAR20);\nif (VAR6 && FUN8(VAR6) == VAR21)\nreturn VAR12;\nif ((FUN9(VAR6) || FUN10(VAR6)) && VAR9 == 0)\nreturn VAR12;\nif (VAR6 &&\nFUN11(VAR4->VAR22) + FUN7(VAR4->VAR23) >\nFUN5(VAR4->VAR24))\nreturn VAR12;\nif (VAR6 && FUN12(VAR4) > VAR2->VAR14.VAR15)\nreturn VAR12;\nVAR7 = FUN7(VAR4->VAR25);\nif (VAR6 && (VAR7 & VAR26) && !VAR2->VAR27)\nreturn VAR12;\nswitch (VAR6 & VAR28) {\ncase VAR29:\ncase VAR30:\ncase VAR31:\ncase VAR32:\nif (VAR4->VAR33 != VAR34)\nreturn VAR12;\nbreak;\ncase VAR35:\ncase VAR36:\ncase VAR37:\nVAR5 = FUN13(VAR4, VAR2, VAR38);\nif (VAR5)\nreturn VAR5;\nbreak;\ncase 0:\nbreak;\ndefault:\nreturn VAR12;\n}\nif (FUN14(VAR4)) {\nVAR5 = FUN13(VAR4, VAR2, VAR39);\nif (VAR5)\nreturn VAR5;\n} else {\nswitch (VAR4->VAR40) {\ncase 0:\ncase VAR41:\nbreak;\ndefault:\nreturn VAR12;\n}\nif (VAR4->VAR23)\nreturn VAR12;\n}\nVAR5 = FUN15(VAR2, FUN11(VAR4->VAR42),\nVAR6, VAR7);\nif (VAR5)\nreturn VAR5;\nif (VAR4->VAR13 < 3)\nreturn NULL;\nVAR8 = FUN5(VAR4->VAR43);\nif ((VAR8 & (VAR44 | VAR45)) &&\n!FUN16(&VAR2->VAR14))\nreturn VAR12;\nif ((VAR8 & VAR44) && (VAR6 & VAR28) != VAR35)\nreturn VAR12;\nif ((VAR8 & VAR44) && (VAR7 & VAR26))\nreturn VAR12;\nif ((VAR8 & VAR44) && (VAR8 & VAR46))\nreturn VAR12;\nVAR5 = FUN17(VAR2, FUN11(VAR4->VAR47),\nVAR6, VAR7, VAR8);\nif (VAR5)\nreturn VAR5;\nreturn NULL;\n}\n",
      "code_before_change_normalized": "VAR1\nFUN1(\nstruct xfs_mount\t*VAR2,\nxfs_ino_t\t\tVAR3,\nstruct xfs_dinode\t*VAR4)\n{\nxfs_failaddr_t\t\tVAR5;\nuint16_t\t\tVAR6;\nuint16_t\t\tVAR7;\nuint64_t\t\tVAR8;\nuint64_t\t\tVAR9;\nif (VAR4->VAR10 != FUN2(VAR11))\nreturn VAR12;\nif (VAR4->VAR13 >= 3) {\nif (!FUN3(&VAR2->VAR14))\nreturn VAR12;\nif (!FUN4((char *)VAR4, VAR2->VAR14.VAR15,\nVAR16))\nreturn VAR12;\nif (FUN5(VAR4->VAR17) != VAR3)\nreturn VAR12;\nif (!FUN6(&VAR4->VAR18, &VAR2->VAR14.VAR19))\nreturn VAR12;\n}\nVAR9 = FUN5(VAR4->VAR9);\nif (VAR9 & (1ULL << 63))\nreturn VAR12;\nVAR6 = FUN7(VAR4->VAR20);\nif (VAR6 && FUN8(VAR6) == VAR21)\nreturn VAR12;\nif ((FUN9(VAR6) || FUN10(VAR6)) && VAR9 == 0)\nreturn VAR12;\nif (VAR6 &&\nFUN11(VAR4->VAR22) + FUN7(VAR4->VAR23) >\nFUN5(VAR4->VAR24))\nreturn VAR12;\nif (VAR6 && FUN12(VAR4) > VAR2->VAR14.VAR15)\nreturn VAR12;\nVAR7 = FUN7(VAR4->VAR25);\nif (VAR6 && (VAR7 & VAR26) && !VAR2->VAR27)\nreturn VAR12;\nswitch (VAR6 & VAR28) {\ncase VAR29:\ncase VAR30:\ncase VAR31:\ncase VAR32:\nif (VAR4->VAR33 != VAR34)\nreturn VAR12;\nbreak;\ncase VAR35:\ncase VAR36:\ncase VAR37:\nswitch (VAR4->VAR33) {\ncase VAR38:\nif (FUN13(VAR6))\nreturn VAR12;\nif (VAR9 > FUN14(VAR4, VAR2))\nreturn VAR12;\nif (VAR4->VAR22)\nreturn VAR12;\ncase VAR39:\ncase VAR40:\nbreak;\ndefault:\nreturn VAR12;\n}\nbreak;\ncase 0:\nbreak;\ndefault:\nreturn VAR12;\n}\nif (FUN15(VAR4)) {\nswitch (VAR4->VAR41) {\ncase VAR38:\nif (VAR4->VAR23)\nreturn VAR12;\ncase VAR39:\ncase VAR40:\nbreak;\ndefault:\nreturn VAR12;\n}\n} else {\nswitch (VAR4->VAR41) {\ncase 0:\ncase VAR39:\nbreak;\ndefault:\nreturn VAR12;\n}\nif (VAR4->VAR23)\nreturn VAR12;\n}\nVAR5 = FUN16(VAR2, FUN11(VAR4->VAR42),\nVAR6, VAR7);\nif (VAR5)\nreturn VAR5;\nif (VAR4->VAR13 < 3)\nreturn NULL;\nVAR8 = FUN5(VAR4->VAR43);\nif ((VAR8 & (VAR44 | VAR45)) &&\n!FUN17(&VAR2->VAR14))\nreturn VAR12;\nif ((VAR8 & VAR44) && (VAR6 & VAR28) != VAR35)\nreturn VAR12;\nif ((VAR8 & VAR44) && (VAR7 & VAR26))\nreturn VAR12;\nif ((VAR8 & VAR44) && (VAR8 & VAR46))\nreturn VAR12;\nVAR5 = FUN18(VAR2, FUN11(VAR4->VAR47),\nVAR6, VAR7, VAR8);\nif (VAR5)\nreturn VAR5;\nreturn NULL;\n}\n",
      "code_after_change_raw": "xfs_failaddr_t\nxfs_dinode_verify(\nstruct xfs_mount\t*mp,\nxfs_ino_t\t\tino,\nstruct xfs_dinode\t*dip)\n{\nxfs_failaddr_t\t\tfa;\nuint16_t\t\tmode;\nuint16_t\t\tflags;\nuint64_t\t\tflags2;\nuint64_t\t\tdi_size;\nif (dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC))\nreturn __this_address;\nif (dip->di_version >= 3) {\nif (!xfs_sb_version_hascrc(&mp->m_sb))\nreturn __this_address;\nif (!xfs_verify_cksum((char *)dip, mp->m_sb.sb_inodesize,\nXFS_DINODE_CRC_OFF))\nreturn __this_address;\nif (be64_to_cpu(dip->di_ino) != ino)\nreturn __this_address;\nif (!uuid_equal(&dip->di_uuid, &mp->m_sb.sb_meta_uuid))\nreturn __this_address;\n}\ndi_size = be64_to_cpu(dip->di_size);\nif (di_size & (1ULL << 63))\nreturn __this_address;\nmode = be16_to_cpu(dip->di_mode);\nif (mode && xfs_mode_to_ftype(mode) == XFS_DIR3_FT_UNKNOWN)\nreturn __this_address;\nif ((S_ISLNK(mode) || S_ISDIR(mode)) && di_size == 0)\nreturn __this_address;\nif (mode &&\nbe32_to_cpu(dip->di_nextents) + be16_to_cpu(dip->di_anextents) >\nbe64_to_cpu(dip->di_nblocks))\nreturn __this_address;\nif (mode && XFS_DFORK_BOFF(dip) > mp->m_sb.sb_inodesize)\nreturn __this_address;\nflags = be16_to_cpu(dip->di_flags);\nif (mode && (flags & XFS_DIFLAG_REALTIME) && !mp->m_rtdev_targp)\nreturn __this_address;\nswitch (mode & S_IFMT) {\ncase S_IFIFO:\ncase S_IFCHR:\ncase S_IFBLK:\ncase S_IFSOCK:\nif (dip->di_format != XFS_DINODE_FMT_DEV)\nreturn __this_address;\nbreak;\ncase S_IFREG:\ncase S_IFLNK:\ncase S_IFDIR:\nfa = xfs_dinode_verify_fork(dip, mp, XFS_DATA_FORK);\nif (fa)\nreturn fa;\nbreak;\ncase 0:\nbreak;\ndefault:\nreturn __this_address;\n}\nif (XFS_DFORK_Q(dip)) {\nfa = xfs_dinode_verify_fork(dip, mp, XFS_ATTR_FORK);\nif (fa)\nreturn fa;\n} else {\nswitch (dip->di_aformat) {\ncase 0:\ncase XFS_DINODE_FMT_EXTENTS:\nbreak;\ndefault:\nreturn __this_address;\n}\nif (dip->di_anextents)\nreturn __this_address;\n}\nfa = xfs_inode_validate_extsize(mp, be32_to_cpu(dip->di_extsize),\nmode, flags);\nif (fa)\nreturn fa;\nif (dip->di_version < 3)\nreturn NULL;\nflags2 = be64_to_cpu(dip->di_flags2);\nif ((flags2 & (XFS_DIFLAG2_REFLINK | XFS_DIFLAG2_COWEXTSIZE)) &&\n!xfs_sb_version_hasreflink(&mp->m_sb))\nreturn __this_address;\nif ((flags2 & XFS_DIFLAG2_REFLINK) && (mode & S_IFMT) != S_IFREG)\nreturn __this_address;\nif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags & XFS_DIFLAG_REALTIME))\nreturn __this_address;\nif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags2 & XFS_DIFLAG2_DAX))\nreturn __this_address;\nfa = xfs_inode_validate_cowextsize(mp, be32_to_cpu(dip->di_cowextsize),\nmode, flags, flags2);\nif (fa)\nreturn fa;\nreturn NULL;\n}\n",
      "code_before_change_raw": "xfs_failaddr_t\nxfs_dinode_verify(\nstruct xfs_mount\t*mp,\nxfs_ino_t\t\tino,\nstruct xfs_dinode\t*dip)\n{\nxfs_failaddr_t\t\tfa;\nuint16_t\t\tmode;\nuint16_t\t\tflags;\nuint64_t\t\tflags2;\nuint64_t\t\tdi_size;\nif (dip->di_magic != cpu_to_be16(XFS_DINODE_MAGIC))\nreturn __this_address;\nif (dip->di_version >= 3) {\nif (!xfs_sb_version_hascrc(&mp->m_sb))\nreturn __this_address;\nif (!xfs_verify_cksum((char *)dip, mp->m_sb.sb_inodesize,\nXFS_DINODE_CRC_OFF))\nreturn __this_address;\nif (be64_to_cpu(dip->di_ino) != ino)\nreturn __this_address;\nif (!uuid_equal(&dip->di_uuid, &mp->m_sb.sb_meta_uuid))\nreturn __this_address;\n}\ndi_size = be64_to_cpu(dip->di_size);\nif (di_size & (1ULL << 63))\nreturn __this_address;\nmode = be16_to_cpu(dip->di_mode);\nif (mode && xfs_mode_to_ftype(mode) == XFS_DIR3_FT_UNKNOWN)\nreturn __this_address;\nif ((S_ISLNK(mode) || S_ISDIR(mode)) && di_size == 0)\nreturn __this_address;\nif (mode &&\nbe32_to_cpu(dip->di_nextents) + be16_to_cpu(dip->di_anextents) >\nbe64_to_cpu(dip->di_nblocks))\nreturn __this_address;\nif (mode && XFS_DFORK_BOFF(dip) > mp->m_sb.sb_inodesize)\nreturn __this_address;\nflags = be16_to_cpu(dip->di_flags);\nif (mode && (flags & XFS_DIFLAG_REALTIME) && !mp->m_rtdev_targp)\nreturn __this_address;\nswitch (mode & S_IFMT) {\ncase S_IFIFO:\ncase S_IFCHR:\ncase S_IFBLK:\ncase S_IFSOCK:\nif (dip->di_format != XFS_DINODE_FMT_DEV)\nreturn __this_address;\nbreak;\ncase S_IFREG:\ncase S_IFLNK:\ncase S_IFDIR:\nswitch (dip->di_format) {\ncase XFS_DINODE_FMT_LOCAL:\nif (S_ISREG(mode))\nreturn __this_address;\nif (di_size > XFS_DFORK_DSIZE(dip, mp))\nreturn __this_address;\nif (dip->di_nextents)\nreturn __this_address;\ncase XFS_DINODE_FMT_EXTENTS:\ncase XFS_DINODE_FMT_BTREE:\nbreak;\ndefault:\nreturn __this_address;\n}\nbreak;\ncase 0:\nbreak;\ndefault:\nreturn __this_address;\n}\nif (XFS_DFORK_Q(dip)) {\nswitch (dip->di_aformat) {\ncase XFS_DINODE_FMT_LOCAL:\nif (dip->di_anextents)\nreturn __this_address;\ncase XFS_DINODE_FMT_EXTENTS:\ncase XFS_DINODE_FMT_BTREE:\nbreak;\ndefault:\nreturn __this_address;\n}\n} else {\nswitch (dip->di_aformat) {\ncase 0:\ncase XFS_DINODE_FMT_EXTENTS:\nbreak;\ndefault:\nreturn __this_address;\n}\nif (dip->di_anextents)\nreturn __this_address;\n}\nfa = xfs_inode_validate_extsize(mp, be32_to_cpu(dip->di_extsize),\nmode, flags);\nif (fa)\nreturn fa;\nif (dip->di_version < 3)\nreturn NULL;\nflags2 = be64_to_cpu(dip->di_flags2);\nif ((flags2 & (XFS_DIFLAG2_REFLINK | XFS_DIFLAG2_COWEXTSIZE)) &&\n!xfs_sb_version_hasreflink(&mp->m_sb))\nreturn __this_address;\nif ((flags2 & XFS_DIFLAG2_REFLINK) && (mode & S_IFMT) != S_IFREG)\nreturn __this_address;\nif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags & XFS_DIFLAG_REALTIME))\nreturn __this_address;\nif ((flags2 & XFS_DIFLAG2_REFLINK) && (flags2 & XFS_DIFLAG2_DAX))\nreturn __this_address;\nfa = xfs_inode_validate_cowextsize(mp, be32_to_cpu(dip->di_cowextsize),\nmode, flags, flags2);\nif (fa)\nreturn fa;\nreturn NULL;\n}\n"
    }
  ],
  "CVE-2018-14610": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of verification that each block group has a corresponding chunk at mount time in the btrfs filesystem.",
        "trigger_condition": "Mounting and operating a crafted btrfs image without proper validation of block group and chunk mappings.",
        "specific_code_behavior_causing_vulnerability": "The code does not verify that each block group has a corresponding chunk during mount time, leading to potential out-of-bounds access in write_extent_buffer(). This lack of validation can result in mismatched block group and chunk mappings, creating a security vulnerability.",
        "specific_solution_to_fix_vulnerability": "To mitigate the vulnerability, it is necessary to add additional checks to ensure consistency between block group and chunk mappings. Specifically, the code should verify that the start and length of the block group match the chunk, and compare the type flags of the block group with the chunk type flags. By performing these validations, the code can prevent out-of-bounds access and ensure that block group and chunk mappings are consistent, thereby enhancing the security of the btrfs filesystem."
      },
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2018-14610, which involves an out-of-bounds access issue in the Linux kernel's btrfs filesystem. The vulnerability occurs due to a lack of verification that each block group has a corresponding chunk at mount time, within the btrfs_read_block_groups function in fs/btrfs/extent-tree.c.\n\nThe modified code snippet addresses this vulnerability by adding the following lines:\n```c\nstruct btrfs_block_group_item bg;\nu64 flags;\n```\nThese lines introduce a new structure `btrfs_block_group_item` and a variable `flags` to store the flags associated with the block group. Additionally, the modified code includes checks to ensure that the block group and chunk mappings are consistent. Specifically, it verifies that the start and length of the block group match the chunk, and it compares the type flags of the block group with the chunk type flags.\n\nBy performing these additional checks and validations, the modified code ensures that there is proper verification of the block group and chunk mappings, thereby preventing the out-of-bounds access issue and enhancing the security of the btrfs filesystem.",
      "GPT_purpose": "Find the first block group within a Btrfs filesystem based on a given key.",
      "GPT_function": "\n1. Find the first block group in a Btrfs file system.\n2. Check if each block group has a corresponding chunk during mount time.\n3. Handle out-of-bounds access in write_extent_buffer().",
      "CVE_id": "CVE-2018-14610",
      "code_before_change": "static int find_first_block_group(struct btrfs_fs_info *fs_info,\n\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t  struct btrfs_key *key)\n{\n\tstruct btrfs_root *root = fs_info->extent_root;\n\tint ret = 0;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tint slot;\n\n\tret = btrfs_search_slot(NULL, root, key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twhile (1) {\n\t\tslot = path->slots[0];\n\t\tleaf = path->nodes[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid >= key->objectid &&\n\t\t    found_key.type == BTRFS_BLOCK_GROUP_ITEM_KEY) {\n\t\t\tstruct extent_map_tree *em_tree;\n\t\t\tstruct extent_map *em;\n\n\t\t\tem_tree = &root->fs_info->mapping_tree.map_tree;\n\t\t\tread_lock(&em_tree->lock);\n\t\t\tem = lookup_extent_mapping(em_tree, found_key.objectid,\n\t\t\t\t\t\t   found_key.offset);\n\t\t\tread_unlock(&em_tree->lock);\n\t\t\tif (!em) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\"logical %llu len %llu found bg but no related chunk\",\n\t\t\t\t\t  found_key.objectid, found_key.offset);\n\t\t\t\tret = -ENOENT;\n\t\t\t} else {\n\t\t\t\tret = 0;\n\t\t\t}\n\t\t\tfree_extent_map(em);\n\t\t\tgoto out;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\nout:\n\treturn ret;\n}",
      "code_after_change": "static int find_first_block_group(struct btrfs_fs_info *fs_info,\n\t\t\t\t  struct btrfs_path *path,\n\t\t\t\t  struct btrfs_key *key)\n{\n\tstruct btrfs_root *root = fs_info->extent_root;\n\tint ret = 0;\n\tstruct btrfs_key found_key;\n\tstruct extent_buffer *leaf;\n\tstruct btrfs_block_group_item bg;\n\tu64 flags;\n\tint slot;\n\n\tret = btrfs_search_slot(NULL, root, key, path, 0, 0);\n\tif (ret < 0)\n\t\tgoto out;\n\n\twhile (1) {\n\t\tslot = path->slots[0];\n\t\tleaf = path->nodes[0];\n\t\tif (slot >= btrfs_header_nritems(leaf)) {\n\t\t\tret = btrfs_next_leaf(root, path);\n\t\t\tif (ret == 0)\n\t\t\t\tcontinue;\n\t\t\tif (ret < 0)\n\t\t\t\tgoto out;\n\t\t\tbreak;\n\t\t}\n\t\tbtrfs_item_key_to_cpu(leaf, &found_key, slot);\n\n\t\tif (found_key.objectid >= key->objectid &&\n\t\t    found_key.type == BTRFS_BLOCK_GROUP_ITEM_KEY) {\n\t\t\tstruct extent_map_tree *em_tree;\n\t\t\tstruct extent_map *em;\n\n\t\t\tem_tree = &root->fs_info->mapping_tree.map_tree;\n\t\t\tread_lock(&em_tree->lock);\n\t\t\tem = lookup_extent_mapping(em_tree, found_key.objectid,\n\t\t\t\t\t\t   found_key.offset);\n\t\t\tread_unlock(&em_tree->lock);\n\t\t\tif (!em) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\t\"logical %llu len %llu found bg but no related chunk\",\n\t\t\t\t\t  found_key.objectid, found_key.offset);\n\t\t\t\tret = -ENOENT;\n\t\t\t} else if (em->start != found_key.objectid ||\n\t\t\t\t   em->len != found_key.offset) {\n\t\t\t\tbtrfs_err(fs_info,\n\t\t\"block group %llu len %llu mismatch with chunk %llu len %llu\",\n\t\t\t\t\t  found_key.objectid, found_key.offset,\n\t\t\t\t\t  em->start, em->len);\n\t\t\t\tret = -EUCLEAN;\n\t\t\t} else {\n\t\t\t\tread_extent_buffer(leaf, &bg,\n\t\t\t\t\tbtrfs_item_ptr_offset(leaf, slot),\n\t\t\t\t\tsizeof(bg));\n\t\t\t\tflags = btrfs_block_group_flags(&bg) &\n\t\t\t\t\tBTRFS_BLOCK_GROUP_TYPE_MASK;\n\n\t\t\t\tif (flags != (em->map_lookup->type &\n\t\t\t\t\t      BTRFS_BLOCK_GROUP_TYPE_MASK)) {\n\t\t\t\t\tbtrfs_err(fs_info,\n\"block group %llu len %llu type flags 0x%llx mismatch with chunk type flags 0x%llx\",\n\t\t\t\t\t\tfound_key.objectid,\n\t\t\t\t\t\tfound_key.offset, flags,\n\t\t\t\t\t\t(BTRFS_BLOCK_GROUP_TYPE_MASK &\n\t\t\t\t\t\t em->map_lookup->type));\n\t\t\t\t\tret = -EUCLEAN;\n\t\t\t\t} else {\n\t\t\t\t\tret = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t\tfree_extent_map(em);\n\t\t\tgoto out;\n\t\t}\n\t\tpath->slots[0]++;\n\t}\nout:\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tstruct btrfs_block_group_item bg;",
          "\tu64 flags;",
          "\t\t\t} else if (em->start != found_key.objectid ||",
          "\t\t\t\t   em->len != found_key.offset) {",
          "\t\t\t\tbtrfs_err(fs_info,",
          "\t\t\"block group %llu len %llu mismatch with chunk %llu len %llu\",",
          "\t\t\t\t\t  found_key.objectid, found_key.offset,",
          "\t\t\t\t\t  em->start, em->len);",
          "\t\t\t\tret = -EUCLEAN;",
          "\t\t\t\tread_extent_buffer(leaf, &bg,",
          "\t\t\t\t\tbtrfs_item_ptr_offset(leaf, slot),",
          "\t\t\t\t\tsizeof(bg));",
          "\t\t\t\tflags = btrfs_block_group_flags(&bg) &",
          "\t\t\t\t\tBTRFS_BLOCK_GROUP_TYPE_MASK;",
          "",
          "\t\t\t\tif (flags != (em->map_lookup->type &",
          "\t\t\t\t\t      BTRFS_BLOCK_GROUP_TYPE_MASK)) {",
          "\t\t\t\t\tbtrfs_err(fs_info,",
          "\"block group %llu len %llu type flags 0x%llx mismatch with chunk type flags 0x%llx\",",
          "\t\t\t\t\t\tfound_key.objectid,",
          "\t\t\t\t\t\tfound_key.offset, flags,",
          "\t\t\t\t\t\t(BTRFS_BLOCK_GROUP_TYPE_MASK &",
          "\t\t\t\t\t\t em->map_lookup->type));",
          "\t\t\t\t\tret = -EUCLEAN;",
          "\t\t\t\t} else {",
          "\t\t\t\t\tret = 0;",
          "\t\t\t\t}"
        ],
        "deleted": [
          "\t\t\t\tret = 0;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of verification that each block group has a corresponding chunk at mount time in the btrfs filesystem.",
      "trigger_condition": "Mounting and operating a crafted btrfs image without proper validation of block group and chunk mappings.",
      "specific_code_behavior_causing_vulnerability": "The code does not verify that each block group has a corresponding chunk during mount time, leading to potential out-of-bounds access in write_extent_buffer(). This lack of validation can result in mismatched block group and chunk mappings, creating a security vulnerability.",
      "id": 31,
      "code_after_change_normalized": "static int FUN1(struct btrfs_fs_info *VAR1,\nstruct btrfs_path *VAR2,\nstruct btrfs_key *VAR3)\n{\nstruct btrfs_root *VAR4 = VAR1->VAR5;\nint VAR6 = 0;\nstruct btrfs_key VAR7;\nstruct extent_buffer *VAR8;\nstruct btrfs_block_group_item VAR9;\nu64 VAR10;\nint VAR11;\nVAR6 = FUN2(NULL, VAR4, VAR3, VAR2, 0, 0);\nif (VAR6 < 0)\ngoto VAR12;\nwhile (1) {\nVAR11 = VAR2->VAR13[0];\nVAR8 = VAR2->VAR14[0];\nif (VAR11 >= FUN3(VAR8)) {\nVAR6 = FUN4(VAR4, VAR2);\nif (VAR6 == 0)\ncontinue;\nif (VAR6 < 0)\ngoto VAR12;\nbreak;\n}\nFUN5(VAR8, &VAR7, VAR11);\nif (VAR7.VAR15 >= VAR3->VAR15 &&\nVAR7.VAR16 == VAR17) {\nstruct extent_map_tree *VAR18;\nstruct extent_map *VAR19;\nVAR18 = &VAR4->VAR1->VAR20.VAR21;\nFUN6(&VAR18->VAR22);\nVAR19 = FUN7(VAR18, VAR7.VAR15,\nVAR7.VAR23);\nFUN8(&VAR18->VAR22);\nif (!VAR19) {\nFUN9(VAR1,\n\"STR\",\nVAR7.VAR15, VAR7.VAR23);\nVAR6 = -VAR24;\n} else if (VAR19->VAR25 != VAR7.VAR15 ||\nVAR19->VAR26 != VAR7.VAR23) {\nFUN9(VAR1,\n\"STR\",\nVAR7.VAR15, VAR7.VAR23,\nVAR19->VAR25, VAR19->VAR26);\nVAR6 = -VAR27;\n} else {\nFUN10(VAR8, &VAR9,\nFUN11(VAR8, VAR11),\nsizeof(VAR9));\nVAR10 = FUN12(&VAR9) &\nVAR28;\nif (VAR10 != (VAR19->VAR29->VAR16 &\nVAR28)) {\nFUN9(VAR1,\n\"STR\",\nVAR7.VAR15,\nVAR7.VAR23, VAR10,\n(VAR28 &\nVAR19->VAR29->VAR16));\nVAR6 = -VAR27;\n} else {\nVAR6 = 0;\n}\n}\nFUN13(VAR19);\ngoto VAR12;\n}\nVAR2->VAR13[0]++;\n}\nVAR12:\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct btrfs_fs_info *VAR1,\nstruct btrfs_path *VAR2,\nstruct btrfs_key *VAR3)\n{\nstruct btrfs_root *VAR4 = VAR1->VAR5;\nint VAR6 = 0;\nstruct btrfs_key VAR7;\nstruct extent_buffer *VAR8;\nint VAR9;\nVAR6 = FUN2(NULL, VAR4, VAR3, VAR2, 0, 0);\nif (VAR6 < 0)\ngoto VAR10;\nwhile (1) {\nVAR9 = VAR2->VAR11[0];\nVAR8 = VAR2->VAR12[0];\nif (VAR9 >= FUN3(VAR8)) {\nVAR6 = FUN4(VAR4, VAR2);\nif (VAR6 == 0)\ncontinue;\nif (VAR6 < 0)\ngoto VAR10;\nbreak;\n}\nFUN5(VAR8, &VAR7, VAR9);\nif (VAR7.VAR13 >= VAR3->VAR13 &&\nVAR7.VAR14 == VAR15) {\nstruct extent_map_tree *VAR16;\nstruct extent_map *VAR17;\nVAR16 = &VAR4->VAR1->VAR18.VAR19;\nFUN6(&VAR16->VAR20);\nVAR17 = FUN7(VAR16, VAR7.VAR13,\nVAR7.VAR21);\nFUN8(&VAR16->VAR20);\nif (!VAR17) {\nFUN9(VAR1,\n\"STR\",\nVAR7.VAR13, VAR7.VAR21);\nVAR6 = -VAR22;\n} else {\nVAR6 = 0;\n}\nFUN10(VAR17);\ngoto VAR10;\n}\nVAR2->VAR11[0]++;\n}\nVAR10:\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static int find_first_block_group(struct btrfs_fs_info *fs_info,\nstruct btrfs_path *path,\nstruct btrfs_key *key)\n{\nstruct btrfs_root *root = fs_info->extent_root;\nint ret = 0;\nstruct btrfs_key found_key;\nstruct extent_buffer *leaf;\nstruct btrfs_block_group_item bg;\nu64 flags;\nint slot;\nret = btrfs_search_slot(NULL, root, key, path, 0, 0);\nif (ret < 0)\ngoto out;\nwhile (1) {\nslot = path->slots[0];\nleaf = path->nodes[0];\nif (slot >= btrfs_header_nritems(leaf)) {\nret = btrfs_next_leaf(root, path);\nif (ret == 0)\ncontinue;\nif (ret < 0)\ngoto out;\nbreak;\n}\nbtrfs_item_key_to_cpu(leaf, &found_key, slot);\nif (found_key.objectid >= key->objectid &&\nfound_key.type == BTRFS_BLOCK_GROUP_ITEM_KEY) {\nstruct extent_map_tree *em_tree;\nstruct extent_map *em;\nem_tree = &root->fs_info->mapping_tree.map_tree;\nread_lock(&em_tree->lock);\nem = lookup_extent_mapping(em_tree, found_key.objectid,\nfound_key.offset);\nread_unlock(&em_tree->lock);\nif (!em) {\nbtrfs_err(fs_info,\n\"logical %llu len %llu found bg but no related chunk\",\nfound_key.objectid, found_key.offset);\nret = -ENOENT;\n} else if (em->start != found_key.objectid ||\nem->len != found_key.offset) {\nbtrfs_err(fs_info,\n\"block group %llu len %llu mismatch with chunk %llu len %llu\",\nfound_key.objectid, found_key.offset,\nem->start, em->len);\nret = -EUCLEAN;\n} else {\nread_extent_buffer(leaf, &bg,\nbtrfs_item_ptr_offset(leaf, slot),\nsizeof(bg));\nflags = btrfs_block_group_flags(&bg) &\nBTRFS_BLOCK_GROUP_TYPE_MASK;\nif (flags != (em->map_lookup->type &\nBTRFS_BLOCK_GROUP_TYPE_MASK)) {\nbtrfs_err(fs_info,\n\"block group %llu len %llu type flags 0x%llx mismatch with chunk type flags 0x%llx\",\nfound_key.objectid,\nfound_key.offset, flags,\n(BTRFS_BLOCK_GROUP_TYPE_MASK &\nem->map_lookup->type));\nret = -EUCLEAN;\n} else {\nret = 0;\n}\n}\nfree_extent_map(em);\ngoto out;\n}\npath->slots[0]++;\n}\nout:\nreturn ret;\n}\n",
      "code_before_change_raw": "static int find_first_block_group(struct btrfs_fs_info *fs_info,\nstruct btrfs_path *path,\nstruct btrfs_key *key)\n{\nstruct btrfs_root *root = fs_info->extent_root;\nint ret = 0;\nstruct btrfs_key found_key;\nstruct extent_buffer *leaf;\nint slot;\nret = btrfs_search_slot(NULL, root, key, path, 0, 0);\nif (ret < 0)\ngoto out;\nwhile (1) {\nslot = path->slots[0];\nleaf = path->nodes[0];\nif (slot >= btrfs_header_nritems(leaf)) {\nret = btrfs_next_leaf(root, path);\nif (ret == 0)\ncontinue;\nif (ret < 0)\ngoto out;\nbreak;\n}\nbtrfs_item_key_to_cpu(leaf, &found_key, slot);\nif (found_key.objectid >= key->objectid &&\nfound_key.type == BTRFS_BLOCK_GROUP_ITEM_KEY) {\nstruct extent_map_tree *em_tree;\nstruct extent_map *em;\nem_tree = &root->fs_info->mapping_tree.map_tree;\nread_lock(&em_tree->lock);\nem = lookup_extent_mapping(em_tree, found_key.objectid,\nfound_key.offset);\nread_unlock(&em_tree->lock);\nif (!em) {\nbtrfs_err(fs_info,\n\"logical %llu len %llu found bg but no related chunk\",\nfound_key.objectid, found_key.offset);\nret = -ENOENT;\n} else {\nret = 0;\n}\nfree_extent_map(em);\ngoto out;\n}\npath->slots[0]++;\n}\nout:\nreturn ret;\n}\n"
    }
  ],
  "CVE-2018-16276": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking and copy operations in the read function of the USB driver.",
        "trigger_condition": "Local attackers could exploit user access read/writes with incorrect bounds checking to crash the kernel or potentially escalate privileges.",
        "specific_code_behavior_causing_vulnerability": "The original code snippet in the read function does not perform proper bounds checking when copying data from the in_buffer to the user space buffer, potentially leading to buffer overflows or out-of-bounds accesses."
      },
      "solution": "To mitigate the vulnerability, the code should be modified to use the simple_read_from_buffer function for copying data from the in_buffer to the user space buffer. This function provides proper bounds checking and helps prevent buffer overflows or out-of-bounds accesses. Additionally, the modification ensures that the mutex is properly unlocked before returning from the function in case the device interface is already disconnected, preventing potential deadlocks or resource leaks.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2018-16276 in the yurex_read function of the yurex USB driver. The vulnerability arises from incorrect bounds checking in the original code snippet, which could be exploited by local attackers to crash the kernel or potentially escalate privileges.\n\nThe modification addresses the vulnerability by replacing the vulnerable bounds checking and copy operations with a safer alternative provided by the simple_read_from_buffer function. This function handles the copying of data from the in_buffer to the user space buffer with proper bounds checking, thus preventing potential buffer overflows or out-of-bounds accesses that could be exploited by attackers.\n\nAdditionally, the modification ensures that the mutex is properly unlocked before returning from the function in case the device interface is already disconnected, preventing potential deadlocks or resource leaks.\n\nBy making these changes, the code is more secure and less susceptible to exploitation, thereby improving the overall security of the yurex USB driver.",
      "GPT_purpose": "Read data from a USB device and copy it to a user buffer with potential vulnerability to incorrect bounds checking.",
      "GPT_function": "\n1. Read data from a USB device using the yurex USB driver.\n2. Check if the device is connected.\n3. Format data to be read and copy it to the user buffer with potential incorrect bounds checking.",
      "CVE_id": "CVE-2018-16276",
      "code_before_change": "static ssize_t yurex_read(struct file *file, char __user *buffer, size_t count,\n\t\t\t  loff_t *ppos)\n{\n\tstruct usb_yurex *dev;\n\tint retval = 0;\n\tint bytes_read = 0;\n\tchar in_buffer[20];\n\tunsigned long flags;\n\n\tdev = file->private_data;\n\n\tmutex_lock(&dev->io_mutex);\n\tif (!dev->interface) {\t\t/* already disconnected */\n\t\tretval = -ENODEV;\n\t\tgoto exit;\n\t}\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tbytes_read = snprintf(in_buffer, 20, \"%lld\\n\", dev->bbu);\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\n\tif (*ppos < bytes_read) {\n\t\tif (copy_to_user(buffer, in_buffer + *ppos, bytes_read - *ppos))\n\t\t\tretval = -EFAULT;\n\t\telse {\n\t\t\tretval = bytes_read - *ppos;\n\t\t\t*ppos += bytes_read;\n\t\t}\n\t}\n\nexit:\n\tmutex_unlock(&dev->io_mutex);\n\treturn retval;\n}",
      "code_after_change": "static ssize_t yurex_read(struct file *file, char __user *buffer, size_t count,\n\t\t\t  loff_t *ppos)\n{\n\tstruct usb_yurex *dev;\n\tint len = 0;\n\tchar in_buffer[20];\n\tunsigned long flags;\n\n\tdev = file->private_data;\n\n\tmutex_lock(&dev->io_mutex);\n\tif (!dev->interface) {\t\t/* already disconnected */\n\t\tmutex_unlock(&dev->io_mutex);\n\t\treturn -ENODEV;\n\t}\n\n\tspin_lock_irqsave(&dev->lock, flags);\n\tlen = snprintf(in_buffer, 20, \"%lld\\n\", dev->bbu);\n\tspin_unlock_irqrestore(&dev->lock, flags);\n\tmutex_unlock(&dev->io_mutex);\n\n\treturn simple_read_from_buffer(buffer, count, ppos, in_buffer, len);\n}",
      "modified_lines": {
        "added": [
          "\tint len = 0;",
          "\t\tmutex_unlock(&dev->io_mutex);",
          "\t\treturn -ENODEV;",
          "\tlen = snprintf(in_buffer, 20, \"%lld\\n\", dev->bbu);",
          "\tmutex_unlock(&dev->io_mutex);",
          "\treturn simple_read_from_buffer(buffer, count, ppos, in_buffer, len);"
        ],
        "deleted": [
          "\tint retval = 0;",
          "\tint bytes_read = 0;",
          "\t\tretval = -ENODEV;",
          "\t\tgoto exit;",
          "\tbytes_read = snprintf(in_buffer, 20, \"%lld\\n\", dev->bbu);",
          "\tif (*ppos < bytes_read) {",
          "\t\tif (copy_to_user(buffer, in_buffer + *ppos, bytes_read - *ppos))",
          "\t\t\tretval = -EFAULT;",
          "\t\telse {",
          "\t\t\tretval = bytes_read - *ppos;",
          "\t\t\t*ppos += bytes_read;",
          "\t\t}",
          "\t}",
          "",
          "exit:",
          "\tmutex_unlock(&dev->io_mutex);",
          "\treturn retval;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking and copy operations in the read function of the USB driver.",
      "trigger_condition": "Local attackers could exploit user access read/writes with incorrect bounds checking to crash the kernel or potentially escalate privileges.",
      "specific_code_behavior_causing_vulnerability": "The original code snippet in the read function does not perform proper bounds checking when copying data from the in_buffer to the user space buffer, potentially leading to buffer overflows or out-of-bounds accesses.",
      "id": 32,
      "code_after_change_normalized": "static ssize_t FUN1(struct VAR1 *VAR1, char __user *VAR2, size_t VAR3,\nloff_t *VAR4)\n{\nstruct usb_yurex *VAR5;\nint VAR6 = 0;\nchar VAR7[20];\nunsigned long VAR8;\nVAR5 = VAR1->VAR9;\nFUN2(&VAR5->VAR10);\nif (!VAR5->VAR11) {\t\t\nFUN3(&VAR5->VAR10);\nreturn -VAR12;\n}\nFUN4(&VAR5->VAR13, VAR8);\nVAR6 = FUN5(VAR7, 20, \"STR\", VAR5->VAR14);\nFUN6(&VAR5->VAR13, VAR8);\nFUN3(&VAR5->VAR10);\nreturn FUN7(VAR2, VAR3, VAR4, VAR7, VAR6);\n}\n",
      "code_before_change_normalized": "static ssize_t FUN1(struct VAR1 *VAR1, char __user *VAR2, size_t VAR3,\nloff_t *VAR4)\n{\nstruct usb_yurex *VAR5;\nint VAR6 = 0;\nint VAR7 = 0;\nchar VAR8[20];\nunsigned long VAR9;\nVAR5 = VAR1->VAR10;\nFUN2(&VAR5->VAR11);\nif (!VAR5->VAR12) {\t\t\nVAR6 = -VAR13;\ngoto VAR14;\n}\nFUN3(&VAR5->VAR15, VAR9);\nVAR7 = FUN4(VAR8, 20, \"STR\", VAR5->VAR16);\nFUN5(&VAR5->VAR15, VAR9);\nif (*VAR4 < VAR7) {\nif (FUN6(VAR2, VAR8 + *VAR4, VAR7 - *VAR4))\nVAR6 = -VAR17;\nelse {\nVAR6 = VAR7 - *VAR4;\n*VAR4 += VAR7;\n}\n}\nVAR14:\nFUN7(&VAR5->VAR11);\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static ssize_t yurex_read(struct file *file, char __user *buffer, size_t count,\nloff_t *ppos)\n{\nstruct usb_yurex *dev;\nint len = 0;\nchar in_buffer[20];\nunsigned long flags;\ndev = file->private_data;\nmutex_lock(&dev->io_mutex);\nif (!dev->interface) {\t\t\nmutex_unlock(&dev->io_mutex);\nreturn -ENODEV;\n}\nspin_lock_irqsave(&dev->lock, flags);\nlen = snprintf(in_buffer, 20, \"%lld\\n\", dev->bbu);\nspin_unlock_irqrestore(&dev->lock, flags);\nmutex_unlock(&dev->io_mutex);\nreturn simple_read_from_buffer(buffer, count, ppos, in_buffer, len);\n}\n",
      "code_before_change_raw": "static ssize_t yurex_read(struct file *file, char __user *buffer, size_t count,\nloff_t *ppos)\n{\nstruct usb_yurex *dev;\nint retval = 0;\nint bytes_read = 0;\nchar in_buffer[20];\nunsigned long flags;\ndev = file->private_data;\nmutex_lock(&dev->io_mutex);\nif (!dev->interface) {\t\t\nretval = -ENODEV;\ngoto exit;\n}\nspin_lock_irqsave(&dev->lock, flags);\nbytes_read = snprintf(in_buffer, 20, \"%lld\\n\", dev->bbu);\nspin_unlock_irqrestore(&dev->lock, flags);\nif (*ppos < bytes_read) {\nif (copy_to_user(buffer, in_buffer + *ppos, bytes_read - *ppos))\nretval = -EFAULT;\nelse {\nretval = bytes_read - *ppos;\n*ppos += bytes_read;\n}\n}\nexit:\nmutex_unlock(&dev->io_mutex);\nreturn retval;\n}\n"
    }
  ],
  "CVE-2018-16880": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper memory allocation size validation and handling in the vhost_net driver's handle_rx() function.",
        "trigger_condition": "A malicious virtual guest, under specific conditions, triggers an out-of-bounds write in a kmalloc-8 slab on a virtual host, leading to kernel memory corruption and a system panic.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate and handle the memory allocation size properly, allowing a malicious virtual guest to trigger an out-of-bounds write in a kmalloc-8 slab, resulting in kernel memory corruption and a system panic."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper memory allocation size validation and handling in the vhost_net driver's handle_rx() function. By adding the correct memory allocation size parameter to the vhost_dev_init function call, the vulnerability can be fixed. This modification prevents the out-of-bounds write issue in the kmalloc-8 slab, thereby addressing the kernel memory corruption and system panic potential.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2018-16880 in the Linux kernel's handle_rx() function in the vhost_net driver. By adding the parameter `UIO_MAXIOV + VHOST_NET_BATCH` to the `vhost_dev_init` function call, you are ensuring that the function is provided with the correct parameters to prevent the out-of-bounds write issue in the kmalloc-8 slab. This modification helps mitigate the potential kernel memory corruption and system panic that could be triggered by a malicious virtual guest under specific conditions. It is important to apply this fix to prevent any possible exploitation of the vulnerability and to maintain the security and stability of the system.",
      "GPT_purpose": "Initialize a vhost_net structure and associated data structures for handling network virtual queues.",
      "GPT_function": "\n1. Allocate memory for vhost_net structure and virtual queues.\n2. Initialize various fields in the vhost_net structure.\n3. Initialize vhost device and virtual queues.\n4. Initialize polling for transmit and receive queues.\n5. Set private data and initialize page fragment and reference count bias.",
      "CVE_id": "CVE-2018-16880",
      "code_before_change": "static int vhost_net_open(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n;\n\tstruct vhost_dev *dev;\n\tstruct vhost_virtqueue **vqs;\n\tvoid **queue;\n\tstruct xdp_buff *xdp;\n\tint i;\n\n\tn = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!n)\n\t\treturn -ENOMEM;\n\tvqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tkvfree(n);\n\t\treturn -ENOMEM;\n\t}\n\n\tqueue = kmalloc_array(VHOST_NET_BATCH, sizeof(void *),\n\t\t\t      GFP_KERNEL);\n\tif (!queue) {\n\t\tkfree(vqs);\n\t\tkvfree(n);\n\t\treturn -ENOMEM;\n\t}\n\tn->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;\n\n\txdp = kmalloc_array(VHOST_NET_BATCH, sizeof(*xdp), GFP_KERNEL);\n\tif (!xdp) {\n\t\tkfree(vqs);\n\t\tkvfree(n);\n\t\tkfree(queue);\n\t\treturn -ENOMEM;\n\t}\n\tn->vqs[VHOST_NET_VQ_TX].xdp = xdp;\n\n\tdev = &n->dev;\n\tvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\n\tvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\n\tn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\n\tn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].ubuf_info = NULL;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].batched_xdp = 0;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t\tn->vqs[i].rx_ring = NULL;\n\t\tvhost_net_buf_init(&n->vqs[i].rxq);\n\t}\n\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);\n\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);\n\n\tf->private_data = n;\n\tn->page_frag.page = NULL;\n\tn->refcnt_bias = 0;\n\n\treturn 0;\n}",
      "code_after_change": "static int vhost_net_open(struct inode *inode, struct file *f)\n{\n\tstruct vhost_net *n;\n\tstruct vhost_dev *dev;\n\tstruct vhost_virtqueue **vqs;\n\tvoid **queue;\n\tstruct xdp_buff *xdp;\n\tint i;\n\n\tn = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!n)\n\t\treturn -ENOMEM;\n\tvqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tkvfree(n);\n\t\treturn -ENOMEM;\n\t}\n\n\tqueue = kmalloc_array(VHOST_NET_BATCH, sizeof(void *),\n\t\t\t      GFP_KERNEL);\n\tif (!queue) {\n\t\tkfree(vqs);\n\t\tkvfree(n);\n\t\treturn -ENOMEM;\n\t}\n\tn->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;\n\n\txdp = kmalloc_array(VHOST_NET_BATCH, sizeof(*xdp), GFP_KERNEL);\n\tif (!xdp) {\n\t\tkfree(vqs);\n\t\tkvfree(n);\n\t\tkfree(queue);\n\t\treturn -ENOMEM;\n\t}\n\tn->vqs[VHOST_NET_VQ_TX].xdp = xdp;\n\n\tdev = &n->dev;\n\tvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\n\tvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\n\tn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\n\tn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\n\tfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\n\t\tn->vqs[i].ubufs = NULL;\n\t\tn->vqs[i].ubuf_info = NULL;\n\t\tn->vqs[i].upend_idx = 0;\n\t\tn->vqs[i].done_idx = 0;\n\t\tn->vqs[i].batched_xdp = 0;\n\t\tn->vqs[i].vhost_hlen = 0;\n\t\tn->vqs[i].sock_hlen = 0;\n\t\tn->vqs[i].rx_ring = NULL;\n\t\tvhost_net_buf_init(&n->vqs[i].rxq);\n\t}\n\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,\n\t\t       UIO_MAXIOV + VHOST_NET_BATCH);\n\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);\n\tvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);\n\n\tf->private_data = n;\n\tn->page_frag.page = NULL;\n\tn->refcnt_bias = 0;\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,",
          "\t\t       UIO_MAXIOV + VHOST_NET_BATCH);"
        ],
        "deleted": [
          "\tvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper memory allocation size validation and handling in the vhost_net driver's handle_rx() function.",
      "trigger_condition": "A malicious virtual guest, under specific conditions, triggers an out-of-bounds write in a kmalloc-8 slab on a virtual host, leading to kernel memory corruption and a system panic.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate and handle the memory allocation size properly, allowing a malicious virtual guest to trigger an out-of-bounds write in a kmalloc-8 slab, resulting in kernel memory corruption and a system panic.",
      "id": 33,
      "code_after_change_normalized": "static int FUN1(struct VAR1 *VAR1, struct file *VAR2)\n{\nstruct vhost_net *VAR3;\nstruct vhost_dev *VAR4;\nstruct vhost_virtqueue **VAR5;\nvoid **VAR6;\nstruct xdp_buff *VAR7;\nint VAR8;\nVAR3 = FUN2(sizeof *VAR3, VAR9 | VAR10);\nif (!VAR3)\nreturn -VAR11;\nVAR5 = FUN3(VAR12, sizeof(*VAR5), VAR9);\nif (!VAR5) {\nFUN4(VAR3);\nreturn -VAR11;\n}\nVAR6 = FUN3(VAR13, sizeof(void *),\nVAR9);\nif (!VAR6) {\nFUN5(VAR5);\nFUN4(VAR3);\nreturn -VAR11;\n}\nVAR3->VAR5[VAR14].VAR15.VAR6 = VAR6;\nVAR7 = FUN3(VAR13, sizeof(*VAR7), VAR9);\nif (!VAR7) {\nFUN5(VAR5);\nFUN4(VAR3);\nFUN5(VAR6);\nreturn -VAR11;\n}\nVAR3->VAR5[VAR16].VAR7 = VAR7;\nVAR4 = &VAR3->VAR4;\nVAR5[VAR16] = &VAR3->VAR5[VAR16].VAR17;\nVAR5[VAR14] = &VAR3->VAR5[VAR14].VAR17;\nVAR3->VAR5[VAR16].VAR17.VAR18 = VAR19;\nVAR3->VAR5[VAR14].VAR17.VAR18 = VAR20;\nfor (VAR8 = 0; VAR8 < VAR12; VAR8++) {\nVAR3->VAR5[VAR8].VAR21 = NULL;\nVAR3->VAR5[VAR8].VAR22 = NULL;\nVAR3->VAR5[VAR8].VAR23 = 0;\nVAR3->VAR5[VAR8].VAR24 = 0;\nVAR3->VAR5[VAR8].VAR25 = 0;\nVAR3->VAR5[VAR8].VAR26 = 0;\nVAR3->VAR5[VAR8].VAR27 = 0;\nVAR3->VAR5[VAR8].VAR28 = NULL;\nFUN6(&VAR3->VAR5[VAR8].VAR15);\n}\nFUN7(VAR4, VAR5, VAR12,\nVAR29 + VAR13);\nFUN8(VAR3->VAR30 + VAR16, VAR31, VAR32, VAR4);\nFUN8(VAR3->VAR30 + VAR14, VAR33, VAR34, VAR4);\nVAR2->VAR35 = VAR3;\nVAR3->VAR36.VAR37 = NULL;\nVAR3->VAR38 = 0;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct VAR1 *VAR1, struct file *VAR2)\n{\nstruct vhost_net *VAR3;\nstruct vhost_dev *VAR4;\nstruct vhost_virtqueue **VAR5;\nvoid **VAR6;\nstruct xdp_buff *VAR7;\nint VAR8;\nVAR3 = FUN2(sizeof *VAR3, VAR9 | VAR10);\nif (!VAR3)\nreturn -VAR11;\nVAR5 = FUN3(VAR12, sizeof(*VAR5), VAR9);\nif (!VAR5) {\nFUN4(VAR3);\nreturn -VAR11;\n}\nVAR6 = FUN3(VAR13, sizeof(void *),\nVAR9);\nif (!VAR6) {\nFUN5(VAR5);\nFUN4(VAR3);\nreturn -VAR11;\n}\nVAR3->VAR5[VAR14].VAR15.VAR6 = VAR6;\nVAR7 = FUN3(VAR13, sizeof(*VAR7), VAR9);\nif (!VAR7) {\nFUN5(VAR5);\nFUN4(VAR3);\nFUN5(VAR6);\nreturn -VAR11;\n}\nVAR3->VAR5[VAR16].VAR7 = VAR7;\nVAR4 = &VAR3->VAR4;\nVAR5[VAR16] = &VAR3->VAR5[VAR16].VAR17;\nVAR5[VAR14] = &VAR3->VAR5[VAR14].VAR17;\nVAR3->VAR5[VAR16].VAR17.VAR18 = VAR19;\nVAR3->VAR5[VAR14].VAR17.VAR18 = VAR20;\nfor (VAR8 = 0; VAR8 < VAR12; VAR8++) {\nVAR3->VAR5[VAR8].VAR21 = NULL;\nVAR3->VAR5[VAR8].VAR22 = NULL;\nVAR3->VAR5[VAR8].VAR23 = 0;\nVAR3->VAR5[VAR8].VAR24 = 0;\nVAR3->VAR5[VAR8].VAR25 = 0;\nVAR3->VAR5[VAR8].VAR26 = 0;\nVAR3->VAR5[VAR8].VAR27 = 0;\nVAR3->VAR5[VAR8].VAR28 = NULL;\nFUN6(&VAR3->VAR5[VAR8].VAR15);\n}\nFUN7(VAR4, VAR5, VAR12);\nFUN8(VAR3->VAR29 + VAR16, VAR30, VAR31, VAR4);\nFUN8(VAR3->VAR29 + VAR14, VAR32, VAR33, VAR4);\nVAR2->VAR34 = VAR3;\nVAR3->VAR35.VAR36 = NULL;\nVAR3->VAR37 = 0;\nreturn 0;\n}\n",
      "code_after_change_raw": "static int vhost_net_open(struct inode *inode, struct file *f)\n{\nstruct vhost_net *n;\nstruct vhost_dev *dev;\nstruct vhost_virtqueue **vqs;\nvoid **queue;\nstruct xdp_buff *xdp;\nint i;\nn = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\nif (!n)\nreturn -ENOMEM;\nvqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);\nif (!vqs) {\nkvfree(n);\nreturn -ENOMEM;\n}\nqueue = kmalloc_array(VHOST_NET_BATCH, sizeof(void *),\nGFP_KERNEL);\nif (!queue) {\nkfree(vqs);\nkvfree(n);\nreturn -ENOMEM;\n}\nn->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;\nxdp = kmalloc_array(VHOST_NET_BATCH, sizeof(*xdp), GFP_KERNEL);\nif (!xdp) {\nkfree(vqs);\nkvfree(n);\nkfree(queue);\nreturn -ENOMEM;\n}\nn->vqs[VHOST_NET_VQ_TX].xdp = xdp;\ndev = &n->dev;\nvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\nvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\nn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\nn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\nfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\nn->vqs[i].ubufs = NULL;\nn->vqs[i].ubuf_info = NULL;\nn->vqs[i].upend_idx = 0;\nn->vqs[i].done_idx = 0;\nn->vqs[i].batched_xdp = 0;\nn->vqs[i].vhost_hlen = 0;\nn->vqs[i].sock_hlen = 0;\nn->vqs[i].rx_ring = NULL;\nvhost_net_buf_init(&n->vqs[i].rxq);\n}\nvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX,\nUIO_MAXIOV + VHOST_NET_BATCH);\nvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);\nvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);\nf->private_data = n;\nn->page_frag.page = NULL;\nn->refcnt_bias = 0;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int vhost_net_open(struct inode *inode, struct file *f)\n{\nstruct vhost_net *n;\nstruct vhost_dev *dev;\nstruct vhost_virtqueue **vqs;\nvoid **queue;\nstruct xdp_buff *xdp;\nint i;\nn = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);\nif (!n)\nreturn -ENOMEM;\nvqs = kmalloc_array(VHOST_NET_VQ_MAX, sizeof(*vqs), GFP_KERNEL);\nif (!vqs) {\nkvfree(n);\nreturn -ENOMEM;\n}\nqueue = kmalloc_array(VHOST_NET_BATCH, sizeof(void *),\nGFP_KERNEL);\nif (!queue) {\nkfree(vqs);\nkvfree(n);\nreturn -ENOMEM;\n}\nn->vqs[VHOST_NET_VQ_RX].rxq.queue = queue;\nxdp = kmalloc_array(VHOST_NET_BATCH, sizeof(*xdp), GFP_KERNEL);\nif (!xdp) {\nkfree(vqs);\nkvfree(n);\nkfree(queue);\nreturn -ENOMEM;\n}\nn->vqs[VHOST_NET_VQ_TX].xdp = xdp;\ndev = &n->dev;\nvqs[VHOST_NET_VQ_TX] = &n->vqs[VHOST_NET_VQ_TX].vq;\nvqs[VHOST_NET_VQ_RX] = &n->vqs[VHOST_NET_VQ_RX].vq;\nn->vqs[VHOST_NET_VQ_TX].vq.handle_kick = handle_tx_kick;\nn->vqs[VHOST_NET_VQ_RX].vq.handle_kick = handle_rx_kick;\nfor (i = 0; i < VHOST_NET_VQ_MAX; i++) {\nn->vqs[i].ubufs = NULL;\nn->vqs[i].ubuf_info = NULL;\nn->vqs[i].upend_idx = 0;\nn->vqs[i].done_idx = 0;\nn->vqs[i].batched_xdp = 0;\nn->vqs[i].vhost_hlen = 0;\nn->vqs[i].sock_hlen = 0;\nn->vqs[i].rx_ring = NULL;\nvhost_net_buf_init(&n->vqs[i].rxq);\n}\nvhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX);\nvhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);\nvhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev);\nf->private_data = n;\nn->page_frag.page = NULL;\nn->refcnt_bias = 0;\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking during memory allocation for arrays in a specific function.",
        "trigger_condition": "A malicious virtual guest triggers an out-of-bounds write in a kmalloc-8 slab on a virtual host, leading to kernel memory corruption and a system panic.",
        "specific_code_behavior_causing_vulnerability": "The code allocates memory for arrays without proper validation of the size, allowing an out-of-bounds write vulnerability to be exploited by a malicious virtual guest."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking during memory allocation for arrays. In this case, the solution involves updating the allocation size for the arrays to a properly defined limit (dev->iov_limit) to prevent out-of-bounds writes and improve the security of the system.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2018-16880. The vulnerability involves an out-of-bounds write in a kmalloc-8 slab, which can lead to kernel memory corruption and a system panic when triggered by a malicious virtual guest under specific conditions.\n\nTo mitigate this vulnerability, the modification changes the allocation size for the `vq->log` and `vq->heads` arrays from `UIO_MAXIOV` to `dev->iov_limit`. By using `dev->iov_limit`, which is presumably a properly defined limit for the number of IO vectors, the code ensures that the memory allocation is within bounds and prevents the out-of-bounds write issue that could lead to memory corruption.\n\nTherefore, updating the allocation size to `dev->iov_limit` in the modified code snippet helps prevent the vulnerability from being exploited and improves the security of the system.",
      "GPT_purpose": "Allocate memory for I/O vectors in a vhost device, potentially vulnerable to an out-of-bounds write in the Linux kernel.",
      "GPT_function": "\n1. Allocate memory for 'indirect' array in each vhost_virtqueue.\n2. Allocate memory for 'log' array in each vhost_virtqueue.\n3. Allocate memory for 'heads' array in each vhost_virtqueue.\n4. Handle error case when memory allocation fails by freeing previously allocated memory.",
      "CVE_id": "CVE-2018-16880",
      "code_before_change": "static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\n{\n\tstruct vhost_virtqueue *vq;\n\tint i;\n\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tvq = dev->vqs[i];\n\t\tvq->indirect = kmalloc_array(UIO_MAXIOV,\n\t\t\t\t\t     sizeof(*vq->indirect),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tvq->log = kmalloc_array(UIO_MAXIOV, sizeof(*vq->log),\n\t\t\t\t\tGFP_KERNEL);\n\t\tvq->heads = kmalloc_array(UIO_MAXIOV, sizeof(*vq->heads),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!vq->indirect || !vq->log || !vq->heads)\n\t\t\tgoto err_nomem;\n\t}\n\treturn 0;\n\nerr_nomem:\n\tfor (; i >= 0; --i)\n\t\tvhost_vq_free_iovecs(dev->vqs[i]);\n\treturn -ENOMEM;\n}",
      "code_after_change": "static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\n{\n\tstruct vhost_virtqueue *vq;\n\tint i;\n\n\tfor (i = 0; i < dev->nvqs; ++i) {\n\t\tvq = dev->vqs[i];\n\t\tvq->indirect = kmalloc_array(UIO_MAXIOV,\n\t\t\t\t\t     sizeof(*vq->indirect),\n\t\t\t\t\t     GFP_KERNEL);\n\t\tvq->log = kmalloc_array(dev->iov_limit, sizeof(*vq->log),\n\t\t\t\t\tGFP_KERNEL);\n\t\tvq->heads = kmalloc_array(dev->iov_limit, sizeof(*vq->heads),\n\t\t\t\t\t  GFP_KERNEL);\n\t\tif (!vq->indirect || !vq->log || !vq->heads)\n\t\t\tgoto err_nomem;\n\t}\n\treturn 0;\n\nerr_nomem:\n\tfor (; i >= 0; --i)\n\t\tvhost_vq_free_iovecs(dev->vqs[i]);\n\treturn -ENOMEM;\n}",
      "modified_lines": {
        "added": [
          "\t\tvq->log = kmalloc_array(dev->iov_limit, sizeof(*vq->log),",
          "\t\tvq->heads = kmalloc_array(dev->iov_limit, sizeof(*vq->heads),"
        ],
        "deleted": [
          "\t\tvq->log = kmalloc_array(UIO_MAXIOV, sizeof(*vq->log),",
          "\t\tvq->heads = kmalloc_array(UIO_MAXIOV, sizeof(*vq->heads),"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking during memory allocation for arrays in a specific function.",
      "trigger_condition": "A malicious virtual guest triggers an out-of-bounds write in a kmalloc-8 slab on a virtual host, leading to kernel memory corruption and a system panic.",
      "specific_code_behavior_causing_vulnerability": "The code allocates memory for arrays without proper validation of the size, allowing an out-of-bounds write vulnerability to be exploited by a malicious virtual guest.",
      "id": 34,
      "code_after_change_normalized": "static long FUN1(struct vhost_dev *VAR1)\n{\nstruct vhost_virtqueue *VAR2;\nint VAR3;\nfor (VAR3 = 0; VAR3 < VAR1->VAR4; ++VAR3) {\nVAR2 = VAR1->VAR5[VAR3];\nVAR2->VAR6 = FUN2(VAR7,\nsizeof(*VAR2->VAR6),\nVAR8);\nVAR2->VAR9 = FUN2(VAR1->VAR10, sizeof(*VAR2->VAR9),\nVAR8);\nVAR2->VAR11 = FUN2(VAR1->VAR10, sizeof(*VAR2->VAR11),\nVAR8);\nif (!VAR2->VAR6 || !VAR2->VAR9 || !VAR2->VAR11)\ngoto VAR12;\n}\nreturn 0;\nVAR12:\nfor (; VAR3 >= 0; --VAR3)\nFUN3(VAR1->VAR5[VAR3]);\nreturn -VAR13;\n}\n",
      "code_before_change_normalized": "static long FUN1(struct vhost_dev *VAR1)\n{\nstruct vhost_virtqueue *VAR2;\nint VAR3;\nfor (VAR3 = 0; VAR3 < VAR1->VAR4; ++VAR3) {\nVAR2 = VAR1->VAR5[VAR3];\nVAR2->VAR6 = FUN2(VAR7,\nsizeof(*VAR2->VAR6),\nVAR8);\nVAR2->VAR9 = FUN2(VAR7, sizeof(*VAR2->VAR9),\nVAR8);\nVAR2->VAR10 = FUN2(VAR7, sizeof(*VAR2->VAR10),\nVAR8);\nif (!VAR2->VAR6 || !VAR2->VAR9 || !VAR2->VAR10)\ngoto VAR11;\n}\nreturn 0;\nVAR11:\nfor (; VAR3 >= 0; --VAR3)\nFUN3(VAR1->VAR5[VAR3]);\nreturn -VAR12;\n}\n",
      "code_after_change_raw": "static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\n{\nstruct vhost_virtqueue *vq;\nint i;\nfor (i = 0; i < dev->nvqs; ++i) {\nvq = dev->vqs[i];\nvq->indirect = kmalloc_array(UIO_MAXIOV,\nsizeof(*vq->indirect),\nGFP_KERNEL);\nvq->log = kmalloc_array(dev->iov_limit, sizeof(*vq->log),\nGFP_KERNEL);\nvq->heads = kmalloc_array(dev->iov_limit, sizeof(*vq->heads),\nGFP_KERNEL);\nif (!vq->indirect || !vq->log || !vq->heads)\ngoto err_nomem;\n}\nreturn 0;\nerr_nomem:\nfor (; i >= 0; --i)\nvhost_vq_free_iovecs(dev->vqs[i]);\nreturn -ENOMEM;\n}\n",
      "code_before_change_raw": "static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)\n{\nstruct vhost_virtqueue *vq;\nint i;\nfor (i = 0; i < dev->nvqs; ++i) {\nvq = dev->vqs[i];\nvq->indirect = kmalloc_array(UIO_MAXIOV,\nsizeof(*vq->indirect),\nGFP_KERNEL);\nvq->log = kmalloc_array(UIO_MAXIOV, sizeof(*vq->log),\nGFP_KERNEL);\nvq->heads = kmalloc_array(UIO_MAXIOV, sizeof(*vq->heads),\nGFP_KERNEL);\nif (!vq->indirect || !vq->log || !vq->heads)\ngoto err_nomem;\n}\nreturn 0;\nerr_nomem:\nfor (; i >= 0; --i)\nvhost_vq_free_iovecs(dev->vqs[i]);\nreturn -ENOMEM;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and bounds checking when processing I/O vectors in a specific function.",
        "trigger_condition": "A malicious virtual guest can trigger an out-of-bounds write in a kmalloc-8 slab on a virtual host under specific conditions, leading to kernel memory corruption and a system panic.",
        "specific_code_behavior_causing_vulnerability": "The code does not enforce proper bounds checking when handling I/O vectors, allowing a malicious guest to write beyond the allocated memory region, leading to memory corruption and potential privilege escalation."
      },
      "solution": "To mitigate the vulnerability, it is necessary to limit the number of I/O vectors processed to prevent out-of-bounds writes. This can be achieved by adding an additional parameter to the function call that initializes the device, specifying the maximum number of I/O vectors allowed. By enforcing this limit, the code prevents malicious guests from triggering out-of-bounds writes and helps avoid kernel memory corruption and potential privilege escalation.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2018-16880 in the code snippet. The vulnerability arises from an out-of-bounds write in a kmalloc-8 slab on a virtual host triggered by a malicious virtual guest under specific conditions. To address this issue, the modification adds an additional parameter `UIO_MAXIOV` to the `vhost_dev_init` function call. This parameter helps prevent the out-of-bounds write by limiting the number of I/O vectors that can be processed, thus mitigating the vulnerability and potential kernel memory corruption.",
      "GPT_purpose": "Open a vhost vsock device, allocate memory for the device structure and virtqueues, initialize the device, and set up necessary data structures.",
      "GPT_function": "\n1. Open a vhost vsock device.\n2. Allocate memory for vsock and vqs.\n3. Initialize vsock and vqs structures.\n4. Initialize vhost device and set private data in the file structure.\n5. Initialize spin lock and work for sending packets.",
      "CVE_id": "CVE-2018-16880",
      "code_before_change": "static int vhost_vsock_dev_open(struct inode *inode, struct file *file)\n{\n\tstruct vhost_virtqueue **vqs;\n\tstruct vhost_vsock *vsock;\n\tint ret;\n\n\t/* This struct is large and allocation could fail, fall back to vmalloc\n\t * if there is no other way.\n\t */\n\tvsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!vsock)\n\t\treturn -ENOMEM;\n\n\tvqs = kmalloc_array(ARRAY_SIZE(vsock->vqs), sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tvsock->guest_cid = 0; /* no CID assigned yet */\n\n\tatomic_set(&vsock->queued_replies, 0);\n\n\tvqs[VSOCK_VQ_TX] = &vsock->vqs[VSOCK_VQ_TX];\n\tvqs[VSOCK_VQ_RX] = &vsock->vqs[VSOCK_VQ_RX];\n\tvsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;\n\tvsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;\n\n\tvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));\n\n\tfile->private_data = vsock;\n\tspin_lock_init(&vsock->send_pkt_list_lock);\n\tINIT_LIST_HEAD(&vsock->send_pkt_list);\n\tvhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);\n\treturn 0;\n\nout:\n\tvhost_vsock_free(vsock);\n\treturn ret;\n}",
      "code_after_change": "static int vhost_vsock_dev_open(struct inode *inode, struct file *file)\n{\n\tstruct vhost_virtqueue **vqs;\n\tstruct vhost_vsock *vsock;\n\tint ret;\n\n\t/* This struct is large and allocation could fail, fall back to vmalloc\n\t * if there is no other way.\n\t */\n\tvsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);\n\tif (!vsock)\n\t\treturn -ENOMEM;\n\n\tvqs = kmalloc_array(ARRAY_SIZE(vsock->vqs), sizeof(*vqs), GFP_KERNEL);\n\tif (!vqs) {\n\t\tret = -ENOMEM;\n\t\tgoto out;\n\t}\n\n\tvsock->guest_cid = 0; /* no CID assigned yet */\n\n\tatomic_set(&vsock->queued_replies, 0);\n\n\tvqs[VSOCK_VQ_TX] = &vsock->vqs[VSOCK_VQ_TX];\n\tvqs[VSOCK_VQ_RX] = &vsock->vqs[VSOCK_VQ_RX];\n\tvsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;\n\tvsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;\n\n\tvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs), UIO_MAXIOV);\n\n\tfile->private_data = vsock;\n\tspin_lock_init(&vsock->send_pkt_list_lock);\n\tINIT_LIST_HEAD(&vsock->send_pkt_list);\n\tvhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);\n\treturn 0;\n\nout:\n\tvhost_vsock_free(vsock);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs), UIO_MAXIOV);"
        ],
        "deleted": [
          "\tvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and bounds checking when processing I/O vectors in a specific function.",
      "trigger_condition": "A malicious virtual guest can trigger an out-of-bounds write in a kmalloc-8 slab on a virtual host under specific conditions, leading to kernel memory corruption and a system panic.",
      "specific_code_behavior_causing_vulnerability": "The code does not enforce proper bounds checking when handling I/O vectors, allowing a malicious guest to write beyond the allocated memory region, leading to memory corruption and potential privilege escalation.",
      "id": 35,
      "code_after_change_normalized": "static int FUN1(struct VAR1 *VAR1, struct VAR2 *VAR2)\n{\nstruct vhost_virtqueue **VAR3;\nstruct vhost_vsock *VAR4;\nint VAR5;\nVAR4 = FUN2(sizeof(*VAR4), VAR6 | VAR7);\nif (!VAR4)\nreturn -VAR8;\nVAR3 = FUN3(FUN4(VAR4->VAR3), sizeof(*VAR3), VAR6);\nif (!VAR3) {\nVAR5 = -VAR8;\ngoto VAR9;\n}\nVAR4->VAR10 = 0; \nFUN5(&VAR4->VAR11, 0);\nVAR3[VAR12] = &VAR4->VAR3[VAR12];\nVAR3[VAR13] = &VAR4->VAR3[VAR13];\nVAR4->VAR3[VAR12].VAR14 = VAR15;\nVAR4->VAR3[VAR13].VAR14 = VAR16;\nFUN6(&VAR4->VAR17, VAR3, FUN4(VAR4->VAR3), VAR18);\nVAR2->VAR19 = VAR4;\nFUN7(&VAR4->VAR20);\nFUN8(&VAR4->VAR21);\nFUN9(&VAR4->VAR22, VAR23);\nreturn 0;\nVAR9:\nFUN10(VAR4);\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct VAR1 *VAR1, struct VAR2 *VAR2)\n{\nstruct vhost_virtqueue **VAR3;\nstruct vhost_vsock *VAR4;\nint VAR5;\nVAR4 = FUN2(sizeof(*VAR4), VAR6 | VAR7);\nif (!VAR4)\nreturn -VAR8;\nVAR3 = FUN3(FUN4(VAR4->VAR3), sizeof(*VAR3), VAR6);\nif (!VAR3) {\nVAR5 = -VAR8;\ngoto VAR9;\n}\nVAR4->VAR10 = 0; \nFUN5(&VAR4->VAR11, 0);\nVAR3[VAR12] = &VAR4->VAR3[VAR12];\nVAR3[VAR13] = &VAR4->VAR3[VAR13];\nVAR4->VAR3[VAR12].VAR14 = VAR15;\nVAR4->VAR3[VAR13].VAR14 = VAR16;\nFUN6(&VAR4->VAR17, VAR3, FUN4(VAR4->VAR3));\nVAR2->VAR18 = VAR4;\nFUN7(&VAR4->VAR19);\nFUN8(&VAR4->VAR20);\nFUN9(&VAR4->VAR21, VAR22);\nreturn 0;\nVAR9:\nFUN10(VAR4);\nreturn VAR5;\n}\n",
      "code_after_change_raw": "static int vhost_vsock_dev_open(struct inode *inode, struct file *file)\n{\nstruct vhost_virtqueue **vqs;\nstruct vhost_vsock *vsock;\nint ret;\nvsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);\nif (!vsock)\nreturn -ENOMEM;\nvqs = kmalloc_array(ARRAY_SIZE(vsock->vqs), sizeof(*vqs), GFP_KERNEL);\nif (!vqs) {\nret = -ENOMEM;\ngoto out;\n}\nvsock->guest_cid = 0; \natomic_set(&vsock->queued_replies, 0);\nvqs[VSOCK_VQ_TX] = &vsock->vqs[VSOCK_VQ_TX];\nvqs[VSOCK_VQ_RX] = &vsock->vqs[VSOCK_VQ_RX];\nvsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;\nvsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;\nvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs), UIO_MAXIOV);\nfile->private_data = vsock;\nspin_lock_init(&vsock->send_pkt_list_lock);\nINIT_LIST_HEAD(&vsock->send_pkt_list);\nvhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);\nreturn 0;\nout:\nvhost_vsock_free(vsock);\nreturn ret;\n}\n",
      "code_before_change_raw": "static int vhost_vsock_dev_open(struct inode *inode, struct file *file)\n{\nstruct vhost_virtqueue **vqs;\nstruct vhost_vsock *vsock;\nint ret;\nvsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);\nif (!vsock)\nreturn -ENOMEM;\nvqs = kmalloc_array(ARRAY_SIZE(vsock->vqs), sizeof(*vqs), GFP_KERNEL);\nif (!vqs) {\nret = -ENOMEM;\ngoto out;\n}\nvsock->guest_cid = 0; \natomic_set(&vsock->queued_replies, 0);\nvqs[VSOCK_VQ_TX] = &vsock->vqs[VSOCK_VQ_TX];\nvqs[VSOCK_VQ_RX] = &vsock->vqs[VSOCK_VQ_RX];\nvsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;\nvsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;\nvhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs));\nfile->private_data = vsock;\nspin_lock_init(&vsock->send_pkt_list_lock);\nINIT_LIST_HEAD(&vsock->send_pkt_list);\nvhost_work_init(&vsock->send_pkt_work, vhost_transport_send_pkt_work);\nreturn 0;\nout:\nvhost_vsock_free(vsock);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2018-5332": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for a value used during DMA page allocation.",
        "trigger_condition": "The rds_message_alloc_sgs() function does not properly validate the value, leading to a heap-based out-of-bounds write.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the nr_local parameter is zero before iterating over the local_vec array. This can result in an out-of-bounds write if nr_local is zero, leading to potential memory corruption and security vulnerabilities."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the nr_local parameter is not zero before iterating over the local_vec array. By adding the check 'if (args->nr_local == 0) return -EINVAL;', the code prevents the loop from running with an invalid index when nr_local is zero, thus avoiding potential memory corruption and security vulnerabilities related to out-of-bounds writes.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-5332, which involves a heap-based out-of-bounds write in the Linux kernel's rds_rdma_extra_size function. By adding the check `if (args->nr_local == 0) return -EINVAL;`, the code ensures that the `nr_local` parameter is not zero, preventing potential issues related to DMA page allocation and out-of-bounds writes.\n\nWithout this check, if `nr_local` is zero, the loop would iterate with an invalid index, leading to potential memory corruption and security vulnerabilities. By adding this check, the code ensures that the loop only runs when `nr_local` is a valid value, thus preventing the vulnerability from being exploited.",
      "GPT_purpose": "Calculate the total size of scatterlist pages based on the number of pages in a vector for RDMA operations.",
      "GPT_function": "\n1. Calculate the total number of pages in the vector.\n2. Check for invalid number of pages in the vector.\n3. Prevent overflow by checking the total number of pages.",
      "CVE_id": "CVE-2018-5332",
      "code_before_change": "int rds_rdma_extra_size(struct rds_rdma_args *args)\n{\n\tstruct rds_iovec vec;\n\tstruct rds_iovec __user *local_vec;\n\tint tot_pages = 0;\n\tunsigned int nr_pages;\n\tunsigned int i;\n\n\tlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\n\n\t/* figure out the number of pages in the vector */\n\tfor (i = 0; i < args->nr_local; i++) {\n\t\tif (copy_from_user(&vec, &local_vec[i],\n\t\t\t\t   sizeof(struct rds_iovec)))\n\t\t\treturn -EFAULT;\n\n\t\tnr_pages = rds_pages_in_vec(&vec);\n\t\tif (nr_pages == 0)\n\t\t\treturn -EINVAL;\n\n\t\ttot_pages += nr_pages;\n\n\t\t/*\n\t\t * nr_pages for one entry is limited to (UINT_MAX>>PAGE_SHIFT)+1,\n\t\t * so tot_pages cannot overflow without first going negative.\n\t\t */\n\t\tif (tot_pages < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn tot_pages * sizeof(struct scatterlist);\n}",
      "code_after_change": "int rds_rdma_extra_size(struct rds_rdma_args *args)\n{\n\tstruct rds_iovec vec;\n\tstruct rds_iovec __user *local_vec;\n\tint tot_pages = 0;\n\tunsigned int nr_pages;\n\tunsigned int i;\n\n\tlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\n\n\tif (args->nr_local == 0)\n\t\treturn -EINVAL;\n\n\t/* figure out the number of pages in the vector */\n\tfor (i = 0; i < args->nr_local; i++) {\n\t\tif (copy_from_user(&vec, &local_vec[i],\n\t\t\t\t   sizeof(struct rds_iovec)))\n\t\t\treturn -EFAULT;\n\n\t\tnr_pages = rds_pages_in_vec(&vec);\n\t\tif (nr_pages == 0)\n\t\t\treturn -EINVAL;\n\n\t\ttot_pages += nr_pages;\n\n\t\t/*\n\t\t * nr_pages for one entry is limited to (UINT_MAX>>PAGE_SHIFT)+1,\n\t\t * so tot_pages cannot overflow without first going negative.\n\t\t */\n\t\tif (tot_pages < 0)\n\t\t\treturn -EINVAL;\n\t}\n\n\treturn tot_pages * sizeof(struct scatterlist);\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (args->nr_local == 0)",
          "\t\treturn -EINVAL;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of validation for a value used during DMA page allocation.",
      "trigger_condition": "The rds_message_alloc_sgs() function does not properly validate the value, leading to a heap-based out-of-bounds write.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the nr_local parameter is zero before iterating over the local_vec array. This can result in an out-of-bounds write if nr_local is zero, leading to potential memory corruption and security vulnerabilities.",
      "id": 36,
      "code_after_change_normalized": "int FUN1(struct rds_rdma_args *VAR1)\n{\nstruct rds_iovec VAR2;\nstruct rds_iovec __user *VAR3;\nint VAR4 = 0;\nunsigned int VAR5;\nunsigned int VAR6;\nVAR3 = (struct rds_iovec VAR7 *)(unsigned long) VAR1->VAR8;\nif (VAR1->VAR9 == 0)\nreturn -VAR10;\nfor (VAR6 = 0; VAR6 < VAR1->VAR9; VAR6++) {\nif (FUN2(&VAR2, &VAR3[VAR6],\nsizeof(struct VAR11)))\nreturn -VAR12;\nVAR5 = FUN3(&VAR2);\nif (VAR5 == 0)\nreturn -VAR10;\nVAR4 += VAR5;\nif (VAR4 < 0)\nreturn -VAR10;\n}\nreturn VAR4 * sizeof(struct VAR13);\n}\n",
      "code_before_change_normalized": "int FUN1(struct rds_rdma_args *VAR1)\n{\nstruct rds_iovec VAR2;\nstruct rds_iovec __user *VAR3;\nint VAR4 = 0;\nunsigned int VAR5;\nunsigned int VAR6;\nVAR3 = (struct rds_iovec VAR7 *)(unsigned long) VAR1->VAR8;\nfor (VAR6 = 0; VAR6 < VAR1->VAR9; VAR6++) {\nif (FUN2(&VAR2, &VAR3[VAR6],\nsizeof(struct VAR10)))\nreturn -VAR11;\nVAR5 = FUN3(&VAR2);\nif (VAR5 == 0)\nreturn -VAR12;\nVAR4 += VAR5;\nif (VAR4 < 0)\nreturn -VAR12;\n}\nreturn VAR4 * sizeof(struct VAR13);\n}\n",
      "code_after_change_raw": "int rds_rdma_extra_size(struct rds_rdma_args *args)\n{\nstruct rds_iovec vec;\nstruct rds_iovec __user *local_vec;\nint tot_pages = 0;\nunsigned int nr_pages;\nunsigned int i;\nlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\nif (args->nr_local == 0)\nreturn -EINVAL;\nfor (i = 0; i < args->nr_local; i++) {\nif (copy_from_user(&vec, &local_vec[i],\nsizeof(struct rds_iovec)))\nreturn -EFAULT;\nnr_pages = rds_pages_in_vec(&vec);\nif (nr_pages == 0)\nreturn -EINVAL;\ntot_pages += nr_pages;\nif (tot_pages < 0)\nreturn -EINVAL;\n}\nreturn tot_pages * sizeof(struct scatterlist);\n}\n",
      "code_before_change_raw": "int rds_rdma_extra_size(struct rds_rdma_args *args)\n{\nstruct rds_iovec vec;\nstruct rds_iovec __user *local_vec;\nint tot_pages = 0;\nunsigned int nr_pages;\nunsigned int i;\nlocal_vec = (struct rds_iovec __user *)(unsigned long) args->local_vec_addr;\nfor (i = 0; i < args->nr_local; i++) {\nif (copy_from_user(&vec, &local_vec[i],\nsizeof(struct rds_iovec)))\nreturn -EFAULT;\nnr_pages = rds_pages_in_vec(&vec);\nif (nr_pages == 0)\nreturn -EINVAL;\ntot_pages += nr_pages;\nif (tot_pages < 0)\nreturn -EINVAL;\n}\nreturn tot_pages * sizeof(struct scatterlist);\n}\n"
    }
  ],
  "CVE-2018-5703": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for TLS protocol updates in the Linux kernel code.",
        "trigger_condition": "Changes in the socket's protocol without rebuilding the TLS protocol for IPv6 sockets.",
        "specific_code_behavior_causing_vulnerability": "The code does not check for changes in the socket's protocol and fails to rebuild the TLS protocol for IPv6 sockets when necessary. This can lead to a vulnerability where the TLS handling is not correctly updated, potentially resulting in a denial of service or other impacts."
      },
      "solution": "To mitigate the vulnerability, it is necessary to introduce a check for changes in the socket's protocol and rebuild the TLS protocol for IPv6 sockets when the protocol changes. This ensures that the TLS handling is correctly updated and maintained, preventing potential vulnerabilities related to TLS operations. In this case, the solution involves adding code to check for changes in the socket's protocol and trigger a rebuild of the TLS protocol for IPv6 sockets when needed.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-5703 related to TLS handling in the Linux kernel. The vulnerability allows attackers to cause a denial of service (slab out-of-bounds write) or potentially have other unspecified impacts via vectors involving TLS.\n\nThe modification introduces the following changes:\n1. Introduces the variable `ip_ver` to determine whether the TLS version is TLSV4 or TLSV6 based on the socket's address family.\n2. Checks if the socket's protocol has changed to trigger a rebuild of the TLS protocol for IPv6.\n3. Ensures that the TLS protocol is correctly updated and maintained for IPv6 sockets to prevent potential vulnerabilities related to TLS handling.\n\nBy adding the code snippet related to IPv6 TLS handling and protocol rebuilding, the modified code aims to enhance the security and stability of TLS operations in the Linux kernel, thereby mitigating the vulnerability and potential risks associated with it.",
      "GPT_purpose": "Initialize a TLS context for a TCP socket in the ESTABLISHED state.",
      "GPT_function": "\n1. Initialize TLS for a socket.\n2. Check if the socket is in TCP_ESTABLISHED state.\n3. Allocate memory for the TLS context.\n4. Set socket options and protocol functions for the TLS context.",
      "CVE_id": "CVE-2018-5703",
      "code_before_change": "static int tls_init(struct sock *sk)\n{\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tls_context *ctx;\n\tint rc = 0;\n\n\t/* The TLS ulp is currently supported only for TCP sockets\n\t * in ESTABLISHED state.\n\t * Supporting sockets in LISTEN state will require us\n\t * to modify the accept implementation to clone rather then\n\t * share the ulp context.\n\t */\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTSUPP;\n\n\t/* allocate tls context */\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\ticsk->icsk_ulp_data = ctx;\n\tctx->setsockopt = sk->sk_prot->setsockopt;\n\tctx->getsockopt = sk->sk_prot->getsockopt;\n\tctx->sk_proto_close = sk->sk_prot->close;\n\n\tctx->tx_conf = TLS_BASE_TX;\n\tupdate_sk_prot(sk, ctx);\nout:\n\treturn rc;\n}",
      "code_after_change": "static int tls_init(struct sock *sk)\n{\n\tint ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;\n\tstruct inet_connection_sock *icsk = inet_csk(sk);\n\tstruct tls_context *ctx;\n\tint rc = 0;\n\n\t/* The TLS ulp is currently supported only for TCP sockets\n\t * in ESTABLISHED state.\n\t * Supporting sockets in LISTEN state will require us\n\t * to modify the accept implementation to clone rather then\n\t * share the ulp context.\n\t */\n\tif (sk->sk_state != TCP_ESTABLISHED)\n\t\treturn -ENOTSUPP;\n\n\t/* allocate tls context */\n\tctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\n\tif (!ctx) {\n\t\trc = -ENOMEM;\n\t\tgoto out;\n\t}\n\ticsk->icsk_ulp_data = ctx;\n\tctx->setsockopt = sk->sk_prot->setsockopt;\n\tctx->getsockopt = sk->sk_prot->getsockopt;\n\tctx->sk_proto_close = sk->sk_prot->close;\n\n\t/* Build IPv6 TLS whenever the address of tcpv6_prot changes */\n\tif (ip_ver == TLSV6 &&\n\t    unlikely(sk->sk_prot != smp_load_acquire(&saved_tcpv6_prot))) {\n\t\tmutex_lock(&tcpv6_prot_mutex);\n\t\tif (likely(sk->sk_prot != saved_tcpv6_prot)) {\n\t\t\tbuild_protos(tls_prots[TLSV6], sk->sk_prot);\n\t\t\tsmp_store_release(&saved_tcpv6_prot, sk->sk_prot);\n\t\t}\n\t\tmutex_unlock(&tcpv6_prot_mutex);\n\t}\n\n\tctx->tx_conf = TLS_BASE_TX;\n\tupdate_sk_prot(sk, ctx);\nout:\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tint ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;",
          "\t/* Build IPv6 TLS whenever the address of tcpv6_prot changes */",
          "\tif (ip_ver == TLSV6 &&",
          "\t    unlikely(sk->sk_prot != smp_load_acquire(&saved_tcpv6_prot))) {",
          "\t\tmutex_lock(&tcpv6_prot_mutex);",
          "\t\tif (likely(sk->sk_prot != saved_tcpv6_prot)) {",
          "\t\t\tbuild_protos(tls_prots[TLSV6], sk->sk_prot);",
          "\t\t\tsmp_store_release(&saved_tcpv6_prot, sk->sk_prot);",
          "\t\t}",
          "\t\tmutex_unlock(&tcpv6_prot_mutex);",
          "\t}",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper handling for TLS protocol updates in the Linux kernel code.",
      "trigger_condition": "Changes in the socket's protocol without rebuilding the TLS protocol for IPv6 sockets.",
      "specific_code_behavior_causing_vulnerability": "The code does not check for changes in the socket's protocol and fails to rebuild the TLS protocol for IPv6 sockets when necessary. This can lead to a vulnerability where the TLS handling is not correctly updated, potentially resulting in a denial of service or other impacts.",
      "id": 37,
      "code_after_change_normalized": "static int FUN1(struct sock *VAR1)\n{\nint VAR2 = VAR1->VAR3 == VAR4 ? VAR5 : VAR6;\nstruct inet_connection_sock *VAR7 = FUN2(VAR1);\nstruct tls_context *VAR8;\nint VAR9 = 0;\nif (VAR1->VAR10 != VAR11)\nreturn -VAR12;\nVAR8 = FUN3(sizeof(*VAR8), VAR13);\nif (!VAR8) {\nVAR9 = -VAR14;\ngoto VAR15;\n}\nVAR7->VAR16 = VAR8;\nVAR8->VAR17 = VAR1->VAR18->VAR17;\nVAR8->VAR19 = VAR1->VAR18->VAR19;\nVAR8->VAR20 = VAR1->VAR18->VAR21;\nif (VAR2 == VAR5 &&\nFUN4(VAR1->VAR18 != FUN5(&VAR22))) {\nFUN6(&VAR23);\nif (FUN7(VAR1->VAR18 != VAR22)) {\nFUN8(VAR24[VAR5], VAR1->VAR18);\nFUN9(&VAR22, VAR1->VAR18);\n}\nFUN10(&VAR23);\n}\nVAR8->VAR25 = VAR26;\nFUN11(VAR1, VAR8);\nVAR15:\nreturn VAR9;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sock *VAR1)\n{\nstruct inet_connection_sock *VAR2 = FUN2(VAR1);\nstruct tls_context *VAR3;\nint VAR4 = 0;\nif (VAR1->VAR5 != VAR6)\nreturn -VAR7;\nVAR3 = FUN3(sizeof(*VAR3), VAR8);\nif (!VAR3) {\nVAR4 = -VAR9;\ngoto VAR10;\n}\nVAR2->VAR11 = VAR3;\nVAR3->VAR12 = VAR1->VAR13->VAR12;\nVAR3->VAR14 = VAR1->VAR13->VAR14;\nVAR3->VAR15 = VAR1->VAR13->VAR16;\nVAR3->VAR17 = VAR18;\nFUN4(VAR1, VAR3);\nVAR10:\nreturn VAR4;\n}\n",
      "code_after_change_raw": "static int tls_init(struct sock *sk)\n{\nint ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;\nstruct inet_connection_sock *icsk = inet_csk(sk);\nstruct tls_context *ctx;\nint rc = 0;\nif (sk->sk_state != TCP_ESTABLISHED)\nreturn -ENOTSUPP;\nctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\nif (!ctx) {\nrc = -ENOMEM;\ngoto out;\n}\nicsk->icsk_ulp_data = ctx;\nctx->setsockopt = sk->sk_prot->setsockopt;\nctx->getsockopt = sk->sk_prot->getsockopt;\nctx->sk_proto_close = sk->sk_prot->close;\nif (ip_ver == TLSV6 &&\nunlikely(sk->sk_prot != smp_load_acquire(&saved_tcpv6_prot))) {\nmutex_lock(&tcpv6_prot_mutex);\nif (likely(sk->sk_prot != saved_tcpv6_prot)) {\nbuild_protos(tls_prots[TLSV6], sk->sk_prot);\nsmp_store_release(&saved_tcpv6_prot, sk->sk_prot);\n}\nmutex_unlock(&tcpv6_prot_mutex);\n}\nctx->tx_conf = TLS_BASE_TX;\nupdate_sk_prot(sk, ctx);\nout:\nreturn rc;\n}\n",
      "code_before_change_raw": "static int tls_init(struct sock *sk)\n{\nstruct inet_connection_sock *icsk = inet_csk(sk);\nstruct tls_context *ctx;\nint rc = 0;\nif (sk->sk_state != TCP_ESTABLISHED)\nreturn -ENOTSUPP;\nctx = kzalloc(sizeof(*ctx), GFP_KERNEL);\nif (!ctx) {\nrc = -ENOMEM;\ngoto out;\n}\nicsk->icsk_ulp_data = ctx;\nctx->setsockopt = sk->sk_prot->setsockopt;\nctx->getsockopt = sk->sk_prot->getsockopt;\nctx->sk_proto_close = sk->sk_prot->close;\nctx->tx_conf = TLS_BASE_TX;\nupdate_sk_prot(sk, ctx);\nout:\nreturn rc;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Directly indexing an array without proper bounds checking based on external input.",
        "trigger_condition": "An attacker provides a malicious input value for the tx_conf field of the tls_context structure, causing an out-of-bounds write when updating the sk_prot field of the socket structure.",
        "specific_code_behavior_causing_vulnerability": "The code snippet directly indexes the tls_prots array using the tx_conf field without validating the input, potentially allowing an attacker to specify an invalid index and trigger an out-of-bounds write vulnerability."
      },
      "solution": "To mitigate the vulnerability, the code should be modified to determine the appropriate index based on the IP version of the socket and then use that index to access the tls_prots array. This ensures that the index is within bounds and prevents potential out-of-bounds writes. In this case, the solution involves calculating the IP version and selecting the correct index to access the tls_prots array based on that calculation.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-5703 by ensuring that the correct TLS protocol version is selected based on the IP version of the socket. \n\nIn the original code snippet, the vulnerability arises from directly indexing the tls_prots array using the tx_conf field of the tls_context structure. This can lead to an out-of-bounds write if the tx_conf value is not within the expected range.\n\nBy modifying the code to determine the IP version (IPv4 or IPv6) and then selecting the appropriate TLS protocol version based on that, the vulnerability is mitigated. This modification ensures that the correct index is used to access the tls_prots array, preventing any potential out-of-bounds writes and improving the overall security of the code.",
      "GPT_purpose": "Update the protocol of a socket with a TLS context.",
      "GPT_function": "\n1. Updates the sk_prot field of a socket structure with a TLS protocol based on the tx_conf field of a tls_context structure.",
      "CVE_id": "CVE-2018-5703",
      "code_before_change": "static inline void update_sk_prot(struct sock *sk, struct tls_context *ctx)\n{\n\tsk->sk_prot = &tls_prots[ctx->tx_conf];\n}",
      "code_after_change": "static inline void update_sk_prot(struct sock *sk, struct tls_context *ctx)\n{\n\tint ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;\n\n\tsk->sk_prot = &tls_prots[ip_ver][ctx->tx_conf];\n}",
      "modified_lines": {
        "added": [
          "\tint ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;",
          "",
          "\tsk->sk_prot = &tls_prots[ip_ver][ctx->tx_conf];"
        ],
        "deleted": [
          "\tsk->sk_prot = &tls_prots[ctx->tx_conf];"
        ]
      },
      "preconditions_for_vulnerability": "Directly indexing an array without proper bounds checking based on external input.",
      "trigger_condition": "An attacker provides a malicious input value for the tx_conf field of the tls_context structure, causing an out-of-bounds write when updating the sk_prot field of the socket structure.",
      "specific_code_behavior_causing_vulnerability": "The code snippet directly indexes the tls_prots array using the tx_conf field without validating the input, potentially allowing an attacker to specify an invalid index and trigger an out-of-bounds write vulnerability.",
      "id": 38,
      "code_after_change_normalized": "static inline void FUN1(struct sock *VAR1, struct tls_context *VAR2)\n{\nint VAR3 = VAR1->VAR4 == VAR5 ? VAR6 : VAR7;\nVAR1->VAR8 = &VAR9[VAR3][VAR2->VAR10];\n}\n",
      "code_before_change_normalized": "static inline void FUN1(struct sock *VAR1, struct tls_context *VAR2)\n{\nVAR1->VAR3 = &VAR4[VAR2->VAR5];\n}\n",
      "code_after_change_raw": "static inline void update_sk_prot(struct sock *sk, struct tls_context *ctx)\n{\nint ip_ver = sk->sk_family == AF_INET6 ? TLSV6 : TLSV4;\nsk->sk_prot = &tls_prots[ip_ver][ctx->tx_conf];\n}\n",
      "code_before_change_raw": "static inline void update_sk_prot(struct sock *sk, struct tls_context *ctx)\n{\nsk->sk_prot = &tls_prots[ctx->tx_conf];\n}\n"
    }
  ],
  "CVE-2018-9385": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when copying data into a buffer, specifically in the driver_override_store function.",
        "trigger_condition": "An input data size larger than the allocated buffer size is provided, leading to a possible out-of-bounds write operation.",
        "specific_code_behavior_causing_vulnerability": "The code does not adequately check if the input data size exceeds the buffer size, allowing for a potential buffer overflow vulnerability.",
        "solution": "To mitigate the vulnerability, it is necessary to include a proper bounds check to ensure that the input data size does not exceed the allocated buffer size. This prevents out-of-bounds write operations and enhances the security of the code. In this case, the solution involves adding a check to verify that the input data size is within the acceptable limits before copying it into the buffer."
      },
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2018-9385 in the code snippet. The vulnerability arises from an incorrect bounds check in the original code, which allows for a possible out-of-bounds write operation. By modifying the code to include the check `if (count >= (PAGE_SIZE - 1))`, the corrected code ensures that there is enough room for the data being copied into the `driver_override` buffer, preventing the out-of-bounds write issue.\n\nIn the corrected code snippet, the check `if (count >= (PAGE_SIZE - 1))` ensures that the `driver_override` buffer has enough space to accommodate the data being copied from `buf`. This check helps prevent buffer overflow vulnerabilities and ensures that the operation stays within the bounds of the allocated memory.\n\nBy making this modification, the code is more secure and less susceptible to exploitation, addressing the vulnerability described in CVE-2018-9385.",
      "GPT_purpose": "Store a driver override for a device in the Linux kernel.",
      "GPT_function": "\n1. Store a driver override for an AMBA device.\n2. Check for the length of the input buffer.\n3. Update the driver override for the device and handle memory allocation.",
      "CVE_id": "CVE-2018-9385",
      "code_before_change": "static ssize_t driver_override_store(struct device *_dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct amba_device *dev = to_amba_device(_dev);\n\tchar *driver_override, *old, *cp;\n\n\tif (count > PATH_MAX)\n\t\treturn -EINVAL;\n\n\tdriver_override = kstrndup(buf, count, GFP_KERNEL);\n\tif (!driver_override)\n\t\treturn -ENOMEM;\n\n\tcp = strchr(driver_override, '\\n');\n\tif (cp)\n\t\t*cp = '\\0';\n\n\tdevice_lock(_dev);\n\told = dev->driver_override;\n\tif (strlen(driver_override)) {\n\t\tdev->driver_override = driver_override;\n\t} else {\n\t       kfree(driver_override);\n\t       dev->driver_override = NULL;\n\t}\n\tdevice_unlock(_dev);\n\n\tkfree(old);\n\n\treturn count;\n}",
      "code_after_change": "static ssize_t driver_override_store(struct device *_dev,\n\t\t\t\t     struct device_attribute *attr,\n\t\t\t\t     const char *buf, size_t count)\n{\n\tstruct amba_device *dev = to_amba_device(_dev);\n\tchar *driver_override, *old, *cp;\n\n\t/* We need to keep extra room for a newline */\n\tif (count >= (PAGE_SIZE - 1))\n\t\treturn -EINVAL;\n\n\tdriver_override = kstrndup(buf, count, GFP_KERNEL);\n\tif (!driver_override)\n\t\treturn -ENOMEM;\n\n\tcp = strchr(driver_override, '\\n');\n\tif (cp)\n\t\t*cp = '\\0';\n\n\tdevice_lock(_dev);\n\told = dev->driver_override;\n\tif (strlen(driver_override)) {\n\t\tdev->driver_override = driver_override;\n\t} else {\n\t       kfree(driver_override);\n\t       dev->driver_override = NULL;\n\t}\n\tdevice_unlock(_dev);\n\n\tkfree(old);\n\n\treturn count;\n}",
      "modified_lines": {
        "added": [
          "\t/* We need to keep extra room for a newline */",
          "\tif (count >= (PAGE_SIZE - 1))"
        ],
        "deleted": [
          "\tif (count > PATH_MAX)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when copying data into a buffer, specifically in the driver_override_store function.",
      "trigger_condition": "An input data size larger than the allocated buffer size is provided, leading to a possible out-of-bounds write operation.",
      "specific_code_behavior_causing_vulnerability": "The code does not adequately check if the input data size exceeds the buffer size, allowing for a potential buffer overflow vulnerability.",
      "solution": "To mitigate the vulnerability, it is necessary to include a proper bounds check to ensure that the input data size does not exceed the allocated buffer size. This prevents out-of-bounds write operations and enhances the security of the code. In this case, the solution involves adding a check to verify that the input data size is within the acceptable limits before copying it into the buffer.",
      "id": 39,
      "code_after_change_normalized": "static ssize_t FUN1(struct device *VAR1,\nstruct device_attribute *VAR2,\nconst char *VAR3, size_t VAR4)\n{\nstruct amba_device *VAR5 = FUN2(VAR1);\nchar *VAR6, *VAR7, *VAR8;\nif (VAR4 >= (VAR9 - 1))\nreturn -VAR10;\nVAR6 = FUN3(VAR3, VAR4, VAR11);\nif (!VAR6)\nreturn -VAR12;\nVAR8 = FUN4(VAR6, );\nif (VAR8)\n*VAR8 = ;\nFUN5(VAR1);\nVAR7 = VAR5->VAR6;\nif (FUN6(VAR6)) {\nVAR5->VAR6 = VAR6;\n} else {\nFUN7(VAR6);\nVAR5->VAR6 = NULL;\n}\nFUN8(VAR1);\nFUN7(VAR7);\nreturn VAR4;\n}\n",
      "code_before_change_normalized": "static ssize_t FUN1(struct device *VAR1,\nstruct device_attribute *VAR2,\nconst char *VAR3, size_t VAR4)\n{\nstruct amba_device *VAR5 = FUN2(VAR1);\nchar *VAR6, *VAR7, *VAR8;\nif (VAR4 > VAR9)\nreturn -VAR10;\nVAR6 = FUN3(VAR3, VAR4, VAR11);\nif (!VAR6)\nreturn -VAR12;\nVAR8 = FUN4(VAR6, );\nif (VAR8)\n*VAR8 = ;\nFUN5(VAR1);\nVAR7 = VAR5->VAR6;\nif (FUN6(VAR6)) {\nVAR5->VAR6 = VAR6;\n} else {\nFUN7(VAR6);\nVAR5->VAR6 = NULL;\n}\nFUN8(VAR1);\nFUN7(VAR7);\nreturn VAR4;\n}\n",
      "code_after_change_raw": "static ssize_t driver_override_store(struct device *_dev,\nstruct device_attribute *attr,\nconst char *buf, size_t count)\n{\nstruct amba_device *dev = to_amba_device(_dev);\nchar *driver_override, *old, *cp;\nif (count >= (PAGE_SIZE - 1))\nreturn -EINVAL;\ndriver_override = kstrndup(buf, count, GFP_KERNEL);\nif (!driver_override)\nreturn -ENOMEM;\ncp = strchr(driver_override, '\\n');\nif (cp)\n*cp = '\\0';\ndevice_lock(_dev);\nold = dev->driver_override;\nif (strlen(driver_override)) {\ndev->driver_override = driver_override;\n} else {\nkfree(driver_override);\ndev->driver_override = NULL;\n}\ndevice_unlock(_dev);\nkfree(old);\nreturn count;\n}\n",
      "code_before_change_raw": "static ssize_t driver_override_store(struct device *_dev,\nstruct device_attribute *attr,\nconst char *buf, size_t count)\n{\nstruct amba_device *dev = to_amba_device(_dev);\nchar *driver_override, *old, *cp;\nif (count > PATH_MAX)\nreturn -EINVAL;\ndriver_override = kstrndup(buf, count, GFP_KERNEL);\nif (!driver_override)\nreturn -ENOMEM;\ncp = strchr(driver_override, '\\n');\nif (cp)\n*cp = '\\0';\ndevice_lock(_dev);\nold = dev->driver_override;\nif (strlen(driver_override)) {\ndev->driver_override = driver_override;\n} else {\nkfree(driver_override);\ndev->driver_override = NULL;\n}\ndevice_unlock(_dev);\nkfree(old);\nreturn count;\n}\n"
    }
  ],
  "CVE-2018-9518": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of bounds checking when calculating the length of a TLV structure based on user-provided input.",
        "trigger_condition": "Passing a large value for the 'uri_len' parameter without proper validation, leading to a potential out-of-bounds write.",
        "specific_code_behavior_causing_vulnerability": "The code calculates 'sdreq->tlv_len' without verifying that the sum of 'uri_len' and other constants does not exceed the maximum value that can be stored in a 'u8' data type. This oversight can result in a buffer overflow or out-of-bounds write vulnerability."
      },
      "solution": "To mitigate the vulnerability, a bounds check should be added to ensure that the calculated 'sdreq->tlv_len' value does not exceed the maximum value representable by a 'u8' data type. By adding the check 'if (WARN_ON_ONCE(uri_len > U8_MAX - 4))', the code prevents potential buffer overflows and out-of-bounds writes that could lead to security vulnerabilities. This solution enhances the security of the code by validating user input and preventing the calculation of invalid TLV lengths.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2018-9518, which involves a possible out-of-bounds write due to a missing bounds check in the original code snippet. Specifically, the vulnerability arises from the calculation of `sdreq->tlv_len` without checking if it exceeds the maximum value that can be stored in a `u8` data type.\n\nBy adding the check `if (WARN_ON_ONCE(uri_len > U8_MAX - 4))`, the modified code ensures that the `uri_len` value does not exceed the maximum value that can be represented by an unsigned 8-bit integer (`u8`). This prevents potential buffer overflows or out-of-bounds writes that could lead to security vulnerabilities, such as local escalation of privilege as described in the vulnerability report.\n\nIn summary, the modification adds a bounds check to prevent the `sdreq->tlv_len` value from exceeding the maximum allowed value for a `u8` data type, thus addressing the vulnerability and enhancing the security of the code.",
      "GPT_purpose": "Build a Service Discovery Request TLV structure for NFC LLCP communication.",
      "GPT_function": "\n1. Build a Service Discovery Request TLV structure for NFC LLCP.\n2. Allocate memory for the TLV structure and initialize its fields.\n3. Copy the provided URI into the TLV structure.\n4. Initialize a timestamp for the TLV structure.",
      "CVE_id": "CVE-2018-9518",
      "code_before_change": "struct nfc_llcp_sdp_tlv *nfc_llcp_build_sdreq_tlv(u8 tid, char *uri,\n\t\t\t\t\t\t  size_t uri_len)\n{\n\tstruct nfc_llcp_sdp_tlv *sdreq;\n\n\tpr_debug(\"uri: %s, len: %zu\\n\", uri, uri_len);\n\n\tsdreq = kzalloc(sizeof(struct nfc_llcp_sdp_tlv), GFP_KERNEL);\n\tif (sdreq == NULL)\n\t\treturn NULL;\n\n\tsdreq->tlv_len = uri_len + 3;\n\n\tif (uri[uri_len - 1] == 0)\n\t\tsdreq->tlv_len--;\n\n\tsdreq->tlv = kzalloc(sdreq->tlv_len + 1, GFP_KERNEL);\n\tif (sdreq->tlv == NULL) {\n\t\tkfree(sdreq);\n\t\treturn NULL;\n\t}\n\n\tsdreq->tlv[0] = LLCP_TLV_SDREQ;\n\tsdreq->tlv[1] = sdreq->tlv_len - 2;\n\tsdreq->tlv[2] = tid;\n\n\tsdreq->tid = tid;\n\tsdreq->uri = sdreq->tlv + 3;\n\tmemcpy(sdreq->uri, uri, uri_len);\n\n\tsdreq->time = jiffies;\n\n\tINIT_HLIST_NODE(&sdreq->node);\n\n\treturn sdreq;\n}",
      "code_after_change": "struct nfc_llcp_sdp_tlv *nfc_llcp_build_sdreq_tlv(u8 tid, char *uri,\n\t\t\t\t\t\t  size_t uri_len)\n{\n\tstruct nfc_llcp_sdp_tlv *sdreq;\n\n\tpr_debug(\"uri: %s, len: %zu\\n\", uri, uri_len);\n\n\t/* sdreq->tlv_len is u8, takes uri_len, + 3 for header, + 1 for NULL */\n\tif (WARN_ON_ONCE(uri_len > U8_MAX - 4))\n\t\treturn NULL;\n\n\tsdreq = kzalloc(sizeof(struct nfc_llcp_sdp_tlv), GFP_KERNEL);\n\tif (sdreq == NULL)\n\t\treturn NULL;\n\n\tsdreq->tlv_len = uri_len + 3;\n\n\tif (uri[uri_len - 1] == 0)\n\t\tsdreq->tlv_len--;\n\n\tsdreq->tlv = kzalloc(sdreq->tlv_len + 1, GFP_KERNEL);\n\tif (sdreq->tlv == NULL) {\n\t\tkfree(sdreq);\n\t\treturn NULL;\n\t}\n\n\tsdreq->tlv[0] = LLCP_TLV_SDREQ;\n\tsdreq->tlv[1] = sdreq->tlv_len - 2;\n\tsdreq->tlv[2] = tid;\n\n\tsdreq->tid = tid;\n\tsdreq->uri = sdreq->tlv + 3;\n\tmemcpy(sdreq->uri, uri, uri_len);\n\n\tsdreq->time = jiffies;\n\n\tINIT_HLIST_NODE(&sdreq->node);\n\n\treturn sdreq;\n}",
      "modified_lines": {
        "added": [
          "",
          "\t/* sdreq->tlv_len is u8, takes uri_len, + 3 for header, + 1 for NULL */",
          "\tif (WARN_ON_ONCE(uri_len > U8_MAX - 4))",
          "\t\treturn NULL;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of bounds checking when calculating the length of a TLV structure based on user-provided input.",
      "trigger_condition": "Passing a large value for the 'uri_len' parameter without proper validation, leading to a potential out-of-bounds write.",
      "specific_code_behavior_causing_vulnerability": "The code calculates 'sdreq->tlv_len' without verifying that the sum of 'uri_len' and other constants does not exceed the maximum value that can be stored in a 'u8' data type. This oversight can result in a buffer overflow or out-of-bounds write vulnerability.",
      "id": 40,
      "code_after_change_normalized": "struct nfc_llcp_sdp_tlv *FUN1(u8 VAR1, char *VAR2,\nsize_t VAR3)\n{\nstruct nfc_llcp_sdp_tlv *VAR4;\nFUN2(\"STR\", VAR2, VAR3);\nif (FUN3(VAR3 > VAR5 - 4))\nreturn NULL;\nVAR4 = FUN4(sizeof(struct VAR6), VAR7);\nif (VAR4 == NULL)\nreturn NULL;\nVAR4->VAR8 = VAR3 + 3;\nif (VAR2[VAR3 - 1] == 0)\nVAR4->VAR8--;\nVAR4->VAR9 = FUN4(VAR4->VAR8 + 1, VAR7);\nif (VAR4->VAR9 == NULL) {\nFUN5(VAR4);\nreturn NULL;\n}\nVAR4->VAR9[0] = VAR10;\nVAR4->VAR9[1] = VAR4->VAR8 - 2;\nVAR4->VAR9[2] = VAR1;\nVAR4->VAR1 = VAR1;\nVAR4->VAR2 = VAR4->VAR9 + 3;\nFUN6(VAR4->VAR2, VAR2, VAR3);\nVAR4->VAR11 = VAR12;\nFUN7(&VAR4->VAR13);\nreturn VAR4;\n}\n",
      "code_before_change_normalized": "struct nfc_llcp_sdp_tlv *FUN1(u8 VAR1, char *VAR2,\nsize_t VAR3)\n{\nstruct nfc_llcp_sdp_tlv *VAR4;\nFUN2(\"STR\", VAR2, VAR3);\nVAR4 = FUN3(sizeof(struct VAR5), VAR6);\nif (VAR4 == NULL)\nreturn NULL;\nVAR4->VAR7 = VAR3 + 3;\nif (VAR2[VAR3 - 1] == 0)\nVAR4->VAR7--;\nVAR4->VAR8 = FUN3(VAR4->VAR7 + 1, VAR6);\nif (VAR4->VAR8 == NULL) {\nFUN4(VAR4);\nreturn NULL;\n}\nVAR4->VAR8[0] = VAR9;\nVAR4->VAR8[1] = VAR4->VAR7 - 2;\nVAR4->VAR8[2] = VAR1;\nVAR4->VAR1 = VAR1;\nVAR4->VAR2 = VAR4->VAR8 + 3;\nFUN5(VAR4->VAR2, VAR2, VAR3);\nVAR4->VAR10 = VAR11;\nFUN6(&VAR4->VAR12);\nreturn VAR4;\n}\n",
      "code_after_change_raw": "struct nfc_llcp_sdp_tlv *nfc_llcp_build_sdreq_tlv(u8 tid, char *uri,\nsize_t uri_len)\n{\nstruct nfc_llcp_sdp_tlv *sdreq;\npr_debug(\"uri: %s, len: %zu\\n\", uri, uri_len);\nif (WARN_ON_ONCE(uri_len > U8_MAX - 4))\nreturn NULL;\nsdreq = kzalloc(sizeof(struct nfc_llcp_sdp_tlv), GFP_KERNEL);\nif (sdreq == NULL)\nreturn NULL;\nsdreq->tlv_len = uri_len + 3;\nif (uri[uri_len - 1] == 0)\nsdreq->tlv_len--;\nsdreq->tlv = kzalloc(sdreq->tlv_len + 1, GFP_KERNEL);\nif (sdreq->tlv == NULL) {\nkfree(sdreq);\nreturn NULL;\n}\nsdreq->tlv[0] = LLCP_TLV_SDREQ;\nsdreq->tlv[1] = sdreq->tlv_len - 2;\nsdreq->tlv[2] = tid;\nsdreq->tid = tid;\nsdreq->uri = sdreq->tlv + 3;\nmemcpy(sdreq->uri, uri, uri_len);\nsdreq->time = jiffies;\nINIT_HLIST_NODE(&sdreq->node);\nreturn sdreq;\n}\n",
      "code_before_change_raw": "struct nfc_llcp_sdp_tlv *nfc_llcp_build_sdreq_tlv(u8 tid, char *uri,\nsize_t uri_len)\n{\nstruct nfc_llcp_sdp_tlv *sdreq;\npr_debug(\"uri: %s, len: %zu\\n\", uri, uri_len);\nsdreq = kzalloc(sizeof(struct nfc_llcp_sdp_tlv), GFP_KERNEL);\nif (sdreq == NULL)\nreturn NULL;\nsdreq->tlv_len = uri_len + 3;\nif (uri[uri_len - 1] == 0)\nsdreq->tlv_len--;\nsdreq->tlv = kzalloc(sdreq->tlv_len + 1, GFP_KERNEL);\nif (sdreq->tlv == NULL) {\nkfree(sdreq);\nreturn NULL;\n}\nsdreq->tlv[0] = LLCP_TLV_SDREQ;\nsdreq->tlv[1] = sdreq->tlv_len - 2;\nsdreq->tlv[2] = tid;\nsdreq->tid = tid;\nsdreq->uri = sdreq->tlv + 3;\nmemcpy(sdreq->uri, uri, uri_len);\nsdreq->time = jiffies;\nINIT_HLIST_NODE(&sdreq->node);\nreturn sdreq;\n}\n"
    }
  ],
  "CVE-2019-11683": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Mishandling of padded UDP packets with a payload size of 0, leading to a slab-out-of-bounds memory corruption.",
        "trigger_condition": "Receiving UDP packets with a payload size of 0, causing the code to process padded packets incorrectly.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly validate the UDP packet length, allowing for the mishandling of padded packets and potential memory corruption. This vulnerability arises from the lack of checks on the UDP packet length, leading to processing errors when dealing with zero-sized payloads."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the UDP packet length is valid and not less than the size of the UDP header or different from the actual length of the received packet. This check prevents the processing of potentially malicious or padded packets with zero-sized payloads, thereby addressing the vulnerability. In this case, the solution involves introducing a validation step for the UDP packet length before further processing.",
      "GPT_analysis": "The modification is necessary to address the vulnerability known as the \"GRO packet of death\" issue (CVE-2019-11683) in the Linux kernel. This vulnerability allows remote attackers to cause a denial of service or potentially have other impacts via UDP packets with a payload size of 0, due to mishandling of padded packets.\n\nThe modification adds the following lines to the code snippet:\n```c\nulen = ntohs(uh->len);\nif (ulen <= sizeof(*uh) || ulen != skb_gro_len(skb)) {\n    NAPI_GRO_CB(skb)->flush = 1;\n    return NULL;\n}\n```\nThese lines introduce a check to ensure that the UDP packet length (ulen) is valid and not less than the size of the UDP header or different from the actual length of the received packet. If the length check fails, the function sets the flush flag and returns NULL, effectively preventing the processing of potentially malicious or padded packets that could trigger the vulnerability.\n\nBy adding these lines, the code is strengthened to handle incoming UDP packets more securely, mitigating the risk of the \"GRO packet of death\" vulnerability being exploited.",
      "GPT_purpose": "Receive and process UDP packets for Generic Receive Offload (GRO) segmentation.",
      "GPT_function": "\n1. Receive and process UDP packets for Generic Receive Offload (GRO).\n2. Check for a non-zero checksum in the UDP header for symmetry with GSO.\n3. Pull the encapsulating UDP header and perform post-pull receive checksum verification.\n4. Match ports of incoming UDP packets to determine if they belong to the same flow.\n5. Terminate the flow based on length mismatch or excessive growth of GRO count.\n6. Handle the case of a mismatch in UDP packets without needing to flush.",
      "CVE_id": "CVE-2019-11683",
      "code_before_change": "static struct sk_buff *udp_gro_receive_segment(struct list_head *head,\n\t\t\t\t\t       struct sk_buff *skb)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *pp = NULL;\n\tstruct udphdr *uh2;\n\tstruct sk_buff *p;\n\n\t/* requires non zero csum, for symmetry with GSO */\n\tif (!uh->check) {\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\t/* pull encapsulating udp header */\n\tskb_gro_pull(skb, sizeof(struct udphdr));\n\tskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\n\n\tlist_for_each_entry(p, head, list) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tuh2 = udp_hdr(p);\n\n\t\t/* Match ports only, as csum is always non zero */\n\t\tif ((*(u32 *)&uh->source != *(u32 *)&uh2->source)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Terminate the flow on len mismatch or if it grow \"too much\".\n\t\t * Under small packet flood GRO count could elsewhere grow a lot\n\t\t * leading to execessive truesize values.\n\t\t * On len mismatch merge the first packet shorter than gso_size,\n\t\t * otherwise complete the GRO packet.\n\t\t */\n\t\tif (uh->len > uh2->len || skb_gro_receive(p, skb) ||\n\t\t    uh->len != uh2->len ||\n\t\t    NAPI_GRO_CB(p)->count >= UDP_GRO_CNT_MAX)\n\t\t\tpp = p;\n\n\t\treturn pp;\n\t}\n\n\t/* mismatch, but we never need to flush */\n\treturn NULL;\n}",
      "code_after_change": "static struct sk_buff *udp_gro_receive_segment(struct list_head *head,\n\t\t\t\t\t       struct sk_buff *skb)\n{\n\tstruct udphdr *uh = udp_hdr(skb);\n\tstruct sk_buff *pp = NULL;\n\tstruct udphdr *uh2;\n\tstruct sk_buff *p;\n\tunsigned int ulen;\n\n\t/* requires non zero csum, for symmetry with GSO */\n\tif (!uh->check) {\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\n\t/* Do not deal with padded or malicious packets, sorry ! */\n\tulen = ntohs(uh->len);\n\tif (ulen <= sizeof(*uh) || ulen != skb_gro_len(skb)) {\n\t\tNAPI_GRO_CB(skb)->flush = 1;\n\t\treturn NULL;\n\t}\n\t/* pull encapsulating udp header */\n\tskb_gro_pull(skb, sizeof(struct udphdr));\n\tskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\n\n\tlist_for_each_entry(p, head, list) {\n\t\tif (!NAPI_GRO_CB(p)->same_flow)\n\t\t\tcontinue;\n\n\t\tuh2 = udp_hdr(p);\n\n\t\t/* Match ports only, as csum is always non zero */\n\t\tif ((*(u32 *)&uh->source != *(u32 *)&uh2->source)) {\n\t\t\tNAPI_GRO_CB(p)->same_flow = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\t/* Terminate the flow on len mismatch or if it grow \"too much\".\n\t\t * Under small packet flood GRO count could elsewhere grow a lot\n\t\t * leading to excessive truesize values.\n\t\t * On len mismatch merge the first packet shorter than gso_size,\n\t\t * otherwise complete the GRO packet.\n\t\t */\n\t\tif (ulen > ntohs(uh2->len) || skb_gro_receive(p, skb) ||\n\t\t    ulen != ntohs(uh2->len) ||\n\t\t    NAPI_GRO_CB(p)->count >= UDP_GRO_CNT_MAX)\n\t\t\tpp = p;\n\n\t\treturn pp;\n\t}\n\n\t/* mismatch, but we never need to flush */\n\treturn NULL;\n}",
      "modified_lines": {
        "added": [
          "\tunsigned int ulen;",
          "\t/* Do not deal with padded or malicious packets, sorry ! */",
          "\tulen = ntohs(uh->len);",
          "\tif (ulen <= sizeof(*uh) || ulen != skb_gro_len(skb)) {",
          "\t\tNAPI_GRO_CB(skb)->flush = 1;",
          "\t\treturn NULL;",
          "\t}",
          "\t\t * leading to excessive truesize values.",
          "\t\tif (ulen > ntohs(uh2->len) || skb_gro_receive(p, skb) ||",
          "\t\t    ulen != ntohs(uh2->len) ||"
        ],
        "deleted": [
          "\t\t * leading to execessive truesize values.",
          "\t\tif (uh->len > uh2->len || skb_gro_receive(p, skb) ||",
          "\t\t    uh->len != uh2->len ||"
        ]
      },
      "preconditions_for_vulnerability": "Mishandling of padded UDP packets with a payload size of 0, leading to a slab-out-of-bounds memory corruption.",
      "trigger_condition": "Receiving UDP packets with a payload size of 0, causing the code to process padded packets incorrectly.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly validate the UDP packet length, allowing for the mishandling of padded packets and potential memory corruption. This vulnerability arises from the lack of checks on the UDP packet length, leading to processing errors when dealing with zero-sized payloads.",
      "id": 41,
      "code_after_change_normalized": "static struct sk_buff *FUN1(struct list_head *VAR1,\nstruct sk_buff *VAR2)\n{\nstruct udphdr *VAR3 = FUN2(VAR2);\nstruct sk_buff *VAR4 = NULL;\nstruct udphdr *VAR5;\nstruct sk_buff *VAR6;\nunsigned int VAR7;\nif (!VAR3->VAR8) {\nFUN3(VAR2)->VAR9 = 1;\nreturn NULL;\n}\nVAR7 = FUN4(VAR3->VAR10);\nif (VAR7 <= sizeof(*VAR3) || VAR7 != FUN5(VAR2)) {\nFUN3(VAR2)->VAR9 = 1;\nreturn NULL;\n}\nFUN6(VAR2, sizeof(struct VAR11));\nFUN7(VAR2, VAR3, sizeof(struct VAR11));\nFUN8(VAR6, VAR1, VAR12) {\nif (!FUN3(VAR6)->VAR13)\ncontinue;\nVAR5 = FUN2(VAR6);\nif ((*(VAR14 *)&VAR3->VAR15 != *(VAR14 *)&VAR5->VAR15)) {\nFUN3(VAR6)->VAR13 = 0;\ncontinue;\n}\nif (VAR7 > FUN4(VAR5->VAR10) || FUN9(VAR6, VAR2) ||\nVAR7 != FUN4(VAR5->VAR10) ||\nFUN3(VAR6)->VAR16 >= VAR17)\nVAR4 = VAR6;\nreturn VAR4;\n}\nreturn NULL;\n}\n",
      "code_before_change_normalized": "static struct sk_buff *FUN1(struct list_head *VAR1,\nstruct sk_buff *VAR2)\n{\nstruct udphdr *VAR3 = FUN2(VAR2);\nstruct sk_buff *VAR4 = NULL;\nstruct udphdr *VAR5;\nstruct sk_buff *VAR6;\nif (!VAR3->VAR7) {\nFUN3(VAR2)->VAR8 = 1;\nreturn NULL;\n}\nFUN4(VAR2, sizeof(struct VAR9));\nFUN5(VAR2, VAR3, sizeof(struct VAR9));\nFUN6(VAR6, VAR1, VAR10) {\nif (!FUN3(VAR6)->VAR11)\ncontinue;\nVAR5 = FUN2(VAR6);\nif ((*(VAR12 *)&VAR3->VAR13 != *(VAR12 *)&VAR5->VAR13)) {\nFUN3(VAR6)->VAR11 = 0;\ncontinue;\n}\nif (VAR3->VAR14 > VAR5->VAR14 || FUN7(VAR6, VAR2) ||\nVAR3->VAR14 != VAR5->VAR14 ||\nFUN3(VAR6)->VAR15 >= VAR16)\nVAR4 = VAR6;\nreturn VAR4;\n}\nreturn NULL;\n}\n",
      "code_after_change_raw": "static struct sk_buff *udp_gro_receive_segment(struct list_head *head,\nstruct sk_buff *skb)\n{\nstruct udphdr *uh = udp_hdr(skb);\nstruct sk_buff *pp = NULL;\nstruct udphdr *uh2;\nstruct sk_buff *p;\nunsigned int ulen;\nif (!uh->check) {\nNAPI_GRO_CB(skb)->flush = 1;\nreturn NULL;\n}\nulen = ntohs(uh->len);\nif (ulen <= sizeof(*uh) || ulen != skb_gro_len(skb)) {\nNAPI_GRO_CB(skb)->flush = 1;\nreturn NULL;\n}\nskb_gro_pull(skb, sizeof(struct udphdr));\nskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\nlist_for_each_entry(p, head, list) {\nif (!NAPI_GRO_CB(p)->same_flow)\ncontinue;\nuh2 = udp_hdr(p);\nif ((*(u32 *)&uh->source != *(u32 *)&uh2->source)) {\nNAPI_GRO_CB(p)->same_flow = 0;\ncontinue;\n}\nif (ulen > ntohs(uh2->len) || skb_gro_receive(p, skb) ||\nulen != ntohs(uh2->len) ||\nNAPI_GRO_CB(p)->count >= UDP_GRO_CNT_MAX)\npp = p;\nreturn pp;\n}\nreturn NULL;\n}\n",
      "code_before_change_raw": "static struct sk_buff *udp_gro_receive_segment(struct list_head *head,\nstruct sk_buff *skb)\n{\nstruct udphdr *uh = udp_hdr(skb);\nstruct sk_buff *pp = NULL;\nstruct udphdr *uh2;\nstruct sk_buff *p;\nif (!uh->check) {\nNAPI_GRO_CB(skb)->flush = 1;\nreturn NULL;\n}\nskb_gro_pull(skb, sizeof(struct udphdr));\nskb_gro_postpull_rcsum(skb, uh, sizeof(struct udphdr));\nlist_for_each_entry(p, head, list) {\nif (!NAPI_GRO_CB(p)->same_flow)\ncontinue;\nuh2 = udp_hdr(p);\nif ((*(u32 *)&uh->source != *(u32 *)&uh2->source)) {\nNAPI_GRO_CB(p)->same_flow = 0;\ncontinue;\n}\nif (uh->len > uh2->len || skb_gro_receive(p, skb) ||\nuh->len != uh2->len ||\nNAPI_GRO_CB(p)->count >= UDP_GRO_CNT_MAX)\npp = p;\nreturn pp;\n}\nreturn NULL;\n}\n"
    }
  ],
  "CVE-2019-12817": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of context IDs during memory allocation process.",
        "trigger_condition": "An mmap operation above 512 TB is performed on certain powerpc systems, allowing unrelated processes to read/write to each other's virtual memory.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly reallocate context IDs, leading to a security vulnerability where unrelated processes can access each other's memory under specific conditions."
      },
      "solution": "To mitigate the vulnerability, it is necessary to reallocate context IDs properly to ensure that context slices are handled correctly. This prevents unauthorized memory access between unrelated processes. The solution involves adding a line to reallocate context IDs before allocating a new context ID, enhancing the security of the code and preventing the vulnerability from being exploited.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-12817 in the code snippet. The vulnerability allows unrelated processes to read/write to each other's virtual memory under certain conditions via an mmap above 512 TB on certain powerpc systems.\n\nThe modification involves adding the line `index = realloc_context_ids(&mm->context);` before the original code that allocates a new context ID. This change ensures that the context IDs are reallocated properly, which helps prevent the vulnerability from being exploited.\n\nBy reallocating the context IDs, the code is updated to handle the context slices correctly and avoid the security issue related to memory access between unrelated processes. This modification enhances the security of the code and mitigates the risk of unauthorized memory access.",
      "GPT_purpose": "Initialize a new memory context for a given mm_struct.",
      "GPT_function": "\n1. Initialize a new context for a memory management structure.\n2. Allocate a context ID for the new context.\n3. Check if the context ID is 0 and initialize context slice details accordingly.\n4. Call functions to initialize subpage protection and process keys for the new context.",
      "CVE_id": "CVE-2019-12817",
      "code_before_change": "static int hash__init_new_context(struct mm_struct *mm)\n{\n\tint index;\n\n\tindex = hash__alloc_context_id();\n\tif (index < 0)\n\t\treturn index;\n\n\t/*\n\t * The old code would re-promote on fork, we don't do that when using\n\t * slices as it could cause problem promoting slices that have been\n\t * forced down to 4K.\n\t *\n\t * For book3s we have MMU_NO_CONTEXT set to be ~0. Hence check\n\t * explicitly against context.id == 0. This ensures that we properly\n\t * initialize context slice details for newly allocated mm's (which will\n\t * have id == 0) and don't alter context slice inherited via fork (which\n\t * will have id != 0).\n\t *\n\t * We should not be calling init_new_context() on init_mm. Hence a\n\t * check against 0 is OK.\n\t */\n\tif (mm->context.id == 0)\n\t\tslice_init_new_context_exec(mm);\n\n\tsubpage_prot_init_new_context(mm);\n\n\tpkey_mm_init(mm);\n\treturn index;\n}",
      "code_after_change": "static int hash__init_new_context(struct mm_struct *mm)\n{\n\tint index;\n\n\t/*\n\t * The old code would re-promote on fork, we don't do that when using\n\t * slices as it could cause problem promoting slices that have been\n\t * forced down to 4K.\n\t *\n\t * For book3s we have MMU_NO_CONTEXT set to be ~0. Hence check\n\t * explicitly against context.id == 0. This ensures that we properly\n\t * initialize context slice details for newly allocated mm's (which will\n\t * have id == 0) and don't alter context slice inherited via fork (which\n\t * will have id != 0).\n\t *\n\t * We should not be calling init_new_context() on init_mm. Hence a\n\t * check against 0 is OK.\n\t */\n\tif (mm->context.id == 0)\n\t\tslice_init_new_context_exec(mm);\n\n\tindex = realloc_context_ids(&mm->context);\n\tif (index < 0)\n\t\treturn index;\n\n\tsubpage_prot_init_new_context(mm);\n\n\tpkey_mm_init(mm);\n\treturn index;\n}",
      "modified_lines": {
        "added": [
          "\tindex = realloc_context_ids(&mm->context);",
          "\tif (index < 0)",
          "\t\treturn index;",
          ""
        ],
        "deleted": [
          "",
          "\tindex = hash__alloc_context_id();",
          "\tif (index < 0)",
          "\t\treturn index;"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of context IDs during memory allocation process.",
      "trigger_condition": "An mmap operation above 512 TB is performed on certain powerpc systems, allowing unrelated processes to read/write to each other's virtual memory.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly reallocate context IDs, leading to a security vulnerability where unrelated processes can access each other's memory under specific conditions.",
      "id": 42,
      "code_after_change_normalized": "static int FUN1(struct mm_struct *VAR1)\n{\nint VAR2;\nif (VAR1->VAR3.VAR4 == 0)\nFUN2(VAR1);\nVAR2 = FUN3(&VAR1->VAR3);\nif (VAR2 < 0)\nreturn VAR2;\nFUN4(VAR1);\nFUN5(VAR1);\nreturn VAR2;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct mm_struct *VAR1)\n{\nint VAR2;\nVAR2 = FUN2();\nif (VAR2 < 0)\nreturn VAR2;\nif (VAR1->VAR3.VAR4 == 0)\nFUN3(VAR1);\nFUN4(VAR1);\nFUN5(VAR1);\nreturn VAR2;\n}\n",
      "code_after_change_raw": "static int hash__init_new_context(struct mm_struct *mm)\n{\nint index;\nif (mm->context.id == 0)\nslice_init_new_context_exec(mm);\nindex = realloc_context_ids(&mm->context);\nif (index < 0)\nreturn index;\nsubpage_prot_init_new_context(mm);\npkey_mm_init(mm);\nreturn index;\n}\n",
      "code_before_change_raw": "static int hash__init_new_context(struct mm_struct *mm)\n{\nint index;\nindex = hash__alloc_context_id();\nif (index < 0)\nreturn index;\nif (mm->context.id == 0)\nslice_init_new_context_exec(mm);\nsubpage_prot_init_new_context(mm);\npkey_mm_init(mm);\nreturn index;\n}\n"
    }
  ],
  "CVE-2019-13631": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking for the indent variable in the TAG_MAIN_COL_START and TAG_MAIN_COL_END cases.",
        "trigger_condition": "A malicious USB device sends an HID report that triggers an out-of-bounds write during the generation of debugging messages.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the bounds of the indent variable before incrementing or decrementing it in the TAG_MAIN_COL_START and TAG_MAIN_COL_END cases, respectively. This can lead to an out-of-bounds write vulnerability if the indent variable exceeds the maximum collection levels allowed.",
        "solution": "To mitigate the vulnerability, it is necessary to add bounds checking for the indent variable in the TAG_MAIN_COL_START and TAG_MAIN_COL_END cases. By adding checks to ensure that the indent variable stays within the bounds of the maximum collection levels, the code can prevent an out-of-bounds write vulnerability. In this case, the solution involves adding checks before incrementing and decrementing the indent variable to ensure it does not exceed the maximum collection levels."
      },
      "GPT_analysis": "The modification is necessary to prevent an out-of-bounds write vulnerability in the code. By adding the check `if (indent == MAX_COLLECTION_LEVELS)` before incrementing the `indent` variable in the `TAG_MAIN_COL_START` case and adding the check `if (indent == 0)` before decrementing the `indent` variable in the `TAG_MAIN_COL_END` case, we ensure that the `indent` variable stays within the bounds of the `MAX_COLLECTION_LEVELS`. This prevents a malicious USB device from triggering an out-of-bounds write during the generation of debugging messages, thus addressing the vulnerability CVE-2019-13631.",
      "GPT_purpose": "This function parses a HID report descriptor to extract information needed for handling input from a device.",
      "GPT_function": "\n1. Parsing a HID report descriptor.\n2. Saving information from the HID report.\n3. Generating debugging messages based on the HID report data.",
      "CVE_id": "CVE-2019-13631",
      "code_before_change": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[10] = \"\";\n\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i++];\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = (1U << PREF_SIZE(prefix)) >> 1;\n\t\tif (i + size > length) {\n\t\t\tdev_err(ddev,\n\t\t\t\t\"Not enough data (need %d, have %d)\\n\",\n\t\t\t\ti + size, length);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\t\t\t\tmaintype = 'E';\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}",
      "code_after_change": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\n\t\t\t\t\tint length)\n{\n\tstruct device *ddev = &device->intf->dev;\n\tint   x, i = 0;\n\n\t/* Tag primitive vars */\n\t__u8   prefix;\n\t__u8   size;\n\t__u8   tag;\n\t__u8   type;\n\t__u8   data   = 0;\n\t__u16  data16 = 0;\n\t__u32  data32 = 0;\n\n\t/* For parsing logic */\n\tint   inputnum = 0;\n\t__u32 usage = 0;\n\n\t/* Global Values, indexed by TAG */\n\t__u32 globalval[TAG_GLOB_MAX];\n\t__u32 oldval[TAG_GLOB_MAX];\n\n\t/* Debug stuff */\n\tchar  maintype = 'x';\n\tchar  globtype[12];\n\tint   indent = 0;\n\tchar  indentstr[MAX_COLLECTION_LEVELS + 1] = { 0 };\n\n\tdev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\n\n\t/* Walk  this report and pull out the info we need */\n\twhile (i < length) {\n\t\tprefix = report[i++];\n\n\t\t/* Determine data size and save the data in the proper variable */\n\t\tsize = (1U << PREF_SIZE(prefix)) >> 1;\n\t\tif (i + size > length) {\n\t\t\tdev_err(ddev,\n\t\t\t\t\"Not enough data (need %d, have %d)\\n\",\n\t\t\t\ti + size, length);\n\t\t\tbreak;\n\t\t}\n\n\t\tswitch (size) {\n\t\tcase 1:\n\t\t\tdata = report[i];\n\t\t\tbreak;\n\t\tcase 2:\n\t\t\tdata16 = get_unaligned_le16(&report[i]);\n\t\t\tbreak;\n\t\tcase 4:\n\t\t\tdata32 = get_unaligned_le32(&report[i]);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Skip size of data */\n\t\ti += size;\n\n\t\t/* What we do depends on the tag type */\n\t\ttag  = PREF_TAG(prefix);\n\t\ttype = PREF_TYPE(prefix);\n\t\tswitch (type) {\n\t\tcase TYPE_MAIN:\n\t\t\tstrcpy(globtype, \"\");\n\t\t\tswitch (tag) {\n\n\t\t\tcase TAG_MAIN_INPUT:\n\t\t\t\t/*\n\t\t\t\t * The INPUT MAIN tag signifies this is\n\t\t\t\t * information from a report.  We need to\n\t\t\t\t * figure out what it is and store the\n\t\t\t\t * min/max values\n\t\t\t\t */\n\n\t\t\t\tmaintype = 'I';\n\t\t\t\tif (data == 2)\n\t\t\t\t\tstrcpy(globtype, \"Variable\");\n\t\t\t\telse if (data == 3)\n\t\t\t\t\tstrcpy(globtype, \"Var|Const\");\n\n\t\t\t\tdev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_ID], inputnum,\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\n\t\t\t\t\tglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\n\t\t\t\t\tglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\n\n\n\t\t\t\t/*\n\t\t\t\t  We can assume that the first two input items\n\t\t\t\t  are always the X and Y coordinates.  After\n\t\t\t\t  that, we look for everything else by\n\t\t\t\t  local usage value\n\t\t\t\t */\n\t\t\t\tswitch (inputnum) {\n\t\t\t\tcase 0:  /* X coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_X == 0) {\n\t\t\t\t\t\tdevice->max_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 1:  /* Y coord */\n\t\t\t\t\tdev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\n\t\t\t\t\tif (device->max_Y == 0) {\n\t\t\t\t\t\tdevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\tdevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t}\n\t\t\t\t\tbreak;\n\n\t\t\t\tdefault:\n\t\t\t\t\t/* Tilt X */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_X) {\n\t\t\t\t\t\tif (device->maxtilt_X == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Tilt Y */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TILT_Y) {\n\t\t\t\t\t\tif (device->maxtilt_Y == 0) {\n\t\t\t\t\t\t\tdevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\t/* Pressure */\n\t\t\t\t\tif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\n\t\t\t\t\t\tif (device->maxpressure == 0) {\n\t\t\t\t\t\t\tdevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\n\t\t\t\t\t\t\tdevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tinputnum++;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_OUTPUT:\n\t\t\t\tmaintype = 'O';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_FEATURE:\n\t\t\t\tmaintype = 'F';\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_START:\n\t\t\t\tmaintype = 'S';\n\n\t\t\t\tif (indent == MAX_COLLECTION_LEVELS) {\n\t\t\t\t\tdev_err(ddev, \"Collection level %d would exceed limit of %d\\n\",\n\t\t\t\t\t\tindent + 1,\n\t\t\t\t\t\tMAX_COLLECTION_LEVELS);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tif (data == 0) {\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>> Physical\\n\");\n\t\t\t\t\tstrcpy(globtype, \"Physical\");\n\t\t\t\t} else\n\t\t\t\t\tdev_dbg(ddev, \"======>>>>>>\\n\");\n\n\t\t\t\t/* Indent the debug output */\n\t\t\t\tindent++;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Save global tags */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\toldval[x] = globalval[x];\n\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_MAIN_COL_END:\n\t\t\t\tmaintype = 'E';\n\n\t\t\t\tif (indent == 0) {\n\t\t\t\t\tdev_err(ddev, \"Collection level already at zero\\n\");\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tdev_dbg(ddev, \"<<<<<<======\\n\");\n\n\t\t\t\tindent--;\n\t\t\t\tfor (x = 0; x < indent; x++)\n\t\t\t\t\tindentstr[x] = '-';\n\t\t\t\tindentstr[x] = 0;\n\n\t\t\t\t/* Copy global tags back */\n\t\t\t\tfor (x = 0; x < TAG_GLOB_MAX; x++)\n\t\t\t\t\tglobalval[x] = oldval[x];\n\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, maintype, size, globtype, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_GLOBAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\t/*\n\t\t\t\t * First time we hit the global usage tag,\n\t\t\t\t * it should tell us the type of device\n\t\t\t\t */\n\t\t\t\tif (device->usage == 0)\n\t\t\t\t\tdevice->usage = data;\n\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"LOG_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"LOG_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MIN:\n\t\t\t\tstrcpy(globtype, \"PHYS_MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PHYS_MAX:\n\t\t\t\tstrcpy(globtype, \"PHYS_MAX\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT_EXP:\n\t\t\t\tstrcpy(globtype, \"EXP\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_UNIT:\n\t\t\t\tstrcpy(globtype, \"UNIT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_SZ:\n\t\t\t\tstrcpy(globtype, \"REPORT_SZ\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_ID:\n\t\t\t\tstrcpy(globtype, \"REPORT_ID\");\n\t\t\t\t/* New report, restart numbering */\n\t\t\t\tinputnum = 0;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_REPORT_CNT:\n\t\t\t\tstrcpy(globtype, \"REPORT_CNT\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_PUSH:\n\t\t\t\tstrcpy(globtype, \"PUSH\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_POP:\n\t\t\t\tstrcpy(globtype, \"POP\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\t/* Check to make sure we have a good tag number\n\t\t\t   so we don't overflow array */\n\t\t\tif (tag < TAG_GLOB_MAX) {\n\t\t\t\tswitch (size) {\n\t\t\t\tcase 1:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data);\n\t\t\t\t\tglobalval[tag] = data;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 2:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data16);\n\t\t\t\t\tglobalval[tag] = data16;\n\t\t\t\t\tbreak;\n\n\t\t\t\tcase 4:\n\t\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\t\tindentstr, globtype, tag, size, data32);\n\t\t\t\t\tglobalval[tag] = data32;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tdev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\n\t\t\t\t\tindentstr, tag, size);\n\t\t\t}\n\t\t\tbreak;\n\n\t\tcase TYPE_LOCAL:\n\t\t\tswitch (tag) {\n\t\t\tcase TAG_GLOB_USAGE:\n\t\t\t\tstrcpy(globtype, \"USAGE\");\n\t\t\t\t/* Always 1 byte */\n\t\t\t\tusage = data;\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MIN:\n\t\t\t\tstrcpy(globtype, \"MIN\");\n\t\t\t\tbreak;\n\n\t\t\tcase TAG_GLOB_LOG_MAX:\n\t\t\t\tstrcpy(globtype, \"MAX\");\n\t\t\t\tbreak;\n\n\t\t\tdefault:\n\t\t\t\tstrcpy(globtype, \"UNKNOWN\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tswitch (size) {\n\t\t\tcase 1:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data);\n\t\t\t\tbreak;\n\n\t\t\tcase 2:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data16);\n\t\t\t\tbreak;\n\n\t\t\tcase 4:\n\t\t\t\tdev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\n\t\t\t\t\tindentstr, tag, globtype, size, data32);\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tbreak;\n\t\t}\n\t}\n}",
      "modified_lines": {
        "added": [
          "\tchar  indentstr[MAX_COLLECTION_LEVELS + 1] = { 0 };",
          "\t\t\t\tif (indent == MAX_COLLECTION_LEVELS) {",
          "\t\t\t\t\tdev_err(ddev, \"Collection level %d would exceed limit of %d\\n\",",
          "\t\t\t\t\t\tindent + 1,",
          "\t\t\t\t\t\tMAX_COLLECTION_LEVELS);",
          "\t\t\t\t\tbreak;",
          "\t\t\t\t}",
          "",
          "\t\t\t\tmaintype = 'E';",
          "",
          "\t\t\t\tif (indent == 0) {",
          "\t\t\t\t\tdev_err(ddev, \"Collection level already at zero\\n\");",
          "\t\t\t\t\tbreak;",
          "\t\t\t\t}",
          "",
          ""
        ],
        "deleted": [
          "\tchar  indentstr[10] = \"\";",
          "",
          "\t\t\t\tmaintype = 'E';"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking for the indent variable in the TAG_MAIN_COL_START and TAG_MAIN_COL_END cases.",
      "trigger_condition": "A malicious USB device sends an HID report that triggers an out-of-bounds write during the generation of debugging messages.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the bounds of the indent variable before incrementing or decrementing it in the TAG_MAIN_COL_START and TAG_MAIN_COL_END cases, respectively. This can lead to an out-of-bounds write vulnerability if the indent variable exceeds the maximum collection levels allowed.",
      "solution": "To mitigate the vulnerability, it is necessary to add bounds checking for the indent variable in the TAG_MAIN_COL_START and TAG_MAIN_COL_END cases. By adding checks to ensure that the indent variable stays within the bounds of the maximum collection levels, the code can prevent an out-of-bounds write vulnerability. In this case, the solution involves adding checks before incrementing and decrementing the indent variable to ensure it does not exceed the maximum collection levels.",
      "id": 43,
      "code_after_change_normalized": "static void FUN1(struct gtco *VAR1, char * VAR2,\nint VAR3)\n{\nstruct VAR1 *VAR4 = &VAR1->VAR5->VAR6;\nint   VAR7, VAR8 = 0;\n__u8   VAR9;\n__u8   VAR10;\n__u8   VAR11;\n__u8   VAR12;\n__u8   VAR13   = 0;\n__u16  VAR14 = 0;\n__u32  VAR15 = 0;\nint   VAR16 = 0;\n__u32 VAR17 = 0;\n__u32 VAR18[VAR19];\n__u32 VAR20[VAR19];\nchar  VAR21 = ;\nchar  VAR22[12];\nint   VAR23 = 0;\nchar  VAR24[VAR25 + 1] = { 0 };\nFUN2(VAR4, \"STR\");\nwhile (VAR8 < VAR3) {\nVAR9 = VAR2[VAR8++];\nVAR10 = (1U << FUN3(VAR9)) >> 1;\nif (VAR8 + VAR10 > VAR3) {\nFUN4(VAR4,\n\"STR\",\nVAR8 + VAR10, VAR3);\nbreak;\n}\nswitch (VAR10) {\ncase 1:\nVAR13 = VAR2[VAR8];\nbreak;\ncase 2:\nVAR14 = FUN5(&VAR2[VAR8]);\nbreak;\ncase 4:\nVAR15 = FUN6(&VAR2[VAR8]);\nbreak;\n}\nVAR8 += VAR10;\nVAR11  = FUN7(VAR9);\nVAR12 = FUN8(VAR9);\nswitch (VAR12) {\ncase VAR26:\nFUN9(VAR22, \"STR\");\nswitch (VAR11) {\ncase VAR27:\nVAR21 = ;\nif (VAR13 == 2)\nFUN9(VAR22, \"STR\");\nelse if (VAR13 == 3)\nFUN9(VAR22, \"STR\");\nFUN2(VAR4, \"STR\",\nVAR18[VAR28], VAR16,\nVAR18[VAR29], VAR18[VAR29],\nVAR18[VAR30], VAR18[VAR30],\nVAR18[VAR31] * VAR18[VAR32]);\nswitch (VAR16) {\ncase 0:  \nFUN2(VAR4, \"STR\", VAR17);\nif (VAR1->VAR33 == 0) {\nVAR1->VAR33 = VAR18[VAR29];\nVAR1->VAR34 = VAR18[VAR30];\n}\nbreak;\ncase 1:  \nFUN2(VAR4, \"STR\", VAR17);\nif (VAR1->VAR35 == 0) {\nVAR1->VAR35 = VAR18[VAR29];\nVAR1->VAR36 = VAR18[VAR30];\n}\nbreak;\ndefault:\nif (VAR17 == VAR37) {\nif (VAR1->VAR38 == 0) {\nVAR1->VAR38 = VAR18[VAR29];\nVAR1->VAR39 = VAR18[VAR30];\n}\n}\nif (VAR17 == VAR40) {\nif (VAR1->VAR41 == 0) {\nVAR1->VAR41 = VAR18[VAR29];\nVAR1->VAR42 = VAR18[VAR30];\n}\n}\nif (VAR17 == VAR43) {\nif (VAR1->VAR44 == 0) {\nVAR1->VAR44 = VAR18[VAR29];\nVAR1->VAR45 = VAR18[VAR30];\n}\n}\nbreak;\n}\nVAR16++;\nbreak;\ncase VAR46:\nVAR21 = ;\nbreak;\ncase VAR47:\nVAR21 = ;\nbreak;\ncase VAR48:\nVAR21 = ;\nif (VAR23 == VAR25) {\nFUN4(VAR4, \"STR\",\nVAR23 + 1,\nVAR25);\nbreak;\n}\nif (VAR13 == 0) {\nFUN2(VAR4, \"STR\");\nFUN9(VAR22, \"STR\");\n} else\nFUN2(VAR4, \"STR\");\nVAR23++;\nfor (VAR7 = 0; VAR7 < VAR23; VAR7++)\nVAR24[VAR7] = ;\nVAR24[VAR7] = 0;\nfor (VAR7 = 0; VAR7 < VAR19; VAR7++)\nVAR20[VAR7] = VAR18[VAR7];\nbreak;\ncase VAR49:\nVAR21 = ;\nif (VAR23 == 0) {\nFUN4(VAR4, \"STR\");\nbreak;\n}\nFUN2(VAR4, \"STR\");\nVAR23--;\nfor (VAR7 = 0; VAR7 < VAR23; VAR7++)\nVAR24[VAR7] = ;\nVAR24[VAR7] = 0;\nfor (VAR7 = 0; VAR7 < VAR19; VAR7++)\nVAR18[VAR7] = VAR20[VAR7];\nbreak;\n}\nswitch (VAR10) {\ncase 1:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR21, VAR10, VAR22, VAR13);\nbreak;\ncase 2:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR21, VAR10, VAR22, VAR14);\nbreak;\ncase 4:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR21, VAR10, VAR22, VAR15);\nbreak;\n}\nbreak;\ncase VAR50:\nswitch (VAR11) {\ncase VAR51:\nif (VAR1->VAR17 == 0)\nVAR1->VAR17 = VAR13;\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR30:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR29:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR52:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR53:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR54:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR55:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR31:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR28:\nFUN9(VAR22, \"STR\");\nVAR16 = 0;\nbreak;\ncase VAR32:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR56:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR57:\nFUN9(VAR22, \"STR\");\nbreak;\n}\nif (VAR11 < VAR19) {\nswitch (VAR10) {\ncase 1:\nFUN2(VAR4, \"STR\",\nVAR24, VAR22, VAR11, VAR10, VAR13);\nVAR18[VAR11] = VAR13;\nbreak;\ncase 2:\nFUN2(VAR4, \"STR\",\nVAR24, VAR22, VAR11, VAR10, VAR14);\nVAR18[VAR11] = VAR14;\nbreak;\ncase 4:\nFUN2(VAR4, \"STR\",\nVAR24, VAR22, VAR11, VAR10, VAR15);\nVAR18[VAR11] = VAR15;\nbreak;\n}\n} else {\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR10);\n}\nbreak;\ncase VAR58:\nswitch (VAR11) {\ncase VAR51:\nFUN9(VAR22, \"STR\");\nVAR17 = VAR13;\nbreak;\ncase VAR30:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR29:\nFUN9(VAR22, \"STR\");\nbreak;\ndefault:\nFUN9(VAR22, \"STR\");\nbreak;\n}\nswitch (VAR10) {\ncase 1:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR22, VAR10, VAR13);\nbreak;\ncase 2:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR22, VAR10, VAR14);\nbreak;\ncase 4:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR22, VAR10, VAR15);\nbreak;\n}\nbreak;\n}\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct gtco *VAR1, char * VAR2,\nint VAR3)\n{\nstruct VAR1 *VAR4 = &VAR1->VAR5->VAR6;\nint   VAR7, VAR8 = 0;\n__u8   VAR9;\n__u8   VAR10;\n__u8   VAR11;\n__u8   VAR12;\n__u8   VAR13   = 0;\n__u16  VAR14 = 0;\n__u32  VAR15 = 0;\nint   VAR16 = 0;\n__u32 VAR17 = 0;\n__u32 VAR18[VAR19];\n__u32 VAR20[VAR19];\nchar  VAR21 = ;\nchar  VAR22[12];\nint   VAR23 = 0;\nchar  VAR24[10] = \"STR\";\nFUN2(VAR4, \"STR\");\nwhile (VAR8 < VAR3) {\nVAR9 = VAR2[VAR8++];\nVAR10 = (1U << FUN3(VAR9)) >> 1;\nif (VAR8 + VAR10 > VAR3) {\nFUN4(VAR4,\n\"STR\",\nVAR8 + VAR10, VAR3);\nbreak;\n}\nswitch (VAR10) {\ncase 1:\nVAR13 = VAR2[VAR8];\nbreak;\ncase 2:\nVAR14 = FUN5(&VAR2[VAR8]);\nbreak;\ncase 4:\nVAR15 = FUN6(&VAR2[VAR8]);\nbreak;\n}\nVAR8 += VAR10;\nVAR11  = FUN7(VAR9);\nVAR12 = FUN8(VAR9);\nswitch (VAR12) {\ncase VAR25:\nFUN9(VAR22, \"STR\");\nswitch (VAR11) {\ncase VAR26:\nVAR21 = ;\nif (VAR13 == 2)\nFUN9(VAR22, \"STR\");\nelse if (VAR13 == 3)\nFUN9(VAR22, \"STR\");\nFUN2(VAR4, \"STR\",\nVAR18[VAR27], VAR16,\nVAR18[VAR28], VAR18[VAR28],\nVAR18[VAR29], VAR18[VAR29],\nVAR18[VAR30] * VAR18[VAR31]);\nswitch (VAR16) {\ncase 0:  \nFUN2(VAR4, \"STR\", VAR17);\nif (VAR1->VAR32 == 0) {\nVAR1->VAR32 = VAR18[VAR28];\nVAR1->VAR33 = VAR18[VAR29];\n}\nbreak;\ncase 1:  \nFUN2(VAR4, \"STR\", VAR17);\nif (VAR1->VAR34 == 0) {\nVAR1->VAR34 = VAR18[VAR28];\nVAR1->VAR35 = VAR18[VAR29];\n}\nbreak;\ndefault:\nif (VAR17 == VAR36) {\nif (VAR1->VAR37 == 0) {\nVAR1->VAR37 = VAR18[VAR28];\nVAR1->VAR38 = VAR18[VAR29];\n}\n}\nif (VAR17 == VAR39) {\nif (VAR1->VAR40 == 0) {\nVAR1->VAR40 = VAR18[VAR28];\nVAR1->VAR41 = VAR18[VAR29];\n}\n}\nif (VAR17 == VAR42) {\nif (VAR1->VAR43 == 0) {\nVAR1->VAR43 = VAR18[VAR28];\nVAR1->VAR44 = VAR18[VAR29];\n}\n}\nbreak;\n}\nVAR16++;\nbreak;\ncase VAR45:\nVAR21 = ;\nbreak;\ncase VAR46:\nVAR21 = ;\nbreak;\ncase VAR47:\nVAR21 = ;\nif (VAR13 == 0) {\nFUN2(VAR4, \"STR\");\nFUN9(VAR22, \"STR\");\n} else\nFUN2(VAR4, \"STR\");\nVAR23++;\nfor (VAR7 = 0; VAR7 < VAR23; VAR7++)\nVAR24[VAR7] = ;\nVAR24[VAR7] = 0;\nfor (VAR7 = 0; VAR7 < VAR19; VAR7++)\nVAR20[VAR7] = VAR18[VAR7];\nbreak;\ncase VAR48:\nFUN2(VAR4, \"STR\");\nVAR21 = ;\nVAR23--;\nfor (VAR7 = 0; VAR7 < VAR23; VAR7++)\nVAR24[VAR7] = ;\nVAR24[VAR7] = 0;\nfor (VAR7 = 0; VAR7 < VAR19; VAR7++)\nVAR18[VAR7] = VAR20[VAR7];\nbreak;\n}\nswitch (VAR10) {\ncase 1:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR21, VAR10, VAR22, VAR13);\nbreak;\ncase 2:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR21, VAR10, VAR22, VAR14);\nbreak;\ncase 4:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR21, VAR10, VAR22, VAR15);\nbreak;\n}\nbreak;\ncase VAR49:\nswitch (VAR11) {\ncase VAR50:\nif (VAR1->VAR17 == 0)\nVAR1->VAR17 = VAR13;\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR29:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR28:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR51:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR52:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR53:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR54:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR30:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR27:\nFUN9(VAR22, \"STR\");\nVAR16 = 0;\nbreak;\ncase VAR31:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR55:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR56:\nFUN9(VAR22, \"STR\");\nbreak;\n}\nif (VAR11 < VAR19) {\nswitch (VAR10) {\ncase 1:\nFUN2(VAR4, \"STR\",\nVAR24, VAR22, VAR11, VAR10, VAR13);\nVAR18[VAR11] = VAR13;\nbreak;\ncase 2:\nFUN2(VAR4, \"STR\",\nVAR24, VAR22, VAR11, VAR10, VAR14);\nVAR18[VAR11] = VAR14;\nbreak;\ncase 4:\nFUN2(VAR4, \"STR\",\nVAR24, VAR22, VAR11, VAR10, VAR15);\nVAR18[VAR11] = VAR15;\nbreak;\n}\n} else {\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR10);\n}\nbreak;\ncase VAR57:\nswitch (VAR11) {\ncase VAR50:\nFUN9(VAR22, \"STR\");\nVAR17 = VAR13;\nbreak;\ncase VAR29:\nFUN9(VAR22, \"STR\");\nbreak;\ncase VAR28:\nFUN9(VAR22, \"STR\");\nbreak;\ndefault:\nFUN9(VAR22, \"STR\");\nbreak;\n}\nswitch (VAR10) {\ncase 1:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR22, VAR10, VAR13);\nbreak;\ncase 2:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR22, VAR10, VAR14);\nbreak;\ncase 4:\nFUN2(VAR4, \"STR\",\nVAR24, VAR11, VAR22, VAR10, VAR15);\nbreak;\n}\nbreak;\n}\n}\n}\n",
      "code_after_change_raw": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\nint length)\n{\nstruct device *ddev = &device->intf->dev;\nint   x, i = 0;\n__u8   prefix;\n__u8   size;\n__u8   tag;\n__u8   type;\n__u8   data   = 0;\n__u16  data16 = 0;\n__u32  data32 = 0;\nint   inputnum = 0;\n__u32 usage = 0;\n__u32 globalval[TAG_GLOB_MAX];\n__u32 oldval[TAG_GLOB_MAX];\nchar  maintype = 'x';\nchar  globtype[12];\nint   indent = 0;\nchar  indentstr[MAX_COLLECTION_LEVELS + 1] = { 0 };\ndev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\nwhile (i < length) {\nprefix = report[i++];\nsize = (1U << PREF_SIZE(prefix)) >> 1;\nif (i + size > length) {\ndev_err(ddev,\n\"Not enough data (need %d, have %d)\\n\",\ni + size, length);\nbreak;\n}\nswitch (size) {\ncase 1:\ndata = report[i];\nbreak;\ncase 2:\ndata16 = get_unaligned_le16(&report[i]);\nbreak;\ncase 4:\ndata32 = get_unaligned_le32(&report[i]);\nbreak;\n}\ni += size;\ntag  = PREF_TAG(prefix);\ntype = PREF_TYPE(prefix);\nswitch (type) {\ncase TYPE_MAIN:\nstrcpy(globtype, \"\");\nswitch (tag) {\ncase TAG_MAIN_INPUT:\nmaintype = 'I';\nif (data == 2)\nstrcpy(globtype, \"Variable\");\nelse if (data == 3)\nstrcpy(globtype, \"Var|Const\");\ndev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\nglobalval[TAG_GLOB_REPORT_ID], inputnum,\nglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\nglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\nglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\nswitch (inputnum) {\ncase 0:  \ndev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\nif (device->max_X == 0) {\ndevice->max_X = globalval[TAG_GLOB_LOG_MAX];\ndevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n}\nbreak;\ncase 1:  \ndev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\nif (device->max_Y == 0) {\ndevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\ndevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n}\nbreak;\ndefault:\nif (usage == DIGITIZER_USAGE_TILT_X) {\nif (device->maxtilt_X == 0) {\ndevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\ndevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n}\n}\nif (usage == DIGITIZER_USAGE_TILT_Y) {\nif (device->maxtilt_Y == 0) {\ndevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\ndevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n}\n}\nif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\nif (device->maxpressure == 0) {\ndevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\ndevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n}\n}\nbreak;\n}\ninputnum++;\nbreak;\ncase TAG_MAIN_OUTPUT:\nmaintype = 'O';\nbreak;\ncase TAG_MAIN_FEATURE:\nmaintype = 'F';\nbreak;\ncase TAG_MAIN_COL_START:\nmaintype = 'S';\nif (indent == MAX_COLLECTION_LEVELS) {\ndev_err(ddev, \"Collection level %d would exceed limit of %d\\n\",\nindent + 1,\nMAX_COLLECTION_LEVELS);\nbreak;\n}\nif (data == 0) {\ndev_dbg(ddev, \"======>>>>>> Physical\\n\");\nstrcpy(globtype, \"Physical\");\n} else\ndev_dbg(ddev, \"======>>>>>>\\n\");\nindent++;\nfor (x = 0; x < indent; x++)\nindentstr[x] = '-';\nindentstr[x] = 0;\nfor (x = 0; x < TAG_GLOB_MAX; x++)\noldval[x] = globalval[x];\nbreak;\ncase TAG_MAIN_COL_END:\nmaintype = 'E';\nif (indent == 0) {\ndev_err(ddev, \"Collection level already at zero\\n\");\nbreak;\n}\ndev_dbg(ddev, \"<<<<<<======\\n\");\nindent--;\nfor (x = 0; x < indent; x++)\nindentstr[x] = '-';\nindentstr[x] = 0;\nfor (x = 0; x < TAG_GLOB_MAX; x++)\nglobalval[x] = oldval[x];\nbreak;\n}\nswitch (size) {\ncase 1:\ndev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\nindentstr, tag, maintype, size, globtype, data);\nbreak;\ncase 2:\ndev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\nindentstr, tag, maintype, size, globtype, data16);\nbreak;\ncase 4:\ndev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\nindentstr, tag, maintype, size, globtype, data32);\nbreak;\n}\nbreak;\ncase TYPE_GLOBAL:\nswitch (tag) {\ncase TAG_GLOB_USAGE:\nif (device->usage == 0)\ndevice->usage = data;\nstrcpy(globtype, \"USAGE\");\nbreak;\ncase TAG_GLOB_LOG_MIN:\nstrcpy(globtype, \"LOG_MIN\");\nbreak;\ncase TAG_GLOB_LOG_MAX:\nstrcpy(globtype, \"LOG_MAX\");\nbreak;\ncase TAG_GLOB_PHYS_MIN:\nstrcpy(globtype, \"PHYS_MIN\");\nbreak;\ncase TAG_GLOB_PHYS_MAX:\nstrcpy(globtype, \"PHYS_MAX\");\nbreak;\ncase TAG_GLOB_UNIT_EXP:\nstrcpy(globtype, \"EXP\");\nbreak;\ncase TAG_GLOB_UNIT:\nstrcpy(globtype, \"UNIT\");\nbreak;\ncase TAG_GLOB_REPORT_SZ:\nstrcpy(globtype, \"REPORT_SZ\");\nbreak;\ncase TAG_GLOB_REPORT_ID:\nstrcpy(globtype, \"REPORT_ID\");\ninputnum = 0;\nbreak;\ncase TAG_GLOB_REPORT_CNT:\nstrcpy(globtype, \"REPORT_CNT\");\nbreak;\ncase TAG_GLOB_PUSH:\nstrcpy(globtype, \"PUSH\");\nbreak;\ncase TAG_GLOB_POP:\nstrcpy(globtype, \"POP\");\nbreak;\n}\nif (tag < TAG_GLOB_MAX) {\nswitch (size) {\ncase 1:\ndev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\nindentstr, globtype, tag, size, data);\nglobalval[tag] = data;\nbreak;\ncase 2:\ndev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\nindentstr, globtype, tag, size, data16);\nglobalval[tag] = data16;\nbreak;\ncase 4:\ndev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\nindentstr, globtype, tag, size, data32);\nglobalval[tag] = data32;\nbreak;\n}\n} else {\ndev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\nindentstr, tag, size);\n}\nbreak;\ncase TYPE_LOCAL:\nswitch (tag) {\ncase TAG_GLOB_USAGE:\nstrcpy(globtype, \"USAGE\");\nusage = data;\nbreak;\ncase TAG_GLOB_LOG_MIN:\nstrcpy(globtype, \"MIN\");\nbreak;\ncase TAG_GLOB_LOG_MAX:\nstrcpy(globtype, \"MAX\");\nbreak;\ndefault:\nstrcpy(globtype, \"UNKNOWN\");\nbreak;\n}\nswitch (size) {\ncase 1:\ndev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\nindentstr, tag, globtype, size, data);\nbreak;\ncase 2:\ndev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\nindentstr, tag, globtype, size, data16);\nbreak;\ncase 4:\ndev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\nindentstr, tag, globtype, size, data32);\nbreak;\n}\nbreak;\n}\n}\n}\n",
      "code_before_change_raw": "static void parse_hid_report_descriptor(struct gtco *device, char * report,\nint length)\n{\nstruct device *ddev = &device->intf->dev;\nint   x, i = 0;\n__u8   prefix;\n__u8   size;\n__u8   tag;\n__u8   type;\n__u8   data   = 0;\n__u16  data16 = 0;\n__u32  data32 = 0;\nint   inputnum = 0;\n__u32 usage = 0;\n__u32 globalval[TAG_GLOB_MAX];\n__u32 oldval[TAG_GLOB_MAX];\nchar  maintype = 'x';\nchar  globtype[12];\nint   indent = 0;\nchar  indentstr[10] = \"\";\ndev_dbg(ddev, \"======>>>>>>PARSE<<<<<<======\\n\");\nwhile (i < length) {\nprefix = report[i++];\nsize = (1U << PREF_SIZE(prefix)) >> 1;\nif (i + size > length) {\ndev_err(ddev,\n\"Not enough data (need %d, have %d)\\n\",\ni + size, length);\nbreak;\n}\nswitch (size) {\ncase 1:\ndata = report[i];\nbreak;\ncase 2:\ndata16 = get_unaligned_le16(&report[i]);\nbreak;\ncase 4:\ndata32 = get_unaligned_le32(&report[i]);\nbreak;\n}\ni += size;\ntag  = PREF_TAG(prefix);\ntype = PREF_TYPE(prefix);\nswitch (type) {\ncase TYPE_MAIN:\nstrcpy(globtype, \"\");\nswitch (tag) {\ncase TAG_MAIN_INPUT:\nmaintype = 'I';\nif (data == 2)\nstrcpy(globtype, \"Variable\");\nelse if (data == 3)\nstrcpy(globtype, \"Var|Const\");\ndev_dbg(ddev, \"::::: Saving Report: %d input #%d Max: 0x%X(%d) Min:0x%X(%d) of %d bits\\n\",\nglobalval[TAG_GLOB_REPORT_ID], inputnum,\nglobalval[TAG_GLOB_LOG_MAX], globalval[TAG_GLOB_LOG_MAX],\nglobalval[TAG_GLOB_LOG_MIN], globalval[TAG_GLOB_LOG_MIN],\nglobalval[TAG_GLOB_REPORT_SZ] * globalval[TAG_GLOB_REPORT_CNT]);\nswitch (inputnum) {\ncase 0:  \ndev_dbg(ddev, \"GER: X Usage: 0x%x\\n\", usage);\nif (device->max_X == 0) {\ndevice->max_X = globalval[TAG_GLOB_LOG_MAX];\ndevice->min_X = globalval[TAG_GLOB_LOG_MIN];\n}\nbreak;\ncase 1:  \ndev_dbg(ddev, \"GER: Y Usage: 0x%x\\n\", usage);\nif (device->max_Y == 0) {\ndevice->max_Y = globalval[TAG_GLOB_LOG_MAX];\ndevice->min_Y = globalval[TAG_GLOB_LOG_MIN];\n}\nbreak;\ndefault:\nif (usage == DIGITIZER_USAGE_TILT_X) {\nif (device->maxtilt_X == 0) {\ndevice->maxtilt_X = globalval[TAG_GLOB_LOG_MAX];\ndevice->mintilt_X = globalval[TAG_GLOB_LOG_MIN];\n}\n}\nif (usage == DIGITIZER_USAGE_TILT_Y) {\nif (device->maxtilt_Y == 0) {\ndevice->maxtilt_Y = globalval[TAG_GLOB_LOG_MAX];\ndevice->mintilt_Y = globalval[TAG_GLOB_LOG_MIN];\n}\n}\nif (usage == DIGITIZER_USAGE_TIP_PRESSURE) {\nif (device->maxpressure == 0) {\ndevice->maxpressure = globalval[TAG_GLOB_LOG_MAX];\ndevice->minpressure = globalval[TAG_GLOB_LOG_MIN];\n}\n}\nbreak;\n}\ninputnum++;\nbreak;\ncase TAG_MAIN_OUTPUT:\nmaintype = 'O';\nbreak;\ncase TAG_MAIN_FEATURE:\nmaintype = 'F';\nbreak;\ncase TAG_MAIN_COL_START:\nmaintype = 'S';\nif (data == 0) {\ndev_dbg(ddev, \"======>>>>>> Physical\\n\");\nstrcpy(globtype, \"Physical\");\n} else\ndev_dbg(ddev, \"======>>>>>>\\n\");\nindent++;\nfor (x = 0; x < indent; x++)\nindentstr[x] = '-';\nindentstr[x] = 0;\nfor (x = 0; x < TAG_GLOB_MAX; x++)\noldval[x] = globalval[x];\nbreak;\ncase TAG_MAIN_COL_END:\ndev_dbg(ddev, \"<<<<<<======\\n\");\nmaintype = 'E';\nindent--;\nfor (x = 0; x < indent; x++)\nindentstr[x] = '-';\nindentstr[x] = 0;\nfor (x = 0; x < TAG_GLOB_MAX; x++)\nglobalval[x] = oldval[x];\nbreak;\n}\nswitch (size) {\ncase 1:\ndev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\nindentstr, tag, maintype, size, globtype, data);\nbreak;\ncase 2:\ndev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\nindentstr, tag, maintype, size, globtype, data16);\nbreak;\ncase 4:\ndev_dbg(ddev, \"%sMAINTAG:(%d) %c SIZE: %d Data: %s 0x%x\\n\",\nindentstr, tag, maintype, size, globtype, data32);\nbreak;\n}\nbreak;\ncase TYPE_GLOBAL:\nswitch (tag) {\ncase TAG_GLOB_USAGE:\nif (device->usage == 0)\ndevice->usage = data;\nstrcpy(globtype, \"USAGE\");\nbreak;\ncase TAG_GLOB_LOG_MIN:\nstrcpy(globtype, \"LOG_MIN\");\nbreak;\ncase TAG_GLOB_LOG_MAX:\nstrcpy(globtype, \"LOG_MAX\");\nbreak;\ncase TAG_GLOB_PHYS_MIN:\nstrcpy(globtype, \"PHYS_MIN\");\nbreak;\ncase TAG_GLOB_PHYS_MAX:\nstrcpy(globtype, \"PHYS_MAX\");\nbreak;\ncase TAG_GLOB_UNIT_EXP:\nstrcpy(globtype, \"EXP\");\nbreak;\ncase TAG_GLOB_UNIT:\nstrcpy(globtype, \"UNIT\");\nbreak;\ncase TAG_GLOB_REPORT_SZ:\nstrcpy(globtype, \"REPORT_SZ\");\nbreak;\ncase TAG_GLOB_REPORT_ID:\nstrcpy(globtype, \"REPORT_ID\");\ninputnum = 0;\nbreak;\ncase TAG_GLOB_REPORT_CNT:\nstrcpy(globtype, \"REPORT_CNT\");\nbreak;\ncase TAG_GLOB_PUSH:\nstrcpy(globtype, \"PUSH\");\nbreak;\ncase TAG_GLOB_POP:\nstrcpy(globtype, \"POP\");\nbreak;\n}\nif (tag < TAG_GLOB_MAX) {\nswitch (size) {\ncase 1:\ndev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\nindentstr, globtype, tag, size, data);\nglobalval[tag] = data;\nbreak;\ncase 2:\ndev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\nindentstr, globtype, tag, size, data16);\nglobalval[tag] = data16;\nbreak;\ncase 4:\ndev_dbg(ddev, \"%sGLOBALTAG:%s(%d) SIZE: %d Data: 0x%x\\n\",\nindentstr, globtype, tag, size, data32);\nglobalval[tag] = data32;\nbreak;\n}\n} else {\ndev_dbg(ddev, \"%sGLOBALTAG: ILLEGAL TAG:%d SIZE: %d\\n\",\nindentstr, tag, size);\n}\nbreak;\ncase TYPE_LOCAL:\nswitch (tag) {\ncase TAG_GLOB_USAGE:\nstrcpy(globtype, \"USAGE\");\nusage = data;\nbreak;\ncase TAG_GLOB_LOG_MIN:\nstrcpy(globtype, \"MIN\");\nbreak;\ncase TAG_GLOB_LOG_MAX:\nstrcpy(globtype, \"MAX\");\nbreak;\ndefault:\nstrcpy(globtype, \"UNKNOWN\");\nbreak;\n}\nswitch (size) {\ncase 1:\ndev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\nindentstr, tag, globtype, size, data);\nbreak;\ncase 2:\ndev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\nindentstr, tag, globtype, size, data16);\nbreak;\ncase 4:\ndev_dbg(ddev, \"%sLOCALTAG:(%d) %s SIZE: %d Data: 0x%x\\n\",\nindentstr, tag, globtype, size, data32);\nbreak;\n}\nbreak;\n}\n}\n}\n"
    }
  ],
  "CVE-2019-14821": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking and atomic read operation for write indices in the Coalesced MMIO write operation.",
        "trigger_condition": "An unprivileged host user or process with access to a specific device could manipulate the write indices 'ring->first' and 'ring->last' values, leading to out-of-bounds access and potential denial of service or privilege escalation.",
        "specific_code_behavior_causing_vulnerability": "The code does not perform proper bounds checking and atomic read operation for the write indices 'ring->first' and 'ring->last', allowing unprivileged users to potentially crash the host kernel or escalate privileges by manipulating these values."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add proper bounds checking and an atomic read operation for the write indices 'ring->first' and 'ring->last' in the Coalesced MMIO write operation. This involves introducing an additional variable to store the value of 'ring->last' atomically and checking if there is room in the ring buffer and if the value is within the valid range before proceeding with the write operation. By ensuring proper bounds checking and atomic read operation, the code prevents out-of-bounds access and addresses the vulnerability.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-14821, which involves an out-of-bounds access issue in the Linux kernel's KVM hypervisor implementation of the Coalesced MMIO write operation. The vulnerability allows an unprivileged host user or process with access to '/dev/kvm' device to potentially crash the host kernel or escalate privileges on the system by manipulating the write indices 'ring->first' and 'ring->last' values.\n\nThe modification introduces the use of an additional variable `insert` to store the value of `ring->last` atomically. By using the `READ_ONCE` macro, the value of `ring->last` is read atomically to prevent any race conditions that could lead to out-of-bounds access. The code then checks if there is room in the ring buffer and if the `insert` value is within the bounds of the buffer before proceeding with the write operation. This ensures that the write operation stays within the valid range of the ring buffer, preventing potential out-of-bounds access and addressing the vulnerability.",
      "GPT_purpose": "Write data to a Coalesced MMIO ring buffer in the Linux kernel's KVM hypervisor.",
      "GPT_function": "\n1. Write data to the coalesced MMIO ring buffer.\n2. Check if the provided address and length are within the supported range.\n3. Ensure there is enough room in the ring buffer for writing data.",
      "CVE_id": "CVE-2019-14821",
      "code_before_change": "static int coalesced_mmio_write(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_io_device *this, gpa_t addr,\n\t\t\t\tint len, const void *val)\n{\n\tstruct kvm_coalesced_mmio_dev *dev = to_mmio(this);\n\tstruct kvm_coalesced_mmio_ring *ring = dev->kvm->coalesced_mmio_ring;\n\n\tif (!coalesced_mmio_in_range(dev, addr, len))\n\t\treturn -EOPNOTSUPP;\n\n\tspin_lock(&dev->kvm->ring_lock);\n\n\tif (!coalesced_mmio_has_room(dev)) {\n\t\tspin_unlock(&dev->kvm->ring_lock);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/* copy data in first free entry of the ring */\n\n\tring->coalesced_mmio[ring->last].phys_addr = addr;\n\tring->coalesced_mmio[ring->last].len = len;\n\tmemcpy(ring->coalesced_mmio[ring->last].data, val, len);\n\tring->coalesced_mmio[ring->last].pio = dev->zone.pio;\n\tsmp_wmb();\n\tring->last = (ring->last + 1) % KVM_COALESCED_MMIO_MAX;\n\tspin_unlock(&dev->kvm->ring_lock);\n\treturn 0;\n}",
      "code_after_change": "static int coalesced_mmio_write(struct kvm_vcpu *vcpu,\n\t\t\t\tstruct kvm_io_device *this, gpa_t addr,\n\t\t\t\tint len, const void *val)\n{\n\tstruct kvm_coalesced_mmio_dev *dev = to_mmio(this);\n\tstruct kvm_coalesced_mmio_ring *ring = dev->kvm->coalesced_mmio_ring;\n\t__u32 insert;\n\n\tif (!coalesced_mmio_in_range(dev, addr, len))\n\t\treturn -EOPNOTSUPP;\n\n\tspin_lock(&dev->kvm->ring_lock);\n\n\tinsert = READ_ONCE(ring->last);\n\tif (!coalesced_mmio_has_room(dev, insert) ||\n\t    insert >= KVM_COALESCED_MMIO_MAX) {\n\t\tspin_unlock(&dev->kvm->ring_lock);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\t/* copy data in first free entry of the ring */\n\n\tring->coalesced_mmio[insert].phys_addr = addr;\n\tring->coalesced_mmio[insert].len = len;\n\tmemcpy(ring->coalesced_mmio[insert].data, val, len);\n\tring->coalesced_mmio[insert].pio = dev->zone.pio;\n\tsmp_wmb();\n\tring->last = (insert + 1) % KVM_COALESCED_MMIO_MAX;\n\tspin_unlock(&dev->kvm->ring_lock);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t__u32 insert;",
          "\tinsert = READ_ONCE(ring->last);",
          "\tif (!coalesced_mmio_has_room(dev, insert) ||",
          "\t    insert >= KVM_COALESCED_MMIO_MAX) {",
          "\tring->coalesced_mmio[insert].phys_addr = addr;",
          "\tring->coalesced_mmio[insert].len = len;",
          "\tmemcpy(ring->coalesced_mmio[insert].data, val, len);",
          "\tring->coalesced_mmio[insert].pio = dev->zone.pio;",
          "\tring->last = (insert + 1) % KVM_COALESCED_MMIO_MAX;"
        ],
        "deleted": [
          "\tif (!coalesced_mmio_has_room(dev)) {",
          "\tring->coalesced_mmio[ring->last].phys_addr = addr;",
          "\tring->coalesced_mmio[ring->last].len = len;",
          "\tmemcpy(ring->coalesced_mmio[ring->last].data, val, len);",
          "\tring->coalesced_mmio[ring->last].pio = dev->zone.pio;",
          "\tring->last = (ring->last + 1) % KVM_COALESCED_MMIO_MAX;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking and atomic read operation for write indices in the Coalesced MMIO write operation.",
      "trigger_condition": "An unprivileged host user or process with access to a specific device could manipulate the write indices 'ring->first' and 'ring->last' values, leading to out-of-bounds access and potential denial of service or privilege escalation.",
      "specific_code_behavior_causing_vulnerability": "The code does not perform proper bounds checking and atomic read operation for the write indices 'ring->first' and 'ring->last', allowing unprivileged users to potentially crash the host kernel or escalate privileges by manipulating these values.",
      "id": 44,
      "code_after_change_normalized": "static int FUN1(struct kvm_vcpu *VAR1,\nstruct kvm_io_device *this, gpa_t VAR2,\nint VAR3, const void *VAR4)\n{\nstruct kvm_coalesced_mmio_dev *VAR5 = FUN2(this);\nstruct kvm_coalesced_mmio_ring *VAR6 = VAR5->VAR7->VAR8;\n__u32 VAR9;\nif (!FUN3(VAR5, VAR2, VAR3))\nreturn -VAR10;\nFUN4(&VAR5->VAR7->VAR11);\nVAR9 = FUN5(VAR6->VAR12);\nif (!FUN6(VAR5, VAR9) ||\nVAR9 >= VAR13) {\nFUN7(&VAR5->VAR7->VAR11);\nreturn -VAR10;\n}\nVAR6->VAR14[VAR9].VAR15 = VAR2;\nVAR6->VAR14[VAR9].VAR3 = VAR3;\nFUN8(VAR6->VAR14[VAR9].VAR16, VAR4, VAR3);\nVAR6->VAR14[VAR9].VAR17 = VAR5->VAR18.VAR17;\nFUN9();\nVAR6->VAR12 = (VAR9 + 1) % VAR13;\nFUN7(&VAR5->VAR7->VAR11);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct kvm_vcpu *VAR1,\nstruct kvm_io_device *this, gpa_t VAR2,\nint VAR3, const void *VAR4)\n{\nstruct kvm_coalesced_mmio_dev *VAR5 = FUN2(this);\nstruct kvm_coalesced_mmio_ring *VAR6 = VAR5->VAR7->VAR8;\nif (!FUN3(VAR5, VAR2, VAR3))\nreturn -VAR9;\nFUN4(&VAR5->VAR7->VAR10);\nif (!FUN5(VAR5)) {\nFUN6(&VAR5->VAR7->VAR10);\nreturn -VAR9;\n}\nVAR6->VAR11[VAR6->VAR12].VAR13 = VAR2;\nVAR6->VAR11[VAR6->VAR12].VAR3 = VAR3;\nFUN7(VAR6->VAR11[VAR6->VAR12].VAR14, VAR4, VAR3);\nVAR6->VAR11[VAR6->VAR12].VAR15 = VAR5->VAR16.VAR15;\nFUN8();\nVAR6->VAR12 = (VAR6->VAR12 + 1) % VAR17;\nFUN6(&VAR5->VAR7->VAR10);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int coalesced_mmio_write(struct kvm_vcpu *vcpu,\nstruct kvm_io_device *this, gpa_t addr,\nint len, const void *val)\n{\nstruct kvm_coalesced_mmio_dev *dev = to_mmio(this);\nstruct kvm_coalesced_mmio_ring *ring = dev->kvm->coalesced_mmio_ring;\n__u32 insert;\nif (!coalesced_mmio_in_range(dev, addr, len))\nreturn -EOPNOTSUPP;\nspin_lock(&dev->kvm->ring_lock);\ninsert = READ_ONCE(ring->last);\nif (!coalesced_mmio_has_room(dev, insert) ||\ninsert >= KVM_COALESCED_MMIO_MAX) {\nspin_unlock(&dev->kvm->ring_lock);\nreturn -EOPNOTSUPP;\n}\nring->coalesced_mmio[insert].phys_addr = addr;\nring->coalesced_mmio[insert].len = len;\nmemcpy(ring->coalesced_mmio[insert].data, val, len);\nring->coalesced_mmio[insert].pio = dev->zone.pio;\nsmp_wmb();\nring->last = (insert + 1) % KVM_COALESCED_MMIO_MAX;\nspin_unlock(&dev->kvm->ring_lock);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int coalesced_mmio_write(struct kvm_vcpu *vcpu,\nstruct kvm_io_device *this, gpa_t addr,\nint len, const void *val)\n{\nstruct kvm_coalesced_mmio_dev *dev = to_mmio(this);\nstruct kvm_coalesced_mmio_ring *ring = dev->kvm->coalesced_mmio_ring;\nif (!coalesced_mmio_in_range(dev, addr, len))\nreturn -EOPNOTSUPP;\nspin_lock(&dev->kvm->ring_lock);\nif (!coalesced_mmio_has_room(dev)) {\nspin_unlock(&dev->kvm->ring_lock);\nreturn -EOPNOTSUPP;\n}\nring->coalesced_mmio[ring->last].phys_addr = addr;\nring->coalesced_mmio[ring->last].len = len;\nmemcpy(ring->coalesced_mmio[ring->last].data, val, len);\nring->coalesced_mmio[ring->last].pio = dev->zone.pio;\nsmp_wmb();\nring->last = (ring->last + 1) % KVM_COALESCED_MMIO_MAX;\nspin_unlock(&dev->kvm->ring_lock);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2019-19319": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for a specific filesystem feature related to journaling.",
        "trigger_condition": "A setxattr operation is performed after mounting a crafted ext4 image, leading to a use-after-free issue due to a large old_size value used in a memset call.",
        "specific_code_behavior_causing_vulnerability": "The code does not protect the reserved inode related to the journal feature, resulting in a use-after-free vulnerability when a large old_size value is used in a memset call during a setxattr operation on a mounted ext4 image."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper protection of the reserved inode associated with the journal feature. This involves adding code to protect the reserved inode if the filesystem has the journal feature enabled and the journal inode number is set in the ext4 superblock. By implementing this protection, the use-after-free issue related to the large old_size value in the memset call can be prevented.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2019-19319, which involves a use-after-free issue in the Linux kernel before version 5.2. Specifically, the vulnerability occurs due to a slab-out-of-bounds write access caused by an ext4_xattr_set_entry use-after-free in fs/ext4/xattr.c when a large old_size value is used in a memset call.\n\nThe fix involves adding the following lines to the code snippet:\n'''\nif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {\n\tret = ext4_protect_reserved_inode(sb,\n\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));\n\tif (ret)\n\t\treturn ret;\n}\n'''\nThese lines ensure that the ext4_protect_reserved_inode function is called to protect the reserved inode if the filesystem has the journal feature enabled and the journal inode number is set in the ext4 superblock.\n\nBy adding this code, the vulnerability is addressed by properly handling the protection of the reserved inode related to the journal feature, preventing the use-after-free issue and potential slab-out-of-bounds write access.",
      "GPT_purpose": "Setting up system zones for an ext4 filesystem based on certain conditions and configurations.",
      "GPT_function": "\n1. `ext4_setup_system_zone`: Sets up system zones for ext4 filesystem.\n2. `ext4_release_system_zone`: Releases system zones if the BLOCK_VALIDITY option is not set.\n3. `ext4_flex_bg_size`: Calculates the flex block group size for ext4 filesystem.\n4. `add_system_zone`: Adds a system zone for a given block range.\n5. `ext4_get_groups_count`: Retrieves the number of block groups in the ext4 filesystem.\n6. `ext4_bg_has_super`: Checks if a block group has a superblock.\n7. `ext4_group_first_block_no`: Retrieves the first block number of a block group.\n8. `ext4_bg_num_gdb`: Retrieves the number of group descriptor blocks in a block group.\n9. `ext4_get_group_desc`: Retrieves the group descriptor for a given block group.\n10. `ext4_block_bitmap`: Retrieves the block bitmap for a given group descriptor.\n11. `ext4_inode_bitmap`: Retrieves the inode bitmap for a given group descriptor.\n12. `ext4_inode_table`: Retrieves the inode table for a given group descriptor.\n13. `debug_print_tree`: Prints the debug tree if the DEBUG option is set.",
      "CVE_id": "CVE-2019-19319",
      "code_before_change": "int ext4_setup_system_zone(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp;\n\text4_group_t i;\n\tint flex_size = ext4_flex_bg_size(sbi);\n\tint ret;\n\n\tif (!test_opt(sb, BLOCK_VALIDITY)) {\n\t\tif (sbi->system_blks.rb_node)\n\t\t\text4_release_system_zone(sb);\n\t\treturn 0;\n\t}\n\tif (sbi->system_blks.rb_node)\n\t\treturn 0;\n\n\tfor (i=0; i < ngroups; i++) {\n\t\tif (ext4_bg_has_super(sb, i) &&\n\t\t    ((i < 5) || ((i % flex_size) == 0)))\n\t\t\tadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\n\t\t\t\t\text4_bg_num_gdb(sb, i) + 1);\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\n\t\t\t\tsbi->s_itb_per_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (test_opt(sb, DEBUG))\n\t\tdebug_print_tree(sbi);\n\treturn 0;\n}",
      "code_after_change": "int ext4_setup_system_zone(struct super_block *sb)\n{\n\text4_group_t ngroups = ext4_get_groups_count(sb);\n\tstruct ext4_sb_info *sbi = EXT4_SB(sb);\n\tstruct ext4_group_desc *gdp;\n\text4_group_t i;\n\tint flex_size = ext4_flex_bg_size(sbi);\n\tint ret;\n\n\tif (!test_opt(sb, BLOCK_VALIDITY)) {\n\t\tif (sbi->system_blks.rb_node)\n\t\t\text4_release_system_zone(sb);\n\t\treturn 0;\n\t}\n\tif (sbi->system_blks.rb_node)\n\t\treturn 0;\n\n\tfor (i=0; i < ngroups; i++) {\n\t\tif (ext4_bg_has_super(sb, i) &&\n\t\t    ((i < 5) || ((i % flex_size) == 0)))\n\t\t\tadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\n\t\t\t\t\text4_bg_num_gdb(sb, i) + 1);\n\t\tgdp = ext4_get_group_desc(sb, i, NULL);\n\t\tret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\n\t\tif (ret)\n\t\t\treturn ret;\n\t\tret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\n\t\t\t\tsbi->s_itb_per_group);\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\tif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {\n\t\tret = ext4_protect_reserved_inode(sb,\n\t\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));\n\t\tif (ret)\n\t\t\treturn ret;\n\t}\n\n\tif (test_opt(sb, DEBUG))\n\t\tdebug_print_tree(sbi);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {",
          "\t\tret = ext4_protect_reserved_inode(sb,",
          "\t\t\t\tle32_to_cpu(sbi->s_es->s_journal_inum));",
          "\t\tif (ret)",
          "\t\t\treturn ret;",
          "\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper handling for a specific filesystem feature related to journaling.",
      "trigger_condition": "A setxattr operation is performed after mounting a crafted ext4 image, leading to a use-after-free issue due to a large old_size value used in a memset call.",
      "specific_code_behavior_causing_vulnerability": "The code does not protect the reserved inode related to the journal feature, resulting in a use-after-free vulnerability when a large old_size value is used in a memset call during a setxattr operation on a mounted ext4 image.",
      "id": 45,
      "code_after_change_normalized": "int FUN1(struct super_block *VAR1)\n{\next4_group_t VAR2 = FUN2(VAR1);\nstruct ext4_sb_info *VAR3 = FUN3(VAR1);\nstruct ext4_group_desc *VAR4;\next4_group_t VAR5;\nint VAR6 = FUN4(VAR3);\nint VAR7;\nif (!FUN5(VAR1, VAR8)) {\nif (VAR3->VAR9.VAR10)\nFUN6(VAR1);\nreturn 0;\n}\nif (VAR3->VAR9.VAR10)\nreturn 0;\nfor (VAR5=0; VAR5 < VAR2; VAR5++) {\nif (FUN7(VAR1, VAR5) &&\n((VAR5 < 5) || ((VAR5 % VAR6) == 0)))\nFUN8(VAR3, FUN9(VAR1, VAR5),\nFUN10(VAR1, VAR5) + 1);\nVAR4 = FUN11(VAR1, VAR5, NULL);\nVAR7 = FUN8(VAR3, FUN12(VAR1, VAR4), 1);\nif (VAR7)\nreturn VAR7;\nVAR7 = FUN8(VAR3, FUN13(VAR1, VAR4), 1);\nif (VAR7)\nreturn VAR7;\nVAR7 = FUN8(VAR3, FUN14(VAR1, VAR4),\nVAR3->VAR11);\nif (VAR7)\nreturn VAR7;\n}\nif (FUN15(VAR1) && VAR3->VAR12->VAR13) {\nVAR7 = FUN16(VAR1,\nFUN17(VAR3->VAR12->VAR13));\nif (VAR7)\nreturn VAR7;\n}\nif (FUN5(VAR1, VAR14))\nFUN18(VAR3);\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct super_block *VAR1)\n{\next4_group_t VAR2 = FUN2(VAR1);\nstruct ext4_sb_info *VAR3 = FUN3(VAR1);\nstruct ext4_group_desc *VAR4;\next4_group_t VAR5;\nint VAR6 = FUN4(VAR3);\nint VAR7;\nif (!FUN5(VAR1, VAR8)) {\nif (VAR3->VAR9.VAR10)\nFUN6(VAR1);\nreturn 0;\n}\nif (VAR3->VAR9.VAR10)\nreturn 0;\nfor (VAR5=0; VAR5 < VAR2; VAR5++) {\nif (FUN7(VAR1, VAR5) &&\n((VAR5 < 5) || ((VAR5 % VAR6) == 0)))\nFUN8(VAR3, FUN9(VAR1, VAR5),\nFUN10(VAR1, VAR5) + 1);\nVAR4 = FUN11(VAR1, VAR5, NULL);\nVAR7 = FUN8(VAR3, FUN12(VAR1, VAR4), 1);\nif (VAR7)\nreturn VAR7;\nVAR7 = FUN8(VAR3, FUN13(VAR1, VAR4), 1);\nif (VAR7)\nreturn VAR7;\nVAR7 = FUN8(VAR3, FUN14(VAR1, VAR4),\nVAR3->VAR11);\nif (VAR7)\nreturn VAR7;\n}\nif (FUN5(VAR1, VAR12))\nFUN15(VAR3);\nreturn 0;\n}\n",
      "code_after_change_raw": "int ext4_setup_system_zone(struct super_block *sb)\n{\next4_group_t ngroups = ext4_get_groups_count(sb);\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\nstruct ext4_group_desc *gdp;\next4_group_t i;\nint flex_size = ext4_flex_bg_size(sbi);\nint ret;\nif (!test_opt(sb, BLOCK_VALIDITY)) {\nif (sbi->system_blks.rb_node)\next4_release_system_zone(sb);\nreturn 0;\n}\nif (sbi->system_blks.rb_node)\nreturn 0;\nfor (i=0; i < ngroups; i++) {\nif (ext4_bg_has_super(sb, i) &&\n((i < 5) || ((i % flex_size) == 0)))\nadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\next4_bg_num_gdb(sb, i) + 1);\ngdp = ext4_get_group_desc(sb, i, NULL);\nret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\nif (ret)\nreturn ret;\nret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\nif (ret)\nreturn ret;\nret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\nsbi->s_itb_per_group);\nif (ret)\nreturn ret;\n}\nif (ext4_has_feature_journal(sb) && sbi->s_es->s_journal_inum) {\nret = ext4_protect_reserved_inode(sb,\nle32_to_cpu(sbi->s_es->s_journal_inum));\nif (ret)\nreturn ret;\n}\nif (test_opt(sb, DEBUG))\ndebug_print_tree(sbi);\nreturn 0;\n}\n",
      "code_before_change_raw": "int ext4_setup_system_zone(struct super_block *sb)\n{\next4_group_t ngroups = ext4_get_groups_count(sb);\nstruct ext4_sb_info *sbi = EXT4_SB(sb);\nstruct ext4_group_desc *gdp;\next4_group_t i;\nint flex_size = ext4_flex_bg_size(sbi);\nint ret;\nif (!test_opt(sb, BLOCK_VALIDITY)) {\nif (sbi->system_blks.rb_node)\next4_release_system_zone(sb);\nreturn 0;\n}\nif (sbi->system_blks.rb_node)\nreturn 0;\nfor (i=0; i < ngroups; i++) {\nif (ext4_bg_has_super(sb, i) &&\n((i < 5) || ((i % flex_size) == 0)))\nadd_system_zone(sbi, ext4_group_first_block_no(sb, i),\next4_bg_num_gdb(sb, i) + 1);\ngdp = ext4_get_group_desc(sb, i, NULL);\nret = add_system_zone(sbi, ext4_block_bitmap(sb, gdp), 1);\nif (ret)\nreturn ret;\nret = add_system_zone(sbi, ext4_inode_bitmap(sb, gdp), 1);\nif (ret)\nreturn ret;\nret = add_system_zone(sbi, ext4_inode_table(sb, gdp),\nsbi->s_itb_per_group);\nif (ret)\nreturn ret;\n}\nif (test_opt(sb, DEBUG))\ndebug_print_tree(sbi);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2019-19332": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of boundary check for memory write operations on the 'entry' array in the KVM hypervisor code.",
        "trigger_condition": "An attacker with access to the '/dev/kvm' device triggers the 'KVM_GET_EMULATED_CPUID' ioctl(2) request, leading to an out-of-bounds memory write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the boundary of the 'entry' array before writing to it, allowing an attacker to trigger a denial of service by causing a crash through the out-of-bounds memory write.",
        "solution": "To mitigate the vulnerability, a boundary check should be added before writing to the 'entry' array in the KVM hypervisor code. By adding a check to ensure that the write operation stays within the bounds of the array, the code becomes more secure and prevents the out-of-bounds memory write vulnerability from being exploited."
      },
      "GPT_analysis": "The modification is necessary to address the out-of-bounds memory write vulnerability (CVE-2019-19332) in the Linux Kernel's KVM hypervisor. By adding the check `WARN_ON(*nent >= maxnent)` before writing to the `entry` array, the code ensures that the write operation stays within the bounds of the array. This check helps prevent a user or process with access to the `/dev/kvm` device from causing a denial of service by crashing the system through exploiting the vulnerability. By adding this check, the code becomes more robust and secure against potential attacks exploiting the vulnerability.",
      "GPT_purpose": "Retrieve CPUID features for KVM virtualization and handle various CPUID function requests.",
      "GPT_function": "\n1. __do_cpuid_func: Handles the KVM_GET_EMULATED_CPUID ioctl request to get CPUID features emulated by the KVM hypervisor.",
      "CVE_id": "CVE-2019-19332",
      "code_before_change": "static inline int __do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 function,\n\t\t\t\t  int *nent, int maxnent)\n{\n\tint r;\n\tunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\n\tunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n\t\t\t\t? F(GBPAGES) : 0;\n\tunsigned f_lm = F(LM);\n#else\n\tunsigned f_gbpages = 0;\n\tunsigned f_lm = 0;\n#endif\n\tunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\n\tunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\n\tunsigned f_intel_pt = kvm_x86_ops->pt_supported() ? F(INTEL_PT) : 0;\n\n\t/* cpuid 1.edx */\n\tconst u32 kvm_cpuid_1_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |\n\t\t0 /* Reserved, DS, ACPI */ | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0 /* HTT, TM, Reserved, PBE */;\n\t/* cpuid 0x80000001.edx */\n\tconst u32 kvm_cpuid_8000_0001_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* Reserved */ |\n\t\tf_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n\t\t0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW);\n\t/* cpuid 1.ecx */\n\tconst u32 kvm_cpuid_1_ecx_x86_features =\n\t\t/* NOTE: MONITOR (and MWAIT) are emulated as NOP,\n\t\t * but *not* advertised to guests via CPUID ! */\n\t\tF(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |\n\t\t0 /* DS-CPL, VMX, SMX, EST */ |\n\t\t0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |\n\t\tF(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |\n\t\tF(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |\n\t\tF(F16C) | F(RDRAND);\n\t/* cpuid 0x80000001.ecx */\n\tconst u32 kvm_cpuid_8000_0001_ecx_x86_features =\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |\n\t\t0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM) |\n\t\tF(TOPOEXT) | F(PERFCTR_CORE);\n\n\t/* cpuid 0x80000008.ebx */\n\tconst u32 kvm_cpuid_8000_0008_ebx_x86_features =\n\t\tF(CLZERO) | F(XSAVEERPTR) |\n\t\tF(WBNOINVD) | F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |\n\t\tF(AMD_SSB_NO) | F(AMD_STIBP) | F(AMD_STIBP_ALWAYS_ON);\n\n\t/* cpuid 0xC0000001.edx */\n\tconst u32 kvm_cpuid_C000_0001_edx_x86_features =\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN);\n\n\t/* cpuid 0xD.1.eax */\n\tconst u32 kvm_cpuid_D_1_eax_x86_features =\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\n\n\t/* all calls to cpuid_count() should be made on the same cpu */\n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tif (*nent >= maxnent)\n\t\tgoto out;\n\n\tdo_host_cpuid(entry, function, 0);\n\t++*nent;\n\n\tswitch (function) {\n\tcase 0:\n\t\t/* Limited to the highest leaf implemented in KVM. */\n\t\tentry->eax = min(entry->eax, 0x1fU);\n\t\tbreak;\n\tcase 1:\n\t\tentry->edx &= kvm_cpuid_1_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_1_EDX);\n\t\tentry->ecx &= kvm_cpuid_1_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_1_ECX);\n\t\t/* we support x2apic emulation even if host does not support\n\t\t * it since we emulate x2apic in software */\n\t\tentry->ecx |= F(X2APIC);\n\t\tbreak;\n\t/* function 2 entries are STATEFUL. That is, repeated cpuid commands\n\t * may return different values. This forces us to get_cpu() before\n\t * issuing the first command, and also to emulate this annoying behavior\n\t * in kvm_emulate_cpuid() using KVM_CPUID_FLAG_STATE_READ_NEXT */\n\tcase 2: {\n\t\tint t, times = entry->eax & 0xff;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\n\t\tfor (t = 1; t < times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_host_cpuid(&entry[t], function, 0);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\t/* functions 4 and 0x8000001d have additional index. */\n\tcase 4:\n\tcase 0x8000001d: {\n\t\tint i, cache_type;\n\n\t\t/* read more entries until cache_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tcache_type = entry[i - 1].eax & 0x1f;\n\t\t\tif (!cache_type)\n\t\t\t\tbreak;\n\t\t\tdo_host_cpuid(&entry[i], function, i);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 6: /* Thermal management */\n\t\tentry->eax = 0x4; /* allow ARAT */\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t/* function 7 has additional index. */\n\tcase 7: {\n\t\tint i;\n\n\t\tfor (i = 0; ; ) {\n\t\t\tdo_cpuid_7_mask(&entry[i], i);\n\t\t\tif (i == entry->eax)\n\t\t\t\tbreak;\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\t++i;\n\t\t\tdo_host_cpuid(&entry[i], function, i);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 9:\n\t\tbreak;\n\tcase 0xa: { /* Architectural Performance Monitoring */\n\t\tstruct x86_pmu_capability cap;\n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tperf_get_x86_pmu_capability(&cap);\n\n\t\t/*\n\t\t * Only support guest architectural pmu on a host\n\t\t * with architectural pmu.\n\t\t */\n\t\tif (!cap.version)\n\t\t\tmemset(&cap, 0, sizeof(cap));\n\n\t\teax.split.version_id = min(cap.version, 2);\n\t\teax.split.num_counters = cap.num_counters_gp;\n\t\teax.split.bit_width = cap.bit_width_gp;\n\t\teax.split.mask_length = cap.events_mask_len;\n\n\t\tedx.split.num_counters_fixed = cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = cap.bit_width_fixed;\n\t\tedx.split.reserved = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\t/*\n\t * Per Intel's SDM, the 0x1f is a superset of 0xb,\n\t * thus they can be handled by common code.\n\t */\n\tcase 0x1f:\n\tcase 0xb: {\n\t\tint i;\n\n\t\t/*\n\t\t * We filled in entry[0] for CPUID(EAX=<function>,\n\t\t * ECX=00H) above.  If its level type (ECX[15:8]) is\n\t\t * zero, then the leaf is unimplemented, and we're\n\t\t * done.  Otherwise, continue to populate entries\n\t\t * until the level type (ECX[15:8]) of the previously\n\t\t * added entry is zero.\n\t\t */\n\t\tfor (i = 1; entry[i - 1].ecx & 0xff00; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_host_cpuid(&entry[i], function, i);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0xd: {\n\t\tint idx, i;\n\t\tu64 supported = kvm_supported_xcr0();\n\n\t\tentry->eax &= supported;\n\t\tentry->ebx = xstate_required_size(supported, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= supported >> 32;\n\t\tif (!supported)\n\t\t\tbreak;\n\n\t\tfor (idx = 1, i = 1; idx < 64; ++idx) {\n\t\t\tu64 mask = ((u64)1 << idx);\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_host_cpuid(&entry[i], function, idx);\n\t\t\tif (idx == 1) {\n\t\t\t\tentry[i].eax &= kvm_cpuid_D_1_eax_x86_features;\n\t\t\t\tcpuid_mask(&entry[i].eax, CPUID_D_1_EAX);\n\t\t\t\tentry[i].ebx = 0;\n\t\t\t\tif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\t\t\tentry[i].ebx =\n\t\t\t\t\t\txstate_required_size(supported,\n\t\t\t\t\t\t\t\t     true);\n\t\t\t} else {\n\t\t\t\tif (entry[i].eax == 0 || !(supported & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (WARN_ON_ONCE(entry[i].ecx & 1))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tentry[i].ecx = 0;\n\t\t\tentry[i].edx = 0;\n\t\t\t++*nent;\n\t\t\t++i;\n\t\t}\n\t\tbreak;\n\t}\n\t/* Intel PT */\n\tcase 0x14: {\n\t\tint t, times = entry->eax;\n\n\t\tif (!f_intel_pt)\n\t\t\tbreak;\n\n\t\tfor (t = 1; t <= times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\t\t\tdo_host_cpuid(&entry[t], function, t);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\n\t\tconst u32 *sigptr = (const u32 *)signature;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT) |\n\t\t\t     (1 << KVM_FEATURE_PV_TLB_FLUSH) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_SEND_IPI) |\n\t\t\t     (1 << KVM_FEATURE_POLL_CONTROL) |\n\t\t\t     (1 << KVM_FEATURE_PV_SCHED_YIELD);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x8000001f);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->edx &= kvm_cpuid_8000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_8000_0001_EDX);\n\t\tentry->ecx &= kvm_cpuid_8000_0001_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_8000_0001_ECX);\n\t\tbreak;\n\tcase 0x80000007: /* Advanced power management */\n\t\t/* invariant TSC is CPUID.80000007H:EDX[8] */\n\t\tentry->edx &= (1 << 8);\n\t\t/* mask against host */\n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\tif (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->edx = 0;\n\t\tentry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;\n\t\tcpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);\n\t\t/*\n\t\t * AMD has separate bits for each SPEC_CTRL bit.\n\t\t * arch/x86/kernel/cpu/bugs.c is kind enough to\n\t\t * record that in cpufeatures so use them.\n\t\t */\n\t\tif (boot_cpu_has(X86_FEATURE_IBPB))\n\t\t\tentry->ebx |= F(AMD_IBPB);\n\t\tif (boot_cpu_has(X86_FEATURE_IBRS))\n\t\t\tentry->ebx |= F(AMD_IBRS);\n\t\tif (boot_cpu_has(X86_FEATURE_STIBP))\n\t\t\tentry->ebx |= F(AMD_STIBP);\n\t\tif (boot_cpu_has(X86_FEATURE_SSBD))\n\t\t\tentry->ebx |= F(AMD_SSBD);\n\t\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\t\tentry->ebx |= F(AMD_SSB_NO);\n\t\t/*\n\t\t * The preference is to use SPEC CTRL MSR instead of the\n\t\t * VIRT_SPEC MSR.\n\t\t */\n\t\tif (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n\t\t    !boot_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\t\tentry->ebx |= F(VIRT_SSBD);\n\t\tbreak;\n\t}\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\tcase 0x8000001e:\n\t\tbreak;\n\t/* Support memory encryption cpuid if host supports it */\n\tcase 0x8000001F:\n\t\tif (!boot_cpu_has(X86_FEATURE_SEV))\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t/*Add support for Centaur's CPUID instruction*/\n\tcase 0xC0000000:\n\t\t/*Just support up to 0xC0000004 now*/\n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tentry->edx &= kvm_cpuid_C000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_C000_0001_EDX);\n\t\tbreak;\n\tcase 3: /* Processor serial number */\n\tcase 5: /* MONITOR/MWAIT */\n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tkvm_x86_ops->set_supported_cpuid(function, entry);\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}",
      "code_after_change": "static inline int __do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 function,\n\t\t\t\t  int *nent, int maxnent)\n{\n\tint r;\n\tunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\n\tunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n\t\t\t\t? F(GBPAGES) : 0;\n\tunsigned f_lm = F(LM);\n#else\n\tunsigned f_gbpages = 0;\n\tunsigned f_lm = 0;\n#endif\n\tunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\n\tunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\n\tunsigned f_intel_pt = kvm_x86_ops->pt_supported() ? F(INTEL_PT) : 0;\n\n\t/* cpuid 1.edx */\n\tconst u32 kvm_cpuid_1_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SEP) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* PSN */ | F(CLFLUSH) |\n\t\t0 /* Reserved, DS, ACPI */ | F(MMX) |\n\t\tF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n\t\t0 /* HTT, TM, Reserved, PBE */;\n\t/* cpuid 0x80000001.edx */\n\tconst u32 kvm_cpuid_8000_0001_edx_x86_features =\n\t\tF(FPU) | F(VME) | F(DE) | F(PSE) |\n\t\tF(TSC) | F(MSR) | F(PAE) | F(MCE) |\n\t\tF(CX8) | F(APIC) | 0 /* Reserved */ | F(SYSCALL) |\n\t\tF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\n\t\tF(PAT) | F(PSE36) | 0 /* Reserved */ |\n\t\tf_nx | 0 /* Reserved */ | F(MMXEXT) | F(MMX) |\n\t\tF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n\t\t0 /* Reserved */ | f_lm | F(3DNOWEXT) | F(3DNOW);\n\t/* cpuid 1.ecx */\n\tconst u32 kvm_cpuid_1_ecx_x86_features =\n\t\t/* NOTE: MONITOR (and MWAIT) are emulated as NOP,\n\t\t * but *not* advertised to guests via CPUID ! */\n\t\tF(XMM3) | F(PCLMULQDQ) | 0 /* DTES64, MONITOR */ |\n\t\t0 /* DS-CPL, VMX, SMX, EST */ |\n\t\t0 /* TM2 */ | F(SSSE3) | 0 /* CNXT-ID */ | 0 /* Reserved */ |\n\t\tF(FMA) | F(CX16) | 0 /* xTPR Update, PDCM */ |\n\t\tF(PCID) | 0 /* Reserved, DCA */ | F(XMM4_1) |\n\t\tF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n\t\t0 /* Reserved*/ | F(AES) | F(XSAVE) | 0 /* OSXSAVE */ | F(AVX) |\n\t\tF(F16C) | F(RDRAND);\n\t/* cpuid 0x80000001.ecx */\n\tconst u32 kvm_cpuid_8000_0001_ecx_x86_features =\n\t\tF(LAHF_LM) | F(CMP_LEGACY) | 0 /*SVM*/ | 0 /* ExtApicSpace */ |\n\t\tF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\n\t\tF(3DNOWPREFETCH) | F(OSVW) | 0 /* IBS */ | F(XOP) |\n\t\t0 /* SKINIT, WDT, LWP */ | F(FMA4) | F(TBM) |\n\t\tF(TOPOEXT) | F(PERFCTR_CORE);\n\n\t/* cpuid 0x80000008.ebx */\n\tconst u32 kvm_cpuid_8000_0008_ebx_x86_features =\n\t\tF(CLZERO) | F(XSAVEERPTR) |\n\t\tF(WBNOINVD) | F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |\n\t\tF(AMD_SSB_NO) | F(AMD_STIBP) | F(AMD_STIBP_ALWAYS_ON);\n\n\t/* cpuid 0xC0000001.edx */\n\tconst u32 kvm_cpuid_C000_0001_edx_x86_features =\n\t\tF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\n\t\tF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\n\t\tF(PMM) | F(PMM_EN);\n\n\t/* cpuid 0xD.1.eax */\n\tconst u32 kvm_cpuid_D_1_eax_x86_features =\n\t\tF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\n\n\t/* all calls to cpuid_count() should be made on the same cpu */\n\tget_cpu();\n\n\tr = -E2BIG;\n\n\tif (WARN_ON(*nent >= maxnent))\n\t\tgoto out;\n\n\tdo_host_cpuid(entry, function, 0);\n\t++*nent;\n\n\tswitch (function) {\n\tcase 0:\n\t\t/* Limited to the highest leaf implemented in KVM. */\n\t\tentry->eax = min(entry->eax, 0x1fU);\n\t\tbreak;\n\tcase 1:\n\t\tentry->edx &= kvm_cpuid_1_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_1_EDX);\n\t\tentry->ecx &= kvm_cpuid_1_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_1_ECX);\n\t\t/* we support x2apic emulation even if host does not support\n\t\t * it since we emulate x2apic in software */\n\t\tentry->ecx |= F(X2APIC);\n\t\tbreak;\n\t/* function 2 entries are STATEFUL. That is, repeated cpuid commands\n\t * may return different values. This forces us to get_cpu() before\n\t * issuing the first command, and also to emulate this annoying behavior\n\t * in kvm_emulate_cpuid() using KVM_CPUID_FLAG_STATE_READ_NEXT */\n\tcase 2: {\n\t\tint t, times = entry->eax & 0xff;\n\n\t\tentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\n\t\tfor (t = 1; t < times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_host_cpuid(&entry[t], function, 0);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\t/* functions 4 and 0x8000001d have additional index. */\n\tcase 4:\n\tcase 0x8000001d: {\n\t\tint i, cache_type;\n\n\t\t/* read more entries until cache_type is zero */\n\t\tfor (i = 1; ; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tcache_type = entry[i - 1].eax & 0x1f;\n\t\t\tif (!cache_type)\n\t\t\t\tbreak;\n\t\t\tdo_host_cpuid(&entry[i], function, i);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 6: /* Thermal management */\n\t\tentry->eax = 0x4; /* allow ARAT */\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\t/* function 7 has additional index. */\n\tcase 7: {\n\t\tint i;\n\n\t\tfor (i = 0; ; ) {\n\t\t\tdo_cpuid_7_mask(&entry[i], i);\n\t\t\tif (i == entry->eax)\n\t\t\t\tbreak;\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\t++i;\n\t\t\tdo_host_cpuid(&entry[i], function, i);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 9:\n\t\tbreak;\n\tcase 0xa: { /* Architectural Performance Monitoring */\n\t\tstruct x86_pmu_capability cap;\n\t\tunion cpuid10_eax eax;\n\t\tunion cpuid10_edx edx;\n\n\t\tperf_get_x86_pmu_capability(&cap);\n\n\t\t/*\n\t\t * Only support guest architectural pmu on a host\n\t\t * with architectural pmu.\n\t\t */\n\t\tif (!cap.version)\n\t\t\tmemset(&cap, 0, sizeof(cap));\n\n\t\teax.split.version_id = min(cap.version, 2);\n\t\teax.split.num_counters = cap.num_counters_gp;\n\t\teax.split.bit_width = cap.bit_width_gp;\n\t\teax.split.mask_length = cap.events_mask_len;\n\n\t\tedx.split.num_counters_fixed = cap.num_counters_fixed;\n\t\tedx.split.bit_width_fixed = cap.bit_width_fixed;\n\t\tedx.split.reserved = 0;\n\n\t\tentry->eax = eax.full;\n\t\tentry->ebx = cap.events_mask;\n\t\tentry->ecx = 0;\n\t\tentry->edx = edx.full;\n\t\tbreak;\n\t}\n\t/*\n\t * Per Intel's SDM, the 0x1f is a superset of 0xb,\n\t * thus they can be handled by common code.\n\t */\n\tcase 0x1f:\n\tcase 0xb: {\n\t\tint i;\n\n\t\t/*\n\t\t * We filled in entry[0] for CPUID(EAX=<function>,\n\t\t * ECX=00H) above.  If its level type (ECX[15:8]) is\n\t\t * zero, then the leaf is unimplemented, and we're\n\t\t * done.  Otherwise, continue to populate entries\n\t\t * until the level type (ECX[15:8]) of the previously\n\t\t * added entry is zero.\n\t\t */\n\t\tfor (i = 1; entry[i - 1].ecx & 0xff00; ++i) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_host_cpuid(&entry[i], function, i);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 0xd: {\n\t\tint idx, i;\n\t\tu64 supported = kvm_supported_xcr0();\n\n\t\tentry->eax &= supported;\n\t\tentry->ebx = xstate_required_size(supported, false);\n\t\tentry->ecx = entry->ebx;\n\t\tentry->edx &= supported >> 32;\n\t\tif (!supported)\n\t\t\tbreak;\n\n\t\tfor (idx = 1, i = 1; idx < 64; ++idx) {\n\t\t\tu64 mask = ((u64)1 << idx);\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\n\t\t\tdo_host_cpuid(&entry[i], function, idx);\n\t\t\tif (idx == 1) {\n\t\t\t\tentry[i].eax &= kvm_cpuid_D_1_eax_x86_features;\n\t\t\t\tcpuid_mask(&entry[i].eax, CPUID_D_1_EAX);\n\t\t\t\tentry[i].ebx = 0;\n\t\t\t\tif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\n\t\t\t\t\tentry[i].ebx =\n\t\t\t\t\t\txstate_required_size(supported,\n\t\t\t\t\t\t\t\t     true);\n\t\t\t} else {\n\t\t\t\tif (entry[i].eax == 0 || !(supported & mask))\n\t\t\t\t\tcontinue;\n\t\t\t\tif (WARN_ON_ONCE(entry[i].ecx & 1))\n\t\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tentry[i].ecx = 0;\n\t\t\tentry[i].edx = 0;\n\t\t\t++*nent;\n\t\t\t++i;\n\t\t}\n\t\tbreak;\n\t}\n\t/* Intel PT */\n\tcase 0x14: {\n\t\tint t, times = entry->eax;\n\n\t\tif (!f_intel_pt)\n\t\t\tbreak;\n\n\t\tfor (t = 1; t <= times; ++t) {\n\t\t\tif (*nent >= maxnent)\n\t\t\t\tgoto out;\n\t\t\tdo_host_cpuid(&entry[t], function, t);\n\t\t\t++*nent;\n\t\t}\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_SIGNATURE: {\n\t\tstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\n\t\tconst u32 *sigptr = (const u32 *)signature;\n\t\tentry->eax = KVM_CPUID_FEATURES;\n\t\tentry->ebx = sigptr[0];\n\t\tentry->ecx = sigptr[1];\n\t\tentry->edx = sigptr[2];\n\t\tbreak;\n\t}\n\tcase KVM_CPUID_FEATURES:\n\t\tentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n\t\t\t     (1 << KVM_FEATURE_NOP_IO_DELAY) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE2) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF) |\n\t\t\t     (1 << KVM_FEATURE_PV_EOI) |\n\t\t\t     (1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_UNHALT) |\n\t\t\t     (1 << KVM_FEATURE_PV_TLB_FLUSH) |\n\t\t\t     (1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |\n\t\t\t     (1 << KVM_FEATURE_PV_SEND_IPI) |\n\t\t\t     (1 << KVM_FEATURE_POLL_CONTROL) |\n\t\t\t     (1 << KVM_FEATURE_PV_SCHED_YIELD);\n\n\t\tif (sched_info_on())\n\t\t\tentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\n\n\t\tentry->ebx = 0;\n\t\tentry->ecx = 0;\n\t\tentry->edx = 0;\n\t\tbreak;\n\tcase 0x80000000:\n\t\tentry->eax = min(entry->eax, 0x8000001f);\n\t\tbreak;\n\tcase 0x80000001:\n\t\tentry->edx &= kvm_cpuid_8000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_8000_0001_EDX);\n\t\tentry->ecx &= kvm_cpuid_8000_0001_ecx_x86_features;\n\t\tcpuid_mask(&entry->ecx, CPUID_8000_0001_ECX);\n\t\tbreak;\n\tcase 0x80000007: /* Advanced power management */\n\t\t/* invariant TSC is CPUID.80000007H:EDX[8] */\n\t\tentry->edx &= (1 << 8);\n\t\t/* mask against host */\n\t\tentry->edx &= boot_cpu_data.x86_power;\n\t\tentry->eax = entry->ebx = entry->ecx = 0;\n\t\tbreak;\n\tcase 0x80000008: {\n\t\tunsigned g_phys_as = (entry->eax >> 16) & 0xff;\n\t\tunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\n\t\tunsigned phys_as = entry->eax & 0xff;\n\n\t\tif (!g_phys_as)\n\t\t\tg_phys_as = phys_as;\n\t\tentry->eax = g_phys_as | (virt_as << 8);\n\t\tentry->edx = 0;\n\t\tentry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;\n\t\tcpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);\n\t\t/*\n\t\t * AMD has separate bits for each SPEC_CTRL bit.\n\t\t * arch/x86/kernel/cpu/bugs.c is kind enough to\n\t\t * record that in cpufeatures so use them.\n\t\t */\n\t\tif (boot_cpu_has(X86_FEATURE_IBPB))\n\t\t\tentry->ebx |= F(AMD_IBPB);\n\t\tif (boot_cpu_has(X86_FEATURE_IBRS))\n\t\t\tentry->ebx |= F(AMD_IBRS);\n\t\tif (boot_cpu_has(X86_FEATURE_STIBP))\n\t\t\tentry->ebx |= F(AMD_STIBP);\n\t\tif (boot_cpu_has(X86_FEATURE_SSBD))\n\t\t\tentry->ebx |= F(AMD_SSBD);\n\t\tif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\n\t\t\tentry->ebx |= F(AMD_SSB_NO);\n\t\t/*\n\t\t * The preference is to use SPEC CTRL MSR instead of the\n\t\t * VIRT_SPEC MSR.\n\t\t */\n\t\tif (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n\t\t    !boot_cpu_has(X86_FEATURE_AMD_SSBD))\n\t\t\tentry->ebx |= F(VIRT_SSBD);\n\t\tbreak;\n\t}\n\tcase 0x80000019:\n\t\tentry->ecx = entry->edx = 0;\n\t\tbreak;\n\tcase 0x8000001a:\n\tcase 0x8000001e:\n\t\tbreak;\n\t/* Support memory encryption cpuid if host supports it */\n\tcase 0x8000001F:\n\t\tif (!boot_cpu_has(X86_FEATURE_SEV))\n\t\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t/*Add support for Centaur's CPUID instruction*/\n\tcase 0xC0000000:\n\t\t/*Just support up to 0xC0000004 now*/\n\t\tentry->eax = min(entry->eax, 0xC0000004);\n\t\tbreak;\n\tcase 0xC0000001:\n\t\tentry->edx &= kvm_cpuid_C000_0001_edx_x86_features;\n\t\tcpuid_mask(&entry->edx, CPUID_C000_0001_EDX);\n\t\tbreak;\n\tcase 3: /* Processor serial number */\n\tcase 5: /* MONITOR/MWAIT */\n\tcase 0xC0000002:\n\tcase 0xC0000003:\n\tcase 0xC0000004:\n\tdefault:\n\t\tentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\n\t\tbreak;\n\t}\n\n\tkvm_x86_ops->set_supported_cpuid(function, entry);\n\n\tr = 0;\n\nout:\n\tput_cpu();\n\n\treturn r;\n}",
      "modified_lines": {
        "added": [
          "\tif (WARN_ON(*nent >= maxnent))"
        ],
        "deleted": [
          "\tif (*nent >= maxnent)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of boundary check for memory write operations on the 'entry' array in the KVM hypervisor code.",
      "trigger_condition": "An attacker with access to the '/dev/kvm' device triggers the 'KVM_GET_EMULATED_CPUID' ioctl(2) request, leading to an out-of-bounds memory write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the boundary of the 'entry' array before writing to it, allowing an attacker to trigger a denial of service by causing a crash through the out-of-bounds memory write.",
      "solution": "To mitigate the vulnerability, a boundary check should be added before writing to the 'entry' array in the KVM hypervisor code. By adding a check to ensure that the write operation stays within the bounds of the array, the code becomes more secure and prevents the out-of-bounds memory write vulnerability from being exploited.",
      "id": 46,
      "code_after_change_normalized": "static inline int FUN1(struct kvm_cpuid_entry2 *VAR1, u32 VAR2,\nint *VAR3, int VAR4)\n{\nint VAR5;\nunsigned VAR6 = FUN2() ? FUN3(VAR7) : 0;\n#ifdef VAR8\nunsigned VAR9 = (VAR10->FUN4() == VAR11)\n? FUN3(VAR12) : 0;\nunsigned VAR13 = FUN3(VAR14);\n#else\nunsigned VAR9 = 0;\nunsigned VAR13 = 0;\n#VAR15\nunsigned VAR16 = VAR10->FUN5() ? FUN3(VAR17) : 0;\nunsigned VAR18 = VAR10->FUN6() ? FUN3(VAR19) : 0;\nunsigned VAR20 = VAR10->FUN7() ? FUN3(VAR21) : 0;\nconst u32 VAR22 =\nFUN3(VAR23) | FUN3(VAR24) | FUN3(VAR25) | FUN3(VAR26) |\nFUN3(VAR27) | FUN3(VAR28) | FUN3(VAR29) | FUN3(VAR30) |\nFUN3(VAR31) | FUN3(VAR32) | 0  | FUN3(VAR33) |\nFUN3(VAR34) | FUN3(VAR35) | FUN3(VAR36) | FUN3(VAR37) |\nFUN3(VAR38) | FUN3(VAR39) | 0  | FUN3(VAR40) |\n0  | FUN3(VAR41) |\nFUN3(VAR42) | FUN3(VAR43) | FUN3(VAR44) | FUN3(VAR45) |\n0 ;\nconst u32 VAR46 =\nFUN3(VAR23) | FUN3(VAR24) | FUN3(VAR25) | FUN3(VAR26) |\nFUN3(VAR27) | FUN3(VAR28) | FUN3(VAR29) | FUN3(VAR30) |\nFUN3(VAR31) | FUN3(VAR32) | 0  | FUN3(VAR47) |\nFUN3(VAR34) | FUN3(VAR35) | FUN3(VAR36) | FUN3(VAR37) |\nFUN3(VAR38) | FUN3(VAR39) | 0  |\nVAR6 | 0  | FUN3(VAR48) | FUN3(VAR41) |\nFUN3(VAR42) | FUN3(VAR49) | VAR9 | VAR16 |\n0  | VAR13 | FUN3(3DNOWEXT) | FUN3(3DNOW);\nconst u32 VAR50 =\nFUN3(VAR51) | FUN3(VAR52) | 0  |\n0  |\n0  | FUN3(VAR53) | 0  | 0  |\nFUN3(VAR54) | FUN3(VAR55) | 0  |\nFUN3(VAR56) | 0  | FUN3(VAR57) |\nFUN3(VAR58) | FUN3(VAR59) | FUN3(VAR60) | FUN3(VAR61) |\n0  | FUN3(VAR62) | FUN3(VAR63) | 0  | FUN3(VAR64) |\nFUN3(VAR65) | FUN3(VAR66);\nconst u32 VAR67 =\nFUN3(VAR68) | FUN3(VAR69) | 0  | 0  |\nFUN3(VAR70) | FUN3(VAR71) | FUN3(VAR72) | FUN3(VAR73) |\nFUN3(3DNOWPREFETCH) | FUN3(VAR74) | 0  | FUN3(VAR75) |\n0  | FUN3(VAR76) | FUN3(VAR77) |\nFUN3(VAR78) | FUN3(VAR79);\nconst u32 VAR80 =\nFUN3(VAR81) | FUN3(VAR82) |\nFUN3(VAR83) | FUN3(VAR84) | FUN3(VAR85) | FUN3(VAR86) | FUN3(VAR87) |\nFUN3(VAR88) | FUN3(VAR89) | FUN3(VAR90);\nconst u32 VAR91 =\nFUN3(VAR92) | FUN3(VAR93) | FUN3(VAR94) | FUN3(VAR95) |\nFUN3(VAR96) | FUN3(VAR97) | FUN3(VAR98) | FUN3(VAR99) |\nFUN3(VAR100) | FUN3(VAR101);\nconst u32 VAR102 =\nFUN3(VAR103) | FUN3(VAR104) | FUN3(VAR105) | VAR18;\nFUN8();\nVAR5 = -VAR106;\nif (FUN9(*VAR3 >= VAR4))\ngoto VAR107;\nFUN10(VAR1, VAR2, 0);\n++*VAR3;\nswitch (VAR2) {\ncase 0:\nVAR1->VAR108 = FUN11(VAR1->VAR108, VAR109);\nbreak;\ncase 1:\nVAR1->VAR110 &= VAR22;\nFUN12(&VAR1->VAR110, VAR111);\nVAR1->VAR112 &= VAR50;\nFUN12(&VAR1->VAR112, VAR113);\nVAR1->VAR112 |= FUN3(VAR59);\nbreak;\ncase 2: {\nint VAR114, VAR115 = VAR1->VAR108 & VAR116;\nVAR1->VAR117 |= VAR118;\nfor (VAR114 = 1; VAR114 < VAR115; ++VAR114) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN10(&VAR1[VAR114], VAR2, 0);\n++*VAR3;\n}\nbreak;\n}\ncase 4:\ncase VAR116: {\nint VAR119, VAR120;\nfor (VAR119 = 1; ; ++VAR119) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nVAR120 = VAR1[VAR119 - 1].VAR108 & VAR116;\nif (!VAR120)\nbreak;\nFUN10(&VAR1[VAR119], VAR2, VAR119);\n++*VAR3;\n}\nbreak;\n}\ncase 6: \nVAR1->VAR108 = VAR116; \nVAR1->VAR121 = 0;\nVAR1->VAR112 = 0;\nVAR1->VAR110 = 0;\nbreak;\ncase 7: {\nint VAR119;\nfor (VAR119 = 0; ; ) {\nFUN13(&VAR1[VAR119], VAR119);\nif (VAR119 == VAR1->VAR108)\nbreak;\nif (*VAR3 >= VAR4)\ngoto VAR107;\n++VAR119;\nFUN10(&VAR1[VAR119], VAR2, VAR119);\n++*VAR3;\n}\nbreak;\n}\ncase 9:\nbreak;\ncase VAR116: { \nstruct x86_pmu_capability VAR122;\nunion cpuid10_eax VAR108;\nunion cpuid10_edx VAR110;\nFUN14(&VAR122);\nif (!VAR122.VAR123)\nFUN15(&VAR122, 0, sizeof(VAR122));\nVAR108.VAR124.VAR125 = FUN11(VAR122.VAR123, 2);\nVAR108.VAR124.VAR126 = VAR122.VAR127;\nVAR108.VAR124.VAR128 = VAR122.VAR129;\nVAR108.VAR124.VAR130 = VAR122.VAR131;\nVAR110.VAR124.VAR132 = VAR122.VAR132;\nVAR110.VAR124.VAR133 = VAR122.VAR133;\nVAR110.VAR124.VAR134 = 0;\nVAR1->VAR108 = VAR108.VAR135;\nVAR1->VAR121 = VAR122.VAR136;\nVAR1->VAR112 = 0;\nVAR1->VAR110 = VAR110.VAR135;\nbreak;\n}\ncase VAR116:\ncase VAR116: {\nint VAR119;\nfor (VAR119 = 1; VAR1[VAR119 - 1].VAR112 & VAR116; ++VAR119) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN10(&VAR1[VAR119], VAR2, VAR119);\n++*VAR3;\n}\nbreak;\n}\ncase VAR116: {\nint VAR137, VAR119;\nu64 VAR138 = FUN16();\nVAR1->VAR108 &= VAR138;\nVAR1->VAR121 = FUN17(VAR138, false);\nVAR1->VAR112 = VAR1->VAR121;\nVAR1->VAR110 &= VAR138 >> 32;\nif (!VAR138)\nbreak;\nfor (VAR137 = 1, VAR119 = 1; VAR137 < 64; ++VAR137) {\nu64 VAR139 = ((VAR140)1 << VAR137);\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN10(&VAR1[VAR119], VAR2, VAR137);\nif (VAR137 == 1) {\nVAR1[VAR119].VAR108 &= VAR102;\nFUN12(&VAR1[VAR119].VAR108, VAR141);\nVAR1[VAR119].VAR121 = 0;\nif (VAR1[VAR119].VAR108 & (FUN3(VAR19)|FUN3(VAR104)))\nVAR1[VAR119].VAR121 =\nFUN17(VAR138,\ntrue);\n} else {\nif (VAR1[VAR119].VAR108 == 0 || !(VAR138 & VAR139))\ncontinue;\nif (FUN18(VAR1[VAR119].VAR112 & 1))\ncontinue;\n}\nVAR1[VAR119].VAR112 = 0;\nVAR1[VAR119].VAR110 = 0;\n++*VAR3;\n++VAR119;\n}\nbreak;\n}\ncase VAR116: {\nint VAR114, VAR115 = VAR1->VAR108;\nif (!VAR20)\nbreak;\nfor (VAR114 = 1; VAR114 <= VAR115; ++VAR114) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN10(&VAR1[VAR114], VAR2, VAR114);\n++*VAR3;\n}\nbreak;\n}\ncase VAR142: {\nstatic const char VAR143[12] = \"STR\";\nconst VAR145 *VAR144 = (const VAR145 *)VAR143;\nVAR1->VAR108 = VAR146;\nVAR1->VAR121 = VAR144[0];\nVAR1->VAR112 = VAR144[1];\nVAR1->VAR110 = VAR144[2];\nbreak;\n}\ncase VAR146:\nVAR1->VAR108 = (1 << VAR147) |\n(1 << VAR148) |\n(1 << VAR149) |\n(1 << VAR150) |\n(1 << VAR151) |\n(1 << VAR152) |\n(1 << VAR153) |\n(1 << VAR154) |\n(1 << VAR155) |\n(1 << VAR156) |\n(1 << VAR157) |\n(1 << VAR158);\nif (FUN19())\nVAR1->VAR108 |= (1 << VAR159);\nVAR1->VAR121 = 0;\nVAR1->VAR112 = 0;\nVAR1->VAR110 = 0;\nbreak;\ncase VAR116:\nVAR1->VAR108 = FUN11(VAR1->VAR108, VAR116);\nbreak;\ncase VAR116:\nVAR1->VAR110 &= VAR46;\nFUN12(&VAR1->VAR110, VAR160);\nVAR1->VAR112 &= VAR67;\nFUN12(&VAR1->VAR112, VAR161);\nbreak;\ncase VAR116: \nVAR1->VAR110 &= (1 << 8);\nVAR1->VAR110 &= VAR162.VAR163;\nVAR1->VAR108 = VAR1->VAR121 = VAR1->VAR112 = 0;\nbreak;\ncase VAR116: {\nunsigned VAR164 = (VAR1->VAR108 >> 16) & VAR116;\nunsigned VAR165 = FUN20((VAR1->VAR108 >> 8) & VAR116, 48U);\nunsigned VAR166 = VAR1->VAR108 & VAR116;\nif (!VAR164)\nVAR164 = VAR166;\nVAR1->VAR108 = VAR164 | (VAR165 << 8);\nVAR1->VAR110 = 0;\nVAR1->VAR121 &= VAR80;\nFUN12(&VAR1->VAR121, VAR167);\nif (FUN21(VAR168))\nVAR1->VAR121 |= FUN3(VAR84);\nif (FUN21(VAR169))\nVAR1->VAR121 |= FUN3(VAR85);\nif (FUN21(VAR170))\nVAR1->VAR121 |= FUN3(VAR89);\nif (FUN21(VAR171))\nVAR1->VAR121 |= FUN3(VAR86);\nif (!FUN22(VAR172))\nVAR1->VAR121 |= FUN3(VAR88);\nif (FUN21(VAR173) &&\n!FUN21(VAR174))\nVAR1->VAR121 |= FUN3(VAR87);\nbreak;\n}\ncase VAR116:\nVAR1->VAR112 = VAR1->VAR110 = 0;\nbreak;\ncase VAR116:\ncase VAR116:\nbreak;\ncase VAR116:\nif (!FUN21(VAR175))\nVAR1->VAR108 = VAR1->VAR121 = VAR1->VAR112 = VAR1->VAR110 = 0;\nbreak;\ncase VAR116:\nVAR1->VAR108 = FUN11(VAR1->VAR108, VAR116);\nbreak;\ncase VAR116:\nVAR1->VAR110 &= VAR91;\nFUN12(&VAR1->VAR110, VAR176);\nbreak;\ncase 3: \ncase 5: \ncase VAR116:\ncase VAR116:\ncase VAR116:\ndefault:\nVAR1->VAR108 = VAR1->VAR121 = VAR1->VAR112 = VAR1->VAR110 = 0;\nbreak;\n}\nVAR10->FUN23(VAR2, VAR1);\nVAR5 = 0;\nVAR107:\nFUN24();\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "static inline int FUN1(struct kvm_cpuid_entry2 *VAR1, u32 VAR2,\nint *VAR3, int VAR4)\n{\nint VAR5;\nunsigned VAR6 = FUN2() ? FUN3(VAR7) : 0;\n#ifdef VAR8\nunsigned VAR9 = (VAR10->FUN4() == VAR11)\n? FUN3(VAR12) : 0;\nunsigned VAR13 = FUN3(VAR14);\n#else\nunsigned VAR9 = 0;\nunsigned VAR13 = 0;\n#VAR15\nunsigned VAR16 = VAR10->FUN5() ? FUN3(VAR17) : 0;\nunsigned VAR18 = VAR10->FUN6() ? FUN3(VAR19) : 0;\nunsigned VAR20 = VAR10->FUN7() ? FUN3(VAR21) : 0;\nconst u32 VAR22 =\nFUN3(VAR23) | FUN3(VAR24) | FUN3(VAR25) | FUN3(VAR26) |\nFUN3(VAR27) | FUN3(VAR28) | FUN3(VAR29) | FUN3(VAR30) |\nFUN3(VAR31) | FUN3(VAR32) | 0  | FUN3(VAR33) |\nFUN3(VAR34) | FUN3(VAR35) | FUN3(VAR36) | FUN3(VAR37) |\nFUN3(VAR38) | FUN3(VAR39) | 0  | FUN3(VAR40) |\n0  | FUN3(VAR41) |\nFUN3(VAR42) | FUN3(VAR43) | FUN3(VAR44) | FUN3(VAR45) |\n0 ;\nconst u32 VAR46 =\nFUN3(VAR23) | FUN3(VAR24) | FUN3(VAR25) | FUN3(VAR26) |\nFUN3(VAR27) | FUN3(VAR28) | FUN3(VAR29) | FUN3(VAR30) |\nFUN3(VAR31) | FUN3(VAR32) | 0  | FUN3(VAR47) |\nFUN3(VAR34) | FUN3(VAR35) | FUN3(VAR36) | FUN3(VAR37) |\nFUN3(VAR38) | FUN3(VAR39) | 0  |\nVAR6 | 0  | FUN3(VAR48) | FUN3(VAR41) |\nFUN3(VAR42) | FUN3(VAR49) | VAR9 | VAR16 |\n0  | VAR13 | FUN3(3DNOWEXT) | FUN3(3DNOW);\nconst u32 VAR50 =\nFUN3(VAR51) | FUN3(VAR52) | 0  |\n0  |\n0  | FUN3(VAR53) | 0  | 0  |\nFUN3(VAR54) | FUN3(VAR55) | 0  |\nFUN3(VAR56) | 0  | FUN3(VAR57) |\nFUN3(VAR58) | FUN3(VAR59) | FUN3(VAR60) | FUN3(VAR61) |\n0  | FUN3(VAR62) | FUN3(VAR63) | 0  | FUN3(VAR64) |\nFUN3(VAR65) | FUN3(VAR66);\nconst u32 VAR67 =\nFUN3(VAR68) | FUN3(VAR69) | 0  | 0  |\nFUN3(VAR70) | FUN3(VAR71) | FUN3(VAR72) | FUN3(VAR73) |\nFUN3(3DNOWPREFETCH) | FUN3(VAR74) | 0  | FUN3(VAR75) |\n0  | FUN3(VAR76) | FUN3(VAR77) |\nFUN3(VAR78) | FUN3(VAR79);\nconst u32 VAR80 =\nFUN3(VAR81) | FUN3(VAR82) |\nFUN3(VAR83) | FUN3(VAR84) | FUN3(VAR85) | FUN3(VAR86) | FUN3(VAR87) |\nFUN3(VAR88) | FUN3(VAR89) | FUN3(VAR90);\nconst u32 VAR91 =\nFUN3(VAR92) | FUN3(VAR93) | FUN3(VAR94) | FUN3(VAR95) |\nFUN3(VAR96) | FUN3(VAR97) | FUN3(VAR98) | FUN3(VAR99) |\nFUN3(VAR100) | FUN3(VAR101);\nconst u32 VAR102 =\nFUN3(VAR103) | FUN3(VAR104) | FUN3(VAR105) | VAR18;\nFUN8();\nVAR5 = -VAR106;\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN9(VAR1, VAR2, 0);\n++*VAR3;\nswitch (VAR2) {\ncase 0:\nVAR1->VAR108 = FUN10(VAR1->VAR108, VAR109);\nbreak;\ncase 1:\nVAR1->VAR110 &= VAR22;\nFUN11(&VAR1->VAR110, VAR111);\nVAR1->VAR112 &= VAR50;\nFUN11(&VAR1->VAR112, VAR113);\nVAR1->VAR112 |= FUN3(VAR59);\nbreak;\ncase 2: {\nint VAR114, VAR115 = VAR1->VAR108 & VAR116;\nVAR1->VAR117 |= VAR118;\nfor (VAR114 = 1; VAR114 < VAR115; ++VAR114) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN9(&VAR1[VAR114], VAR2, 0);\n++*VAR3;\n}\nbreak;\n}\ncase 4:\ncase VAR116: {\nint VAR119, VAR120;\nfor (VAR119 = 1; ; ++VAR119) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nVAR120 = VAR1[VAR119 - 1].VAR108 & VAR116;\nif (!VAR120)\nbreak;\nFUN9(&VAR1[VAR119], VAR2, VAR119);\n++*VAR3;\n}\nbreak;\n}\ncase 6: \nVAR1->VAR108 = VAR116; \nVAR1->VAR121 = 0;\nVAR1->VAR112 = 0;\nVAR1->VAR110 = 0;\nbreak;\ncase 7: {\nint VAR119;\nfor (VAR119 = 0; ; ) {\nFUN12(&VAR1[VAR119], VAR119);\nif (VAR119 == VAR1->VAR108)\nbreak;\nif (*VAR3 >= VAR4)\ngoto VAR107;\n++VAR119;\nFUN9(&VAR1[VAR119], VAR2, VAR119);\n++*VAR3;\n}\nbreak;\n}\ncase 9:\nbreak;\ncase VAR116: { \nstruct x86_pmu_capability VAR122;\nunion cpuid10_eax VAR108;\nunion cpuid10_edx VAR110;\nFUN13(&VAR122);\nif (!VAR122.VAR123)\nFUN14(&VAR122, 0, sizeof(VAR122));\nVAR108.VAR124.VAR125 = FUN10(VAR122.VAR123, 2);\nVAR108.VAR124.VAR126 = VAR122.VAR127;\nVAR108.VAR124.VAR128 = VAR122.VAR129;\nVAR108.VAR124.VAR130 = VAR122.VAR131;\nVAR110.VAR124.VAR132 = VAR122.VAR132;\nVAR110.VAR124.VAR133 = VAR122.VAR133;\nVAR110.VAR124.VAR134 = 0;\nVAR1->VAR108 = VAR108.VAR135;\nVAR1->VAR121 = VAR122.VAR136;\nVAR1->VAR112 = 0;\nVAR1->VAR110 = VAR110.VAR135;\nbreak;\n}\ncase VAR116:\ncase VAR116: {\nint VAR119;\nfor (VAR119 = 1; VAR1[VAR119 - 1].VAR112 & VAR116; ++VAR119) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN9(&VAR1[VAR119], VAR2, VAR119);\n++*VAR3;\n}\nbreak;\n}\ncase VAR116: {\nint VAR137, VAR119;\nu64 VAR138 = FUN15();\nVAR1->VAR108 &= VAR138;\nVAR1->VAR121 = FUN16(VAR138, false);\nVAR1->VAR112 = VAR1->VAR121;\nVAR1->VAR110 &= VAR138 >> 32;\nif (!VAR138)\nbreak;\nfor (VAR137 = 1, VAR119 = 1; VAR137 < 64; ++VAR137) {\nu64 VAR139 = ((VAR140)1 << VAR137);\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN9(&VAR1[VAR119], VAR2, VAR137);\nif (VAR137 == 1) {\nVAR1[VAR119].VAR108 &= VAR102;\nFUN11(&VAR1[VAR119].VAR108, VAR141);\nVAR1[VAR119].VAR121 = 0;\nif (VAR1[VAR119].VAR108 & (FUN3(VAR19)|FUN3(VAR104)))\nVAR1[VAR119].VAR121 =\nFUN16(VAR138,\ntrue);\n} else {\nif (VAR1[VAR119].VAR108 == 0 || !(VAR138 & VAR139))\ncontinue;\nif (FUN17(VAR1[VAR119].VAR112 & 1))\ncontinue;\n}\nVAR1[VAR119].VAR112 = 0;\nVAR1[VAR119].VAR110 = 0;\n++*VAR3;\n++VAR119;\n}\nbreak;\n}\ncase VAR116: {\nint VAR114, VAR115 = VAR1->VAR108;\nif (!VAR20)\nbreak;\nfor (VAR114 = 1; VAR114 <= VAR115; ++VAR114) {\nif (*VAR3 >= VAR4)\ngoto VAR107;\nFUN9(&VAR1[VAR114], VAR2, VAR114);\n++*VAR3;\n}\nbreak;\n}\ncase VAR142: {\nstatic const char VAR143[12] = \"STR\";\nconst VAR145 *VAR144 = (const VAR145 *)VAR143;\nVAR1->VAR108 = VAR146;\nVAR1->VAR121 = VAR144[0];\nVAR1->VAR112 = VAR144[1];\nVAR1->VAR110 = VAR144[2];\nbreak;\n}\ncase VAR146:\nVAR1->VAR108 = (1 << VAR147) |\n(1 << VAR148) |\n(1 << VAR149) |\n(1 << VAR150) |\n(1 << VAR151) |\n(1 << VAR152) |\n(1 << VAR153) |\n(1 << VAR154) |\n(1 << VAR155) |\n(1 << VAR156) |\n(1 << VAR157) |\n(1 << VAR158);\nif (FUN18())\nVAR1->VAR108 |= (1 << VAR159);\nVAR1->VAR121 = 0;\nVAR1->VAR112 = 0;\nVAR1->VAR110 = 0;\nbreak;\ncase VAR116:\nVAR1->VAR108 = FUN10(VAR1->VAR108, VAR116);\nbreak;\ncase VAR116:\nVAR1->VAR110 &= VAR46;\nFUN11(&VAR1->VAR110, VAR160);\nVAR1->VAR112 &= VAR67;\nFUN11(&VAR1->VAR112, VAR161);\nbreak;\ncase VAR116: \nVAR1->VAR110 &= (1 << 8);\nVAR1->VAR110 &= VAR162.VAR163;\nVAR1->VAR108 = VAR1->VAR121 = VAR1->VAR112 = 0;\nbreak;\ncase VAR116: {\nunsigned VAR164 = (VAR1->VAR108 >> 16) & VAR116;\nunsigned VAR165 = FUN19((VAR1->VAR108 >> 8) & VAR116, 48U);\nunsigned VAR166 = VAR1->VAR108 & VAR116;\nif (!VAR164)\nVAR164 = VAR166;\nVAR1->VAR108 = VAR164 | (VAR165 << 8);\nVAR1->VAR110 = 0;\nVAR1->VAR121 &= VAR80;\nFUN11(&VAR1->VAR121, VAR167);\nif (FUN20(VAR168))\nVAR1->VAR121 |= FUN3(VAR84);\nif (FUN20(VAR169))\nVAR1->VAR121 |= FUN3(VAR85);\nif (FUN20(VAR170))\nVAR1->VAR121 |= FUN3(VAR89);\nif (FUN20(VAR171))\nVAR1->VAR121 |= FUN3(VAR86);\nif (!FUN21(VAR172))\nVAR1->VAR121 |= FUN3(VAR88);\nif (FUN20(VAR173) &&\n!FUN20(VAR174))\nVAR1->VAR121 |= FUN3(VAR87);\nbreak;\n}\ncase VAR116:\nVAR1->VAR112 = VAR1->VAR110 = 0;\nbreak;\ncase VAR116:\ncase VAR116:\nbreak;\ncase VAR116:\nif (!FUN20(VAR175))\nVAR1->VAR108 = VAR1->VAR121 = VAR1->VAR112 = VAR1->VAR110 = 0;\nbreak;\ncase VAR116:\nVAR1->VAR108 = FUN10(VAR1->VAR108, VAR116);\nbreak;\ncase VAR116:\nVAR1->VAR110 &= VAR91;\nFUN11(&VAR1->VAR110, VAR176);\nbreak;\ncase 3: \ncase 5: \ncase VAR116:\ncase VAR116:\ncase VAR116:\ndefault:\nVAR1->VAR108 = VAR1->VAR121 = VAR1->VAR112 = VAR1->VAR110 = 0;\nbreak;\n}\nVAR10->FUN22(VAR2, VAR1);\nVAR5 = 0;\nVAR107:\nFUN23();\nreturn VAR5;\n}\n",
      "code_after_change_raw": "static inline int __do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 function,\nint *nent, int maxnent)\n{\nint r;\nunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\nunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n? F(GBPAGES) : 0;\nunsigned f_lm = F(LM);\n#else\nunsigned f_gbpages = 0;\nunsigned f_lm = 0;\n#endif\nunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\nunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\nunsigned f_intel_pt = kvm_x86_ops->pt_supported() ? F(INTEL_PT) : 0;\nconst u32 kvm_cpuid_1_edx_x86_features =\nF(FPU) | F(VME) | F(DE) | F(PSE) |\nF(TSC) | F(MSR) | F(PAE) | F(MCE) |\nF(CX8) | F(APIC) | 0  | F(SEP) |\nF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\nF(PAT) | F(PSE36) | 0  | F(CLFLUSH) |\n0  | F(MMX) |\nF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n0 ;\nconst u32 kvm_cpuid_8000_0001_edx_x86_features =\nF(FPU) | F(VME) | F(DE) | F(PSE) |\nF(TSC) | F(MSR) | F(PAE) | F(MCE) |\nF(CX8) | F(APIC) | 0  | F(SYSCALL) |\nF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\nF(PAT) | F(PSE36) | 0  |\nf_nx | 0  | F(MMXEXT) | F(MMX) |\nF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n0  | f_lm | F(3DNOWEXT) | F(3DNOW);\nconst u32 kvm_cpuid_1_ecx_x86_features =\nF(XMM3) | F(PCLMULQDQ) | 0  |\n0  |\n0  | F(SSSE3) | 0  | 0  |\nF(FMA) | F(CX16) | 0  |\nF(PCID) | 0  | F(XMM4_1) |\nF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n0  | F(AES) | F(XSAVE) | 0  | F(AVX) |\nF(F16C) | F(RDRAND);\nconst u32 kvm_cpuid_8000_0001_ecx_x86_features =\nF(LAHF_LM) | F(CMP_LEGACY) | 0  | 0  |\nF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\nF(3DNOWPREFETCH) | F(OSVW) | 0  | F(XOP) |\n0  | F(FMA4) | F(TBM) |\nF(TOPOEXT) | F(PERFCTR_CORE);\nconst u32 kvm_cpuid_8000_0008_ebx_x86_features =\nF(CLZERO) | F(XSAVEERPTR) |\nF(WBNOINVD) | F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |\nF(AMD_SSB_NO) | F(AMD_STIBP) | F(AMD_STIBP_ALWAYS_ON);\nconst u32 kvm_cpuid_C000_0001_edx_x86_features =\nF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\nF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\nF(PMM) | F(PMM_EN);\nconst u32 kvm_cpuid_D_1_eax_x86_features =\nF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\nget_cpu();\nr = -E2BIG;\nif (WARN_ON(*nent >= maxnent))\ngoto out;\ndo_host_cpuid(entry, function, 0);\n++*nent;\nswitch (function) {\ncase 0:\nentry->eax = min(entry->eax, 0x1fU);\nbreak;\ncase 1:\nentry->edx &= kvm_cpuid_1_edx_x86_features;\ncpuid_mask(&entry->edx, CPUID_1_EDX);\nentry->ecx &= kvm_cpuid_1_ecx_x86_features;\ncpuid_mask(&entry->ecx, CPUID_1_ECX);\nentry->ecx |= F(X2APIC);\nbreak;\ncase 2: {\nint t, times = entry->eax & 0xff;\nentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\nfor (t = 1; t < times; ++t) {\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[t], function, 0);\n++*nent;\n}\nbreak;\n}\ncase 4:\ncase 0x8000001d: {\nint i, cache_type;\nfor (i = 1; ; ++i) {\nif (*nent >= maxnent)\ngoto out;\ncache_type = entry[i - 1].eax & 0x1f;\nif (!cache_type)\nbreak;\ndo_host_cpuid(&entry[i], function, i);\n++*nent;\n}\nbreak;\n}\ncase 6: \nentry->eax = 0x4; \nentry->ebx = 0;\nentry->ecx = 0;\nentry->edx = 0;\nbreak;\ncase 7: {\nint i;\nfor (i = 0; ; ) {\ndo_cpuid_7_mask(&entry[i], i);\nif (i == entry->eax)\nbreak;\nif (*nent >= maxnent)\ngoto out;\n++i;\ndo_host_cpuid(&entry[i], function, i);\n++*nent;\n}\nbreak;\n}\ncase 9:\nbreak;\ncase 0xa: { \nstruct x86_pmu_capability cap;\nunion cpuid10_eax eax;\nunion cpuid10_edx edx;\nperf_get_x86_pmu_capability(&cap);\nif (!cap.version)\nmemset(&cap, 0, sizeof(cap));\neax.split.version_id = min(cap.version, 2);\neax.split.num_counters = cap.num_counters_gp;\neax.split.bit_width = cap.bit_width_gp;\neax.split.mask_length = cap.events_mask_len;\nedx.split.num_counters_fixed = cap.num_counters_fixed;\nedx.split.bit_width_fixed = cap.bit_width_fixed;\nedx.split.reserved = 0;\nentry->eax = eax.full;\nentry->ebx = cap.events_mask;\nentry->ecx = 0;\nentry->edx = edx.full;\nbreak;\n}\ncase 0x1f:\ncase 0xb: {\nint i;\nfor (i = 1; entry[i - 1].ecx & 0xff00; ++i) {\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[i], function, i);\n++*nent;\n}\nbreak;\n}\ncase 0xd: {\nint idx, i;\nu64 supported = kvm_supported_xcr0();\nentry->eax &= supported;\nentry->ebx = xstate_required_size(supported, false);\nentry->ecx = entry->ebx;\nentry->edx &= supported >> 32;\nif (!supported)\nbreak;\nfor (idx = 1, i = 1; idx < 64; ++idx) {\nu64 mask = ((u64)1 << idx);\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[i], function, idx);\nif (idx == 1) {\nentry[i].eax &= kvm_cpuid_D_1_eax_x86_features;\ncpuid_mask(&entry[i].eax, CPUID_D_1_EAX);\nentry[i].ebx = 0;\nif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\nentry[i].ebx =\nxstate_required_size(supported,\ntrue);\n} else {\nif (entry[i].eax == 0 || !(supported & mask))\ncontinue;\nif (WARN_ON_ONCE(entry[i].ecx & 1))\ncontinue;\n}\nentry[i].ecx = 0;\nentry[i].edx = 0;\n++*nent;\n++i;\n}\nbreak;\n}\ncase 0x14: {\nint t, times = entry->eax;\nif (!f_intel_pt)\nbreak;\nfor (t = 1; t <= times; ++t) {\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[t], function, t);\n++*nent;\n}\nbreak;\n}\ncase KVM_CPUID_SIGNATURE: {\nstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\nconst u32 *sigptr = (const u32 *)signature;\nentry->eax = KVM_CPUID_FEATURES;\nentry->ebx = sigptr[0];\nentry->ecx = sigptr[1];\nentry->edx = sigptr[2];\nbreak;\n}\ncase KVM_CPUID_FEATURES:\nentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n(1 << KVM_FEATURE_NOP_IO_DELAY) |\n(1 << KVM_FEATURE_CLOCKSOURCE2) |\n(1 << KVM_FEATURE_ASYNC_PF) |\n(1 << KVM_FEATURE_PV_EOI) |\n(1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n(1 << KVM_FEATURE_PV_UNHALT) |\n(1 << KVM_FEATURE_PV_TLB_FLUSH) |\n(1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |\n(1 << KVM_FEATURE_PV_SEND_IPI) |\n(1 << KVM_FEATURE_POLL_CONTROL) |\n(1 << KVM_FEATURE_PV_SCHED_YIELD);\nif (sched_info_on())\nentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\nentry->ebx = 0;\nentry->ecx = 0;\nentry->edx = 0;\nbreak;\ncase 0x80000000:\nentry->eax = min(entry->eax, 0x8000001f);\nbreak;\ncase 0x80000001:\nentry->edx &= kvm_cpuid_8000_0001_edx_x86_features;\ncpuid_mask(&entry->edx, CPUID_8000_0001_EDX);\nentry->ecx &= kvm_cpuid_8000_0001_ecx_x86_features;\ncpuid_mask(&entry->ecx, CPUID_8000_0001_ECX);\nbreak;\ncase 0x80000007: \nentry->edx &= (1 << 8);\nentry->edx &= boot_cpu_data.x86_power;\nentry->eax = entry->ebx = entry->ecx = 0;\nbreak;\ncase 0x80000008: {\nunsigned g_phys_as = (entry->eax >> 16) & 0xff;\nunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\nunsigned phys_as = entry->eax & 0xff;\nif (!g_phys_as)\ng_phys_as = phys_as;\nentry->eax = g_phys_as | (virt_as << 8);\nentry->edx = 0;\nentry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;\ncpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);\nif (boot_cpu_has(X86_FEATURE_IBPB))\nentry->ebx |= F(AMD_IBPB);\nif (boot_cpu_has(X86_FEATURE_IBRS))\nentry->ebx |= F(AMD_IBRS);\nif (boot_cpu_has(X86_FEATURE_STIBP))\nentry->ebx |= F(AMD_STIBP);\nif (boot_cpu_has(X86_FEATURE_SSBD))\nentry->ebx |= F(AMD_SSBD);\nif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\nentry->ebx |= F(AMD_SSB_NO);\nif (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n!boot_cpu_has(X86_FEATURE_AMD_SSBD))\nentry->ebx |= F(VIRT_SSBD);\nbreak;\n}\ncase 0x80000019:\nentry->ecx = entry->edx = 0;\nbreak;\ncase 0x8000001a:\ncase 0x8000001e:\nbreak;\ncase 0x8000001F:\nif (!boot_cpu_has(X86_FEATURE_SEV))\nentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\nbreak;\ncase 0xC0000000:\nentry->eax = min(entry->eax, 0xC0000004);\nbreak;\ncase 0xC0000001:\nentry->edx &= kvm_cpuid_C000_0001_edx_x86_features;\ncpuid_mask(&entry->edx, CPUID_C000_0001_EDX);\nbreak;\ncase 3: \ncase 5: \ncase 0xC0000002:\ncase 0xC0000003:\ncase 0xC0000004:\ndefault:\nentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\nbreak;\n}\nkvm_x86_ops->set_supported_cpuid(function, entry);\nr = 0;\nout:\nput_cpu();\nreturn r;\n}\n",
      "code_before_change_raw": "static inline int __do_cpuid_func(struct kvm_cpuid_entry2 *entry, u32 function,\nint *nent, int maxnent)\n{\nint r;\nunsigned f_nx = is_efer_nx() ? F(NX) : 0;\n#ifdef CONFIG_X86_64\nunsigned f_gbpages = (kvm_x86_ops->get_lpage_level() == PT_PDPE_LEVEL)\n? F(GBPAGES) : 0;\nunsigned f_lm = F(LM);\n#else\nunsigned f_gbpages = 0;\nunsigned f_lm = 0;\n#endif\nunsigned f_rdtscp = kvm_x86_ops->rdtscp_supported() ? F(RDTSCP) : 0;\nunsigned f_xsaves = kvm_x86_ops->xsaves_supported() ? F(XSAVES) : 0;\nunsigned f_intel_pt = kvm_x86_ops->pt_supported() ? F(INTEL_PT) : 0;\nconst u32 kvm_cpuid_1_edx_x86_features =\nF(FPU) | F(VME) | F(DE) | F(PSE) |\nF(TSC) | F(MSR) | F(PAE) | F(MCE) |\nF(CX8) | F(APIC) | 0  | F(SEP) |\nF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\nF(PAT) | F(PSE36) | 0  | F(CLFLUSH) |\n0  | F(MMX) |\nF(FXSR) | F(XMM) | F(XMM2) | F(SELFSNOOP) |\n0 ;\nconst u32 kvm_cpuid_8000_0001_edx_x86_features =\nF(FPU) | F(VME) | F(DE) | F(PSE) |\nF(TSC) | F(MSR) | F(PAE) | F(MCE) |\nF(CX8) | F(APIC) | 0  | F(SYSCALL) |\nF(MTRR) | F(PGE) | F(MCA) | F(CMOV) |\nF(PAT) | F(PSE36) | 0  |\nf_nx | 0  | F(MMXEXT) | F(MMX) |\nF(FXSR) | F(FXSR_OPT) | f_gbpages | f_rdtscp |\n0  | f_lm | F(3DNOWEXT) | F(3DNOW);\nconst u32 kvm_cpuid_1_ecx_x86_features =\nF(XMM3) | F(PCLMULQDQ) | 0  |\n0  |\n0  | F(SSSE3) | 0  | 0  |\nF(FMA) | F(CX16) | 0  |\nF(PCID) | 0  | F(XMM4_1) |\nF(XMM4_2) | F(X2APIC) | F(MOVBE) | F(POPCNT) |\n0  | F(AES) | F(XSAVE) | 0  | F(AVX) |\nF(F16C) | F(RDRAND);\nconst u32 kvm_cpuid_8000_0001_ecx_x86_features =\nF(LAHF_LM) | F(CMP_LEGACY) | 0  | 0  |\nF(CR8_LEGACY) | F(ABM) | F(SSE4A) | F(MISALIGNSSE) |\nF(3DNOWPREFETCH) | F(OSVW) | 0  | F(XOP) |\n0  | F(FMA4) | F(TBM) |\nF(TOPOEXT) | F(PERFCTR_CORE);\nconst u32 kvm_cpuid_8000_0008_ebx_x86_features =\nF(CLZERO) | F(XSAVEERPTR) |\nF(WBNOINVD) | F(AMD_IBPB) | F(AMD_IBRS) | F(AMD_SSBD) | F(VIRT_SSBD) |\nF(AMD_SSB_NO) | F(AMD_STIBP) | F(AMD_STIBP_ALWAYS_ON);\nconst u32 kvm_cpuid_C000_0001_edx_x86_features =\nF(XSTORE) | F(XSTORE_EN) | F(XCRYPT) | F(XCRYPT_EN) |\nF(ACE2) | F(ACE2_EN) | F(PHE) | F(PHE_EN) |\nF(PMM) | F(PMM_EN);\nconst u32 kvm_cpuid_D_1_eax_x86_features =\nF(XSAVEOPT) | F(XSAVEC) | F(XGETBV1) | f_xsaves;\nget_cpu();\nr = -E2BIG;\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(entry, function, 0);\n++*nent;\nswitch (function) {\ncase 0:\nentry->eax = min(entry->eax, 0x1fU);\nbreak;\ncase 1:\nentry->edx &= kvm_cpuid_1_edx_x86_features;\ncpuid_mask(&entry->edx, CPUID_1_EDX);\nentry->ecx &= kvm_cpuid_1_ecx_x86_features;\ncpuid_mask(&entry->ecx, CPUID_1_ECX);\nentry->ecx |= F(X2APIC);\nbreak;\ncase 2: {\nint t, times = entry->eax & 0xff;\nentry->flags |= KVM_CPUID_FLAG_STATE_READ_NEXT;\nfor (t = 1; t < times; ++t) {\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[t], function, 0);\n++*nent;\n}\nbreak;\n}\ncase 4:\ncase 0x8000001d: {\nint i, cache_type;\nfor (i = 1; ; ++i) {\nif (*nent >= maxnent)\ngoto out;\ncache_type = entry[i - 1].eax & 0x1f;\nif (!cache_type)\nbreak;\ndo_host_cpuid(&entry[i], function, i);\n++*nent;\n}\nbreak;\n}\ncase 6: \nentry->eax = 0x4; \nentry->ebx = 0;\nentry->ecx = 0;\nentry->edx = 0;\nbreak;\ncase 7: {\nint i;\nfor (i = 0; ; ) {\ndo_cpuid_7_mask(&entry[i], i);\nif (i == entry->eax)\nbreak;\nif (*nent >= maxnent)\ngoto out;\n++i;\ndo_host_cpuid(&entry[i], function, i);\n++*nent;\n}\nbreak;\n}\ncase 9:\nbreak;\ncase 0xa: { \nstruct x86_pmu_capability cap;\nunion cpuid10_eax eax;\nunion cpuid10_edx edx;\nperf_get_x86_pmu_capability(&cap);\nif (!cap.version)\nmemset(&cap, 0, sizeof(cap));\neax.split.version_id = min(cap.version, 2);\neax.split.num_counters = cap.num_counters_gp;\neax.split.bit_width = cap.bit_width_gp;\neax.split.mask_length = cap.events_mask_len;\nedx.split.num_counters_fixed = cap.num_counters_fixed;\nedx.split.bit_width_fixed = cap.bit_width_fixed;\nedx.split.reserved = 0;\nentry->eax = eax.full;\nentry->ebx = cap.events_mask;\nentry->ecx = 0;\nentry->edx = edx.full;\nbreak;\n}\ncase 0x1f:\ncase 0xb: {\nint i;\nfor (i = 1; entry[i - 1].ecx & 0xff00; ++i) {\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[i], function, i);\n++*nent;\n}\nbreak;\n}\ncase 0xd: {\nint idx, i;\nu64 supported = kvm_supported_xcr0();\nentry->eax &= supported;\nentry->ebx = xstate_required_size(supported, false);\nentry->ecx = entry->ebx;\nentry->edx &= supported >> 32;\nif (!supported)\nbreak;\nfor (idx = 1, i = 1; idx < 64; ++idx) {\nu64 mask = ((u64)1 << idx);\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[i], function, idx);\nif (idx == 1) {\nentry[i].eax &= kvm_cpuid_D_1_eax_x86_features;\ncpuid_mask(&entry[i].eax, CPUID_D_1_EAX);\nentry[i].ebx = 0;\nif (entry[i].eax & (F(XSAVES)|F(XSAVEC)))\nentry[i].ebx =\nxstate_required_size(supported,\ntrue);\n} else {\nif (entry[i].eax == 0 || !(supported & mask))\ncontinue;\nif (WARN_ON_ONCE(entry[i].ecx & 1))\ncontinue;\n}\nentry[i].ecx = 0;\nentry[i].edx = 0;\n++*nent;\n++i;\n}\nbreak;\n}\ncase 0x14: {\nint t, times = entry->eax;\nif (!f_intel_pt)\nbreak;\nfor (t = 1; t <= times; ++t) {\nif (*nent >= maxnent)\ngoto out;\ndo_host_cpuid(&entry[t], function, t);\n++*nent;\n}\nbreak;\n}\ncase KVM_CPUID_SIGNATURE: {\nstatic const char signature[12] = \"KVMKVMKVM\\0\\0\";\nconst u32 *sigptr = (const u32 *)signature;\nentry->eax = KVM_CPUID_FEATURES;\nentry->ebx = sigptr[0];\nentry->ecx = sigptr[1];\nentry->edx = sigptr[2];\nbreak;\n}\ncase KVM_CPUID_FEATURES:\nentry->eax = (1 << KVM_FEATURE_CLOCKSOURCE) |\n(1 << KVM_FEATURE_NOP_IO_DELAY) |\n(1 << KVM_FEATURE_CLOCKSOURCE2) |\n(1 << KVM_FEATURE_ASYNC_PF) |\n(1 << KVM_FEATURE_PV_EOI) |\n(1 << KVM_FEATURE_CLOCKSOURCE_STABLE_BIT) |\n(1 << KVM_FEATURE_PV_UNHALT) |\n(1 << KVM_FEATURE_PV_TLB_FLUSH) |\n(1 << KVM_FEATURE_ASYNC_PF_VMEXIT) |\n(1 << KVM_FEATURE_PV_SEND_IPI) |\n(1 << KVM_FEATURE_POLL_CONTROL) |\n(1 << KVM_FEATURE_PV_SCHED_YIELD);\nif (sched_info_on())\nentry->eax |= (1 << KVM_FEATURE_STEAL_TIME);\nentry->ebx = 0;\nentry->ecx = 0;\nentry->edx = 0;\nbreak;\ncase 0x80000000:\nentry->eax = min(entry->eax, 0x8000001f);\nbreak;\ncase 0x80000001:\nentry->edx &= kvm_cpuid_8000_0001_edx_x86_features;\ncpuid_mask(&entry->edx, CPUID_8000_0001_EDX);\nentry->ecx &= kvm_cpuid_8000_0001_ecx_x86_features;\ncpuid_mask(&entry->ecx, CPUID_8000_0001_ECX);\nbreak;\ncase 0x80000007: \nentry->edx &= (1 << 8);\nentry->edx &= boot_cpu_data.x86_power;\nentry->eax = entry->ebx = entry->ecx = 0;\nbreak;\ncase 0x80000008: {\nunsigned g_phys_as = (entry->eax >> 16) & 0xff;\nunsigned virt_as = max((entry->eax >> 8) & 0xff, 48U);\nunsigned phys_as = entry->eax & 0xff;\nif (!g_phys_as)\ng_phys_as = phys_as;\nentry->eax = g_phys_as | (virt_as << 8);\nentry->edx = 0;\nentry->ebx &= kvm_cpuid_8000_0008_ebx_x86_features;\ncpuid_mask(&entry->ebx, CPUID_8000_0008_EBX);\nif (boot_cpu_has(X86_FEATURE_IBPB))\nentry->ebx |= F(AMD_IBPB);\nif (boot_cpu_has(X86_FEATURE_IBRS))\nentry->ebx |= F(AMD_IBRS);\nif (boot_cpu_has(X86_FEATURE_STIBP))\nentry->ebx |= F(AMD_STIBP);\nif (boot_cpu_has(X86_FEATURE_SSBD))\nentry->ebx |= F(AMD_SSBD);\nif (!boot_cpu_has_bug(X86_BUG_SPEC_STORE_BYPASS))\nentry->ebx |= F(AMD_SSB_NO);\nif (boot_cpu_has(X86_FEATURE_LS_CFG_SSBD) &&\n!boot_cpu_has(X86_FEATURE_AMD_SSBD))\nentry->ebx |= F(VIRT_SSBD);\nbreak;\n}\ncase 0x80000019:\nentry->ecx = entry->edx = 0;\nbreak;\ncase 0x8000001a:\ncase 0x8000001e:\nbreak;\ncase 0x8000001F:\nif (!boot_cpu_has(X86_FEATURE_SEV))\nentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\nbreak;\ncase 0xC0000000:\nentry->eax = min(entry->eax, 0xC0000004);\nbreak;\ncase 0xC0000001:\nentry->edx &= kvm_cpuid_C000_0001_edx_x86_features;\ncpuid_mask(&entry->edx, CPUID_C000_0001_EDX);\nbreak;\ncase 3: \ncase 5: \ncase 0xC0000002:\ncase 0xC0000003:\ncase 0xC0000004:\ndefault:\nentry->eax = entry->ebx = entry->ecx = entry->edx = 0;\nbreak;\n}\nkvm_x86_ops->set_supported_cpuid(function, entry);\nr = 0;\nout:\nput_cpu();\nreturn r;\n}\n"
    }
  ],
  "CVE-2019-19532": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper initialization of input device and input report structures before accessing them.",
        "trigger_condition": "Malicious USB device can exploit the lack of proper initialization to trigger out-of-bounds write bugs in the HID drivers.",
        "specific_code_behavior_causing_vulnerability": "The code accesses input device and input report structures without ensuring they are properly initialized, leading to potential out-of-bounds writes.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure that input device and input report structures are correctly initialized before accessing them. This involves initializing the input device and input report structures before performing any operations that rely on them. By ensuring proper initialization, the code can prevent out-of-bounds write vulnerabilities in the HID drivers."
      },
      "GPT_analysis": "The modification is necessary to address the out-of-bounds write vulnerability (CVE-2019-19532) in the Linux kernel HID drivers. By making the changes as suggested, the code is updated to ensure that the input device and input report structures are properly accessed and handled. Specifically, the modification ensures that the `hidinput` and `dev` variables are correctly initialized before accessing them, which helps prevent potential out-of-bounds writes that could be exploited by a malicious USB device.\n\nBy following the corrected code snippet, the vulnerability related to out-of-bounds writes in the HID drivers is mitigated, thus improving the security of the system. It is important to always address known vulnerabilities promptly to prevent potential security risks and ensure the integrity of the software.",
      "GPT_purpose": "Initialize force feedback for EMS based devices.",
      "GPT_function": "\n1. Initialize the EMSFF device.\n2. Check for the presence of output reports and fields in the report.\n3. Allocate memory for the EMSFF device.\n4. Create force feedback effects for the device.\n5. Set specific values in the report field.\n6. Send a HID request to set the report.\n7. Log information about force feedback for EMS based devices.",
      "CVE_id": "CVE-2019-19532",
      "code_before_change": "static int emsff_init(struct hid_device *hid)\n{\n\tstruct emsff_device *emsff;\n\tstruct hid_report *report;\n\tstruct hid_input *hidinput = list_first_entry(&hid->inputs,\n\t\t\t\t\t\tstruct hid_input, list);\n\tstruct list_head *report_list =\n\t\t\t&hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tstruct input_dev *dev = hidinput->input;\n\tint error;\n\n\tif (list_empty(report_list)) {\n\t\thid_err(hid, \"no output reports found\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treport = list_first_entry(report_list, struct hid_report, list);\n\tif (report->maxfield < 1) {\n\t\thid_err(hid, \"no fields in the report\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (report->field[0]->report_count < 7) {\n\t\thid_err(hid, \"not enough values in the field\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\temsff = kzalloc(sizeof(struct emsff_device), GFP_KERNEL);\n\tif (!emsff)\n\t\treturn -ENOMEM;\n\n\tset_bit(FF_RUMBLE, dev->ffbit);\n\n\terror = input_ff_create_memless(dev, emsff, emsff_play);\n\tif (error) {\n\t\tkfree(emsff);\n\t\treturn error;\n\t}\n\n\temsff->report = report;\n\temsff->report->field[0]->value[0] = 0x01;\n\temsff->report->field[0]->value[1] = 0x00;\n\temsff->report->field[0]->value[2] = 0x00;\n\temsff->report->field[0]->value[3] = 0x00;\n\temsff->report->field[0]->value[4] = 0x00;\n\temsff->report->field[0]->value[5] = 0x00;\n\temsff->report->field[0]->value[6] = 0x00;\n\thid_hw_request(hid, emsff->report, HID_REQ_SET_REPORT);\n\n\thid_info(hid, \"force feedback for EMS based devices by Ignaz Forster <ignaz.forster@gmx.de>\\n\");\n\n\treturn 0;\n}",
      "code_after_change": "static int emsff_init(struct hid_device *hid)\n{\n\tstruct emsff_device *emsff;\n\tstruct hid_report *report;\n\tstruct hid_input *hidinput;\n\tstruct list_head *report_list =\n\t\t\t&hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tstruct input_dev *dev;\n\tint error;\n\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\treturn -ENODEV;\n\t}\n\thidinput = list_first_entry(&hid->inputs, struct hid_input, list);\n\tdev = hidinput->input;\n\n\tif (list_empty(report_list)) {\n\t\thid_err(hid, \"no output reports found\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\treport = list_first_entry(report_list, struct hid_report, list);\n\tif (report->maxfield < 1) {\n\t\thid_err(hid, \"no fields in the report\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\tif (report->field[0]->report_count < 7) {\n\t\thid_err(hid, \"not enough values in the field\\n\");\n\t\treturn -ENODEV;\n\t}\n\n\temsff = kzalloc(sizeof(struct emsff_device), GFP_KERNEL);\n\tif (!emsff)\n\t\treturn -ENOMEM;\n\n\tset_bit(FF_RUMBLE, dev->ffbit);\n\n\terror = input_ff_create_memless(dev, emsff, emsff_play);\n\tif (error) {\n\t\tkfree(emsff);\n\t\treturn error;\n\t}\n\n\temsff->report = report;\n\temsff->report->field[0]->value[0] = 0x01;\n\temsff->report->field[0]->value[1] = 0x00;\n\temsff->report->field[0]->value[2] = 0x00;\n\temsff->report->field[0]->value[3] = 0x00;\n\temsff->report->field[0]->value[4] = 0x00;\n\temsff->report->field[0]->value[5] = 0x00;\n\temsff->report->field[0]->value[6] = 0x00;\n\thid_hw_request(hid, emsff->report, HID_REQ_SET_REPORT);\n\n\thid_info(hid, \"force feedback for EMS based devices by Ignaz Forster <ignaz.forster@gmx.de>\\n\");\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tstruct hid_input *hidinput;",
          "\tstruct input_dev *dev;",
          "",
          "\tif (list_empty(&hid->inputs)) {",
          "\t\thid_err(hid, \"no inputs found\\n\");",
          "\t\treturn -ENODEV;",
          "\t}",
          "\thidinput = list_first_entry(&hid->inputs, struct hid_input, list);",
          "\tdev = hidinput->input;"
        ],
        "deleted": [
          "\tstruct hid_input *hidinput = list_first_entry(&hid->inputs,",
          "\t\t\t\t\t\tstruct hid_input, list);",
          "\tstruct input_dev *dev = hidinput->input;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper initialization of input device and input report structures before accessing them.",
      "trigger_condition": "Malicious USB device can exploit the lack of proper initialization to trigger out-of-bounds write bugs in the HID drivers.",
      "specific_code_behavior_causing_vulnerability": "The code accesses input device and input report structures without ensuring they are properly initialized, leading to potential out-of-bounds writes.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure that input device and input report structures are correctly initialized before accessing them. This involves initializing the input device and input report structures before performing any operations that rely on them. By ensuring proper initialization, the code can prevent out-of-bounds write vulnerabilities in the HID drivers.",
      "id": 47,
      "code_after_change_normalized": "static int FUN1(struct hid_device *VAR1)\n{\nstruct emsff_device *VAR2;\nstruct hid_report *VAR3;\nstruct hid_input *VAR4;\nstruct list_head *VAR5 =\n&VAR1->VAR6[VAR7].VAR5;\nstruct input_dev *VAR8;\nint VAR9;\nif (FUN2(&VAR1->VAR10)) {\nFUN3(VAR1, \"STR\");\nreturn -VAR11;\n}\nVAR4 = FUN4(&VAR1->VAR10, struct VAR12, VAR13);\nVAR8 = VAR4->VAR14;\nif (FUN2(VAR5)) {\nFUN3(VAR1, \"STR\");\nreturn -VAR11;\n}\nVAR3 = FUN4(VAR5, struct VAR15, VAR13);\nif (VAR3->VAR16 < 1) {\nFUN3(VAR1, \"STR\");\nreturn -VAR11;\n}\nif (VAR3->VAR17[0]->VAR18 < 7) {\nFUN3(VAR1, \"STR\");\nreturn -VAR11;\n}\nVAR2 = FUN5(sizeof(struct VAR19), VAR20);\nif (!VAR2)\nreturn -VAR21;\nFUN6(VAR22, VAR8->VAR23);\nVAR9 = FUN7(VAR8, VAR2, VAR24);\nif (VAR9) {\nFUN8(VAR2);\nreturn VAR9;\n}\nVAR2->VAR3 = VAR3;\nVAR2->VAR3->VAR17[0]->VAR25[0] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[1] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[2] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[3] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[4] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[5] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[6] = VAR26;\nFUN9(VAR1, VAR2->VAR3, VAR27);\nFUN10(VAR1, \"STR\");\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct hid_device *VAR1)\n{\nstruct emsff_device *VAR2;\nstruct hid_report *VAR3;\nstruct hid_input *VAR4 = FUN2(&VAR1->VAR5,\nstruct VAR6, VAR7);\nstruct list_head *VAR8 =\n&VAR1->VAR9[VAR10].VAR8;\nstruct input_dev *VAR11 = VAR4->VAR12;\nint VAR13;\nif (FUN3(VAR8)) {\nFUN4(VAR1, \"STR\");\nreturn -VAR14;\n}\nVAR3 = FUN2(VAR8, struct VAR15, VAR7);\nif (VAR3->VAR16 < 1) {\nFUN4(VAR1, \"STR\");\nreturn -VAR14;\n}\nif (VAR3->VAR17[0]->VAR18 < 7) {\nFUN4(VAR1, \"STR\");\nreturn -VAR14;\n}\nVAR2 = FUN5(sizeof(struct VAR19), VAR20);\nif (!VAR2)\nreturn -VAR21;\nFUN6(VAR22, VAR11->VAR23);\nVAR13 = FUN7(VAR11, VAR2, VAR24);\nif (VAR13) {\nFUN8(VAR2);\nreturn VAR13;\n}\nVAR2->VAR3 = VAR3;\nVAR2->VAR3->VAR17[0]->VAR25[0] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[1] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[2] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[3] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[4] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[5] = VAR26;\nVAR2->VAR3->VAR17[0]->VAR25[6] = VAR26;\nFUN9(VAR1, VAR2->VAR3, VAR27);\nFUN10(VAR1, \"STR\");\nreturn 0;\n}\n",
      "code_after_change_raw": "static int emsff_init(struct hid_device *hid)\n{\nstruct emsff_device *emsff;\nstruct hid_report *report;\nstruct hid_input *hidinput;\nstruct list_head *report_list =\n&hid->report_enum[HID_OUTPUT_REPORT].report_list;\nstruct input_dev *dev;\nint error;\nif (list_empty(&hid->inputs)) {\nhid_err(hid, \"no inputs found\\n\");\nreturn -ENODEV;\n}\nhidinput = list_first_entry(&hid->inputs, struct hid_input, list);\ndev = hidinput->input;\nif (list_empty(report_list)) {\nhid_err(hid, \"no output reports found\\n\");\nreturn -ENODEV;\n}\nreport = list_first_entry(report_list, struct hid_report, list);\nif (report->maxfield < 1) {\nhid_err(hid, \"no fields in the report\\n\");\nreturn -ENODEV;\n}\nif (report->field[0]->report_count < 7) {\nhid_err(hid, \"not enough values in the field\\n\");\nreturn -ENODEV;\n}\nemsff = kzalloc(sizeof(struct emsff_device), GFP_KERNEL);\nif (!emsff)\nreturn -ENOMEM;\nset_bit(FF_RUMBLE, dev->ffbit);\nerror = input_ff_create_memless(dev, emsff, emsff_play);\nif (error) {\nkfree(emsff);\nreturn error;\n}\nemsff->report = report;\nemsff->report->field[0]->value[0] = 0x01;\nemsff->report->field[0]->value[1] = 0x00;\nemsff->report->field[0]->value[2] = 0x00;\nemsff->report->field[0]->value[3] = 0x00;\nemsff->report->field[0]->value[4] = 0x00;\nemsff->report->field[0]->value[5] = 0x00;\nemsff->report->field[0]->value[6] = 0x00;\nhid_hw_request(hid, emsff->report, HID_REQ_SET_REPORT);\nhid_info(hid, \"force feedback for EMS based devices by Ignaz Forster <ignaz.forster@gmx.de>\\n\");\nreturn 0;\n}\n",
      "code_before_change_raw": "static int emsff_init(struct hid_device *hid)\n{\nstruct emsff_device *emsff;\nstruct hid_report *report;\nstruct hid_input *hidinput = list_first_entry(&hid->inputs,\nstruct hid_input, list);\nstruct list_head *report_list =\n&hid->report_enum[HID_OUTPUT_REPORT].report_list;\nstruct input_dev *dev = hidinput->input;\nint error;\nif (list_empty(report_list)) {\nhid_err(hid, \"no output reports found\\n\");\nreturn -ENODEV;\n}\nreport = list_first_entry(report_list, struct hid_report, list);\nif (report->maxfield < 1) {\nhid_err(hid, \"no fields in the report\\n\");\nreturn -ENODEV;\n}\nif (report->field[0]->report_count < 7) {\nhid_err(hid, \"not enough values in the field\\n\");\nreturn -ENODEV;\n}\nemsff = kzalloc(sizeof(struct emsff_device), GFP_KERNEL);\nif (!emsff)\nreturn -ENOMEM;\nset_bit(FF_RUMBLE, dev->ffbit);\nerror = input_ff_create_memless(dev, emsff, emsff_play);\nif (error) {\nkfree(emsff);\nreturn error;\n}\nemsff->report = report;\nemsff->report->field[0]->value[0] = 0x01;\nemsff->report->field[0]->value[1] = 0x00;\nemsff->report->field[0]->value[2] = 0x00;\nemsff->report->field[0]->value[3] = 0x00;\nemsff->report->field[0]->value[4] = 0x00;\nemsff->report->field[0]->value[5] = 0x00;\nemsff->report->field[0]->value[6] = 0x00;\nhid_hw_request(hid, emsff->report, HID_REQ_SET_REPORT);\nhid_info(hid, \"force feedback for EMS based devices by Ignaz Forster <ignaz.forster@gmx.de>\\n\");\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper input validation and handling for HID devices during initialization process.",
        "trigger_condition": "A malicious USB device is connected, triggering out-of-bounds write bugs in the Linux kernel HID drivers.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly handle HID input devices, leading to potential out-of-bounds write vulnerabilities when processing HID reports and device properties. This vulnerability can be exploited by a malicious USB device to trigger out-of-bounds writes in various HID drivers.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure proper input validation and handling for HID devices during the initialization process. This involves validating input data, checking for empty input lists, and ensuring correct access to HID input structures. In this case, the solution includes adding a check for empty input lists and properly initializing the HID input and device structures to prevent out-of-bounds writes."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-19532, which involves multiple out-of-bounds write bugs that can be exploited by a malicious USB device in the Linux kernel HID drivers. By adding the check `if (list_empty(&hid->inputs))` and handling the `hidinput` and `dev` variables properly, we can prevent potential out-of-bounds writes that could be triggered by a malicious USB device. This modification ensures that the code is more robust and secure, reducing the risk of exploitation through the identified vulnerability.",
      "GPT_purpose": "Initialize Logitech force feedback device properties and handle potential vulnerabilities.",
      "GPT_function": "\n1. Initialize a Logitech gaming device.\n2. Handle multimode wheel connection.\n3. Set supported force feedback capabilities.\n4. Create sysfs interface for device properties.\n5. Register LED subsystem for specific Logitech wheels.\n6. Return with error handling if initialization fails.",
      "CVE_id": "CVE-2019-19532",
      "code_before_change": "int lg4ff_init(struct hid_device *hid)\n{\n\tstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);\n\tstruct input_dev *dev = hidinput->input;\n\tstruct list_head *report_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tstruct hid_report *report = list_entry(report_list->next, struct hid_report, list);\n\tconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\n\tconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\n\tconst struct lg4ff_multimode_wheel *mmode_wheel = NULL;\n\tstruct lg4ff_device_entry *entry;\n\tstruct lg_drv_data *drv_data;\n\tint error, i, j;\n\tint mmode_ret, mmode_idx = -1;\n\tu16 real_product_id;\n\n\t/* Check that the report looks ok */\n\tif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\n\t\treturn -1;\n\n\tdrv_data = hid_get_drvdata(hid);\n\tif (!drv_data) {\n\t\thid_err(hid, \"Cannot add device, private driver data not allocated\\n\");\n\t\treturn -1;\n\t}\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&entry->report_lock);\n\tentry->report = report;\n\tdrv_data->device_props = entry;\n\n\t/* Check if a multimode wheel has been connected and\n\t * handle it appropriately */\n\tmmode_ret = lg4ff_handle_multimode_wheel(hid, &real_product_id, bcdDevice);\n\n\t/* Wheel has been told to switch to native mode. There is no point in going on\n\t * with the initialization as the wheel will do a USB reset when it switches mode\n\t */\n\tif (mmode_ret == LG4FF_MMODE_SWITCHED)\n\t\treturn 0;\n\telse if (mmode_ret < 0) {\n\t\thid_err(hid, \"Unable to switch device mode during initialization, errno %d\\n\", mmode_ret);\n\t\terror = mmode_ret;\n\t\tgoto err_init;\n\t}\n\n\t/* Check what wheel has been connected */\n\tfor (i = 0; i < ARRAY_SIZE(lg4ff_devices); i++) {\n\t\tif (hid->product == lg4ff_devices[i].product_id) {\n\t\t\tdbg_hid(\"Found compatible device, product ID %04X\\n\", lg4ff_devices[i].product_id);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == ARRAY_SIZE(lg4ff_devices)) {\n\t\thid_err(hid, \"This device is flagged to be handled by the lg4ff module but this module does not know how to handle it. \"\n\t\t\t     \"Please report this as a bug to LKML, Simon Wood <simon@mungewell.org> or \"\n\t\t\t     \"Michal Maly <madcatxster@devoid-pointer.net>\\n\");\n\t\terror = -1;\n\t\tgoto err_init;\n\t}\n\n\tif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\n\t\tfor (mmode_idx = 0; mmode_idx < ARRAY_SIZE(lg4ff_multimode_wheels); mmode_idx++) {\n\t\t\tif (real_product_id == lg4ff_multimode_wheels[mmode_idx].product_id)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (mmode_idx == ARRAY_SIZE(lg4ff_multimode_wheels)) {\n\t\t\thid_err(hid, \"Device product ID %X is not listed as a multimode wheel\", real_product_id);\n\t\t\terror = -1;\n\t\t\tgoto err_init;\n\t\t}\n\t}\n\n\t/* Set supported force feedback capabilities */\n\tfor (j = 0; lg4ff_devices[i].ff_effects[j] >= 0; j++)\n\t\tset_bit(lg4ff_devices[i].ff_effects[j], dev->ffbit);\n\n\terror = input_ff_create_memless(dev, NULL, lg4ff_play);\n\n\tif (error)\n\t\tgoto err_init;\n\n\t/* Initialize device properties */\n\tif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\n\t\tBUG_ON(mmode_idx == -1);\n\t\tmmode_wheel = &lg4ff_multimode_wheels[mmode_idx];\n\t}\n\tlg4ff_init_wheel_data(&entry->wdata, &lg4ff_devices[i], mmode_wheel, real_product_id);\n\n\t/* Check if autocentering is available and\n\t * set the centering force to zero by default */\n\tif (test_bit(FF_AUTOCENTER, dev->ffbit)) {\n\t\t/* Formula Force EX expects different autocentering command */\n\t\tif ((bcdDevice >> 8) == LG4FF_FFEX_REV_MAJ &&\n\t\t    (bcdDevice & 0xff) == LG4FF_FFEX_REV_MIN)\n\t\t\tdev->ff->set_autocenter = lg4ff_set_autocenter_ffex;\n\t\telse\n\t\t\tdev->ff->set_autocenter = lg4ff_set_autocenter_default;\n\n\t\tdev->ff->set_autocenter(dev, 0);\n\t}\n\n\t/* Create sysfs interface */\n\terror = device_create_file(&hid->dev, &dev_attr_combine_pedals);\n\tif (error)\n\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"combine\\\", errno %d\\n\", error);\n\terror = device_create_file(&hid->dev, &dev_attr_range);\n\tif (error)\n\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"range\\\", errno %d\\n\", error);\n\tif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\n\t\terror = device_create_file(&hid->dev, &dev_attr_real_id);\n\t\tif (error)\n\t\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"real_id\\\", errno %d\\n\", error);\n\t\terror = device_create_file(&hid->dev, &dev_attr_alternate_modes);\n\t\tif (error)\n\t\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"alternate_modes\\\", errno %d\\n\", error);\n\t}\n\tdbg_hid(\"sysfs interface created\\n\");\n\n\t/* Set the maximum range to start with */\n\tentry->wdata.range = entry->wdata.max_range;\n\tif (entry->wdata.set_range)\n\t\tentry->wdata.set_range(hid, entry->wdata.range);\n\n#ifdef CONFIG_LEDS_CLASS\n\t/* register led subsystem - G27/G29 only */\n\tentry->wdata.led_state = 0;\n\tfor (j = 0; j < 5; j++)\n\t\tentry->wdata.led[j] = NULL;\n\n\tif (lg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G27_WHEEL ||\n\t\t\tlg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G29_WHEEL) {\n\t\tstruct led_classdev *led;\n\t\tsize_t name_sz;\n\t\tchar *name;\n\n\t\tlg4ff_set_leds(hid, 0);\n\n\t\tname_sz = strlen(dev_name(&hid->dev)) + 8;\n\n\t\tfor (j = 0; j < 5; j++) {\n\t\t\tled = kzalloc(sizeof(struct led_classdev)+name_sz, GFP_KERNEL);\n\t\t\tif (!led) {\n\t\t\t\thid_err(hid, \"can't allocate memory for LED %d\\n\", j);\n\t\t\t\tgoto err_leds;\n\t\t\t}\n\n\t\t\tname = (void *)(&led[1]);\n\t\t\tsnprintf(name, name_sz, \"%s::RPM%d\", dev_name(&hid->dev), j+1);\n\t\t\tled->name = name;\n\t\t\tled->brightness = 0;\n\t\t\tled->max_brightness = 1;\n\t\t\tled->brightness_get = lg4ff_led_get_brightness;\n\t\t\tled->brightness_set = lg4ff_led_set_brightness;\n\n\t\t\tentry->wdata.led[j] = led;\n\t\t\terror = led_classdev_register(&hid->dev, led);\n\n\t\t\tif (error) {\n\t\t\t\thid_err(hid, \"failed to register LED %d. Aborting.\\n\", j);\nerr_leds:\n\t\t\t\t/* Deregister LEDs (if any) */\n\t\t\t\tfor (j = 0; j < 5; j++) {\n\t\t\t\t\tled = entry->wdata.led[j];\n\t\t\t\t\tentry->wdata.led[j] = NULL;\n\t\t\t\t\tif (!led)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tled_classdev_unregister(led);\n\t\t\t\t\tkfree(led);\n\t\t\t\t}\n\t\t\t\tgoto out;\t/* Let the driver continue without LEDs */\n\t\t\t}\n\t\t}\n\t}\nout:\n#endif\n\thid_info(hid, \"Force feedback support for Logitech Gaming Wheels\\n\");\n\treturn 0;\n\nerr_init:\n\tdrv_data->device_props = NULL;\n\tkfree(entry);\n\treturn error;\n}",
      "code_after_change": "int lg4ff_init(struct hid_device *hid)\n{\n\tstruct hid_input *hidinput;\n\tstruct input_dev *dev;\n\tstruct list_head *report_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tstruct hid_report *report = list_entry(report_list->next, struct hid_report, list);\n\tconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\n\tconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\n\tconst struct lg4ff_multimode_wheel *mmode_wheel = NULL;\n\tstruct lg4ff_device_entry *entry;\n\tstruct lg_drv_data *drv_data;\n\tint error, i, j;\n\tint mmode_ret, mmode_idx = -1;\n\tu16 real_product_id;\n\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\treturn -ENODEV;\n\t}\n\thidinput = list_entry(hid->inputs.next, struct hid_input, list);\n\tdev = hidinput->input;\n\n\t/* Check that the report looks ok */\n\tif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\n\t\treturn -1;\n\n\tdrv_data = hid_get_drvdata(hid);\n\tif (!drv_data) {\n\t\thid_err(hid, \"Cannot add device, private driver data not allocated\\n\");\n\t\treturn -1;\n\t}\n\tentry = kzalloc(sizeof(*entry), GFP_KERNEL);\n\tif (!entry)\n\t\treturn -ENOMEM;\n\tspin_lock_init(&entry->report_lock);\n\tentry->report = report;\n\tdrv_data->device_props = entry;\n\n\t/* Check if a multimode wheel has been connected and\n\t * handle it appropriately */\n\tmmode_ret = lg4ff_handle_multimode_wheel(hid, &real_product_id, bcdDevice);\n\n\t/* Wheel has been told to switch to native mode. There is no point in going on\n\t * with the initialization as the wheel will do a USB reset when it switches mode\n\t */\n\tif (mmode_ret == LG4FF_MMODE_SWITCHED)\n\t\treturn 0;\n\telse if (mmode_ret < 0) {\n\t\thid_err(hid, \"Unable to switch device mode during initialization, errno %d\\n\", mmode_ret);\n\t\terror = mmode_ret;\n\t\tgoto err_init;\n\t}\n\n\t/* Check what wheel has been connected */\n\tfor (i = 0; i < ARRAY_SIZE(lg4ff_devices); i++) {\n\t\tif (hid->product == lg4ff_devices[i].product_id) {\n\t\t\tdbg_hid(\"Found compatible device, product ID %04X\\n\", lg4ff_devices[i].product_id);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tif (i == ARRAY_SIZE(lg4ff_devices)) {\n\t\thid_err(hid, \"This device is flagged to be handled by the lg4ff module but this module does not know how to handle it. \"\n\t\t\t     \"Please report this as a bug to LKML, Simon Wood <simon@mungewell.org> or \"\n\t\t\t     \"Michal Maly <madcatxster@devoid-pointer.net>\\n\");\n\t\terror = -1;\n\t\tgoto err_init;\n\t}\n\n\tif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\n\t\tfor (mmode_idx = 0; mmode_idx < ARRAY_SIZE(lg4ff_multimode_wheels); mmode_idx++) {\n\t\t\tif (real_product_id == lg4ff_multimode_wheels[mmode_idx].product_id)\n\t\t\t\tbreak;\n\t\t}\n\n\t\tif (mmode_idx == ARRAY_SIZE(lg4ff_multimode_wheels)) {\n\t\t\thid_err(hid, \"Device product ID %X is not listed as a multimode wheel\", real_product_id);\n\t\t\terror = -1;\n\t\t\tgoto err_init;\n\t\t}\n\t}\n\n\t/* Set supported force feedback capabilities */\n\tfor (j = 0; lg4ff_devices[i].ff_effects[j] >= 0; j++)\n\t\tset_bit(lg4ff_devices[i].ff_effects[j], dev->ffbit);\n\n\terror = input_ff_create_memless(dev, NULL, lg4ff_play);\n\n\tif (error)\n\t\tgoto err_init;\n\n\t/* Initialize device properties */\n\tif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\n\t\tBUG_ON(mmode_idx == -1);\n\t\tmmode_wheel = &lg4ff_multimode_wheels[mmode_idx];\n\t}\n\tlg4ff_init_wheel_data(&entry->wdata, &lg4ff_devices[i], mmode_wheel, real_product_id);\n\n\t/* Check if autocentering is available and\n\t * set the centering force to zero by default */\n\tif (test_bit(FF_AUTOCENTER, dev->ffbit)) {\n\t\t/* Formula Force EX expects different autocentering command */\n\t\tif ((bcdDevice >> 8) == LG4FF_FFEX_REV_MAJ &&\n\t\t    (bcdDevice & 0xff) == LG4FF_FFEX_REV_MIN)\n\t\t\tdev->ff->set_autocenter = lg4ff_set_autocenter_ffex;\n\t\telse\n\t\t\tdev->ff->set_autocenter = lg4ff_set_autocenter_default;\n\n\t\tdev->ff->set_autocenter(dev, 0);\n\t}\n\n\t/* Create sysfs interface */\n\terror = device_create_file(&hid->dev, &dev_attr_combine_pedals);\n\tif (error)\n\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"combine\\\", errno %d\\n\", error);\n\terror = device_create_file(&hid->dev, &dev_attr_range);\n\tif (error)\n\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"range\\\", errno %d\\n\", error);\n\tif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\n\t\terror = device_create_file(&hid->dev, &dev_attr_real_id);\n\t\tif (error)\n\t\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"real_id\\\", errno %d\\n\", error);\n\t\terror = device_create_file(&hid->dev, &dev_attr_alternate_modes);\n\t\tif (error)\n\t\t\thid_warn(hid, \"Unable to create sysfs interface for \\\"alternate_modes\\\", errno %d\\n\", error);\n\t}\n\tdbg_hid(\"sysfs interface created\\n\");\n\n\t/* Set the maximum range to start with */\n\tentry->wdata.range = entry->wdata.max_range;\n\tif (entry->wdata.set_range)\n\t\tentry->wdata.set_range(hid, entry->wdata.range);\n\n#ifdef CONFIG_LEDS_CLASS\n\t/* register led subsystem - G27/G29 only */\n\tentry->wdata.led_state = 0;\n\tfor (j = 0; j < 5; j++)\n\t\tentry->wdata.led[j] = NULL;\n\n\tif (lg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G27_WHEEL ||\n\t\t\tlg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G29_WHEEL) {\n\t\tstruct led_classdev *led;\n\t\tsize_t name_sz;\n\t\tchar *name;\n\n\t\tlg4ff_set_leds(hid, 0);\n\n\t\tname_sz = strlen(dev_name(&hid->dev)) + 8;\n\n\t\tfor (j = 0; j < 5; j++) {\n\t\t\tled = kzalloc(sizeof(struct led_classdev)+name_sz, GFP_KERNEL);\n\t\t\tif (!led) {\n\t\t\t\thid_err(hid, \"can't allocate memory for LED %d\\n\", j);\n\t\t\t\tgoto err_leds;\n\t\t\t}\n\n\t\t\tname = (void *)(&led[1]);\n\t\t\tsnprintf(name, name_sz, \"%s::RPM%d\", dev_name(&hid->dev), j+1);\n\t\t\tled->name = name;\n\t\t\tled->brightness = 0;\n\t\t\tled->max_brightness = 1;\n\t\t\tled->brightness_get = lg4ff_led_get_brightness;\n\t\t\tled->brightness_set = lg4ff_led_set_brightness;\n\n\t\t\tentry->wdata.led[j] = led;\n\t\t\terror = led_classdev_register(&hid->dev, led);\n\n\t\t\tif (error) {\n\t\t\t\thid_err(hid, \"failed to register LED %d. Aborting.\\n\", j);\nerr_leds:\n\t\t\t\t/* Deregister LEDs (if any) */\n\t\t\t\tfor (j = 0; j < 5; j++) {\n\t\t\t\t\tled = entry->wdata.led[j];\n\t\t\t\t\tentry->wdata.led[j] = NULL;\n\t\t\t\t\tif (!led)\n\t\t\t\t\t\tcontinue;\n\t\t\t\t\tled_classdev_unregister(led);\n\t\t\t\t\tkfree(led);\n\t\t\t\t}\n\t\t\t\tgoto out;\t/* Let the driver continue without LEDs */\n\t\t\t}\n\t\t}\n\t}\nout:\n#endif\n\thid_info(hid, \"Force feedback support for Logitech Gaming Wheels\\n\");\n\treturn 0;\n\nerr_init:\n\tdrv_data->device_props = NULL;\n\tkfree(entry);\n\treturn error;\n}",
      "modified_lines": {
        "added": [
          "\tstruct hid_input *hidinput;",
          "\tstruct input_dev *dev;",
          "",
          "\tif (list_empty(&hid->inputs)) {",
          "\t\thid_err(hid, \"no inputs found\\n\");",
          "\t\treturn -ENODEV;",
          "\t}",
          "\thidinput = list_entry(hid->inputs.next, struct hid_input, list);",
          "\tdev = hidinput->input;"
        ],
        "deleted": [
          "\tstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);",
          "\tstruct input_dev *dev = hidinput->input;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper input validation and handling for HID devices during initialization process.",
      "trigger_condition": "A malicious USB device is connected, triggering out-of-bounds write bugs in the Linux kernel HID drivers.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly handle HID input devices, leading to potential out-of-bounds write vulnerabilities when processing HID reports and device properties. This vulnerability can be exploited by a malicious USB device to trigger out-of-bounds writes in various HID drivers.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper input validation and handling for HID devices during the initialization process. This involves validating input data, checking for empty input lists, and ensuring correct access to HID input structures. In this case, the solution includes adding a check for empty input lists and properly initializing the HID input and device structures to prevent out-of-bounds writes.",
      "id": 48,
      "code_after_change_normalized": "int FUN1(struct hid_device *VAR1)\n{\nstruct hid_input *VAR2;\nstruct input_dev *VAR3;\nstruct list_head *VAR4 = &VAR1->VAR5[VAR6].VAR4;\nstruct VAR9 *VAR7 = FUN2(VAR4->VAR8, struct VAR9, VAR10);\nconst struct usb_device_descriptor *VAR11 = &(FUN3(VAR1)->VAR12);\nconst u16 VAR13 = FUN4(VAR11->VAR13);\nconst struct lg4ff_multimode_wheel *VAR14 = NULL;\nstruct lg4ff_device_entry *VAR15;\nstruct lg_drv_data *VAR16;\nint VAR17, VAR18, VAR19;\nint VAR20, VAR21 = -1;\nu16 VAR22;\nif (FUN5(&VAR1->VAR23)) {\nFUN6(VAR1, \"STR\");\nreturn -VAR24;\n}\nVAR2 = FUN2(VAR1->VAR23.VAR8, struct VAR25, VAR10);\nVAR3 = VAR2->VAR26;\nif (!FUN7(VAR1, VAR6, 0, 0, 7))\nreturn -1;\nVAR16 = FUN8(VAR1);\nif (!VAR16) {\nFUN6(VAR1, \"STR\");\nreturn -1;\n}\nVAR15 = FUN9(sizeof(*VAR15), VAR27);\nif (!VAR15)\nreturn -VAR28;\nFUN10(&VAR15->VAR29);\nVAR15->VAR7 = VAR7;\nVAR16->VAR30 = VAR15;\nVAR20 = FUN11(VAR1, &VAR22, VAR13);\nif (VAR20 == VAR31)\nreturn 0;\nelse if (VAR20 < 0) {\nFUN6(VAR1, \"STR\", VAR20);\nVAR17 = VAR20;\ngoto VAR32;\n}\nfor (VAR18 = 0; VAR18 < FUN12(VAR33); VAR18++) {\nif (VAR1->VAR34 == VAR33[VAR18].VAR35) {\nFUN13(\"STR\", VAR33[VAR18].VAR35);\nbreak;\n}\n}\nif (VAR18 == FUN12(VAR33)) {\nFUN6(VAR1, \"STR\"\n\"STR\"\n\"STR\");\nVAR17 = -1;\ngoto VAR32;\n}\nif (VAR20 == VAR36) {\nfor (VAR21 = 0; VAR21 < FUN12(VAR37); VAR21++) {\nif (VAR22 == VAR37[VAR21].VAR35)\nbreak;\n}\nif (VAR21 == FUN12(VAR37)) {\nFUN6(VAR1, \"STR\", VAR22);\nVAR17 = -1;\ngoto VAR32;\n}\n}\nfor (VAR19 = 0; VAR33[VAR18].VAR38[VAR19] >= 0; VAR19++)\nFUN14(VAR33[VAR18].VAR38[VAR19], VAR3->VAR39);\nVAR17 = FUN15(VAR3, NULL, VAR40);\nif (VAR17)\ngoto VAR32;\nif (VAR20 == VAR36) {\nFUN16(VAR21 == -1);\nVAR14 = &VAR37[VAR21];\n}\nFUN17(&VAR15->VAR41, &VAR33[VAR18], VAR14, VAR22);\nif (FUN18(VAR42, VAR3->VAR39)) {\nif ((VAR13 >> 8) == VAR43 &&\n(VAR13 & VAR44) == VAR45)\nVAR3->VAR46->VAR47 = VAR48;\nelse\nVAR3->VAR46->VAR47 = VAR49;\nVAR3->VAR46->FUN19(VAR3, 0);\n}\nVAR17 = FUN20(&VAR1->VAR3, &VAR50);\nif (VAR17)\nFUN21(VAR1, \"STR\", VAR17);\nVAR17 = FUN20(&VAR1->VAR3, &VAR51);\nif (VAR17)\nFUN21(VAR1, \"STR\", VAR17);\nif (VAR20 == VAR36) {\nVAR17 = FUN20(&VAR1->VAR3, &VAR52);\nif (VAR17)\nFUN21(VAR1, \"STR\", VAR17);\nVAR17 = FUN20(&VAR1->VAR3, &VAR53);\nif (VAR17)\nFUN21(VAR1, \"STR\", VAR17);\n}\nFUN13(\"STR\");\nVAR15->VAR41.VAR54 = VAR15->VAR41.VAR55;\nif (VAR15->VAR41.VAR56)\nVAR15->VAR41.FUN22(VAR1, VAR15->VAR41.VAR54);\n#ifdef VAR57\nVAR15->VAR41.VAR58 = 0;\nfor (VAR19 = 0; VAR19 < 5; VAR19++)\nVAR15->VAR41.VAR59[VAR19] = NULL;\nif (VAR33[VAR18].VAR35 == VAR60 ||\nVAR33[VAR18].VAR35 == VAR61) {\nstruct led_classdev *VAR59;\nsize_t VAR62;\nchar *VAR63;\nFUN23(VAR1, 0);\nVAR62 = FUN24(FUN25(&VAR1->VAR3)) + 8;\nfor (VAR19 = 0; VAR19 < 5; VAR19++) {\nVAR59 = FUN9(sizeof(struct VAR64)+VAR62, VAR27);\nif (!VAR59) {\nFUN6(VAR1, \"STR\", VAR19);\ngoto VAR65;\n}\nVAR63 = (void *)(&VAR59[1]);\nFUN26(VAR63, VAR62, \"STR\", FUN25(&VAR1->VAR3), VAR19+1);\nVAR59->VAR63 = VAR63;\nVAR59->VAR66 = 0;\nVAR59->VAR67 = 1;\nVAR59->VAR68 = VAR69;\nVAR59->VAR70 = VAR71;\nVAR15->VAR41.VAR59[VAR19] = VAR59;\nVAR17 = FUN27(&VAR1->VAR3, VAR59);\nif (VAR17) {\nFUN6(VAR1, \"STR\", VAR19);\nVAR65:\nfor (VAR19 = 0; VAR19 < 5; VAR19++) {\nVAR59 = VAR15->VAR41.VAR59[VAR19];\nVAR15->VAR41.VAR59[VAR19] = NULL;\nif (!VAR59)\ncontinue;\nFUN28(VAR59);\nFUN29(VAR59);\n}\ngoto VAR72;\t\n}\n}\n}\nVAR72:\n#VAR73\nFUN30(VAR1, \"STR\");\nreturn 0;\nVAR32:\nVAR16->VAR30 = NULL;\nFUN29(VAR15);\nreturn VAR17;\n}\n",
      "code_before_change_normalized": "int FUN1(struct hid_device *VAR1)\n{\nstruct VAR5 *VAR2 = FUN2(VAR1->VAR3.VAR4, struct VAR5, VAR6);\nstruct input_dev *VAR7 = VAR2->VAR8;\nstruct list_head *VAR9 = &VAR1->VAR10[VAR11].VAR9;\nstruct VAR13 *VAR12 = FUN2(VAR9->VAR4, struct VAR13, VAR6);\nconst struct usb_device_descriptor *VAR14 = &(FUN3(VAR1)->VAR15);\nconst u16 VAR16 = FUN4(VAR14->VAR16);\nconst struct lg4ff_multimode_wheel *VAR17 = NULL;\nstruct lg4ff_device_entry *VAR18;\nstruct lg_drv_data *VAR19;\nint VAR20, VAR21, VAR22;\nint VAR23, VAR24 = -1;\nu16 VAR25;\nif (!FUN5(VAR1, VAR11, 0, 0, 7))\nreturn -1;\nVAR19 = FUN6(VAR1);\nif (!VAR19) {\nFUN7(VAR1, \"STR\");\nreturn -1;\n}\nVAR18 = FUN8(sizeof(*VAR18), VAR26);\nif (!VAR18)\nreturn -VAR27;\nFUN9(&VAR18->VAR28);\nVAR18->VAR12 = VAR12;\nVAR19->VAR29 = VAR18;\nVAR23 = FUN10(VAR1, &VAR25, VAR16);\nif (VAR23 == VAR30)\nreturn 0;\nelse if (VAR23 < 0) {\nFUN7(VAR1, \"STR\", VAR23);\nVAR20 = VAR23;\ngoto VAR31;\n}\nfor (VAR21 = 0; VAR21 < FUN11(VAR32); VAR21++) {\nif (VAR1->VAR33 == VAR32[VAR21].VAR34) {\nFUN12(\"STR\", VAR32[VAR21].VAR34);\nbreak;\n}\n}\nif (VAR21 == FUN11(VAR32)) {\nFUN7(VAR1, \"STR\"\n\"STR\"\n\"STR\");\nVAR20 = -1;\ngoto VAR31;\n}\nif (VAR23 == VAR35) {\nfor (VAR24 = 0; VAR24 < FUN11(VAR36); VAR24++) {\nif (VAR25 == VAR36[VAR24].VAR34)\nbreak;\n}\nif (VAR24 == FUN11(VAR36)) {\nFUN7(VAR1, \"STR\", VAR25);\nVAR20 = -1;\ngoto VAR31;\n}\n}\nfor (VAR22 = 0; VAR32[VAR21].VAR37[VAR22] >= 0; VAR22++)\nFUN13(VAR32[VAR21].VAR37[VAR22], VAR7->VAR38);\nVAR20 = FUN14(VAR7, NULL, VAR39);\nif (VAR20)\ngoto VAR31;\nif (VAR23 == VAR35) {\nFUN15(VAR24 == -1);\nVAR17 = &VAR36[VAR24];\n}\nFUN16(&VAR18->VAR40, &VAR32[VAR21], VAR17, VAR25);\nif (FUN17(VAR41, VAR7->VAR38)) {\nif ((VAR16 >> 8) == VAR42 &&\n(VAR16 & VAR43) == VAR44)\nVAR7->VAR45->VAR46 = VAR47;\nelse\nVAR7->VAR45->VAR46 = VAR48;\nVAR7->VAR45->FUN18(VAR7, 0);\n}\nVAR20 = FUN19(&VAR1->VAR7, &VAR49);\nif (VAR20)\nFUN20(VAR1, \"STR\", VAR20);\nVAR20 = FUN19(&VAR1->VAR7, &VAR50);\nif (VAR20)\nFUN20(VAR1, \"STR\", VAR20);\nif (VAR23 == VAR35) {\nVAR20 = FUN19(&VAR1->VAR7, &VAR51);\nif (VAR20)\nFUN20(VAR1, \"STR\", VAR20);\nVAR20 = FUN19(&VAR1->VAR7, &VAR52);\nif (VAR20)\nFUN20(VAR1, \"STR\", VAR20);\n}\nFUN12(\"STR\");\nVAR18->VAR40.VAR53 = VAR18->VAR40.VAR54;\nif (VAR18->VAR40.VAR55)\nVAR18->VAR40.FUN21(VAR1, VAR18->VAR40.VAR53);\n#ifdef VAR56\nVAR18->VAR40.VAR57 = 0;\nfor (VAR22 = 0; VAR22 < 5; VAR22++)\nVAR18->VAR40.VAR58[VAR22] = NULL;\nif (VAR32[VAR21].VAR34 == VAR59 ||\nVAR32[VAR21].VAR34 == VAR60) {\nstruct led_classdev *VAR58;\nsize_t VAR61;\nchar *VAR62;\nFUN22(VAR1, 0);\nVAR61 = FUN23(FUN24(&VAR1->VAR7)) + 8;\nfor (VAR22 = 0; VAR22 < 5; VAR22++) {\nVAR58 = FUN8(sizeof(struct VAR63)+VAR61, VAR26);\nif (!VAR58) {\nFUN7(VAR1, \"STR\", VAR22);\ngoto VAR64;\n}\nVAR62 = (void *)(&VAR58[1]);\nFUN25(VAR62, VAR61, \"STR\", FUN24(&VAR1->VAR7), VAR22+1);\nVAR58->VAR62 = VAR62;\nVAR58->VAR65 = 0;\nVAR58->VAR66 = 1;\nVAR58->VAR67 = VAR68;\nVAR58->VAR69 = VAR70;\nVAR18->VAR40.VAR58[VAR22] = VAR58;\nVAR20 = FUN26(&VAR1->VAR7, VAR58);\nif (VAR20) {\nFUN7(VAR1, \"STR\", VAR22);\nVAR64:\nfor (VAR22 = 0; VAR22 < 5; VAR22++) {\nVAR58 = VAR18->VAR40.VAR58[VAR22];\nVAR18->VAR40.VAR58[VAR22] = NULL;\nif (!VAR58)\ncontinue;\nFUN27(VAR58);\nFUN28(VAR58);\n}\ngoto VAR71;\t\n}\n}\n}\nVAR71:\n#VAR72\nFUN29(VAR1, \"STR\");\nreturn 0;\nVAR31:\nVAR19->VAR29 = NULL;\nFUN28(VAR18);\nreturn VAR20;\n}\n",
      "code_after_change_raw": "int lg4ff_init(struct hid_device *hid)\n{\nstruct hid_input *hidinput;\nstruct input_dev *dev;\nstruct list_head *report_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\nstruct hid_report *report = list_entry(report_list->next, struct hid_report, list);\nconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\nconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\nconst struct lg4ff_multimode_wheel *mmode_wheel = NULL;\nstruct lg4ff_device_entry *entry;\nstruct lg_drv_data *drv_data;\nint error, i, j;\nint mmode_ret, mmode_idx = -1;\nu16 real_product_id;\nif (list_empty(&hid->inputs)) {\nhid_err(hid, \"no inputs found\\n\");\nreturn -ENODEV;\n}\nhidinput = list_entry(hid->inputs.next, struct hid_input, list);\ndev = hidinput->input;\nif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\nreturn -1;\ndrv_data = hid_get_drvdata(hid);\nif (!drv_data) {\nhid_err(hid, \"Cannot add device, private driver data not allocated\\n\");\nreturn -1;\n}\nentry = kzalloc(sizeof(*entry), GFP_KERNEL);\nif (!entry)\nreturn -ENOMEM;\nspin_lock_init(&entry->report_lock);\nentry->report = report;\ndrv_data->device_props = entry;\nmmode_ret = lg4ff_handle_multimode_wheel(hid, &real_product_id, bcdDevice);\nif (mmode_ret == LG4FF_MMODE_SWITCHED)\nreturn 0;\nelse if (mmode_ret < 0) {\nhid_err(hid, \"Unable to switch device mode during initialization, errno %d\\n\", mmode_ret);\nerror = mmode_ret;\ngoto err_init;\n}\nfor (i = 0; i < ARRAY_SIZE(lg4ff_devices); i++) {\nif (hid->product == lg4ff_devices[i].product_id) {\ndbg_hid(\"Found compatible device, product ID %04X\\n\", lg4ff_devices[i].product_id);\nbreak;\n}\n}\nif (i == ARRAY_SIZE(lg4ff_devices)) {\nhid_err(hid, \"This device is flagged to be handled by the lg4ff module but this module does not know how to handle it. \"\n\"Please report this as a bug to LKML, Simon Wood <simon@mungewell.org> or \"\n\"Michal Maly <madcatxster@devoid-pointer.net>\\n\");\nerror = -1;\ngoto err_init;\n}\nif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\nfor (mmode_idx = 0; mmode_idx < ARRAY_SIZE(lg4ff_multimode_wheels); mmode_idx++) {\nif (real_product_id == lg4ff_multimode_wheels[mmode_idx].product_id)\nbreak;\n}\nif (mmode_idx == ARRAY_SIZE(lg4ff_multimode_wheels)) {\nhid_err(hid, \"Device product ID %X is not listed as a multimode wheel\", real_product_id);\nerror = -1;\ngoto err_init;\n}\n}\nfor (j = 0; lg4ff_devices[i].ff_effects[j] >= 0; j++)\nset_bit(lg4ff_devices[i].ff_effects[j], dev->ffbit);\nerror = input_ff_create_memless(dev, NULL, lg4ff_play);\nif (error)\ngoto err_init;\nif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\nBUG_ON(mmode_idx == -1);\nmmode_wheel = &lg4ff_multimode_wheels[mmode_idx];\n}\nlg4ff_init_wheel_data(&entry->wdata, &lg4ff_devices[i], mmode_wheel, real_product_id);\nif (test_bit(FF_AUTOCENTER, dev->ffbit)) {\nif ((bcdDevice >> 8) == LG4FF_FFEX_REV_MAJ &&\n(bcdDevice & 0xff) == LG4FF_FFEX_REV_MIN)\ndev->ff->set_autocenter = lg4ff_set_autocenter_ffex;\nelse\ndev->ff->set_autocenter = lg4ff_set_autocenter_default;\ndev->ff->set_autocenter(dev, 0);\n}\nerror = device_create_file(&hid->dev, &dev_attr_combine_pedals);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"combine\\\", errno %d\\n\", error);\nerror = device_create_file(&hid->dev, &dev_attr_range);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"range\\\", errno %d\\n\", error);\nif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\nerror = device_create_file(&hid->dev, &dev_attr_real_id);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"real_id\\\", errno %d\\n\", error);\nerror = device_create_file(&hid->dev, &dev_attr_alternate_modes);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"alternate_modes\\\", errno %d\\n\", error);\n}\ndbg_hid(\"sysfs interface created\\n\");\nentry->wdata.range = entry->wdata.max_range;\nif (entry->wdata.set_range)\nentry->wdata.set_range(hid, entry->wdata.range);\n#ifdef CONFIG_LEDS_CLASS\nentry->wdata.led_state = 0;\nfor (j = 0; j < 5; j++)\nentry->wdata.led[j] = NULL;\nif (lg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G27_WHEEL ||\nlg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G29_WHEEL) {\nstruct led_classdev *led;\nsize_t name_sz;\nchar *name;\nlg4ff_set_leds(hid, 0);\nname_sz = strlen(dev_name(&hid->dev)) + 8;\nfor (j = 0; j < 5; j++) {\nled = kzalloc(sizeof(struct led_classdev)+name_sz, GFP_KERNEL);\nif (!led) {\nhid_err(hid, \"can't allocate memory for LED %d\\n\", j);\ngoto err_leds;\n}\nname = (void *)(&led[1]);\nsnprintf(name, name_sz, \"%s::RPM%d\", dev_name(&hid->dev), j+1);\nled->name = name;\nled->brightness = 0;\nled->max_brightness = 1;\nled->brightness_get = lg4ff_led_get_brightness;\nled->brightness_set = lg4ff_led_set_brightness;\nentry->wdata.led[j] = led;\nerror = led_classdev_register(&hid->dev, led);\nif (error) {\nhid_err(hid, \"failed to register LED %d. Aborting.\\n\", j);\nerr_leds:\nfor (j = 0; j < 5; j++) {\nled = entry->wdata.led[j];\nentry->wdata.led[j] = NULL;\nif (!led)\ncontinue;\nled_classdev_unregister(led);\nkfree(led);\n}\ngoto out;\t\n}\n}\n}\nout:\n#endif\nhid_info(hid, \"Force feedback support for Logitech Gaming Wheels\\n\");\nreturn 0;\nerr_init:\ndrv_data->device_props = NULL;\nkfree(entry);\nreturn error;\n}\n",
      "code_before_change_raw": "int lg4ff_init(struct hid_device *hid)\n{\nstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);\nstruct input_dev *dev = hidinput->input;\nstruct list_head *report_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\nstruct hid_report *report = list_entry(report_list->next, struct hid_report, list);\nconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\nconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\nconst struct lg4ff_multimode_wheel *mmode_wheel = NULL;\nstruct lg4ff_device_entry *entry;\nstruct lg_drv_data *drv_data;\nint error, i, j;\nint mmode_ret, mmode_idx = -1;\nu16 real_product_id;\nif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\nreturn -1;\ndrv_data = hid_get_drvdata(hid);\nif (!drv_data) {\nhid_err(hid, \"Cannot add device, private driver data not allocated\\n\");\nreturn -1;\n}\nentry = kzalloc(sizeof(*entry), GFP_KERNEL);\nif (!entry)\nreturn -ENOMEM;\nspin_lock_init(&entry->report_lock);\nentry->report = report;\ndrv_data->device_props = entry;\nmmode_ret = lg4ff_handle_multimode_wheel(hid, &real_product_id, bcdDevice);\nif (mmode_ret == LG4FF_MMODE_SWITCHED)\nreturn 0;\nelse if (mmode_ret < 0) {\nhid_err(hid, \"Unable to switch device mode during initialization, errno %d\\n\", mmode_ret);\nerror = mmode_ret;\ngoto err_init;\n}\nfor (i = 0; i < ARRAY_SIZE(lg4ff_devices); i++) {\nif (hid->product == lg4ff_devices[i].product_id) {\ndbg_hid(\"Found compatible device, product ID %04X\\n\", lg4ff_devices[i].product_id);\nbreak;\n}\n}\nif (i == ARRAY_SIZE(lg4ff_devices)) {\nhid_err(hid, \"This device is flagged to be handled by the lg4ff module but this module does not know how to handle it. \"\n\"Please report this as a bug to LKML, Simon Wood <simon@mungewell.org> or \"\n\"Michal Maly <madcatxster@devoid-pointer.net>\\n\");\nerror = -1;\ngoto err_init;\n}\nif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\nfor (mmode_idx = 0; mmode_idx < ARRAY_SIZE(lg4ff_multimode_wheels); mmode_idx++) {\nif (real_product_id == lg4ff_multimode_wheels[mmode_idx].product_id)\nbreak;\n}\nif (mmode_idx == ARRAY_SIZE(lg4ff_multimode_wheels)) {\nhid_err(hid, \"Device product ID %X is not listed as a multimode wheel\", real_product_id);\nerror = -1;\ngoto err_init;\n}\n}\nfor (j = 0; lg4ff_devices[i].ff_effects[j] >= 0; j++)\nset_bit(lg4ff_devices[i].ff_effects[j], dev->ffbit);\nerror = input_ff_create_memless(dev, NULL, lg4ff_play);\nif (error)\ngoto err_init;\nif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\nBUG_ON(mmode_idx == -1);\nmmode_wheel = &lg4ff_multimode_wheels[mmode_idx];\n}\nlg4ff_init_wheel_data(&entry->wdata, &lg4ff_devices[i], mmode_wheel, real_product_id);\nif (test_bit(FF_AUTOCENTER, dev->ffbit)) {\nif ((bcdDevice >> 8) == LG4FF_FFEX_REV_MAJ &&\n(bcdDevice & 0xff) == LG4FF_FFEX_REV_MIN)\ndev->ff->set_autocenter = lg4ff_set_autocenter_ffex;\nelse\ndev->ff->set_autocenter = lg4ff_set_autocenter_default;\ndev->ff->set_autocenter(dev, 0);\n}\nerror = device_create_file(&hid->dev, &dev_attr_combine_pedals);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"combine\\\", errno %d\\n\", error);\nerror = device_create_file(&hid->dev, &dev_attr_range);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"range\\\", errno %d\\n\", error);\nif (mmode_ret == LG4FF_MMODE_IS_MULTIMODE) {\nerror = device_create_file(&hid->dev, &dev_attr_real_id);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"real_id\\\", errno %d\\n\", error);\nerror = device_create_file(&hid->dev, &dev_attr_alternate_modes);\nif (error)\nhid_warn(hid, \"Unable to create sysfs interface for \\\"alternate_modes\\\", errno %d\\n\", error);\n}\ndbg_hid(\"sysfs interface created\\n\");\nentry->wdata.range = entry->wdata.max_range;\nif (entry->wdata.set_range)\nentry->wdata.set_range(hid, entry->wdata.range);\n#ifdef CONFIG_LEDS_CLASS\nentry->wdata.led_state = 0;\nfor (j = 0; j < 5; j++)\nentry->wdata.led[j] = NULL;\nif (lg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G27_WHEEL ||\nlg4ff_devices[i].product_id == USB_DEVICE_ID_LOGITECH_G29_WHEEL) {\nstruct led_classdev *led;\nsize_t name_sz;\nchar *name;\nlg4ff_set_leds(hid, 0);\nname_sz = strlen(dev_name(&hid->dev)) + 8;\nfor (j = 0; j < 5; j++) {\nled = kzalloc(sizeof(struct led_classdev)+name_sz, GFP_KERNEL);\nif (!led) {\nhid_err(hid, \"can't allocate memory for LED %d\\n\", j);\ngoto err_leds;\n}\nname = (void *)(&led[1]);\nsnprintf(name, name_sz, \"%s::RPM%d\", dev_name(&hid->dev), j+1);\nled->name = name;\nled->brightness = 0;\nled->max_brightness = 1;\nled->brightness_get = lg4ff_led_get_brightness;\nled->brightness_set = lg4ff_led_set_brightness;\nentry->wdata.led[j] = led;\nerror = led_classdev_register(&hid->dev, led);\nif (error) {\nhid_err(hid, \"failed to register LED %d. Aborting.\\n\", j);\nerr_leds:\nfor (j = 0; j < 5; j++) {\nled = entry->wdata.led[j];\nentry->wdata.led[j] = NULL;\nif (!led)\ncontinue;\nled_classdev_unregister(led);\nkfree(led);\n}\ngoto out;\t\n}\n}\n}\nout:\n#endif\nhid_info(hid, \"Force feedback support for Logitech Gaming Wheels\\n\");\nreturn 0;\nerr_init:\ndrv_data->device_props = NULL;\nkfree(entry);\nreturn error;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for HID input devices, allowing a malicious USB device to trigger out-of-bounds write bugs.",
        "trigger_condition": "A malicious USB device exploits the lack of bounds checking in the HID drivers, leading to out-of-bounds write vulnerabilities.",
        "specific_code_behavior_causing_vulnerability": "The code accesses HID input devices without verifying the integrity of the data, potentially allowing an attacker to manipulate the input in a way that triggers out-of-bounds writes."
      },
      "solution": "To mitigate the vulnerability, it is necessary to validate the HID input devices properly before accessing them. This involves checking the input data for correctness and ensuring that any write operations are within the bounds of the allocated memory. In this case, the solution includes adding a check to verify the presence of input devices before proceeding with data access, thereby preventing out-of-bounds write vulnerabilities.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-19532, which involves out-of-bounds write bugs that can be exploited by a malicious USB device in the Linux kernel HID drivers. By making the modification as suggested, the code ensures that the list of inputs associated with the HID device is checked for emptiness before attempting to access its elements. This prevents potential out-of-bounds access and helps mitigate the security risk posed by the vulnerability.",
      "GPT_purpose": "Initialize force feedback for Logitech force feedback devices.",
      "GPT_function": "\n1. Initialize Logitech force feedback device.\n2. Check the validity of the HID output report.\n3. Set force feedback bits based on device identification.\n4. Create memoryless force feedback input.\n5. Set autocenter for force feedback devices.",
      "CVE_id": "CVE-2019-19532",
      "code_before_change": "int lgff_init(struct hid_device* hid)\n{\n\tstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);\n\tstruct input_dev *dev = hidinput->input;\n\tconst signed short *ff_bits = ff_joystick;\n\tint error;\n\tint i;\n\n\t/* Check that the report looks ok */\n\tif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\n\t\treturn -ENODEV;\n\n\tfor (i = 0; i < ARRAY_SIZE(devices); i++) {\n\t\tif (dev->id.vendor == devices[i].idVendor &&\n\t\t    dev->id.product == devices[i].idProduct) {\n\t\t\tff_bits = devices[i].ff;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (i = 0; ff_bits[i] >= 0; i++)\n\t\tset_bit(ff_bits[i], dev->ffbit);\n\n\terror = input_ff_create_memless(dev, NULL, hid_lgff_play);\n\tif (error)\n\t\treturn error;\n\n\tif ( test_bit(FF_AUTOCENTER, dev->ffbit) )\n\t\tdev->ff->set_autocenter = hid_lgff_set_autocenter;\n\n\tpr_info(\"Force feedback for Logitech force feedback devices by Johann Deneux <johann.deneux@it.uu.se>\\n\");\n\n\treturn 0;\n}",
      "code_after_change": "int lgff_init(struct hid_device* hid)\n{\n\tstruct hid_input *hidinput;\n\tstruct input_dev *dev;\n\tconst signed short *ff_bits = ff_joystick;\n\tint error;\n\tint i;\n\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\treturn -ENODEV;\n\t}\n\thidinput = list_entry(hid->inputs.next, struct hid_input, list);\n\tdev = hidinput->input;\n\n\t/* Check that the report looks ok */\n\tif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\n\t\treturn -ENODEV;\n\n\tfor (i = 0; i < ARRAY_SIZE(devices); i++) {\n\t\tif (dev->id.vendor == devices[i].idVendor &&\n\t\t    dev->id.product == devices[i].idProduct) {\n\t\t\tff_bits = devices[i].ff;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tfor (i = 0; ff_bits[i] >= 0; i++)\n\t\tset_bit(ff_bits[i], dev->ffbit);\n\n\terror = input_ff_create_memless(dev, NULL, hid_lgff_play);\n\tif (error)\n\t\treturn error;\n\n\tif ( test_bit(FF_AUTOCENTER, dev->ffbit) )\n\t\tdev->ff->set_autocenter = hid_lgff_set_autocenter;\n\n\tpr_info(\"Force feedback for Logitech force feedback devices by Johann Deneux <johann.deneux@it.uu.se>\\n\");\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tstruct hid_input *hidinput;",
          "\tstruct input_dev *dev;",
          "",
          "\tif (list_empty(&hid->inputs)) {",
          "\t\thid_err(hid, \"no inputs found\\n\");",
          "\t\treturn -ENODEV;",
          "\t}",
          "\thidinput = list_entry(hid->inputs.next, struct hid_input, list);",
          "\tdev = hidinput->input;"
        ],
        "deleted": [
          "\tstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);",
          "\tstruct input_dev *dev = hidinput->input;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for HID input devices, allowing a malicious USB device to trigger out-of-bounds write bugs.",
      "trigger_condition": "A malicious USB device exploits the lack of bounds checking in the HID drivers, leading to out-of-bounds write vulnerabilities.",
      "specific_code_behavior_causing_vulnerability": "The code accesses HID input devices without verifying the integrity of the data, potentially allowing an attacker to manipulate the input in a way that triggers out-of-bounds writes.",
      "id": 49,
      "code_after_change_normalized": "int FUN1(struct VAR1* VAR2)\n{\nstruct hid_input *VAR3;\nstruct input_dev *VAR4;\nconst signed short *VAR5 = VAR6;\nint VAR7;\nint VAR8;\nif (FUN2(&VAR2->VAR9)) {\nFUN3(VAR2, \"STR\");\nreturn -VAR10;\n}\nVAR3 = FUN4(VAR2->VAR9.VAR11, struct VAR12, VAR13);\nVAR4 = VAR3->VAR14;\nif (!FUN5(VAR2, VAR15, 0, 0, 7))\nreturn -VAR10;\nfor (VAR8 = 0; VAR8 < FUN6(VAR16); VAR8++) {\nif (VAR4->VAR17.VAR18 == VAR16[VAR8].VAR19 &&\nVAR4->VAR17.VAR20 == VAR16[VAR8].VAR21) {\nVAR5 = VAR16[VAR8].VAR22;\nbreak;\n}\n}\nfor (VAR8 = 0; VAR5[VAR8] >= 0; VAR8++)\nFUN7(VAR5[VAR8], VAR4->VAR23);\nVAR7 = FUN8(VAR4, NULL, VAR24);\nif (VAR7)\nreturn VAR7;\nif ( FUN9(VAR25, VAR4->VAR23) )\nVAR4->VAR22->VAR26 = VAR27;\nFUN10(\"STR\");\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct VAR1* VAR2)\n{\nstruct VAR6 *VAR3 = FUN2(VAR2->VAR4.VAR5, struct VAR6, VAR7);\nstruct input_dev *VAR8 = VAR3->VAR9;\nconst signed short *VAR10 = VAR11;\nint VAR12;\nint VAR13;\nif (!FUN3(VAR2, VAR14, 0, 0, 7))\nreturn -VAR15;\nfor (VAR13 = 0; VAR13 < FUN4(VAR16); VAR13++) {\nif (VAR8->VAR17.VAR18 == VAR16[VAR13].VAR19 &&\nVAR8->VAR17.VAR20 == VAR16[VAR13].VAR21) {\nVAR10 = VAR16[VAR13].VAR22;\nbreak;\n}\n}\nfor (VAR13 = 0; VAR10[VAR13] >= 0; VAR13++)\nFUN5(VAR10[VAR13], VAR8->VAR23);\nVAR12 = FUN6(VAR8, NULL, VAR24);\nif (VAR12)\nreturn VAR12;\nif ( FUN7(VAR25, VAR8->VAR23) )\nVAR8->VAR22->VAR26 = VAR27;\nFUN8(\"STR\");\nreturn 0;\n}\n",
      "code_after_change_raw": "int lgff_init(struct hid_device* hid)\n{\nstruct hid_input *hidinput;\nstruct input_dev *dev;\nconst signed short *ff_bits = ff_joystick;\nint error;\nint i;\nif (list_empty(&hid->inputs)) {\nhid_err(hid, \"no inputs found\\n\");\nreturn -ENODEV;\n}\nhidinput = list_entry(hid->inputs.next, struct hid_input, list);\ndev = hidinput->input;\nif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\nreturn -ENODEV;\nfor (i = 0; i < ARRAY_SIZE(devices); i++) {\nif (dev->id.vendor == devices[i].idVendor &&\ndev->id.product == devices[i].idProduct) {\nff_bits = devices[i].ff;\nbreak;\n}\n}\nfor (i = 0; ff_bits[i] >= 0; i++)\nset_bit(ff_bits[i], dev->ffbit);\nerror = input_ff_create_memless(dev, NULL, hid_lgff_play);\nif (error)\nreturn error;\nif ( test_bit(FF_AUTOCENTER, dev->ffbit) )\ndev->ff->set_autocenter = hid_lgff_set_autocenter;\npr_info(\"Force feedback for Logitech force feedback devices by Johann Deneux <johann.deneux@it.uu.se>\\n\");\nreturn 0;\n}\n",
      "code_before_change_raw": "int lgff_init(struct hid_device* hid)\n{\nstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);\nstruct input_dev *dev = hidinput->input;\nconst signed short *ff_bits = ff_joystick;\nint error;\nint i;\nif (!hid_validate_values(hid, HID_OUTPUT_REPORT, 0, 0, 7))\nreturn -ENODEV;\nfor (i = 0; i < ARRAY_SIZE(devices); i++) {\nif (dev->id.vendor == devices[i].idVendor &&\ndev->id.product == devices[i].idProduct) {\nff_bits = devices[i].ff;\nbreak;\n}\n}\nfor (i = 0; ff_bits[i] >= 0; i++)\nset_bit(ff_bits[i], dev->ffbit);\nerror = input_ff_create_memless(dev, NULL, hid_lgff_play);\nif (error)\nreturn error;\nif ( test_bit(FF_AUTOCENTER, dev->ffbit) )\ndev->ff->set_autocenter = hid_lgff_set_autocenter;\npr_info(\"Force feedback for Logitech force feedback devices by Johann Deneux <johann.deneux@it.uu.se>\\n\");\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper initialization and validation of input variables, specifically 'hidinput' and 'dev', before accessing them in the code.",
        "trigger_condition": "A malicious USB device can exploit the lack of proper initialization and validation of input variables to trigger out-of-bounds write bugs in the HID drivers.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the list of inputs is empty before attempting to access 'hidinput' and 'dev'. This can lead to out-of-bounds write vulnerabilities if the input variables are not properly initialized, allowing a malicious USB device to manipulate memory beyond the intended boundaries."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the list of inputs is not empty before accessing 'hidinput' and 'dev'. By verifying the proper initialization of these variables, the code can prevent out-of-bounds write vulnerabilities that could be exploited by a malicious USB device. In this case, the solution involves adding a conditional check to confirm the presence of inputs before proceeding with accessing the input variables.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2019-19532, which involves multiple out-of-bounds write bugs that can be exploited by a malicious USB device in the Linux kernel HID drivers. \n\nThe modification specifically addresses the vulnerability by adding a check to ensure that the `hidinput` and `dev` variables are properly initialized before accessing them. By checking if the list of inputs is empty and then initializing `hidinput` and `dev` accordingly, the code prevents potential out-of-bounds write vulnerabilities that could be exploited by a malicious USB device.\n\nBy making these changes, the code becomes more robust and secure, reducing the risk of exploitation through out-of-bounds write vulnerabilities in the HID drivers.",
      "GPT_purpose": "Initialize force feedback support for a HID++ device, setting capabilities, creating FF device, allocating memory for private data, setting function pointers, creating sysfs interface, and initializing hardware command queue.",
      "GPT_function": "\n1. Initialize force feedback for a HID++ device.\n2. Set supported force feedback capabilities based on firmware version.\n3. Read the number of slots available in the device.\n4. Create a force feedback device.\n5. Allocate memory for private data related to force feedback.\n6. Initialize various parameters for force feedback.\n7. Reset all forces on the device.\n8. Read the current range of force feedback.\n9. Create a sysfs interface for force feedback range.\n10. Read the current gain values for force feedback.",
      "CVE_id": "CVE-2019-19532",
      "code_before_change": "static int hidpp_ff_init(struct hidpp_device *hidpp, u8 feature_index)\n{\n\tstruct hid_device *hid = hidpp->hid_dev;\n\tstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);\n\tstruct input_dev *dev = hidinput->input;\n\tconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\n\tconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\n\tstruct ff_device *ff;\n\tstruct hidpp_report response;\n\tstruct hidpp_ff_private_data *data;\n\tint error, j, num_slots;\n\tu8 version;\n\n\tif (!dev) {\n\t\thid_err(hid, \"Struct input_dev not set!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Get firmware release */\n\tversion = bcdDevice & 255;\n\n\t/* Set supported force feedback capabilities */\n\tfor (j = 0; hidpp_ff_effects[j] >= 0; j++)\n\t\tset_bit(hidpp_ff_effects[j], dev->ffbit);\n\tif (version > 1)\n\t\tfor (j = 0; hidpp_ff_effects_v2[j] >= 0; j++)\n\t\t\tset_bit(hidpp_ff_effects_v2[j], dev->ffbit);\n\n\t/* Read number of slots available in device */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_GET_INFO, NULL, 0, &response);\n\tif (error) {\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\thid_err(hidpp->hid_dev, \"%s: received protocol error 0x%02x\\n\",\n\t\t\t__func__, error);\n\t\treturn -EPROTO;\n\t}\n\n\tnum_slots = response.fap.params[0] - HIDPP_FF_RESERVED_SLOTS;\n\n\terror = input_ff_create(dev, num_slots);\n\n\tif (error) {\n\t\thid_err(dev, \"Failed to create FF device!\\n\");\n\t\treturn error;\n\t}\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\tdata->effect_ids = kcalloc(num_slots, sizeof(int), GFP_KERNEL);\n\tif (!data->effect_ids) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tdata->wq = create_singlethread_workqueue(\"hidpp-ff-sendqueue\");\n\tif (!data->wq) {\n\t\tkfree(data->effect_ids);\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\n\tdata->hidpp = hidpp;\n\tdata->feature_index = feature_index;\n\tdata->version = version;\n\tdata->slot_autocenter = 0;\n\tdata->num_effects = num_slots;\n\tfor (j = 0; j < num_slots; j++)\n\t\tdata->effect_ids[j] = -1;\n\n\tff = dev->ff;\n\tff->private = data;\n\n\tff->upload = hidpp_ff_upload_effect;\n\tff->erase = hidpp_ff_erase_effect;\n\tff->playback = hidpp_ff_playback;\n\tff->set_gain = hidpp_ff_set_gain;\n\tff->set_autocenter = hidpp_ff_set_autocenter;\n\tff->destroy = hidpp_ff_destroy;\n\n\n\t/* reset all forces */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_RESET_ALL, NULL, 0, &response);\n\n\t/* Read current Range */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_GET_APERTURE, NULL, 0, &response);\n\tif (error)\n\t\thid_warn(hidpp->hid_dev, \"Failed to read range from device!\\n\");\n\tdata->range = error ? 900 : get_unaligned_be16(&response.fap.params[0]);\n\n\t/* Create sysfs interface */\n\terror = device_create_file(&(hidpp->hid_dev->dev), &dev_attr_range);\n\tif (error)\n\t\thid_warn(hidpp->hid_dev, \"Unable to create sysfs interface for \\\"range\\\", errno %d!\\n\", error);\n\n\t/* Read the current gain values */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_GET_GLOBAL_GAINS, NULL, 0, &response);\n\tif (error)\n\t\thid_warn(hidpp->hid_dev, \"Failed to read gain values from device!\\n\");\n\tdata->gain = error ? 0xffff : get_unaligned_be16(&response.fap.params[0]);\n\t/* ignore boost value at response.fap.params[2] */\n\n\t/* init the hardware command queue */\n\tatomic_set(&data->workqueue_size, 0);\n\n\t/* initialize with zero autocenter to get wheel in usable state */\n\thidpp_ff_set_autocenter(dev, 0);\n\n\thid_info(hid, \"Force feedback support loaded (firmware release %d).\\n\",\n\t\t version);\n\n\treturn 0;\n}",
      "code_after_change": "static int hidpp_ff_init(struct hidpp_device *hidpp, u8 feature_index)\n{\n\tstruct hid_device *hid = hidpp->hid_dev;\n\tstruct hid_input *hidinput;\n\tstruct input_dev *dev;\n\tconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\n\tconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\n\tstruct ff_device *ff;\n\tstruct hidpp_report response;\n\tstruct hidpp_ff_private_data *data;\n\tint error, j, num_slots;\n\tu8 version;\n\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\treturn -ENODEV;\n\t}\n\thidinput = list_entry(hid->inputs.next, struct hid_input, list);\n\tdev = hidinput->input;\n\n\tif (!dev) {\n\t\thid_err(hid, \"Struct input_dev not set!\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Get firmware release */\n\tversion = bcdDevice & 255;\n\n\t/* Set supported force feedback capabilities */\n\tfor (j = 0; hidpp_ff_effects[j] >= 0; j++)\n\t\tset_bit(hidpp_ff_effects[j], dev->ffbit);\n\tif (version > 1)\n\t\tfor (j = 0; hidpp_ff_effects_v2[j] >= 0; j++)\n\t\t\tset_bit(hidpp_ff_effects_v2[j], dev->ffbit);\n\n\t/* Read number of slots available in device */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_GET_INFO, NULL, 0, &response);\n\tif (error) {\n\t\tif (error < 0)\n\t\t\treturn error;\n\t\thid_err(hidpp->hid_dev, \"%s: received protocol error 0x%02x\\n\",\n\t\t\t__func__, error);\n\t\treturn -EPROTO;\n\t}\n\n\tnum_slots = response.fap.params[0] - HIDPP_FF_RESERVED_SLOTS;\n\n\terror = input_ff_create(dev, num_slots);\n\n\tif (error) {\n\t\thid_err(dev, \"Failed to create FF device!\\n\");\n\t\treturn error;\n\t}\n\n\tdata = kzalloc(sizeof(*data), GFP_KERNEL);\n\tif (!data)\n\t\treturn -ENOMEM;\n\tdata->effect_ids = kcalloc(num_slots, sizeof(int), GFP_KERNEL);\n\tif (!data->effect_ids) {\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\tdata->wq = create_singlethread_workqueue(\"hidpp-ff-sendqueue\");\n\tif (!data->wq) {\n\t\tkfree(data->effect_ids);\n\t\tkfree(data);\n\t\treturn -ENOMEM;\n\t}\n\n\tdata->hidpp = hidpp;\n\tdata->feature_index = feature_index;\n\tdata->version = version;\n\tdata->slot_autocenter = 0;\n\tdata->num_effects = num_slots;\n\tfor (j = 0; j < num_slots; j++)\n\t\tdata->effect_ids[j] = -1;\n\n\tff = dev->ff;\n\tff->private = data;\n\n\tff->upload = hidpp_ff_upload_effect;\n\tff->erase = hidpp_ff_erase_effect;\n\tff->playback = hidpp_ff_playback;\n\tff->set_gain = hidpp_ff_set_gain;\n\tff->set_autocenter = hidpp_ff_set_autocenter;\n\tff->destroy = hidpp_ff_destroy;\n\n\n\t/* reset all forces */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_RESET_ALL, NULL, 0, &response);\n\n\t/* Read current Range */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_GET_APERTURE, NULL, 0, &response);\n\tif (error)\n\t\thid_warn(hidpp->hid_dev, \"Failed to read range from device!\\n\");\n\tdata->range = error ? 900 : get_unaligned_be16(&response.fap.params[0]);\n\n\t/* Create sysfs interface */\n\terror = device_create_file(&(hidpp->hid_dev->dev), &dev_attr_range);\n\tif (error)\n\t\thid_warn(hidpp->hid_dev, \"Unable to create sysfs interface for \\\"range\\\", errno %d!\\n\", error);\n\n\t/* Read the current gain values */\n\terror = hidpp_send_fap_command_sync(hidpp, feature_index,\n\t\tHIDPP_FF_GET_GLOBAL_GAINS, NULL, 0, &response);\n\tif (error)\n\t\thid_warn(hidpp->hid_dev, \"Failed to read gain values from device!\\n\");\n\tdata->gain = error ? 0xffff : get_unaligned_be16(&response.fap.params[0]);\n\t/* ignore boost value at response.fap.params[2] */\n\n\t/* init the hardware command queue */\n\tatomic_set(&data->workqueue_size, 0);\n\n\t/* initialize with zero autocenter to get wheel in usable state */\n\thidpp_ff_set_autocenter(dev, 0);\n\n\thid_info(hid, \"Force feedback support loaded (firmware release %d).\\n\",\n\t\t version);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tstruct hid_input *hidinput;",
          "\tstruct input_dev *dev;",
          "",
          "\tif (list_empty(&hid->inputs)) {",
          "\t\thid_err(hid, \"no inputs found\\n\");",
          "\t\treturn -ENODEV;",
          "\t}",
          "\thidinput = list_entry(hid->inputs.next, struct hid_input, list);",
          "\tdev = hidinput->input;"
        ],
        "deleted": [
          "\tstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);",
          "\tstruct input_dev *dev = hidinput->input;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper initialization and validation of input variables, specifically 'hidinput' and 'dev', before accessing them in the code.",
      "trigger_condition": "A malicious USB device can exploit the lack of proper initialization and validation of input variables to trigger out-of-bounds write bugs in the HID drivers.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the list of inputs is empty before attempting to access 'hidinput' and 'dev'. This can lead to out-of-bounds write vulnerabilities if the input variables are not properly initialized, allowing a malicious USB device to manipulate memory beyond the intended boundaries.",
      "id": 50,
      "code_after_change_normalized": "static int FUN1(struct hidpp_device *VAR1, u8 VAR2)\n{\nstruct hid_device *VAR3 = VAR1->VAR4;\nstruct hid_input *VAR5;\nstruct input_dev *VAR6;\nconst struct usb_device_descriptor *VAR7 = &(FUN2(VAR3)->VAR8);\nconst u16 VAR9 = FUN3(VAR7->VAR9);\nstruct ff_device *VAR10;\nstruct hidpp_report VAR11;\nstruct hidpp_ff_private_data *VAR12;\nint VAR13, VAR14, VAR15;\nu8 VAR16;\nif (FUN4(&VAR3->VAR17)) {\nFUN5(VAR3, \"STR\");\nreturn -VAR18;\n}\nVAR5 = FUN6(VAR3->VAR17.VAR19, struct VAR20, VAR21);\nVAR6 = VAR5->VAR22;\nif (!VAR6) {\nFUN5(VAR3, \"STR\");\nreturn -VAR23;\n}\nVAR16 = VAR9 & 255;\nfor (VAR14 = 0; VAR24[VAR14] >= 0; VAR14++)\nFUN7(VAR24[VAR14], VAR6->VAR25);\nif (VAR16 > 1)\nfor (VAR14 = 0; VAR26[VAR14] >= 0; VAR14++)\nFUN7(VAR26[VAR14], VAR6->VAR25);\nVAR13 = FUN8(VAR1, VAR2,\nVAR27, NULL, 0, &VAR11);\nif (VAR13) {\nif (VAR13 < 0)\nreturn VAR13;\nFUN5(VAR1->VAR4, \"STR\",\nVAR28, VAR13);\nreturn -VAR29;\n}\nVAR15 = VAR11.VAR30.VAR31[0] - VAR32;\nVAR13 = FUN9(VAR6, VAR15);\nif (VAR13) {\nFUN5(VAR6, \"STR\");\nreturn VAR13;\n}\nVAR12 = FUN10(sizeof(*VAR12), VAR33);\nif (!VAR12)\nreturn -VAR34;\nVAR12->VAR35 = FUN11(VAR15, sizeof(int), VAR33);\nif (!VAR12->VAR35) {\nFUN12(VAR12);\nreturn -VAR34;\n}\nVAR12->VAR36 = FUN13(\"STR\");\nif (!VAR12->VAR36) {\nFUN12(VAR12->VAR35);\nFUN12(VAR12);\nreturn -VAR34;\n}\nVAR12->VAR1 = VAR1;\nVAR12->VAR2 = VAR2;\nVAR12->VAR16 = VAR16;\nVAR12->VAR37 = 0;\nVAR12->VAR38 = VAR15;\nfor (VAR14 = 0; VAR14 < VAR15; VAR14++)\nVAR12->VAR35[VAR14] = -1;\nVAR10 = VAR6->VAR10;\nVAR10->private = VAR12;\nVAR10->VAR39 = VAR40;\nVAR10->VAR41 = VAR42;\nVAR10->VAR43 = VAR44;\nVAR10->VAR45 = VAR46;\nVAR10->VAR47 = VAR48;\nVAR10->VAR49 = VAR50;\nVAR13 = FUN8(VAR1, VAR2,\nVAR51, NULL, 0, &VAR11);\nVAR13 = FUN8(VAR1, VAR2,\nVAR52, NULL, 0, &VAR11);\nif (VAR13)\nFUN14(VAR1->VAR4, \"STR\");\nVAR12->VAR53 = VAR13 ? 900 : FUN15(&VAR11.VAR30.VAR31[0]);\nVAR13 = FUN16(&(VAR1->VAR4->VAR6), &VAR54);\nif (VAR13)\nFUN14(VAR1->VAR4, \"STR\", VAR13);\nVAR13 = FUN8(VAR1, VAR2,\nVAR55, NULL, 0, &VAR11);\nif (VAR13)\nFUN14(VAR1->VAR4, \"STR\");\nVAR12->VAR56 = VAR13 ? VAR57 : FUN15(&VAR11.VAR30.VAR31[0]);\nFUN17(&VAR12->VAR58, 0);\nFUN18(VAR6, 0);\nFUN19(VAR3, \"STR\",\nVAR16);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct hidpp_device *VAR1, u8 VAR2)\n{\nstruct hid_device *VAR3 = VAR1->VAR4;\nstruct VAR8 *VAR5 = FUN2(VAR3->VAR6.VAR7, struct VAR8, VAR9);\nstruct input_dev *VAR10 = VAR5->VAR11;\nconst struct usb_device_descriptor *VAR12 = &(FUN3(VAR3)->VAR13);\nconst u16 VAR14 = FUN4(VAR12->VAR14);\nstruct ff_device *VAR15;\nstruct hidpp_report VAR16;\nstruct hidpp_ff_private_data *VAR17;\nint VAR18, VAR19, VAR20;\nu8 VAR21;\nif (!VAR10) {\nFUN5(VAR3, \"STR\");\nreturn -VAR22;\n}\nVAR21 = VAR14 & 255;\nfor (VAR19 = 0; VAR23[VAR19] >= 0; VAR19++)\nFUN6(VAR23[VAR19], VAR10->VAR24);\nif (VAR21 > 1)\nfor (VAR19 = 0; VAR25[VAR19] >= 0; VAR19++)\nFUN6(VAR25[VAR19], VAR10->VAR24);\nVAR18 = FUN7(VAR1, VAR2,\nVAR26, NULL, 0, &VAR16);\nif (VAR18) {\nif (VAR18 < 0)\nreturn VAR18;\nFUN5(VAR1->VAR4, \"STR\",\nVAR27, VAR18);\nreturn -VAR28;\n}\nVAR20 = VAR16.VAR29.VAR30[0] - VAR31;\nVAR18 = FUN8(VAR10, VAR20);\nif (VAR18) {\nFUN5(VAR10, \"STR\");\nreturn VAR18;\n}\nVAR17 = FUN9(sizeof(*VAR17), VAR32);\nif (!VAR17)\nreturn -VAR33;\nVAR17->VAR34 = FUN10(VAR20, sizeof(int), VAR32);\nif (!VAR17->VAR34) {\nFUN11(VAR17);\nreturn -VAR33;\n}\nVAR17->VAR35 = FUN12(\"STR\");\nif (!VAR17->VAR35) {\nFUN11(VAR17->VAR34);\nFUN11(VAR17);\nreturn -VAR33;\n}\nVAR17->VAR1 = VAR1;\nVAR17->VAR2 = VAR2;\nVAR17->VAR21 = VAR21;\nVAR17->VAR36 = 0;\nVAR17->VAR37 = VAR20;\nfor (VAR19 = 0; VAR19 < VAR20; VAR19++)\nVAR17->VAR34[VAR19] = -1;\nVAR15 = VAR10->VAR15;\nVAR15->private = VAR17;\nVAR15->VAR38 = VAR39;\nVAR15->VAR40 = VAR41;\nVAR15->VAR42 = VAR43;\nVAR15->VAR44 = VAR45;\nVAR15->VAR46 = VAR47;\nVAR15->VAR48 = VAR49;\nVAR18 = FUN7(VAR1, VAR2,\nVAR50, NULL, 0, &VAR16);\nVAR18 = FUN7(VAR1, VAR2,\nVAR51, NULL, 0, &VAR16);\nif (VAR18)\nFUN13(VAR1->VAR4, \"STR\");\nVAR17->VAR52 = VAR18 ? 900 : FUN14(&VAR16.VAR29.VAR30[0]);\nVAR18 = FUN15(&(VAR1->VAR4->VAR10), &VAR53);\nif (VAR18)\nFUN13(VAR1->VAR4, \"STR\", VAR18);\nVAR18 = FUN7(VAR1, VAR2,\nVAR54, NULL, 0, &VAR16);\nif (VAR18)\nFUN13(VAR1->VAR4, \"STR\");\nVAR17->VAR55 = VAR18 ? VAR56 : FUN14(&VAR16.VAR29.VAR30[0]);\nFUN16(&VAR17->VAR57, 0);\nFUN17(VAR10, 0);\nFUN18(VAR3, \"STR\",\nVAR21);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int hidpp_ff_init(struct hidpp_device *hidpp, u8 feature_index)\n{\nstruct hid_device *hid = hidpp->hid_dev;\nstruct hid_input *hidinput;\nstruct input_dev *dev;\nconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\nconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\nstruct ff_device *ff;\nstruct hidpp_report response;\nstruct hidpp_ff_private_data *data;\nint error, j, num_slots;\nu8 version;\nif (list_empty(&hid->inputs)) {\nhid_err(hid, \"no inputs found\\n\");\nreturn -ENODEV;\n}\nhidinput = list_entry(hid->inputs.next, struct hid_input, list);\ndev = hidinput->input;\nif (!dev) {\nhid_err(hid, \"Struct input_dev not set!\\n\");\nreturn -EINVAL;\n}\nversion = bcdDevice & 255;\nfor (j = 0; hidpp_ff_effects[j] >= 0; j++)\nset_bit(hidpp_ff_effects[j], dev->ffbit);\nif (version > 1)\nfor (j = 0; hidpp_ff_effects_v2[j] >= 0; j++)\nset_bit(hidpp_ff_effects_v2[j], dev->ffbit);\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_GET_INFO, NULL, 0, &response);\nif (error) {\nif (error < 0)\nreturn error;\nhid_err(hidpp->hid_dev, \"%s: received protocol error 0x%02x\\n\",\n__func__, error);\nreturn -EPROTO;\n}\nnum_slots = response.fap.params[0] - HIDPP_FF_RESERVED_SLOTS;\nerror = input_ff_create(dev, num_slots);\nif (error) {\nhid_err(dev, \"Failed to create FF device!\\n\");\nreturn error;\n}\ndata = kzalloc(sizeof(*data), GFP_KERNEL);\nif (!data)\nreturn -ENOMEM;\ndata->effect_ids = kcalloc(num_slots, sizeof(int), GFP_KERNEL);\nif (!data->effect_ids) {\nkfree(data);\nreturn -ENOMEM;\n}\ndata->wq = create_singlethread_workqueue(\"hidpp-ff-sendqueue\");\nif (!data->wq) {\nkfree(data->effect_ids);\nkfree(data);\nreturn -ENOMEM;\n}\ndata->hidpp = hidpp;\ndata->feature_index = feature_index;\ndata->version = version;\ndata->slot_autocenter = 0;\ndata->num_effects = num_slots;\nfor (j = 0; j < num_slots; j++)\ndata->effect_ids[j] = -1;\nff = dev->ff;\nff->private = data;\nff->upload = hidpp_ff_upload_effect;\nff->erase = hidpp_ff_erase_effect;\nff->playback = hidpp_ff_playback;\nff->set_gain = hidpp_ff_set_gain;\nff->set_autocenter = hidpp_ff_set_autocenter;\nff->destroy = hidpp_ff_destroy;\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_RESET_ALL, NULL, 0, &response);\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_GET_APERTURE, NULL, 0, &response);\nif (error)\nhid_warn(hidpp->hid_dev, \"Failed to read range from device!\\n\");\ndata->range = error ? 900 : get_unaligned_be16(&response.fap.params[0]);\nerror = device_create_file(&(hidpp->hid_dev->dev), &dev_attr_range);\nif (error)\nhid_warn(hidpp->hid_dev, \"Unable to create sysfs interface for \\\"range\\\", errno %d!\\n\", error);\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_GET_GLOBAL_GAINS, NULL, 0, &response);\nif (error)\nhid_warn(hidpp->hid_dev, \"Failed to read gain values from device!\\n\");\ndata->gain = error ? 0xffff : get_unaligned_be16(&response.fap.params[0]);\natomic_set(&data->workqueue_size, 0);\nhidpp_ff_set_autocenter(dev, 0);\nhid_info(hid, \"Force feedback support loaded (firmware release %d).\\n\",\nversion);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int hidpp_ff_init(struct hidpp_device *hidpp, u8 feature_index)\n{\nstruct hid_device *hid = hidpp->hid_dev;\nstruct hid_input *hidinput = list_entry(hid->inputs.next, struct hid_input, list);\nstruct input_dev *dev = hidinput->input;\nconst struct usb_device_descriptor *udesc = &(hid_to_usb_dev(hid)->descriptor);\nconst u16 bcdDevice = le16_to_cpu(udesc->bcdDevice);\nstruct ff_device *ff;\nstruct hidpp_report response;\nstruct hidpp_ff_private_data *data;\nint error, j, num_slots;\nu8 version;\nif (!dev) {\nhid_err(hid, \"Struct input_dev not set!\\n\");\nreturn -EINVAL;\n}\nversion = bcdDevice & 255;\nfor (j = 0; hidpp_ff_effects[j] >= 0; j++)\nset_bit(hidpp_ff_effects[j], dev->ffbit);\nif (version > 1)\nfor (j = 0; hidpp_ff_effects_v2[j] >= 0; j++)\nset_bit(hidpp_ff_effects_v2[j], dev->ffbit);\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_GET_INFO, NULL, 0, &response);\nif (error) {\nif (error < 0)\nreturn error;\nhid_err(hidpp->hid_dev, \"%s: received protocol error 0x%02x\\n\",\n__func__, error);\nreturn -EPROTO;\n}\nnum_slots = response.fap.params[0] - HIDPP_FF_RESERVED_SLOTS;\nerror = input_ff_create(dev, num_slots);\nif (error) {\nhid_err(dev, \"Failed to create FF device!\\n\");\nreturn error;\n}\ndata = kzalloc(sizeof(*data), GFP_KERNEL);\nif (!data)\nreturn -ENOMEM;\ndata->effect_ids = kcalloc(num_slots, sizeof(int), GFP_KERNEL);\nif (!data->effect_ids) {\nkfree(data);\nreturn -ENOMEM;\n}\ndata->wq = create_singlethread_workqueue(\"hidpp-ff-sendqueue\");\nif (!data->wq) {\nkfree(data->effect_ids);\nkfree(data);\nreturn -ENOMEM;\n}\ndata->hidpp = hidpp;\ndata->feature_index = feature_index;\ndata->version = version;\ndata->slot_autocenter = 0;\ndata->num_effects = num_slots;\nfor (j = 0; j < num_slots; j++)\ndata->effect_ids[j] = -1;\nff = dev->ff;\nff->private = data;\nff->upload = hidpp_ff_upload_effect;\nff->erase = hidpp_ff_erase_effect;\nff->playback = hidpp_ff_playback;\nff->set_gain = hidpp_ff_set_gain;\nff->set_autocenter = hidpp_ff_set_autocenter;\nff->destroy = hidpp_ff_destroy;\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_RESET_ALL, NULL, 0, &response);\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_GET_APERTURE, NULL, 0, &response);\nif (error)\nhid_warn(hidpp->hid_dev, \"Failed to read range from device!\\n\");\ndata->range = error ? 900 : get_unaligned_be16(&response.fap.params[0]);\nerror = device_create_file(&(hidpp->hid_dev->dev), &dev_attr_range);\nif (error)\nhid_warn(hidpp->hid_dev, \"Unable to create sysfs interface for \\\"range\\\", errno %d!\\n\", error);\nerror = hidpp_send_fap_command_sync(hidpp, feature_index,\nHIDPP_FF_GET_GLOBAL_GAINS, NULL, 0, &response);\nif (error)\nhid_warn(hidpp->hid_dev, \"Failed to read gain values from device!\\n\");\ndata->gain = error ? 0xffff : get_unaligned_be16(&response.fap.params[0]);\natomic_set(&data->workqueue_size, 0);\nhidpp_ff_set_autocenter(dev, 0);\nhid_info(hid, \"Force feedback support loaded (firmware release %d).\\n\",\nversion);\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation before accessing a list of inputs in the HID drivers.",
        "trigger_condition": "A malicious USB device can exploit the lack of validation to trigger out-of-bounds write bugs in the HID drivers.",
        "specific_code_behavior_causing_vulnerability": "The original code snippet does not check if the list of inputs is empty or corrupted before accessing it, potentially leading to out-of-bounds writes if the list is not properly initialized or contains invalid data."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure the list of inputs is not empty before attempting to access it. By verifying the validity of the input list, the code can prevent out-of-bounds write bugs and enhance the security of the HID drivers. In this case, the solution involves adding a check to verify if the list of inputs is empty before proceeding with input-related operations.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-19532, which involves out-of-bounds write bugs that can be exploited by a malicious USB device in the Linux kernel HID drivers. \n\nThe original code snippet does not perform proper validation before accessing the `hdev->inputs` list, which can lead to out-of-bounds writes if the list is empty or corrupted. By adding the check `if (list_empty(&hdev->inputs))` and handling the case where no inputs are found, the modified code snippet ensures that the code operates safely and avoids potential vulnerabilities related to accessing uninitialized or invalid memory.\n\nTherefore, the modification helps prevent potential security risks associated with out-of-bounds writes and enhances the overall robustness of the code.",
      "GPT_purpose": "Initialize force feedback (FF) capabilities for a Microsoft (MS) device, checking for a specific quirk and setting up FF related structures.",
      "GPT_function": "\n1. Initialize force feedback for a HID device.\n2. Check for a specific quirk related to force feedback.\n3. Allocate memory for an output report buffer.\n4. Set capability for force feedback rumble effect.\n5. Create a memoryless force feedback input device.",
      "CVE_id": "CVE-2019-19532",
      "code_before_change": "static int ms_init_ff(struct hid_device *hdev)\n{\n\tstruct hid_input *hidinput = list_entry(hdev->inputs.next,\n\t\t\t\t\t\tstruct hid_input, list);\n\tstruct input_dev *input_dev = hidinput->input;\n\tstruct ms_data *ms = hid_get_drvdata(hdev);\n\n\tif (!(ms->quirks & MS_QUIRK_FF))\n\t\treturn 0;\n\n\tms->hdev = hdev;\n\tINIT_WORK(&ms->ff_worker, ms_ff_worker);\n\n\tms->output_report_dmabuf = devm_kzalloc(&hdev->dev,\n\t\t\t\t\t\tsizeof(struct xb1s_ff_report),\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (ms->output_report_dmabuf == NULL)\n\t\treturn -ENOMEM;\n\n\tinput_set_capability(input_dev, EV_FF, FF_RUMBLE);\n\treturn input_ff_create_memless(input_dev, NULL, ms_play_effect);\n}",
      "code_after_change": "static int ms_init_ff(struct hid_device *hdev)\n{\n\tstruct hid_input *hidinput;\n\tstruct input_dev *input_dev;\n\tstruct ms_data *ms = hid_get_drvdata(hdev);\n\n\tif (list_empty(&hdev->inputs)) {\n\t\thid_err(hdev, \"no inputs found\\n\");\n\t\treturn -ENODEV;\n\t}\n\thidinput = list_entry(hdev->inputs.next, struct hid_input, list);\n\tinput_dev = hidinput->input;\n\n\tif (!(ms->quirks & MS_QUIRK_FF))\n\t\treturn 0;\n\n\tms->hdev = hdev;\n\tINIT_WORK(&ms->ff_worker, ms_ff_worker);\n\n\tms->output_report_dmabuf = devm_kzalloc(&hdev->dev,\n\t\t\t\t\t\tsizeof(struct xb1s_ff_report),\n\t\t\t\t\t\tGFP_KERNEL);\n\tif (ms->output_report_dmabuf == NULL)\n\t\treturn -ENOMEM;\n\n\tinput_set_capability(input_dev, EV_FF, FF_RUMBLE);\n\treturn input_ff_create_memless(input_dev, NULL, ms_play_effect);\n}",
      "modified_lines": {
        "added": [
          "\tstruct hid_input *hidinput;",
          "\tstruct input_dev *input_dev;",
          "",
          "\tif (list_empty(&hdev->inputs)) {",
          "\t\thid_err(hdev, \"no inputs found\\n\");",
          "\t\treturn -ENODEV;",
          "\t}",
          "\thidinput = list_entry(hdev->inputs.next, struct hid_input, list);",
          "\tinput_dev = hidinput->input;"
        ],
        "deleted": [
          "\tstruct hid_input *hidinput = list_entry(hdev->inputs.next,",
          "\t\t\t\t\t\tstruct hid_input, list);",
          "\tstruct input_dev *input_dev = hidinput->input;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation before accessing a list of inputs in the HID drivers.",
      "trigger_condition": "A malicious USB device can exploit the lack of validation to trigger out-of-bounds write bugs in the HID drivers.",
      "specific_code_behavior_causing_vulnerability": "The original code snippet does not check if the list of inputs is empty or corrupted before accessing it, potentially leading to out-of-bounds writes if the list is not properly initialized or contains invalid data.",
      "id": 51,
      "code_after_change_normalized": "static int FUN1(struct hid_device *VAR1)\n{\nstruct hid_input *VAR2;\nstruct VAR3 *VAR3;\nstruct ms_data *VAR4 = FUN2(VAR1);\nif (FUN3(&VAR1->VAR5)) {\nFUN4(VAR1, \"STR\");\nreturn -VAR6;\n}\nVAR2 = FUN5(VAR1->VAR5.VAR7, struct VAR8, VAR9);\nVAR3 = VAR2->VAR10;\nif (!(VAR4->VAR11 & VAR12))\nreturn 0;\nVAR4->VAR1 = VAR1;\nFUN6(&VAR4->VAR13, VAR14);\nVAR4->VAR15 = FUN7(&VAR1->VAR16,\nsizeof(struct VAR17),\nVAR18);\nif (VAR4->VAR15 == NULL)\nreturn -VAR19;\nFUN8(VAR3, VAR20, VAR21);\nreturn FUN9(VAR3, NULL, VAR22);\n}\n",
      "code_before_change_normalized": "static int FUN1(struct hid_device *VAR1)\n{\nstruct hid_input *VAR2 = FUN2(VAR1->VAR3.VAR4,\nstruct VAR5, VAR6);\nstruct VAR7 *VAR7 = VAR2->VAR8;\nstruct ms_data *VAR9 = FUN3(VAR1);\nif (!(VAR9->VAR10 & VAR11))\nreturn 0;\nVAR9->VAR1 = VAR1;\nFUN4(&VAR9->VAR12, VAR13);\nVAR9->VAR14 = FUN5(&VAR1->VAR15,\nsizeof(struct VAR16),\nVAR17);\nif (VAR9->VAR14 == NULL)\nreturn -VAR18;\nFUN6(VAR7, VAR19, VAR20);\nreturn FUN7(VAR7, NULL, VAR21);\n}\n",
      "code_after_change_raw": "static int ms_init_ff(struct hid_device *hdev)\n{\nstruct hid_input *hidinput;\nstruct input_dev *input_dev;\nstruct ms_data *ms = hid_get_drvdata(hdev);\nif (list_empty(&hdev->inputs)) {\nhid_err(hdev, \"no inputs found\\n\");\nreturn -ENODEV;\n}\nhidinput = list_entry(hdev->inputs.next, struct hid_input, list);\ninput_dev = hidinput->input;\nif (!(ms->quirks & MS_QUIRK_FF))\nreturn 0;\nms->hdev = hdev;\nINIT_WORK(&ms->ff_worker, ms_ff_worker);\nms->output_report_dmabuf = devm_kzalloc(&hdev->dev,\nsizeof(struct xb1s_ff_report),\nGFP_KERNEL);\nif (ms->output_report_dmabuf == NULL)\nreturn -ENOMEM;\ninput_set_capability(input_dev, EV_FF, FF_RUMBLE);\nreturn input_ff_create_memless(input_dev, NULL, ms_play_effect);\n}\n",
      "code_before_change_raw": "static int ms_init_ff(struct hid_device *hdev)\n{\nstruct hid_input *hidinput = list_entry(hdev->inputs.next,\nstruct hid_input, list);\nstruct input_dev *input_dev = hidinput->input;\nstruct ms_data *ms = hid_get_drvdata(hdev);\nif (!(ms->quirks & MS_QUIRK_FF))\nreturn 0;\nms->hdev = hdev;\nINIT_WORK(&ms->ff_worker, ms_ff_worker);\nms->output_report_dmabuf = devm_kzalloc(&hdev->dev,\nsizeof(struct xb1s_ff_report),\nGFP_KERNEL);\nif (ms->output_report_dmabuf == NULL)\nreturn -ENOMEM;\ninput_set_capability(input_dev, EV_FF, FF_RUMBLE);\nreturn input_ff_create_memless(input_dev, NULL, ms_play_effect);\n}\n"
    }
  ],
  "CVE-2019-19816": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of inode type validation during filesystem operations.",
        "trigger_condition": "Mounting a crafted btrfs filesystem image and performing certain operations that involve inode type validation.",
        "specific_code_behavior_causing_vulnerability": "The code fails to validate the inode type retrieved during filesystem operations, leading to a potential mismatch between the expected type and the actual type of the inode. This can result in a slab-out-of-bounds write access vulnerability, especially when a value of 1 for the number of data stripes is mishandled.",
        "specific_solution_to_fix_vulnerability": "To mitigate the vulnerability, it is necessary to introduce a new variable to store the inode type and validate this type against the expected type after retrieving the inode. By performing this additional check, the code ensures that the inode type matches the expected type, thereby preventing potential slab-out-of-bounds write access vulnerabilities. The solution involves adding the new variable and the validation check to verify the correctness of the inode type before proceeding with further operations."
      },
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2019-19816 related to a slab-out-of-bounds write access in the __btrfs_map_block function in the Linux kernel. The vulnerability occurs due to mishandling a value of 1 for the number of data stripes.\n\nThe key changes made in the modified code snippet are as follows:\n1. Added a new variable `di_type` of type `u8` to store the inode type.\n2. Updated the `btrfs_inode_by_name` function call to pass the `di_type` variable as an additional argument to retrieve the inode type.\n3. Added an extra check after obtaining the inode to compare the inode type with the `di_type` value. If there is a mismatch, it logs an error message and returns an error code `-EUCLEAN`.\n\nBy adding the `di_type` variable and performing the additional check against the inode type, the modified code ensures that the inode mode matches the expected type. This helps prevent the slab-out-of-bounds write access vulnerability by verifying the correctness of the inode type before proceeding with further operations.",
      "GPT_purpose": "This function is used to lookup a dentry within a Btrfs filesystem and return the corresponding inode.",
      "GPT_function": "\n1. Look up a dentry in a Btrfs filesystem.\n2. Check for the length of the dentry name.\n3. Retrieve the inode based on the dentry name.\n4. Handle different cases for inode retrieval.\n5. Perform cleanup work if needed before returning the inode.",
      "CVE_id": "CVE-2019-19816",
      "code_before_change": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tint index;\n\tint ret = 0;\n\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tret = btrfs_inode_by_name(dir, dentry, &location);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\treturn inode;\n\t}\n\n\tindex = srcu_read_lock(&fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(fs_info, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&fs_info->subvol_srcu, index);\n\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&fs_info->cleanup_work_sem);\n\t\tif (!sb_rdonly(inode->i_sb))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&fs_info->cleanup_work_sem);\n\t\tif (ret) {\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(ret);\n\t\t}\n\t}\n\n\treturn inode;\n}",
      "code_after_change": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\n\tstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\n\tstruct inode *inode;\n\tstruct btrfs_root *root = BTRFS_I(dir)->root;\n\tstruct btrfs_root *sub_root = root;\n\tstruct btrfs_key location;\n\tu8 di_type = 0;\n\tint index;\n\tint ret = 0;\n\n\tif (dentry->d_name.len > BTRFS_NAME_LEN)\n\t\treturn ERR_PTR(-ENAMETOOLONG);\n\n\tret = btrfs_inode_by_name(dir, dentry, &location, &di_type);\n\tif (ret < 0)\n\t\treturn ERR_PTR(ret);\n\n\tif (location.type == BTRFS_INODE_ITEM_KEY) {\n\t\tinode = btrfs_iget(dir->i_sb, &location, root, NULL);\n\t\tif (IS_ERR(inode))\n\t\t\treturn inode;\n\n\t\t/* Do extra check against inode mode with di_type */\n\t\tif (btrfs_inode_type(inode) != di_type) {\n\t\t\tbtrfs_crit(fs_info,\n\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",\n\t\t\t\t  inode->i_mode, btrfs_inode_type(inode),\n\t\t\t\t  di_type);\n\t\t\tiput(inode);\n\t\t\treturn ERR_PTR(-EUCLEAN);\n\t\t}\n\t\treturn inode;\n\t}\n\n\tindex = srcu_read_lock(&fs_info->subvol_srcu);\n\tret = fixup_tree_root_location(fs_info, dir, dentry,\n\t\t\t\t       &location, &sub_root);\n\tif (ret < 0) {\n\t\tif (ret != -ENOENT)\n\t\t\tinode = ERR_PTR(ret);\n\t\telse\n\t\t\tinode = new_simple_dir(dir->i_sb, &location, sub_root);\n\t} else {\n\t\tinode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n\t}\n\tsrcu_read_unlock(&fs_info->subvol_srcu, index);\n\n\tif (!IS_ERR(inode) && root != sub_root) {\n\t\tdown_read(&fs_info->cleanup_work_sem);\n\t\tif (!sb_rdonly(inode->i_sb))\n\t\t\tret = btrfs_orphan_cleanup(sub_root);\n\t\tup_read(&fs_info->cleanup_work_sem);\n\t\tif (ret) {\n\t\t\tiput(inode);\n\t\t\tinode = ERR_PTR(ret);\n\t\t}\n\t}\n\n\treturn inode;\n}",
      "modified_lines": {
        "added": [
          "\tu8 di_type = 0;",
          "\tret = btrfs_inode_by_name(dir, dentry, &location, &di_type);",
          "\t\tif (IS_ERR(inode))",
          "\t\t\treturn inode;",
          "",
          "\t\t/* Do extra check against inode mode with di_type */",
          "\t\tif (btrfs_inode_type(inode) != di_type) {",
          "\t\t\tbtrfs_crit(fs_info,",
          "\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",",
          "\t\t\t\t  inode->i_mode, btrfs_inode_type(inode),",
          "\t\t\t\t  di_type);",
          "\t\t\tiput(inode);",
          "\t\t\treturn ERR_PTR(-EUCLEAN);",
          "\t\t}"
        ],
        "deleted": [
          "\tret = btrfs_inode_by_name(dir, dentry, &location);"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of inode type validation during filesystem operations.",
      "trigger_condition": "Mounting a crafted btrfs filesystem image and performing certain operations that involve inode type validation.",
      "specific_code_behavior_causing_vulnerability": "The code fails to validate the inode type retrieved during filesystem operations, leading to a potential mismatch between the expected type and the actual type of the inode. This can result in a slab-out-of-bounds write access vulnerability, especially when a value of 1 for the number of data stripes is mishandled.",
      "id": 52,
      "code_after_change_normalized": "struct inode *FUN1(struct inode *VAR1, struct VAR2 *VAR2)\n{\nstruct btrfs_fs_info *VAR3 = FUN2(VAR1->VAR4);\nstruct VAR5 *VAR5;\nstruct btrfs_root *VAR6 = FUN3(VAR1)->VAR6;\nstruct btrfs_root *VAR7 = VAR6;\nstruct btrfs_key VAR8;\nu8 VAR9 = 0;\nint VAR10;\nint VAR11 = 0;\nif (VAR2->VAR12.VAR13 > VAR14)\nreturn FUN4(-VAR15);\nVAR11 = FUN5(VAR1, VAR2, &VAR8, &VAR9);\nif (VAR11 < 0)\nreturn FUN4(VAR11);\nif (VAR8.VAR16 == VAR17) {\nVAR5 = FUN6(VAR1->VAR4, &VAR8, VAR6, NULL);\nif (FUN7(VAR5))\nreturn VAR5;\nif (FUN8(VAR5) != VAR9) {\nFUN9(VAR3,\n\"STR\",\nVAR5->VAR18, FUN8(VAR5),\nVAR9);\nFUN10(VAR5);\nreturn FUN4(-VAR19);\n}\nreturn VAR5;\n}\nVAR10 = FUN11(&VAR3->VAR20);\nVAR11 = FUN12(VAR3, VAR1, VAR2,\n&VAR8, &VAR7);\nif (VAR11 < 0) {\nif (VAR11 != -VAR21)\nVAR5 = FUN4(VAR11);\nelse\nVAR5 = FUN13(VAR1->VAR4, &VAR8, VAR7);\n} else {\nVAR5 = FUN6(VAR1->VAR4, &VAR8, VAR7, NULL);\n}\nFUN14(&VAR3->VAR20, VAR10);\nif (!FUN7(VAR5) && VAR6 != VAR7) {\nFUN15(&VAR3->VAR22);\nif (!FUN16(VAR5->VAR4))\nVAR11 = FUN17(VAR7);\nFUN18(&VAR3->VAR22);\nif (VAR11) {\nFUN10(VAR5);\nVAR5 = FUN4(VAR11);\n}\n}\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "struct inode *FUN1(struct inode *VAR1, struct VAR2 *VAR2)\n{\nstruct btrfs_fs_info *VAR3 = FUN2(VAR1->VAR4);\nstruct VAR5 *VAR5;\nstruct btrfs_root *VAR6 = FUN3(VAR1)->VAR6;\nstruct btrfs_root *VAR7 = VAR6;\nstruct btrfs_key VAR8;\nint VAR9;\nint VAR10 = 0;\nif (VAR2->VAR11.VAR12 > VAR13)\nreturn FUN4(-VAR14);\nVAR10 = FUN5(VAR1, VAR2, &VAR8);\nif (VAR10 < 0)\nreturn FUN4(VAR10);\nif (VAR8.VAR15 == VAR16) {\nVAR5 = FUN6(VAR1->VAR4, &VAR8, VAR6, NULL);\nreturn VAR5;\n}\nVAR9 = FUN7(&VAR3->VAR17);\nVAR10 = FUN8(VAR3, VAR1, VAR2,\n&VAR8, &VAR7);\nif (VAR10 < 0) {\nif (VAR10 != -VAR18)\nVAR5 = FUN4(VAR10);\nelse\nVAR5 = FUN9(VAR1->VAR4, &VAR8, VAR7);\n} else {\nVAR5 = FUN6(VAR1->VAR4, &VAR8, VAR7, NULL);\n}\nFUN10(&VAR3->VAR17, VAR9);\nif (!FUN11(VAR5) && VAR6 != VAR7) {\nFUN12(&VAR3->VAR19);\nif (!FUN13(VAR5->VAR4))\nVAR10 = FUN14(VAR7);\nFUN15(&VAR3->VAR19);\nif (VAR10) {\nFUN16(VAR5);\nVAR5 = FUN4(VAR10);\n}\n}\nreturn VAR5;\n}\n",
      "code_after_change_raw": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\nstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\nstruct inode *inode;\nstruct btrfs_root *root = BTRFS_I(dir)->root;\nstruct btrfs_root *sub_root = root;\nstruct btrfs_key location;\nu8 di_type = 0;\nint index;\nint ret = 0;\nif (dentry->d_name.len > BTRFS_NAME_LEN)\nreturn ERR_PTR(-ENAMETOOLONG);\nret = btrfs_inode_by_name(dir, dentry, &location, &di_type);\nif (ret < 0)\nreturn ERR_PTR(ret);\nif (location.type == BTRFS_INODE_ITEM_KEY) {\ninode = btrfs_iget(dir->i_sb, &location, root, NULL);\nif (IS_ERR(inode))\nreturn inode;\nif (btrfs_inode_type(inode) != di_type) {\nbtrfs_crit(fs_info,\n\"inode mode mismatch with dir: inode mode=0%o btrfs type=%u dir type=%u\",\ninode->i_mode, btrfs_inode_type(inode),\ndi_type);\niput(inode);\nreturn ERR_PTR(-EUCLEAN);\n}\nreturn inode;\n}\nindex = srcu_read_lock(&fs_info->subvol_srcu);\nret = fixup_tree_root_location(fs_info, dir, dentry,\n&location, &sub_root);\nif (ret < 0) {\nif (ret != -ENOENT)\ninode = ERR_PTR(ret);\nelse\ninode = new_simple_dir(dir->i_sb, &location, sub_root);\n} else {\ninode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n}\nsrcu_read_unlock(&fs_info->subvol_srcu, index);\nif (!IS_ERR(inode) && root != sub_root) {\ndown_read(&fs_info->cleanup_work_sem);\nif (!sb_rdonly(inode->i_sb))\nret = btrfs_orphan_cleanup(sub_root);\nup_read(&fs_info->cleanup_work_sem);\nif (ret) {\niput(inode);\ninode = ERR_PTR(ret);\n}\n}\nreturn inode;\n}\n",
      "code_before_change_raw": "struct inode *btrfs_lookup_dentry(struct inode *dir, struct dentry *dentry)\n{\nstruct btrfs_fs_info *fs_info = btrfs_sb(dir->i_sb);\nstruct inode *inode;\nstruct btrfs_root *root = BTRFS_I(dir)->root;\nstruct btrfs_root *sub_root = root;\nstruct btrfs_key location;\nint index;\nint ret = 0;\nif (dentry->d_name.len > BTRFS_NAME_LEN)\nreturn ERR_PTR(-ENAMETOOLONG);\nret = btrfs_inode_by_name(dir, dentry, &location);\nif (ret < 0)\nreturn ERR_PTR(ret);\nif (location.type == BTRFS_INODE_ITEM_KEY) {\ninode = btrfs_iget(dir->i_sb, &location, root, NULL);\nreturn inode;\n}\nindex = srcu_read_lock(&fs_info->subvol_srcu);\nret = fixup_tree_root_location(fs_info, dir, dentry,\n&location, &sub_root);\nif (ret < 0) {\nif (ret != -ENOENT)\ninode = ERR_PTR(ret);\nelse\ninode = new_simple_dir(dir->i_sb, &location, sub_root);\n} else {\ninode = btrfs_iget(dir->i_sb, &location, sub_root, NULL);\n}\nsrcu_read_unlock(&fs_info->subvol_srcu, index);\nif (!IS_ERR(inode) && root != sub_root) {\ndown_read(&fs_info->cleanup_work_sem);\nif (!sb_rdonly(inode->i_sb))\nret = btrfs_orphan_cleanup(sub_root);\nup_read(&fs_info->cleanup_work_sem);\nif (ret) {\niput(inode);\ninode = ERR_PTR(ret);\n}\n}\nreturn inode;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Mishandling a value of 1 for the number of data stripes in the code.",
        "trigger_condition": "Mounting a crafted btrfs filesystem image and performing specific operations.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly handle the value of 1 for the number of data stripes, leading to a slab-out-of-bounds write access in the __btrfs_map_block function."
      },
      "solution": "To mitigate the vulnerability, it is necessary to update the code to handle the value of 1 for the number of data stripes correctly. In this case, the solution involves modifying the code to ensure proper handling of the number of data stripes to prevent the slab-out-of-bounds write access vulnerability.",
      "GPT_analysis": "The modification is necessary to address a vulnerability (CVE-2019-19816) related to slab-out-of-bounds write access in the Linux kernel 5.0.21. The vulnerability occurs in the `__btrfs_map_block` function in `fs/btrfs/volumes.c` when a value of 1 for the number of data stripes is mishandled. By modifying the code snippet as shown, you are ensuring that the code is updated to prevent the vulnerability from being exploited, thus enhancing the security and stability of the system.",
      "GPT_purpose": "The function `test_btrfs_get_extent` is used to test the `btrfs_get_extent` function in the Btrfs filesystem by creating various scenarios and checking the expected outcomes.",
      "GPT_function": "\n1. Allocate and initialize necessary structures for testing btrfs_get_extent function.\n2. Perform various tests on btrfs_get_extent function with different scenarios.\n3. Free allocated resources and return the test result.",
      "CVE_id": "CVE-2019-19816",
      "code_before_change": "static noinline int test_btrfs_get_extent(u32 sectorsize, u32 nodesize)\n{\n\tstruct btrfs_fs_info *fs_info = NULL;\n\tstruct inode *inode = NULL;\n\tstruct btrfs_root *root = NULL;\n\tstruct extent_map *em = NULL;\n\tu64 orig_start;\n\tu64 disk_bytenr;\n\tu64 offset;\n\tint ret = -ENOMEM;\n\n\ttest_msg(\"running btrfs_get_extent tests\");\n\n\tinode = btrfs_new_test_inode();\n\tif (!inode) {\n\t\ttest_std_err(TEST_ALLOC_INODE);\n\t\treturn ret;\n\t}\n\n\tBTRFS_I(inode)->location.type = BTRFS_INODE_ITEM_KEY;\n\tBTRFS_I(inode)->location.objectid = BTRFS_FIRST_FREE_OBJECTID;\n\tBTRFS_I(inode)->location.offset = 0;\n\n\tfs_info = btrfs_alloc_dummy_fs_info(nodesize, sectorsize);\n\tif (!fs_info) {\n\t\ttest_std_err(TEST_ALLOC_FS_INFO);\n\t\tgoto out;\n\t}\n\n\troot = btrfs_alloc_dummy_root(fs_info);\n\tif (IS_ERR(root)) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\n\troot->node = alloc_dummy_extent_buffer(fs_info, nodesize);\n\tif (!root->node) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\n\tbtrfs_set_header_nritems(root->node, 0);\n\tbtrfs_set_header_level(root->node, 0);\n\tret = -EINVAL;\n\n\t/* First with no extents */\n\tBTRFS_I(inode)->root = root;\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\tem = NULL;\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tfree_extent_map(em);\n\tbtrfs_drop_extent_cache(BTRFS_I(inode), 0, (u64)-1, 0);\n\n\t/*\n\t * All of the magic numbers are based on the mapping setup in\n\t * setup_file_extents, so if you change anything there you need to\n\t * update the comment and update the expected values below.\n\t */\n\tsetup_file_extents(root, sectorsize);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, (u64)-1, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != 0 || em->len != 5) {\n\t\ttest_err(\n\t\t\"unexpected extent wanted start 0 len 5, got start %llu len %llu\",\n\t\t\tem->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_INLINE) {\n\t\ttest_err(\"expected an inline, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\n\tif (em->start != offset || em->len != (sectorsize - 5)) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 1, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't test anything else for inline since it doesn't get set\n\t * unless we have a page for it to write into.  Maybe we should change\n\t * this?\n\t */\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 4) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 4, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Regular extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize - 1) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 4095, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* The next 3 are split extents */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\t\"unexpected extent start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tdisk_bytenr = em->block_start;\n\torig_start = em->start;\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t orig_start, em->orig_start);\n\t\tgoto out;\n\t}\n\tdisk_bytenr += (em->start - orig_start);\n\tif (em->block_start != disk_bytenr) {\n\t\ttest_err(\"wrong block start, want %llu, have %llu\",\n\t\t\t disk_bytenr, em->block_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Prealloc extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != prealloc_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t prealloc_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* The next 3 are a half written prealloc extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != prealloc_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t prealloc_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tdisk_bytenr = em->block_start;\n\torig_start = em->start;\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"unexpected orig offset, wanted %llu, have %llu\",\n\t\t\t orig_start, em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\n\t\ttest_err(\"unexpected block start, wanted %llu, have %llu\",\n\t\t\t disk_bytenr + (em->start - em->orig_start),\n\t\t\t em->block_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != prealloc_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t prealloc_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", orig_start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\n\t\ttest_err(\"unexpected block start, wanted %llu, have %llu\",\n\t\t\t disk_bytenr + (em->start - em->orig_start),\n\t\t\t em->block_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Now for the compressed extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != compressed_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t compressed_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t em->start, em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\n\t\ttest_err(\"unexpected compress type, wanted %d, got %d\",\n\t\t\t BTRFS_COMPRESS_ZLIB, em->compress_type);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Split compressed extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != compressed_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t compressed_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t em->start, em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\n\t\ttest_err(\"unexpected compress type, wanted %d, got %d\",\n\t\t\t BTRFS_COMPRESS_ZLIB, em->compress_type);\n\t\tgoto out;\n\t}\n\tdisk_bytenr = em->block_start;\n\torig_start = em->start;\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != disk_bytenr) {\n\t\ttest_err(\"block start does not match, want %llu got %llu\",\n\t\t\t disk_bytenr, em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != compressed_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t compressed_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t em->start, orig_start);\n\t\tgoto out;\n\t}\n\tif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\n\t\ttest_err(\"unexpected compress type, wanted %d, got %d\",\n\t\t\t BTRFS_COMPRESS_ZLIB, em->compress_type);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* A hole between regular extents but no hole extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset + 6,\n\t\t\tsectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, SZ_4M, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\t/*\n\t * Currently we just return a length that we requested rather than the\n\t * length of the actual hole, if this changes we'll have to change this\n\t * test.\n\t */\n\tif (em->start != offset || em->len != 3 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 3 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != vacancy_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t vacancy_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tret = 0;\nout:\n\tif (!IS_ERR(em))\n\t\tfree_extent_map(em);\n\tiput(inode);\n\tbtrfs_free_dummy_root(root);\n\tbtrfs_free_dummy_fs_info(fs_info);\n\treturn ret;\n}",
      "code_after_change": "static noinline int test_btrfs_get_extent(u32 sectorsize, u32 nodesize)\n{\n\tstruct btrfs_fs_info *fs_info = NULL;\n\tstruct inode *inode = NULL;\n\tstruct btrfs_root *root = NULL;\n\tstruct extent_map *em = NULL;\n\tu64 orig_start;\n\tu64 disk_bytenr;\n\tu64 offset;\n\tint ret = -ENOMEM;\n\n\ttest_msg(\"running btrfs_get_extent tests\");\n\n\tinode = btrfs_new_test_inode();\n\tif (!inode) {\n\t\ttest_std_err(TEST_ALLOC_INODE);\n\t\treturn ret;\n\t}\n\n\tinode->i_mode = S_IFREG;\n\tBTRFS_I(inode)->location.type = BTRFS_INODE_ITEM_KEY;\n\tBTRFS_I(inode)->location.objectid = BTRFS_FIRST_FREE_OBJECTID;\n\tBTRFS_I(inode)->location.offset = 0;\n\n\tfs_info = btrfs_alloc_dummy_fs_info(nodesize, sectorsize);\n\tif (!fs_info) {\n\t\ttest_std_err(TEST_ALLOC_FS_INFO);\n\t\tgoto out;\n\t}\n\n\troot = btrfs_alloc_dummy_root(fs_info);\n\tif (IS_ERR(root)) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\n\troot->node = alloc_dummy_extent_buffer(fs_info, nodesize);\n\tif (!root->node) {\n\t\ttest_std_err(TEST_ALLOC_ROOT);\n\t\tgoto out;\n\t}\n\n\tbtrfs_set_header_nritems(root->node, 0);\n\tbtrfs_set_header_level(root->node, 0);\n\tret = -EINVAL;\n\n\t/* First with no extents */\n\tBTRFS_I(inode)->root = root;\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\tem = NULL;\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tfree_extent_map(em);\n\tbtrfs_drop_extent_cache(BTRFS_I(inode), 0, (u64)-1, 0);\n\n\t/*\n\t * All of the magic numbers are based on the mapping setup in\n\t * setup_file_extents, so if you change anything there you need to\n\t * update the comment and update the expected values below.\n\t */\n\tsetup_file_extents(root, sectorsize);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, (u64)-1, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != 0 || em->len != 5) {\n\t\ttest_err(\n\t\t\"unexpected extent wanted start 0 len 5, got start %llu len %llu\",\n\t\t\tem->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_INLINE) {\n\t\ttest_err(\"expected an inline, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\n\tif (em->start != offset || em->len != (sectorsize - 5)) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 1, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\t/*\n\t * We don't test anything else for inline since it doesn't get set\n\t * unless we have a page for it to write into.  Maybe we should change\n\t * this?\n\t */\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 4) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 4, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Regular extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize - 1) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len 4095, got start %llu len %llu\",\n\t\t\toffset, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* The next 3 are split extents */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\t\"unexpected extent start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tdisk_bytenr = em->block_start;\n\torig_start = em->start;\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t orig_start, em->orig_start);\n\t\tgoto out;\n\t}\n\tdisk_bytenr += (em->start - orig_start);\n\tif (em->block_start != disk_bytenr) {\n\t\ttest_err(\"wrong block start, want %llu, have %llu\",\n\t\t\t disk_bytenr, em->block_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Prealloc extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != prealloc_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t prealloc_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* The next 3 are a half written prealloc extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != prealloc_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t prealloc_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tdisk_bytenr = em->block_start;\n\torig_start = em->start;\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"unexpected orig offset, wanted %llu, have %llu\",\n\t\t\t orig_start, em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\n\t\ttest_err(\"unexpected block start, wanted %llu, have %llu\",\n\t\t\t disk_bytenr + (em->start - em->orig_start),\n\t\t\t em->block_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != prealloc_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t prealloc_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", orig_start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\n\t\ttest_err(\"unexpected block start, wanted %llu, have %llu\",\n\t\t\t disk_bytenr + (em->start - em->orig_start),\n\t\t\t em->block_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Now for the compressed extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != compressed_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t compressed_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t em->start, em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\n\t\ttest_err(\"unexpected compress type, wanted %d, got %d\",\n\t\t\t BTRFS_COMPRESS_ZLIB, em->compress_type);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* Split compressed extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != compressed_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t compressed_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t em->start, em->orig_start);\n\t\tgoto out;\n\t}\n\tif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\n\t\ttest_err(\"unexpected compress type, wanted %d, got %d\",\n\t\t\t BTRFS_COMPRESS_ZLIB, em->compress_type);\n\t\tgoto out;\n\t}\n\tdisk_bytenr = em->block_start;\n\torig_start = em->start;\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != disk_bytenr) {\n\t\ttest_err(\"block start does not match, want %llu got %llu\",\n\t\t\t disk_bytenr, em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != 2 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 2 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != compressed_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t compressed_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != orig_start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\",\n\t\t\t em->start, orig_start);\n\t\tgoto out;\n\t}\n\tif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\n\t\ttest_err(\"unexpected compress type, wanted %d, got %d\",\n\t\t\t BTRFS_COMPRESS_ZLIB, em->compress_type);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\t/* A hole between regular extents but no hole extent */\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset + 6,\n\t\t\tsectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, SZ_4M, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start != EXTENT_MAP_HOLE) {\n\t\ttest_err(\"expected a hole extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\t/*\n\t * Currently we just return a length that we requested rather than the\n\t * length of the actual hole, if this changes we'll have to change this\n\t * test.\n\t */\n\tif (em->start != offset || em->len != 3 * sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, 3 * sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != vacancy_only) {\n\t\ttest_err(\"unexpected flags set, want %lu have %lu\",\n\t\t\t vacancy_only, em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\toffset = em->start + em->len;\n\tfree_extent_map(em);\n\n\tem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\n\tif (IS_ERR(em)) {\n\t\ttest_err(\"got an error when we shouldn't have\");\n\t\tgoto out;\n\t}\n\tif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\n\t\ttest_err(\"expected a real extent, got %llu\", em->block_start);\n\t\tgoto out;\n\t}\n\tif (em->start != offset || em->len != sectorsize) {\n\t\ttest_err(\n\t\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\n\t\t\toffset, sectorsize, em->start, em->len);\n\t\tgoto out;\n\t}\n\tif (em->flags != 0) {\n\t\ttest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\n\t\tgoto out;\n\t}\n\tif (em->orig_start != em->start) {\n\t\ttest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\n\t\t\t em->orig_start);\n\t\tgoto out;\n\t}\n\tret = 0;\nout:\n\tif (!IS_ERR(em))\n\t\tfree_extent_map(em);\n\tiput(inode);\n\tbtrfs_free_dummy_root(root);\n\tbtrfs_free_dummy_fs_info(fs_info);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tinode->i_mode = S_IFREG;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Mishandling a value of 1 for the number of data stripes in the code.",
      "trigger_condition": "Mounting a crafted btrfs filesystem image and performing specific operations.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly handle the value of 1 for the number of data stripes, leading to a slab-out-of-bounds write access in the __btrfs_map_block function.",
      "id": 53,
      "code_after_change_normalized": "static noinline int FUN1(u32 VAR1, u32 VAR2)\n{\nstruct btrfs_fs_info *VAR3 = NULL;\nstruct VAR4 *VAR4 = NULL;\nstruct btrfs_root *VAR5 = NULL;\nstruct extent_map *VAR6 = NULL;\nu64 VAR7;\nu64 VAR8;\nu64 VAR9;\nint VAR10 = -VAR11;\nFUN2(\"STR\");\nVAR4 = FUN3();\nif (!VAR4) {\nFUN4(VAR12);\nreturn VAR10;\n}\nVAR4->VAR13 = VAR14;\nFUN5(VAR4)->VAR15.VAR16 = VAR17;\nFUN5(VAR4)->VAR15.VAR18 = VAR19;\nFUN5(VAR4)->VAR15.VAR9 = 0;\nVAR3 = FUN6(VAR2, VAR1);\nif (!VAR3) {\nFUN4(VAR20);\ngoto VAR21;\n}\nVAR5 = FUN7(VAR3);\nif (FUN8(VAR5)) {\nFUN4(VAR22);\ngoto VAR21;\n}\nVAR5->VAR23 = FUN9(VAR3, VAR2);\nif (!VAR5->VAR23) {\nFUN4(VAR22);\ngoto VAR21;\n}\nFUN10(VAR5->VAR23, 0);\nFUN11(VAR5->VAR23, 0);\nVAR10 = -VAR24;\nFUN5(VAR4)->VAR5 = VAR5;\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, 0, VAR1, 0);\nif (FUN8(VAR6)) {\nVAR6 = NULL;\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR26) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nFUN14(VAR6);\nFUN15(FUN5(VAR4), 0, (VAR27)-1, 0);\nFUN16(VAR5, VAR1);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, 0, (VAR27)-1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR26) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != 0 || VAR6->VAR29 != 5) {\nFUN13(\n\"STR\",\nVAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR31) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != (VAR1 - 5)) {\nFUN13(\n\"STR\",\nVAR9, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR26) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != 4) {\nFUN13(\n\"STR\",\nVAR9, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1 - 1) {\nFUN13(\n\"STR\",\nVAR9, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR8 = VAR6->VAR25;\nVAR7 = VAR6->VAR28;\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR26) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\",\nVAR7, VAR6->VAR7);\ngoto VAR21;\n}\nVAR8 += (VAR6->VAR28 - VAR7);\nif (VAR6->VAR25 != VAR8) {\nFUN13(\"STR\",\nVAR8, VAR6->VAR25);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR33) {\nFUN13(\"STR\",\nVAR33, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR33) {\nFUN13(\"STR\",\nVAR33, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR8 = VAR6->VAR25;\nVAR7 = VAR6->VAR28;\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR26) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\",\nVAR7, VAR6->VAR7);\ngoto VAR21;\n}\nif (VAR6->VAR25 != (VAR8 + (VAR6->VAR28 - VAR6->VAR7))) {\nFUN13(\"STR\",\nVAR8 + (VAR6->VAR28 - VAR6->VAR7),\nVAR6->VAR25);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR33) {\nFUN13(\"STR\",\nVAR33, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\", VAR7,\nVAR6->VAR7);\ngoto VAR21;\n}\nif (VAR6->VAR25 != (VAR8 + (VAR6->VAR28 - VAR6->VAR7))) {\nFUN13(\"STR\",\nVAR8 + (VAR6->VAR28 - VAR6->VAR7),\nVAR6->VAR25);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR34) {\nFUN13(\"STR\",\nVAR34, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\",\nVAR6->VAR28, VAR6->VAR7);\ngoto VAR21;\n}\nif (VAR6->VAR35 != VAR36) {\nFUN13(\"STR\",\nVAR36, VAR6->VAR35);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR34) {\nFUN13(\"STR\",\nVAR34, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\",\nVAR6->VAR28, VAR6->VAR7);\ngoto VAR21;\n}\nif (VAR6->VAR35 != VAR36) {\nFUN13(\"STR\",\nVAR36, VAR6->VAR35);\ngoto VAR21;\n}\nVAR8 = VAR6->VAR25;\nVAR7 = VAR6->VAR28;\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR8) {\nFUN13(\"STR\",\nVAR8, VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR34) {\nFUN13(\"STR\",\nVAR34, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\",\nVAR6->VAR28, VAR7);\ngoto VAR21;\n}\nif (VAR6->VAR35 != VAR36) {\nFUN13(\"STR\",\nVAR36, VAR6->VAR35);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9 + 6,\nVAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR37, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 != VAR26) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != 3 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 3 * VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != VAR38) {\nFUN13(\"STR\",\nVAR38, VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR9 = VAR6->VAR28 + VAR6->VAR29;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR21;\n}\nif (VAR6->VAR25 >= VAR32) {\nFUN13(\"STR\", VAR6->VAR25);\ngoto VAR21;\n}\nif (VAR6->VAR28 != VAR9 || VAR6->VAR29 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR28, VAR6->VAR29);\ngoto VAR21;\n}\nif (VAR6->VAR30 != 0) {\nFUN13(\"STR\", VAR6->VAR30);\ngoto VAR21;\n}\nif (VAR6->VAR7 != VAR6->VAR28) {\nFUN13(\"STR\", VAR6->VAR28,\nVAR6->VAR7);\ngoto VAR21;\n}\nVAR10 = 0;\nVAR21:\nif (!FUN8(VAR6))\nFUN14(VAR6);\nFUN17(VAR4);\nFUN18(VAR5);\nFUN19(VAR3);\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "static noinline int FUN1(u32 VAR1, u32 VAR2)\n{\nstruct btrfs_fs_info *VAR3 = NULL;\nstruct VAR4 *VAR4 = NULL;\nstruct btrfs_root *VAR5 = NULL;\nstruct extent_map *VAR6 = NULL;\nu64 VAR7;\nu64 VAR8;\nu64 VAR9;\nint VAR10 = -VAR11;\nFUN2(\"STR\");\nVAR4 = FUN3();\nif (!VAR4) {\nFUN4(VAR12);\nreturn VAR10;\n}\nFUN5(VAR4)->VAR13.VAR14 = VAR15;\nFUN5(VAR4)->VAR13.VAR16 = VAR17;\nFUN5(VAR4)->VAR13.VAR9 = 0;\nVAR3 = FUN6(VAR2, VAR1);\nif (!VAR3) {\nFUN4(VAR18);\ngoto VAR19;\n}\nVAR5 = FUN7(VAR3);\nif (FUN8(VAR5)) {\nFUN4(VAR20);\ngoto VAR19;\n}\nVAR5->VAR21 = FUN9(VAR3, VAR2);\nif (!VAR5->VAR21) {\nFUN4(VAR20);\ngoto VAR19;\n}\nFUN10(VAR5->VAR21, 0);\nFUN11(VAR5->VAR21, 0);\nVAR10 = -VAR22;\nFUN5(VAR4)->VAR5 = VAR5;\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, 0, VAR1, 0);\nif (FUN8(VAR6)) {\nVAR6 = NULL;\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR24) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nFUN14(VAR6);\nFUN15(FUN5(VAR4), 0, (VAR25)-1, 0);\nFUN16(VAR5, VAR1);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, 0, (VAR25)-1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR24) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != 0 || VAR6->VAR27 != 5) {\nFUN13(\n\"STR\",\nVAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR29) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != (VAR1 - 5)) {\nFUN13(\n\"STR\",\nVAR9, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR24) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != 4) {\nFUN13(\n\"STR\",\nVAR9, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1 - 1) {\nFUN13(\n\"STR\",\nVAR9, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR8 = VAR6->VAR23;\nVAR7 = VAR6->VAR26;\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR24) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\",\nVAR7, VAR6->VAR7);\ngoto VAR19;\n}\nVAR8 += (VAR6->VAR26 - VAR7);\nif (VAR6->VAR23 != VAR8) {\nFUN13(\"STR\",\nVAR8, VAR6->VAR23);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR31) {\nFUN13(\"STR\",\nVAR31, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR31) {\nFUN13(\"STR\",\nVAR31, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR8 = VAR6->VAR23;\nVAR7 = VAR6->VAR26;\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR24) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\",\nVAR7, VAR6->VAR7);\ngoto VAR19;\n}\nif (VAR6->VAR23 != (VAR8 + (VAR6->VAR26 - VAR6->VAR7))) {\nFUN13(\"STR\",\nVAR8 + (VAR6->VAR26 - VAR6->VAR7),\nVAR6->VAR23);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR31) {\nFUN13(\"STR\",\nVAR31, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\", VAR7,\nVAR6->VAR7);\ngoto VAR19;\n}\nif (VAR6->VAR23 != (VAR8 + (VAR6->VAR26 - VAR6->VAR7))) {\nFUN13(\"STR\",\nVAR8 + (VAR6->VAR26 - VAR6->VAR7),\nVAR6->VAR23);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR32) {\nFUN13(\"STR\",\nVAR32, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\",\nVAR6->VAR26, VAR6->VAR7);\ngoto VAR19;\n}\nif (VAR6->VAR33 != VAR34) {\nFUN13(\"STR\",\nVAR34, VAR6->VAR33);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR32) {\nFUN13(\"STR\",\nVAR32, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\",\nVAR6->VAR26, VAR6->VAR7);\ngoto VAR19;\n}\nif (VAR6->VAR33 != VAR34) {\nFUN13(\"STR\",\nVAR34, VAR6->VAR33);\ngoto VAR19;\n}\nVAR8 = VAR6->VAR23;\nVAR7 = VAR6->VAR26;\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR8) {\nFUN13(\"STR\",\nVAR8, VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != 2 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 2 * VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR32) {\nFUN13(\"STR\",\nVAR32, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR7) {\nFUN13(\"STR\",\nVAR6->VAR26, VAR7);\ngoto VAR19;\n}\nif (VAR6->VAR33 != VAR34) {\nFUN13(\"STR\",\nVAR34, VAR6->VAR33);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9 + 6,\nVAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR35, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 != VAR24) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != 3 * VAR1) {\nFUN13(\n\"STR\",\nVAR9, 3 * VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != VAR36) {\nFUN13(\"STR\",\nVAR36, VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR9 = VAR6->VAR26 + VAR6->VAR27;\nFUN14(VAR6);\nVAR6 = FUN12(FUN5(VAR4), NULL, 0, VAR9, VAR1, 0);\nif (FUN8(VAR6)) {\nFUN13(\"STR\");\ngoto VAR19;\n}\nif (VAR6->VAR23 >= VAR30) {\nFUN13(\"STR\", VAR6->VAR23);\ngoto VAR19;\n}\nif (VAR6->VAR26 != VAR9 || VAR6->VAR27 != VAR1) {\nFUN13(\n\"STR\",\nVAR9, VAR1, VAR6->VAR26, VAR6->VAR27);\ngoto VAR19;\n}\nif (VAR6->VAR28 != 0) {\nFUN13(\"STR\", VAR6->VAR28);\ngoto VAR19;\n}\nif (VAR6->VAR7 != VAR6->VAR26) {\nFUN13(\"STR\", VAR6->VAR26,\nVAR6->VAR7);\ngoto VAR19;\n}\nVAR10 = 0;\nVAR19:\nif (!FUN8(VAR6))\nFUN14(VAR6);\nFUN17(VAR4);\nFUN18(VAR5);\nFUN19(VAR3);\nreturn VAR10;\n}\n",
      "code_after_change_raw": "static noinline int test_btrfs_get_extent(u32 sectorsize, u32 nodesize)\n{\nstruct btrfs_fs_info *fs_info = NULL;\nstruct inode *inode = NULL;\nstruct btrfs_root *root = NULL;\nstruct extent_map *em = NULL;\nu64 orig_start;\nu64 disk_bytenr;\nu64 offset;\nint ret = -ENOMEM;\ntest_msg(\"running btrfs_get_extent tests\");\ninode = btrfs_new_test_inode();\nif (!inode) {\ntest_std_err(TEST_ALLOC_INODE);\nreturn ret;\n}\ninode->i_mode = S_IFREG;\nBTRFS_I(inode)->location.type = BTRFS_INODE_ITEM_KEY;\nBTRFS_I(inode)->location.objectid = BTRFS_FIRST_FREE_OBJECTID;\nBTRFS_I(inode)->location.offset = 0;\nfs_info = btrfs_alloc_dummy_fs_info(nodesize, sectorsize);\nif (!fs_info) {\ntest_std_err(TEST_ALLOC_FS_INFO);\ngoto out;\n}\nroot = btrfs_alloc_dummy_root(fs_info);\nif (IS_ERR(root)) {\ntest_std_err(TEST_ALLOC_ROOT);\ngoto out;\n}\nroot->node = alloc_dummy_extent_buffer(fs_info, nodesize);\nif (!root->node) {\ntest_std_err(TEST_ALLOC_ROOT);\ngoto out;\n}\nbtrfs_set_header_nritems(root->node, 0);\nbtrfs_set_header_level(root->node, 0);\nret = -EINVAL;\nBTRFS_I(inode)->root = root;\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, sectorsize, 0);\nif (IS_ERR(em)) {\nem = NULL;\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nfree_extent_map(em);\nbtrfs_drop_extent_cache(BTRFS_I(inode), 0, (u64)-1, 0);\nsetup_file_extents(root, sectorsize);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, (u64)-1, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != 0 || em->len != 5) {\ntest_err(\n\"unexpected extent wanted start 0 len 5, got start %llu len %llu\",\nem->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_INLINE) {\ntest_err(\"expected an inline, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != (sectorsize - 5)) {\ntest_err(\n\"unexpected extent wanted start %llu len 1, got start %llu len %llu\",\noffset, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 4) {\ntest_err(\n\"unexpected extent wanted start %llu len 4, got start %llu len %llu\",\noffset, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize - 1) {\ntest_err(\n\"unexpected extent wanted start %llu len 4095, got start %llu len %llu\",\noffset, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\ndisk_bytenr = em->block_start;\norig_start = em->start;\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\norig_start, em->orig_start);\ngoto out;\n}\ndisk_bytenr += (em->start - orig_start);\nif (em->block_start != disk_bytenr) {\ntest_err(\"wrong block start, want %llu, have %llu\",\ndisk_bytenr, em->block_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != prealloc_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nprealloc_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != prealloc_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nprealloc_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\ndisk_bytenr = em->block_start;\norig_start = em->start;\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_HOLE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"unexpected orig offset, wanted %llu, have %llu\",\norig_start, em->orig_start);\ngoto out;\n}\nif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\ntest_err(\"unexpected block start, wanted %llu, have %llu\",\ndisk_bytenr + (em->start - em->orig_start),\nem->block_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != prealloc_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nprealloc_only, em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", orig_start,\nem->orig_start);\ngoto out;\n}\nif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\ntest_err(\"unexpected block start, wanted %llu, have %llu\",\ndisk_bytenr + (em->start - em->orig_start),\nem->block_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != compressed_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\ncompressed_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\nem->start, em->orig_start);\ngoto out;\n}\nif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\ntest_err(\"unexpected compress type, wanted %d, got %d\",\nBTRFS_COMPRESS_ZLIB, em->compress_type);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != compressed_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\ncompressed_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\nem->start, em->orig_start);\ngoto out;\n}\nif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\ntest_err(\"unexpected compress type, wanted %d, got %d\",\nBTRFS_COMPRESS_ZLIB, em->compress_type);\ngoto out;\n}\ndisk_bytenr = em->block_start;\norig_start = em->start;\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != disk_bytenr) {\ntest_err(\"block start does not match, want %llu got %llu\",\ndisk_bytenr, em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != compressed_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\ncompressed_only, em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\nem->start, orig_start);\ngoto out;\n}\nif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\ntest_err(\"unexpected compress type, wanted %d, got %d\",\nBTRFS_COMPRESS_ZLIB, em->compress_type);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset + 6,\nsectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, SZ_4M, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 3 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 3 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != vacancy_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nvacancy_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\nret = 0;\nout:\nif (!IS_ERR(em))\nfree_extent_map(em);\niput(inode);\nbtrfs_free_dummy_root(root);\nbtrfs_free_dummy_fs_info(fs_info);\nreturn ret;\n}\n",
      "code_before_change_raw": "static noinline int test_btrfs_get_extent(u32 sectorsize, u32 nodesize)\n{\nstruct btrfs_fs_info *fs_info = NULL;\nstruct inode *inode = NULL;\nstruct btrfs_root *root = NULL;\nstruct extent_map *em = NULL;\nu64 orig_start;\nu64 disk_bytenr;\nu64 offset;\nint ret = -ENOMEM;\ntest_msg(\"running btrfs_get_extent tests\");\ninode = btrfs_new_test_inode();\nif (!inode) {\ntest_std_err(TEST_ALLOC_INODE);\nreturn ret;\n}\nBTRFS_I(inode)->location.type = BTRFS_INODE_ITEM_KEY;\nBTRFS_I(inode)->location.objectid = BTRFS_FIRST_FREE_OBJECTID;\nBTRFS_I(inode)->location.offset = 0;\nfs_info = btrfs_alloc_dummy_fs_info(nodesize, sectorsize);\nif (!fs_info) {\ntest_std_err(TEST_ALLOC_FS_INFO);\ngoto out;\n}\nroot = btrfs_alloc_dummy_root(fs_info);\nif (IS_ERR(root)) {\ntest_std_err(TEST_ALLOC_ROOT);\ngoto out;\n}\nroot->node = alloc_dummy_extent_buffer(fs_info, nodesize);\nif (!root->node) {\ntest_std_err(TEST_ALLOC_ROOT);\ngoto out;\n}\nbtrfs_set_header_nritems(root->node, 0);\nbtrfs_set_header_level(root->node, 0);\nret = -EINVAL;\nBTRFS_I(inode)->root = root;\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, sectorsize, 0);\nif (IS_ERR(em)) {\nem = NULL;\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nfree_extent_map(em);\nbtrfs_drop_extent_cache(BTRFS_I(inode), 0, (u64)-1, 0);\nsetup_file_extents(root, sectorsize);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, 0, (u64)-1, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != 0 || em->len != 5) {\ntest_err(\n\"unexpected extent wanted start 0 len 5, got start %llu len %llu\",\nem->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_INLINE) {\ntest_err(\"expected an inline, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != (sectorsize - 5)) {\ntest_err(\n\"unexpected extent wanted start %llu len 1, got start %llu len %llu\",\noffset, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 4) {\ntest_err(\n\"unexpected extent wanted start %llu len 4, got start %llu len %llu\",\noffset, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize - 1) {\ntest_err(\n\"unexpected extent wanted start %llu len 4095, got start %llu len %llu\",\noffset, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\ndisk_bytenr = em->block_start;\norig_start = em->start;\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\norig_start, em->orig_start);\ngoto out;\n}\ndisk_bytenr += (em->start - orig_start);\nif (em->block_start != disk_bytenr) {\ntest_err(\"wrong block start, want %llu, have %llu\",\ndisk_bytenr, em->block_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != prealloc_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nprealloc_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != prealloc_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nprealloc_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\ndisk_bytenr = em->block_start;\norig_start = em->start;\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_HOLE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"unexpected orig offset, wanted %llu, have %llu\",\norig_start, em->orig_start);\ngoto out;\n}\nif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\ntest_err(\"unexpected block start, wanted %llu, have %llu\",\ndisk_bytenr + (em->start - em->orig_start),\nem->block_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != prealloc_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nprealloc_only, em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", orig_start,\nem->orig_start);\ngoto out;\n}\nif (em->block_start != (disk_bytenr + (em->start - em->orig_start))) {\ntest_err(\"unexpected block start, wanted %llu, have %llu\",\ndisk_bytenr + (em->start - em->orig_start),\nem->block_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != compressed_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\ncompressed_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\nem->start, em->orig_start);\ngoto out;\n}\nif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\ntest_err(\"unexpected compress type, wanted %d, got %d\",\nBTRFS_COMPRESS_ZLIB, em->compress_type);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != compressed_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\ncompressed_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\nem->start, em->orig_start);\ngoto out;\n}\nif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\ntest_err(\"unexpected compress type, wanted %d, got %d\",\nBTRFS_COMPRESS_ZLIB, em->compress_type);\ngoto out;\n}\ndisk_bytenr = em->block_start;\norig_start = em->start;\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != disk_bytenr) {\ntest_err(\"block start does not match, want %llu got %llu\",\ndisk_bytenr, em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 2 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 2 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != compressed_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\ncompressed_only, em->flags);\ngoto out;\n}\nif (em->orig_start != orig_start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\",\nem->start, orig_start);\ngoto out;\n}\nif (em->compress_type != BTRFS_COMPRESS_ZLIB) {\ntest_err(\"unexpected compress type, wanted %d, got %d\",\nBTRFS_COMPRESS_ZLIB, em->compress_type);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset + 6,\nsectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, SZ_4M, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start != EXTENT_MAP_HOLE) {\ntest_err(\"expected a hole extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != 3 * sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, 3 * sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != vacancy_only) {\ntest_err(\"unexpected flags set, want %lu have %lu\",\nvacancy_only, em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\noffset = em->start + em->len;\nfree_extent_map(em);\nem = btrfs_get_extent(BTRFS_I(inode), NULL, 0, offset, sectorsize, 0);\nif (IS_ERR(em)) {\ntest_err(\"got an error when we shouldn't have\");\ngoto out;\n}\nif (em->block_start >= EXTENT_MAP_LAST_BYTE) {\ntest_err(\"expected a real extent, got %llu\", em->block_start);\ngoto out;\n}\nif (em->start != offset || em->len != sectorsize) {\ntest_err(\n\"unexpected extent wanted start %llu len %u, got start %llu len %llu\",\noffset, sectorsize, em->start, em->len);\ngoto out;\n}\nif (em->flags != 0) {\ntest_err(\"unexpected flags set, want 0 have %lu\", em->flags);\ngoto out;\n}\nif (em->orig_start != em->start) {\ntest_err(\"wrong orig offset, want %llu, have %llu\", em->start,\nem->orig_start);\ngoto out;\n}\nret = 0;\nout:\nif (!IS_ERR(em))\nfree_extent_map(em);\niput(inode);\nbtrfs_free_dummy_root(root);\nbtrfs_free_dummy_fs_info(fs_info);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2019-20636": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for the old keycode value retrieved during a keymap update process.",
        "trigger_condition": "A crafted keycode table is provided, leading to an out-of-bounds write when processing the old keycode value.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the old keycode value retrieved during a keymap update, allowing it to be used without ensuring it falls within the valid range. This can result in an out-of-bounds write if the old keycode value is not properly constrained."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to verify that the old keycode value falls within the valid range before further processing. By adding the validation step, the code ensures that the old keycode is within acceptable bounds, preventing potential out-of-bounds writes and enhancing the security of the keymap update process.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-20636, which involves out-of-bounds writes via a crafted keycode table in the Linux kernel before version 5.4.12. By adding the check `if (old_keycode > KEY_MAX)` before further processing the old keycode, the code ensures that the old keycode is within the valid range. If the old keycode is greater than `KEY_MAX`, a warning message is logged, preventing potential out-of-bounds writes and improving the overall security of the code.\n\nThis modification helps prevent the exploitation of the vulnerability by ensuring that the old keycode is properly validated before proceeding with any operations that could lead to out-of-bounds writes.",
      "GPT_purpose": "Set a keycode for a given input device and handle potential keymap changes.",
      "GPT_function": "\n1. Set a keycode for an input device.\n2. Check if the provided keycode is within the valid range.\n3. Update the keycode mapping for the input device.\n4. Ensure that KEY_RESERVED is not enabled.\n5. Simulate a keyup event if the keycode is not present in the keymap anymore.",
      "CVE_id": "CVE-2019-20636",
      "code_before_change": "int input_set_keycode(struct input_dev *dev,\n\t\t      const struct input_keymap_entry *ke)\n{\n\tunsigned long flags;\n\tunsigned int old_keycode;\n\tint retval;\n\n\tif (ke->keycode > KEY_MAX)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&dev->event_lock, flags);\n\n\tretval = dev->setkeycode(dev, ke, &old_keycode);\n\tif (retval)\n\t\tgoto out;\n\n\t/* Make sure KEY_RESERVED did not get enabled. */\n\t__clear_bit(KEY_RESERVED, dev->keybit);\n\n\t/*\n\t * Simulate keyup event if keycode is not present\n\t * in the keymap anymore\n\t */\n\tif (test_bit(EV_KEY, dev->evbit) &&\n\t    !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&\n\t    __test_and_clear_bit(old_keycode, dev->key)) {\n\t\tstruct input_value vals[] =  {\n\t\t\t{ EV_KEY, old_keycode, 0 },\n\t\t\tinput_value_sync\n\t\t};\n\n\t\tinput_pass_values(dev, vals, ARRAY_SIZE(vals));\n\t}\n\n out:\n\tspin_unlock_irqrestore(&dev->event_lock, flags);\n\n\treturn retval;\n}",
      "code_after_change": "int input_set_keycode(struct input_dev *dev,\n\t\t      const struct input_keymap_entry *ke)\n{\n\tunsigned long flags;\n\tunsigned int old_keycode;\n\tint retval;\n\n\tif (ke->keycode > KEY_MAX)\n\t\treturn -EINVAL;\n\n\tspin_lock_irqsave(&dev->event_lock, flags);\n\n\tretval = dev->setkeycode(dev, ke, &old_keycode);\n\tif (retval)\n\t\tgoto out;\n\n\t/* Make sure KEY_RESERVED did not get enabled. */\n\t__clear_bit(KEY_RESERVED, dev->keybit);\n\n\t/*\n\t * Simulate keyup event if keycode is not present\n\t * in the keymap anymore\n\t */\n\tif (old_keycode > KEY_MAX) {\n\t\tdev_warn(dev->dev.parent ?: &dev->dev,\n\t\t\t \"%s: got too big old keycode %#x\\n\",\n\t\t\t __func__, old_keycode);\n\t} else if (test_bit(EV_KEY, dev->evbit) &&\n\t\t   !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&\n\t\t   __test_and_clear_bit(old_keycode, dev->key)) {\n\t\tstruct input_value vals[] =  {\n\t\t\t{ EV_KEY, old_keycode, 0 },\n\t\t\tinput_value_sync\n\t\t};\n\n\t\tinput_pass_values(dev, vals, ARRAY_SIZE(vals));\n\t}\n\n out:\n\tspin_unlock_irqrestore(&dev->event_lock, flags);\n\n\treturn retval;\n}",
      "modified_lines": {
        "added": [
          "\tif (old_keycode > KEY_MAX) {",
          "\t\tdev_warn(dev->dev.parent ?: &dev->dev,",
          "\t\t\t \"%s: got too big old keycode %#x\\n\",",
          "\t\t\t __func__, old_keycode);",
          "\t} else if (test_bit(EV_KEY, dev->evbit) &&",
          "\t\t   !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&",
          "\t\t   __test_and_clear_bit(old_keycode, dev->key)) {"
        ],
        "deleted": [
          "\tif (test_bit(EV_KEY, dev->evbit) &&",
          "\t    !is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&",
          "\t    __test_and_clear_bit(old_keycode, dev->key)) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for the old keycode value retrieved during a keymap update process.",
      "trigger_condition": "A crafted keycode table is provided, leading to an out-of-bounds write when processing the old keycode value.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the old keycode value retrieved during a keymap update, allowing it to be used without ensuring it falls within the valid range. This can result in an out-of-bounds write if the old keycode value is not properly constrained.",
      "id": 54,
      "code_after_change_normalized": "int FUN1(struct input_dev *VAR1,\nconst struct input_keymap_entry *VAR2)\n{\nunsigned long VAR3;\nunsigned int VAR4;\nint VAR5;\nif (VAR2->VAR6 > VAR7)\nreturn -VAR8;\nFUN2(&VAR1->VAR9, VAR3);\nVAR5 = VAR1->FUN3(VAR1, VAR2, &VAR4);\nif (VAR5)\ngoto VAR10;\nFUN4(VAR11, VAR1->VAR12);\nif (VAR4 > VAR7) {\nFUN5(VAR1->VAR1.VAR13 ?: &VAR1->VAR1,\n\"STR\",\nVAR14, VAR4);\n} else if (FUN6(VAR15, VAR1->VAR16) &&\n!FUN7(VAR4, VAR1->VAR12, VAR7) &&\nFUN8(VAR4, VAR1->VAR17)) {\nstruct input_value VAR18[] =  {\n{ VAR15, VAR4, 0 },\nVAR19\n};\nFUN9(VAR1, VAR18, FUN10(VAR18));\n}\nVAR10:\nFUN11(&VAR1->VAR9, VAR3);\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "int FUN1(struct input_dev *VAR1,\nconst struct input_keymap_entry *VAR2)\n{\nunsigned long VAR3;\nunsigned int VAR4;\nint VAR5;\nif (VAR2->VAR6 > VAR7)\nreturn -VAR8;\nFUN2(&VAR1->VAR9, VAR3);\nVAR5 = VAR1->FUN3(VAR1, VAR2, &VAR4);\nif (VAR5)\ngoto VAR10;\nFUN4(VAR11, VAR1->VAR12);\nif (FUN5(VAR13, VAR1->VAR14) &&\n!FUN6(VAR4, VAR1->VAR12, VAR7) &&\nFUN7(VAR4, VAR1->VAR15)) {\nstruct input_value VAR16[] =  {\n{ VAR13, VAR4, 0 },\nVAR17\n};\nFUN8(VAR1, VAR16, FUN9(VAR16));\n}\nVAR10:\nFUN10(&VAR1->VAR9, VAR3);\nreturn VAR5;\n}\n",
      "code_after_change_raw": "int input_set_keycode(struct input_dev *dev,\nconst struct input_keymap_entry *ke)\n{\nunsigned long flags;\nunsigned int old_keycode;\nint retval;\nif (ke->keycode > KEY_MAX)\nreturn -EINVAL;\nspin_lock_irqsave(&dev->event_lock, flags);\nretval = dev->setkeycode(dev, ke, &old_keycode);\nif (retval)\ngoto out;\n__clear_bit(KEY_RESERVED, dev->keybit);\nif (old_keycode > KEY_MAX) {\ndev_warn(dev->dev.parent ?: &dev->dev,\n\"%s: got too big old keycode %#x\\n\",\n__func__, old_keycode);\n} else if (test_bit(EV_KEY, dev->evbit) &&\n!is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&\n__test_and_clear_bit(old_keycode, dev->key)) {\nstruct input_value vals[] =  {\n{ EV_KEY, old_keycode, 0 },\ninput_value_sync\n};\ninput_pass_values(dev, vals, ARRAY_SIZE(vals));\n}\nout:\nspin_unlock_irqrestore(&dev->event_lock, flags);\nreturn retval;\n}\n",
      "code_before_change_raw": "int input_set_keycode(struct input_dev *dev,\nconst struct input_keymap_entry *ke)\n{\nunsigned long flags;\nunsigned int old_keycode;\nint retval;\nif (ke->keycode > KEY_MAX)\nreturn -EINVAL;\nspin_lock_irqsave(&dev->event_lock, flags);\nretval = dev->setkeycode(dev, ke, &old_keycode);\nif (retval)\ngoto out;\n__clear_bit(KEY_RESERVED, dev->keybit);\nif (test_bit(EV_KEY, dev->evbit) &&\n!is_event_supported(old_keycode, dev->keybit, KEY_MAX) &&\n__test_and_clear_bit(old_keycode, dev->key)) {\nstruct input_value vals[] =  {\n{ EV_KEY, old_keycode, 0 },\ninput_value_sync\n};\ninput_pass_values(dev, vals, ARRAY_SIZE(vals));\n}\nout:\nspin_unlock_irqrestore(&dev->event_lock, flags);\nreturn retval;\n}\n"
    }
  ],
  "CVE-2019-2181": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for the size of extra_buffers_size after incrementing it, leading to a potential integer overflow.",
        "trigger_condition": "The code increments extra_buffers_size without checking for potential overflow, which can occur if the added size exceeds the maximum value that the variable can hold.",
        "specific_code_behavior_causing_vulnerability": "The code snippet does not include a check to ensure that the value of extra_buffers_size does not overflow after adding the size of secctx. This can lead to an integer overflow vulnerability, potentially resulting in security risks such as out-of-bounds writes or memory corruption.",
        "solution": "To mitigate the vulnerability, it is necessary to add a check after incrementing extra_buffers_size to verify that it does not overflow. By adding the check 'if (extra_buffers_size < added_size)' after incrementing extra_buffers_size, the code can prevent the integer overflow and associated security risks. This solution ensures that the code handles the size calculation safely and avoids potential vulnerabilities related to integer overflows."
      },
      "GPT_analysis": "The modification is necessary to prevent an integer overflow vulnerability in the code snippet. By adding the check `if (extra_buffers_size < added_size)` after incrementing `extra_buffers_size`, we ensure that the value of `extra_buffers_size` does not overflow. This check helps prevent potential security risks associated with integer overflows, such as out-of-bounds writes and other memory corruption issues. By adding this check, we can mitigate the vulnerability described in CVE-2019-2181 and improve the overall security of the code.",
      "GPT_purpose": "Handle binder transactions in the Android kernel, including processing incoming and outgoing transactions, managing transaction data, and interacting with target processes and nodes.",
      "GPT_function": "\n1. Handle binder transactions between processes.\n2. Manage transaction data and buffers.\n3. Perform security checks and validations.\n4. Enqueue and process transaction work.\n5. Handle errors and cleanup resources.",
      "CVE_id": "CVE-2019-2181",
      "code_before_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\textra_buffers_size += ALIGN(secctx_sz, sizeof(u64));\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t    t->buffer, buf_offset,\n\t\t\t\t\t    secctx, secctx_sz);\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size;\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tbinder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t      &object_offset,\n\t\t\t\t\t      t->buffer,\n\t\t\t\t\t      buffer_offset,\n\t\t\t\t\t      sizeof(object_offset));\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    fp, sizeof(*fp));\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    fp, sizeof(*fp));\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tfp->pad_binder = 0;\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    fp, sizeof(*fp));\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    bp, sizeof(*bp));\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
      "code_after_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t    t->buffer, buf_offset,\n\t\t\t\t\t    secctx, secctx_sz);\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size;\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tbinder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t      &object_offset,\n\t\t\t\t\t      t->buffer,\n\t\t\t\t\t      buffer_offset,\n\t\t\t\t\t      sizeof(object_offset));\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    fp, sizeof(*fp));\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    fp, sizeof(*fp));\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tfp->pad_binder = 0;\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    fp, sizeof(*fp));\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tbinder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t    t->buffer, object_offset,\n\t\t\t\t\t\t    bp, sizeof(*bp));\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
      "modified_lines": {
        "added": [
          "\t\tsize_t added_size;",
          "\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));",
          "\t\textra_buffers_size += added_size;",
          "\t\tif (extra_buffers_size < added_size) {",
          "\t\t\t/* integer overflow of extra_buffers_size */",
          "\t\t\treturn_error = BR_FAILED_REPLY;",
          "\t\t\treturn_error_param = EINVAL;",
          "\t\t\treturn_error_line = __LINE__;",
          "\t\t\tgoto err_bad_extra_size;",
          "\t\t}",
          "err_bad_extra_size:"
        ],
        "deleted": [
          "\t\textra_buffers_size += ALIGN(secctx_sz, sizeof(u64));"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for the size of extra_buffers_size after incrementing it, leading to a potential integer overflow.",
      "trigger_condition": "The code increments extra_buffers_size without checking for potential overflow, which can occur if the added size exceeds the maximum value that the variable can hold.",
      "specific_code_behavior_causing_vulnerability": "The code snippet does not include a check to ensure that the value of extra_buffers_size does not overflow after adding the size of secctx. This can lead to an integer overflow vulnerability, potentially resulting in security risks such as out-of-bounds writes or memory corruption.",
      "solution": "To mitigate the vulnerability, it is necessary to add a check after incrementing extra_buffers_size to verify that it does not overflow. By adding the check 'if (extra_buffers_size < added_size)' after incrementing extra_buffers_size, the code can prevent the integer overflow and associated security risks. This solution ensures that the code handles the size calculation safely and avoids potential vulnerabilities related to integer overflows.",
      "id": 55,
      "code_after_change_normalized": "static void FUN1(struct binder_proc *VAR1,\nstruct binder_thread *VAR2,\nstruct binder_transaction_data *VAR3, int VAR4,\nbinder_size_t VAR5)\n{\nint VAR6;\nstruct binder_transaction *VAR7;\nstruct binder_work *VAR8;\nstruct binder_work *VAR9;\nbinder_size_t VAR10 = 0;\nbinder_size_t VAR11, VAR12;\nbinder_size_t VAR13;\nbinder_size_t VAR14, VAR15;\nstruct binder_proc *VAR16 = NULL;\nstruct binder_thread *VAR17 = NULL;\nstruct binder_node *VAR18 = NULL;\nstruct binder_transaction *VAR19 = NULL;\nstruct binder_transaction_log_entry *VAR20;\nuint32_t VAR21 = 0;\nuint32_t VAR22 = 0;\nuint32_t VAR23 = 0;\nbinder_size_t VAR24 = 0;\nbinder_size_t VAR25 = 0;\nstruct binder_context *VAR26 = VAR1->VAR26;\nint VAR27 = FUN2(&VAR28);\nchar *VAR29 = NULL;\nu32 VAR30 = 0;\nVAR20 = FUN3(&VAR31);\nVAR20->VAR32 = VAR27;\nVAR20->VAR33 = VAR4 ? 2 : !!(VAR3->VAR34 & VAR35);\nVAR20->VAR36 = VAR1->VAR37;\nVAR20->VAR38 = VAR2->VAR37;\nVAR20->VAR39 = VAR3->VAR40.VAR41;\nVAR20->VAR42 = VAR3->VAR42;\nVAR20->VAR43 = VAR3->VAR43;\nVAR20->VAR44 = VAR1->VAR26->VAR45;\nif (VAR4) {\nFUN4(VAR1);\nVAR19 = VAR2->VAR46;\nif (VAR19 == NULL) {\nFUN5(VAR1);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR50;\n}\nif (VAR19->VAR51 != VAR2) {\nFUN7(&VAR19->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR19->VAR32,\nVAR19->VAR53 ?\nVAR19->VAR53->VAR37 : 0,\nVAR19->VAR51 ?\nVAR19->VAR51->VAR37 : 0);\nFUN8(&VAR19->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\ngoto VAR54;\n}\nVAR2->VAR46 = VAR19->VAR55;\nFUN5(VAR1);\nFUN9(VAR19->VAR56);\nVAR17 = FUN10(VAR19);\nif (VAR17 == NULL) {\nFUN11(&VAR17->VAR1->VAR57);\nVAR21 = VAR58;\nVAR23 = VAR49;\ngoto VAR59;\n}\nif (VAR17->VAR46 != VAR19) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\nVAR17->VAR46 ?\nVAR17->VAR46->VAR32 : 0,\nVAR19->VAR32);\nFUN5(VAR17->VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\nVAR17 = NULL;\ngoto VAR59;\n}\nVAR16 = VAR17->VAR1;\nVAR16->VAR60++;\nFUN5(VAR17->VAR1);\n} else {\nif (VAR3->VAR40.VAR41) {\nstruct binder_ref *VAR61;\nFUN12(VAR1);\nVAR61 = FUN13(VAR1, VAR3->VAR40.VAR41,\ntrue);\nif (VAR61) {\nVAR18 = FUN14(\nVAR61->VAR62, &VAR16,\n&VAR21);\n} else {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\n}\nFUN15(VAR1);\n} else {\nFUN16(&VAR26->VAR63);\nVAR18 = VAR26->VAR64;\nif (VAR18)\nVAR18 = FUN14(\nVAR18, &VAR16,\n&VAR21);\nelse\nVAR21 = VAR58;\nFUN17(&VAR26->VAR63);\nif (VAR18 && VAR16 == VAR1) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR66;\n}\n}\nif (!VAR18) {\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR59;\n}\nVAR20->VAR67 = VAR18->VAR32;\nif (FUN18(VAR1->VAR68,\nVAR16->VAR68) < 0) {\nVAR21 = VAR47;\nVAR22 = -VAR69;\nVAR23 = VAR49;\ngoto VAR66;\n}\nFUN4(VAR1);\nVAR8 = FUN19(&VAR2->VAR70,\nstruct VAR71, VAR72);\nif (!(VAR3->VAR34 & VAR35) && VAR8 &&\nVAR8->VAR73 == VAR74) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR75;\n}\nif (!(VAR3->VAR34 & VAR35) && VAR2->VAR46) {\nstruct binder_transaction *VAR76;\nVAR76 = VAR2->VAR46;\nif (VAR76->VAR51 != VAR2) {\nFUN7(&VAR76->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR76->VAR32,\nVAR76->VAR53 ? VAR76->VAR53->VAR37 : 0,\nVAR76->VAR51 ?\nVAR76->VAR51->VAR37 : 0);\nFUN8(&VAR76->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR54;\n}\nwhile (VAR76) {\nstruct binder_thread *VAR77;\nFUN7(&VAR76->VAR52);\nVAR77 = VAR76->VAR77;\nif (VAR77 && VAR77->VAR1 == VAR16) {\nFUN20(&VAR77->VAR60);\nVAR17 = VAR77;\nFUN8(&VAR76->VAR52);\nbreak;\n}\nFUN8(&VAR76->VAR52);\nVAR76 = VAR76->VAR78;\n}\n}\nFUN5(VAR1);\n}\nif (VAR17)\nVAR20->VAR51 = VAR17->VAR37;\nVAR20->VAR53 = VAR16->VAR37;\nVAR7 = FUN21(sizeof(*VAR7), VAR79);\nif (VAR7 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR81;\n}\nFUN22(&VAR7->VAR82);\nFUN23(VAR83);\nFUN24(&VAR7->VAR52);\nVAR9 = FUN21(sizeof(*VAR9), VAR79);\nif (VAR9 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR84;\n}\nFUN23(VAR85);\nVAR7->VAR32 = VAR27;\nif (VAR4)\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR17->VAR37,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nelse\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR18->VAR32,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nif (!VAR4 && !(VAR3->VAR34 & VAR35))\nVAR7->VAR77 = VAR2;\nelse\nVAR7->VAR77 = NULL;\nVAR7->VAR92 = FUN26(VAR1->VAR68);\nVAR7->VAR53 = VAR16;\nVAR7->VAR51 = VAR17;\nVAR7->VAR93 = VAR3->VAR93;\nVAR7->VAR34 = VAR3->VAR34;\nVAR7->VAR94 = FUN27(VAR95);\nif (VAR18 && VAR18->VAR96) {\nu32 VAR97;\nsize_t VAR98;\nFUN28(VAR1->VAR68, &VAR97);\nVAR6 = FUN29(VAR97, &VAR29, &VAR30);\nif (VAR6) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR99;\n}\nVAR98 = FUN30(VAR30, sizeof(VAR87));\nVAR5 += VAR98;\nif (VAR5 < VAR98) {\nVAR21 = VAR47;\nVAR22 = VAR65;\nVAR23 = VAR49;\ngoto VAR100;\n}\n}\nFUN31(VAR4, VAR7, VAR18);\nVAR7->VAR90 = FUN32(&VAR16->VAR101, VAR3->VAR42,\nVAR3->VAR43, VAR5,\n!VAR4 && (VAR7->VAR34 & VAR35));\nif (FUN33(VAR7->VAR90)) {\nVAR22 = FUN34(VAR7->VAR90);\nVAR21 = VAR22 == -VAR102 ?\nVAR58 : VAR47;\nVAR23 = VAR49;\nVAR7->VAR90 = NULL;\ngoto VAR103;\n}\nif (VAR29) {\nsize_t VAR104 = FUN30(VAR3->VAR42, sizeof(void *)) +\nFUN30(VAR3->VAR43, sizeof(void *)) +\nFUN30(VAR5, sizeof(void *)) -\nFUN30(VAR30, sizeof(VAR87));\nVAR7->VAR105 = (VAR106)VAR7->VAR90->VAR107 + VAR104;\nFUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR104,\nVAR29, VAR30);\nFUN36(VAR29, VAR30);\nVAR29 = NULL;\n}\nVAR7->VAR90->VAR32 = VAR7->VAR32;\nVAR7->VAR90->VAR108 = VAR7;\nVAR7->VAR90->VAR18 = VAR18;\nFUN37(VAR7->VAR90);\nif (FUN38(\n&VAR16->VAR101,\nVAR7->VAR90, 0,\n(const void VAR109 *)\n(VAR106)VAR3->VAR88.VAR89.VAR90,\nVAR3->VAR42)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR110;\nVAR23 = VAR49;\ngoto VAR111;\n}\nif (FUN38(\n&VAR16->VAR101,\nVAR7->VAR90,\nFUN30(VAR3->VAR42, sizeof(void *)),\n(const void VAR109 *)\n(VAR106)VAR3->VAR88.VAR89.VAR91,\nVAR3->VAR43)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR110;\nVAR23 = VAR49;\ngoto VAR111;\n}\nif (!FUN39(VAR3->VAR43, sizeof(VAR112))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, (VAR87)VAR3->VAR43);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR113;\n}\nif (!FUN39(VAR5, sizeof(VAR87))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR5);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR113;\n}\nVAR11 = FUN30(VAR3->VAR42, sizeof(void *));\nVAR10 = VAR11;\nVAR12 = VAR11 + VAR3->VAR43;\nVAR14 = FUN30(VAR12, sizeof(void *));\nVAR15 = VAR14 + VAR5;\nVAR13 = 0;\nfor (VAR10 = VAR11; VAR10 < VAR12;\nVAR10 += sizeof(VAR112)) {\nstruct binder_object_header *VAR114;\nsize_t VAR115;\nstruct binder_object VAR116;\nbinder_size_t VAR117;\nFUN40(&VAR16->VAR101,\n&VAR117,\nVAR7->VAR90,\nVAR10,\nsizeof(VAR117));\nVAR115 = FUN41(VAR16, VAR7->VAR90,\nVAR117, &VAR116);\nif (VAR115 == 0 || VAR117 < VAR13) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR117,\n(VAR87)VAR13,\n(VAR87)VAR7->VAR90->VAR42);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR113;\n}\nVAR114 = &VAR116.VAR114;\nVAR13 = VAR117 + VAR115;\nswitch (VAR114->VAR73) {\ncase VAR118:\ncase VAR119: {\nstruct flat_binder_object *VAR120;\nVAR120 = FUN42(VAR114);\nVAR6 = FUN43(VAR120, VAR7, VAR2);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR121;\n}\nFUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR117,\nVAR120, sizeof(*VAR120));\n} break;\ncase VAR122:\ncase VAR123: {\nstruct flat_binder_object *VAR120;\nVAR120 = FUN42(VAR114);\nVAR6 = FUN44(VAR120, VAR7, VAR2);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR121;\n}\nFUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR117,\nVAR120, sizeof(*VAR120));\n} break;\ncase VAR124: {\nstruct binder_fd_object *VAR120 = FUN45(VAR114);\nbinder_size_t VAR125 = VAR117 +\n(VAR106)&VAR120->VAR126 - (VAR106)VAR120;\nint VAR6 = FUN46(VAR120->VAR126, VAR125, VAR7,\nVAR2, VAR19);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR121;\n}\nVAR120->VAR127 = 0;\nFUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR117,\nVAR120, sizeof(*VAR120));\n} break;\ncase VAR128: {\nstruct binder_object VAR129;\nbinder_size_t VAR130;\nstruct binder_fd_array_object *VAR131 =\nFUN47(VAR114);\nsize_t VAR132 = (VAR10 - VAR11) *\nsizeof(VAR112);\nstruct binder_buffer_object *VAR133 =\nFUN48(VAR16, VAR7->VAR90,\n&VAR129, VAR131->VAR133,\nVAR11,\n&VAR130,\nVAR132);\nif (!VAR133) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR134;\n}\nif (!FUN49(VAR16, VAR7->VAR90,\nVAR11,\nVAR130,\nVAR131->VAR130,\nVAR24,\nVAR25)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR134;\n}\nVAR6 = FUN50(VAR131, VAR133, VAR7, VAR2,\nVAR19);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR121;\n}\nVAR24 = VAR130;\nVAR25 =\nVAR131->VAR130 + sizeof(VAR135) * VAR131->VAR136;\n} break;\ncase VAR137: {\nstruct binder_buffer_object *VAR138 =\nFUN51(VAR114);\nsize_t VAR139 = VAR15 - VAR14;\nsize_t VAR132;\nif (VAR138->VAR140 > VAR139) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR113;\n}\nif (FUN38(\n&VAR16->VAR101,\nVAR7->VAR90,\nVAR14,\n(const void VAR109 *)\n(VAR106)VAR138->VAR90,\nVAR138->VAR140)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR22 = -VAR110;\nVAR21 = VAR47;\nVAR23 = VAR49;\ngoto VAR111;\n}\nVAR138->VAR90 = (VAR106)\nVAR7->VAR90->VAR107 + VAR14;\nVAR14 += FUN30(VAR138->VAR140, sizeof(VAR87));\nVAR132 = (VAR10 - VAR11) *\nsizeof(VAR112);\nVAR6 = FUN52(VAR7, VAR2, VAR138,\nVAR11,\nVAR132,\nVAR24,\nVAR25);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR121;\n}\nFUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR117,\nVAR138, sizeof(*VAR138));\nVAR24 = VAR117;\nVAR25 = 0;\n} break;\ndefault:\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR114->VAR73);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR141;\n}\n}\nVAR9->VAR73 = VAR142;\nVAR7->VAR143.VAR73 = VAR74;\nif (VAR4) {\nFUN53(VAR2, VAR9);\nFUN4(VAR16);\nif (VAR17->VAR144) {\nFUN5(VAR16);\ngoto VAR145;\n}\nFUN54(VAR7->VAR90->VAR146 != 0);\nFUN55(VAR17, VAR19);\nFUN56(VAR17, &VAR7->VAR143);\nFUN5(VAR16);\nFUN57(&VAR17->VAR147);\nFUN58(VAR19);\n} else if (!(VAR7->VAR34 & VAR35)) {\nFUN54(VAR7->VAR90->VAR146 != 0);\nFUN4(VAR1);\nFUN59(VAR2, VAR9);\nVAR7->VAR148 = 1;\nVAR7->VAR78 = VAR2->VAR46;\nVAR2->VAR46 = VAR7;\nFUN5(VAR1);\nif (!FUN60(VAR7, VAR16, VAR17)) {\nFUN4(VAR1);\nFUN55(VAR2, VAR7);\nFUN5(VAR1);\ngoto VAR145;\n}\n} else {\nFUN54(VAR18 == NULL);\nFUN54(VAR7->VAR90->VAR146 != 1);\nFUN53(VAR2, VAR9);\nif (!FUN60(VAR7, VAR16, NULL))\ngoto VAR145;\n}\nif (VAR17)\nFUN61(VAR17);\nFUN62(VAR16);\nif (VAR18)\nFUN63(VAR18);\nFUN64();\nFUN65(VAR20->VAR149, VAR27);\nreturn;\nVAR145:\nVAR21 = VAR58;\nVAR23 = VAR49;\nFUN66(VAR1, VAR9);\nVAR121:\nVAR141:\nVAR113:\nVAR134:\nVAR111:\nFUN67(VAR7);\nFUN68(VAR7->VAR90);\nFUN69(VAR16, VAR7->VAR90,\nVAR10, true);\nif (VAR18)\nFUN63(VAR18);\nVAR18 = NULL;\nVAR7->VAR90->VAR108 = NULL;\nFUN70(&VAR16->VAR101, VAR7->VAR90);\nVAR103:\nVAR100:\nif (VAR29)\nFUN36(VAR29, VAR30);\nVAR99:\nFUN71(VAR9);\nFUN72(VAR85);\nVAR84:\nFUN71(VAR7);\nFUN72(VAR83);\nVAR81:\nVAR75:\nVAR54:\nVAR50:\nVAR59:\nVAR66:\nif (VAR17)\nFUN61(VAR17);\nif (VAR16)\nFUN62(VAR16);\nif (VAR18) {\nFUN73(VAR18, 1, 0);\nFUN63(VAR18);\n}\nFUN25(VAR150,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR21, VAR22,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\nVAR23);\n{\nstruct binder_transaction_log_entry *VAR151;\nVAR20->VAR21 = VAR21;\nVAR20->VAR22 = VAR22;\nVAR20->VAR23 = VAR23;\nVAR151 = FUN3(&VAR152);\n*VAR151 = *VAR20;\nFUN64();\nFUN65(VAR20->VAR149, VAR27);\nFUN65(VAR151->VAR149, VAR27);\n}\nFUN54(VAR2->VAR21.VAR153 != VAR154);\nif (VAR19) {\nVAR2->VAR21.VAR153 = VAR155;\nFUN53(VAR2, &VAR2->VAR21.VAR143);\nFUN74(VAR19, VAR21);\n} else {\nVAR2->VAR21.VAR153 = VAR21;\nFUN53(VAR2, &VAR2->VAR21.VAR143);\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct binder_proc *VAR1,\nstruct binder_thread *VAR2,\nstruct binder_transaction_data *VAR3, int VAR4,\nbinder_size_t VAR5)\n{\nint VAR6;\nstruct binder_transaction *VAR7;\nstruct binder_work *VAR8;\nstruct binder_work *VAR9;\nbinder_size_t VAR10 = 0;\nbinder_size_t VAR11, VAR12;\nbinder_size_t VAR13;\nbinder_size_t VAR14, VAR15;\nstruct binder_proc *VAR16 = NULL;\nstruct binder_thread *VAR17 = NULL;\nstruct binder_node *VAR18 = NULL;\nstruct binder_transaction *VAR19 = NULL;\nstruct binder_transaction_log_entry *VAR20;\nuint32_t VAR21 = 0;\nuint32_t VAR22 = 0;\nuint32_t VAR23 = 0;\nbinder_size_t VAR24 = 0;\nbinder_size_t VAR25 = 0;\nstruct binder_context *VAR26 = VAR1->VAR26;\nint VAR27 = FUN2(&VAR28);\nchar *VAR29 = NULL;\nu32 VAR30 = 0;\nVAR20 = FUN3(&VAR31);\nVAR20->VAR32 = VAR27;\nVAR20->VAR33 = VAR4 ? 2 : !!(VAR3->VAR34 & VAR35);\nVAR20->VAR36 = VAR1->VAR37;\nVAR20->VAR38 = VAR2->VAR37;\nVAR20->VAR39 = VAR3->VAR40.VAR41;\nVAR20->VAR42 = VAR3->VAR42;\nVAR20->VAR43 = VAR3->VAR43;\nVAR20->VAR44 = VAR1->VAR26->VAR45;\nif (VAR4) {\nFUN4(VAR1);\nVAR19 = VAR2->VAR46;\nif (VAR19 == NULL) {\nFUN5(VAR1);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR50;\n}\nif (VAR19->VAR51 != VAR2) {\nFUN7(&VAR19->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR19->VAR32,\nVAR19->VAR53 ?\nVAR19->VAR53->VAR37 : 0,\nVAR19->VAR51 ?\nVAR19->VAR51->VAR37 : 0);\nFUN8(&VAR19->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\ngoto VAR54;\n}\nVAR2->VAR46 = VAR19->VAR55;\nFUN5(VAR1);\nFUN9(VAR19->VAR56);\nVAR17 = FUN10(VAR19);\nif (VAR17 == NULL) {\nFUN11(&VAR17->VAR1->VAR57);\nVAR21 = VAR58;\nVAR23 = VAR49;\ngoto VAR59;\n}\nif (VAR17->VAR46 != VAR19) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\nVAR17->VAR46 ?\nVAR17->VAR46->VAR32 : 0,\nVAR19->VAR32);\nFUN5(VAR17->VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\nVAR17 = NULL;\ngoto VAR59;\n}\nVAR16 = VAR17->VAR1;\nVAR16->VAR60++;\nFUN5(VAR17->VAR1);\n} else {\nif (VAR3->VAR40.VAR41) {\nstruct binder_ref *VAR61;\nFUN12(VAR1);\nVAR61 = FUN13(VAR1, VAR3->VAR40.VAR41,\ntrue);\nif (VAR61) {\nVAR18 = FUN14(\nVAR61->VAR62, &VAR16,\n&VAR21);\n} else {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\n}\nFUN15(VAR1);\n} else {\nFUN16(&VAR26->VAR63);\nVAR18 = VAR26->VAR64;\nif (VAR18)\nVAR18 = FUN14(\nVAR18, &VAR16,\n&VAR21);\nelse\nVAR21 = VAR58;\nFUN17(&VAR26->VAR63);\nif (VAR18 && VAR16 == VAR1) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR66;\n}\n}\nif (!VAR18) {\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR59;\n}\nVAR20->VAR67 = VAR18->VAR32;\nif (FUN18(VAR1->VAR68,\nVAR16->VAR68) < 0) {\nVAR21 = VAR47;\nVAR22 = -VAR69;\nVAR23 = VAR49;\ngoto VAR66;\n}\nFUN4(VAR1);\nVAR8 = FUN19(&VAR2->VAR70,\nstruct VAR71, VAR72);\nif (!(VAR3->VAR34 & VAR35) && VAR8 &&\nVAR8->VAR73 == VAR74) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR75;\n}\nif (!(VAR3->VAR34 & VAR35) && VAR2->VAR46) {\nstruct binder_transaction *VAR76;\nVAR76 = VAR2->VAR46;\nif (VAR76->VAR51 != VAR2) {\nFUN7(&VAR76->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR76->VAR32,\nVAR76->VAR53 ? VAR76->VAR53->VAR37 : 0,\nVAR76->VAR51 ?\nVAR76->VAR51->VAR37 : 0);\nFUN8(&VAR76->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR54;\n}\nwhile (VAR76) {\nstruct binder_thread *VAR77;\nFUN7(&VAR76->VAR52);\nVAR77 = VAR76->VAR77;\nif (VAR77 && VAR77->VAR1 == VAR16) {\nFUN20(&VAR77->VAR60);\nVAR17 = VAR77;\nFUN8(&VAR76->VAR52);\nbreak;\n}\nFUN8(&VAR76->VAR52);\nVAR76 = VAR76->VAR78;\n}\n}\nFUN5(VAR1);\n}\nif (VAR17)\nVAR20->VAR51 = VAR17->VAR37;\nVAR20->VAR53 = VAR16->VAR37;\nVAR7 = FUN21(sizeof(*VAR7), VAR79);\nif (VAR7 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR81;\n}\nFUN22(&VAR7->VAR82);\nFUN23(VAR83);\nFUN24(&VAR7->VAR52);\nVAR9 = FUN21(sizeof(*VAR9), VAR79);\nif (VAR9 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR84;\n}\nFUN23(VAR85);\nVAR7->VAR32 = VAR27;\nif (VAR4)\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR17->VAR37,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nelse\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR18->VAR32,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nif (!VAR4 && !(VAR3->VAR34 & VAR35))\nVAR7->VAR77 = VAR2;\nelse\nVAR7->VAR77 = NULL;\nVAR7->VAR92 = FUN26(VAR1->VAR68);\nVAR7->VAR53 = VAR16;\nVAR7->VAR51 = VAR17;\nVAR7->VAR93 = VAR3->VAR93;\nVAR7->VAR34 = VAR3->VAR34;\nVAR7->VAR94 = FUN27(VAR95);\nif (VAR18 && VAR18->VAR96) {\nu32 VAR97;\nFUN28(VAR1->VAR68, &VAR97);\nVAR6 = FUN29(VAR97, &VAR29, &VAR30);\nif (VAR6) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR98;\n}\nVAR5 += FUN30(VAR30, sizeof(VAR87));\n}\nFUN31(VAR4, VAR7, VAR18);\nVAR7->VAR90 = FUN32(&VAR16->VAR99, VAR3->VAR42,\nVAR3->VAR43, VAR5,\n!VAR4 && (VAR7->VAR34 & VAR35));\nif (FUN33(VAR7->VAR90)) {\nVAR22 = FUN34(VAR7->VAR90);\nVAR21 = VAR22 == -VAR100 ?\nVAR58 : VAR47;\nVAR23 = VAR49;\nVAR7->VAR90 = NULL;\ngoto VAR101;\n}\nif (VAR29) {\nsize_t VAR102 = FUN30(VAR3->VAR42, sizeof(void *)) +\nFUN30(VAR3->VAR43, sizeof(void *)) +\nFUN30(VAR5, sizeof(void *)) -\nFUN30(VAR30, sizeof(VAR87));\nVAR7->VAR103 = (VAR104)VAR7->VAR90->VAR105 + VAR102;\nFUN35(&VAR16->VAR99,\nVAR7->VAR90, VAR102,\nVAR29, VAR30);\nFUN36(VAR29, VAR30);\nVAR29 = NULL;\n}\nVAR7->VAR90->VAR32 = VAR7->VAR32;\nVAR7->VAR90->VAR106 = VAR7;\nVAR7->VAR90->VAR18 = VAR18;\nFUN37(VAR7->VAR90);\nif (FUN38(\n&VAR16->VAR99,\nVAR7->VAR90, 0,\n(const void VAR107 *)\n(VAR104)VAR3->VAR88.VAR89.VAR90,\nVAR3->VAR42)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR108;\nVAR23 = VAR49;\ngoto VAR109;\n}\nif (FUN38(\n&VAR16->VAR99,\nVAR7->VAR90,\nFUN30(VAR3->VAR42, sizeof(void *)),\n(const void VAR107 *)\n(VAR104)VAR3->VAR88.VAR89.VAR91,\nVAR3->VAR43)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR108;\nVAR23 = VAR49;\ngoto VAR109;\n}\nif (!FUN39(VAR3->VAR43, sizeof(VAR110))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, (VAR87)VAR3->VAR43);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR111;\n}\nif (!FUN39(VAR5, sizeof(VAR87))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR5);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR111;\n}\nVAR11 = FUN30(VAR3->VAR42, sizeof(void *));\nVAR10 = VAR11;\nVAR12 = VAR11 + VAR3->VAR43;\nVAR14 = FUN30(VAR12, sizeof(void *));\nVAR15 = VAR14 + VAR5;\nVAR13 = 0;\nfor (VAR10 = VAR11; VAR10 < VAR12;\nVAR10 += sizeof(VAR110)) {\nstruct binder_object_header *VAR112;\nsize_t VAR113;\nstruct binder_object VAR114;\nbinder_size_t VAR115;\nFUN40(&VAR16->VAR99,\n&VAR115,\nVAR7->VAR90,\nVAR10,\nsizeof(VAR115));\nVAR113 = FUN41(VAR16, VAR7->VAR90,\nVAR115, &VAR114);\nif (VAR113 == 0 || VAR115 < VAR13) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR115,\n(VAR87)VAR13,\n(VAR87)VAR7->VAR90->VAR42);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR111;\n}\nVAR112 = &VAR114.VAR112;\nVAR13 = VAR115 + VAR113;\nswitch (VAR112->VAR73) {\ncase VAR116:\ncase VAR117: {\nstruct flat_binder_object *VAR118;\nVAR118 = FUN42(VAR112);\nVAR6 = FUN43(VAR118, VAR7, VAR2);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR119;\n}\nFUN35(&VAR16->VAR99,\nVAR7->VAR90, VAR115,\nVAR118, sizeof(*VAR118));\n} break;\ncase VAR120:\ncase VAR121: {\nstruct flat_binder_object *VAR118;\nVAR118 = FUN42(VAR112);\nVAR6 = FUN44(VAR118, VAR7, VAR2);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR119;\n}\nFUN35(&VAR16->VAR99,\nVAR7->VAR90, VAR115,\nVAR118, sizeof(*VAR118));\n} break;\ncase VAR122: {\nstruct binder_fd_object *VAR118 = FUN45(VAR112);\nbinder_size_t VAR123 = VAR115 +\n(VAR104)&VAR118->VAR124 - (VAR104)VAR118;\nint VAR6 = FUN46(VAR118->VAR124, VAR123, VAR7,\nVAR2, VAR19);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR119;\n}\nVAR118->VAR125 = 0;\nFUN35(&VAR16->VAR99,\nVAR7->VAR90, VAR115,\nVAR118, sizeof(*VAR118));\n} break;\ncase VAR126: {\nstruct binder_object VAR127;\nbinder_size_t VAR128;\nstruct binder_fd_array_object *VAR129 =\nFUN47(VAR112);\nsize_t VAR130 = (VAR10 - VAR11) *\nsizeof(VAR110);\nstruct binder_buffer_object *VAR131 =\nFUN48(VAR16, VAR7->VAR90,\n&VAR127, VAR129->VAR131,\nVAR11,\n&VAR128,\nVAR130);\nif (!VAR131) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR132;\n}\nif (!FUN49(VAR16, VAR7->VAR90,\nVAR11,\nVAR128,\nVAR129->VAR128,\nVAR24,\nVAR25)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR132;\n}\nVAR6 = FUN50(VAR129, VAR131, VAR7, VAR2,\nVAR19);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR119;\n}\nVAR24 = VAR128;\nVAR25 =\nVAR129->VAR128 + sizeof(VAR133) * VAR129->VAR134;\n} break;\ncase VAR135: {\nstruct binder_buffer_object *VAR136 =\nFUN51(VAR112);\nsize_t VAR137 = VAR15 - VAR14;\nsize_t VAR130;\nif (VAR136->VAR138 > VAR137) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR111;\n}\nif (FUN38(\n&VAR16->VAR99,\nVAR7->VAR90,\nVAR14,\n(const void VAR107 *)\n(VAR104)VAR136->VAR90,\nVAR136->VAR138)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR22 = -VAR108;\nVAR21 = VAR47;\nVAR23 = VAR49;\ngoto VAR109;\n}\nVAR136->VAR90 = (VAR104)\nVAR7->VAR90->VAR105 + VAR14;\nVAR14 += FUN30(VAR136->VAR138, sizeof(VAR87));\nVAR130 = (VAR10 - VAR11) *\nsizeof(VAR110);\nVAR6 = FUN52(VAR7, VAR2, VAR136,\nVAR11,\nVAR130,\nVAR24,\nVAR25);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR119;\n}\nFUN35(&VAR16->VAR99,\nVAR7->VAR90, VAR115,\nVAR136, sizeof(*VAR136));\nVAR24 = VAR115;\nVAR25 = 0;\n} break;\ndefault:\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR112->VAR73);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR139;\n}\n}\nVAR9->VAR73 = VAR140;\nVAR7->VAR141.VAR73 = VAR74;\nif (VAR4) {\nFUN53(VAR2, VAR9);\nFUN4(VAR16);\nif (VAR17->VAR142) {\nFUN5(VAR16);\ngoto VAR143;\n}\nFUN54(VAR7->VAR90->VAR144 != 0);\nFUN55(VAR17, VAR19);\nFUN56(VAR17, &VAR7->VAR141);\nFUN5(VAR16);\nFUN57(&VAR17->VAR145);\nFUN58(VAR19);\n} else if (!(VAR7->VAR34 & VAR35)) {\nFUN54(VAR7->VAR90->VAR144 != 0);\nFUN4(VAR1);\nFUN59(VAR2, VAR9);\nVAR7->VAR146 = 1;\nVAR7->VAR78 = VAR2->VAR46;\nVAR2->VAR46 = VAR7;\nFUN5(VAR1);\nif (!FUN60(VAR7, VAR16, VAR17)) {\nFUN4(VAR1);\nFUN55(VAR2, VAR7);\nFUN5(VAR1);\ngoto VAR143;\n}\n} else {\nFUN54(VAR18 == NULL);\nFUN54(VAR7->VAR90->VAR144 != 1);\nFUN53(VAR2, VAR9);\nif (!FUN60(VAR7, VAR16, NULL))\ngoto VAR143;\n}\nif (VAR17)\nFUN61(VAR17);\nFUN62(VAR16);\nif (VAR18)\nFUN63(VAR18);\nFUN64();\nFUN65(VAR20->VAR147, VAR27);\nreturn;\nVAR143:\nVAR21 = VAR58;\nVAR23 = VAR49;\nFUN66(VAR1, VAR9);\nVAR119:\nVAR139:\nVAR111:\nVAR132:\nVAR109:\nFUN67(VAR7);\nFUN68(VAR7->VAR90);\nFUN69(VAR16, VAR7->VAR90,\nVAR10, true);\nif (VAR18)\nFUN63(VAR18);\nVAR18 = NULL;\nVAR7->VAR90->VAR106 = NULL;\nFUN70(&VAR16->VAR99, VAR7->VAR90);\nVAR101:\nif (VAR29)\nFUN36(VAR29, VAR30);\nVAR98:\nFUN71(VAR9);\nFUN72(VAR85);\nVAR84:\nFUN71(VAR7);\nFUN72(VAR83);\nVAR81:\nVAR75:\nVAR54:\nVAR50:\nVAR59:\nVAR66:\nif (VAR17)\nFUN61(VAR17);\nif (VAR16)\nFUN62(VAR16);\nif (VAR18) {\nFUN73(VAR18, 1, 0);\nFUN63(VAR18);\n}\nFUN25(VAR148,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR21, VAR22,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\nVAR23);\n{\nstruct binder_transaction_log_entry *VAR149;\nVAR20->VAR21 = VAR21;\nVAR20->VAR22 = VAR22;\nVAR20->VAR23 = VAR23;\nVAR149 = FUN3(&VAR150);\n*VAR149 = *VAR20;\nFUN64();\nFUN65(VAR20->VAR147, VAR27);\nFUN65(VAR149->VAR147, VAR27);\n}\nFUN54(VAR2->VAR21.VAR151 != VAR152);\nif (VAR19) {\nVAR2->VAR21.VAR151 = VAR153;\nFUN53(VAR2, &VAR2->VAR21.VAR141);\nFUN74(VAR19, VAR21);\n} else {\nVAR2->VAR21.VAR151 = VAR21;\nFUN53(VAR2, &VAR2->VAR21.VAR141);\n}\n}\n",
      "code_after_change_raw": "static void binder_transaction(struct binder_proc *proc,\nstruct binder_thread *thread,\nstruct binder_transaction_data *tr, int reply,\nbinder_size_t extra_buffers_size)\n{\nint ret;\nstruct binder_transaction *t;\nstruct binder_work *w;\nstruct binder_work *tcomplete;\nbinder_size_t buffer_offset = 0;\nbinder_size_t off_start_offset, off_end_offset;\nbinder_size_t off_min;\nbinder_size_t sg_buf_offset, sg_buf_end_offset;\nstruct binder_proc *target_proc = NULL;\nstruct binder_thread *target_thread = NULL;\nstruct binder_node *target_node = NULL;\nstruct binder_transaction *in_reply_to = NULL;\nstruct binder_transaction_log_entry *e;\nuint32_t return_error = 0;\nuint32_t return_error_param = 0;\nuint32_t return_error_line = 0;\nbinder_size_t last_fixup_obj_off = 0;\nbinder_size_t last_fixup_min_off = 0;\nstruct binder_context *context = proc->context;\nint t_debug_id = atomic_inc_return(&binder_last_id);\nchar *secctx = NULL;\nu32 secctx_sz = 0;\ne = binder_transaction_log_add(&binder_transaction_log);\ne->debug_id = t_debug_id;\ne->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\ne->from_proc = proc->pid;\ne->from_thread = thread->pid;\ne->target_handle = tr->target.handle;\ne->data_size = tr->data_size;\ne->offsets_size = tr->offsets_size;\ne->context_name = proc->context->name;\nif (reply) {\nbinder_inner_proc_lock(proc);\nin_reply_to = thread->transaction_stack;\nif (in_reply_to == NULL) {\nbinder_inner_proc_unlock(proc);\nbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_empty_call_stack;\n}\nif (in_reply_to->to_thread != thread) {\nspin_lock(&in_reply_to->lock);\nbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, in_reply_to->debug_id,\nin_reply_to->to_proc ?\nin_reply_to->to_proc->pid : 0,\nin_reply_to->to_thread ?\nin_reply_to->to_thread->pid : 0);\nspin_unlock(&in_reply_to->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ngoto err_bad_call_stack;\n}\nthread->transaction_stack = in_reply_to->to_parent;\nbinder_inner_proc_unlock(proc);\nbinder_set_nice(in_reply_to->saved_priority);\ntarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\nif (target_thread == NULL) {\n__release(&target_thread->proc->inner_lock);\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\nif (target_thread->transaction_stack != in_reply_to) {\nbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\nproc->pid, thread->pid,\ntarget_thread->transaction_stack ?\ntarget_thread->transaction_stack->debug_id : 0,\nin_reply_to->debug_id);\nbinder_inner_proc_unlock(target_thread->proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ntarget_thread = NULL;\ngoto err_dead_binder;\n}\ntarget_proc = target_thread->proc;\ntarget_proc->tmp_ref++;\nbinder_inner_proc_unlock(target_thread->proc);\n} else {\nif (tr->target.handle) {\nstruct binder_ref *ref;\nbinder_proc_lock(proc);\nref = binder_get_ref_olocked(proc, tr->target.handle,\ntrue);\nif (ref) {\ntarget_node = binder_get_node_refs_for_txn(\nref->node, &target_proc,\n&return_error);\n} else {\nbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\n}\nbinder_proc_unlock(proc);\n} else {\nmutex_lock(&context->context_mgr_node_lock);\ntarget_node = context->binder_context_mgr_node;\nif (target_node)\ntarget_node = binder_get_node_refs_for_txn(\ntarget_node, &target_proc,\n&return_error);\nelse\nreturn_error = BR_DEAD_REPLY;\nmutex_unlock(&context->context_mgr_node_lock);\nif (target_node && target_proc == proc) {\nbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\n}\nif (!target_node) {\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\ne->to_node = target_node->debug_id;\nif (security_binder_transaction(proc->tsk,\ntarget_proc->tsk) < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPERM;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\nbinder_inner_proc_lock(proc);\nw = list_first_entry_or_null(&thread->todo,\nstruct binder_work, entry);\nif (!(tr->flags & TF_ONE_WAY) && w &&\nw->type == BINDER_WORK_TRANSACTION) {\nbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\nproc->pid, thread->pid);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_todo_list;\n}\nif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\nstruct binder_transaction *tmp;\ntmp = thread->transaction_stack;\nif (tmp->to_thread != thread) {\nspin_lock(&tmp->lock);\nbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, tmp->debug_id,\ntmp->to_proc ? tmp->to_proc->pid : 0,\ntmp->to_thread ?\ntmp->to_thread->pid : 0);\nspin_unlock(&tmp->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_call_stack;\n}\nwhile (tmp) {\nstruct binder_thread *from;\nspin_lock(&tmp->lock);\nfrom = tmp->from;\nif (from && from->proc == target_proc) {\natomic_inc(&from->tmp_ref);\ntarget_thread = from;\nspin_unlock(&tmp->lock);\nbreak;\n}\nspin_unlock(&tmp->lock);\ntmp = tmp->from_parent;\n}\n}\nbinder_inner_proc_unlock(proc);\n}\nif (target_thread)\ne->to_thread = target_thread->pid;\ne->to_proc = target_proc->pid;\nt = kzalloc(sizeof(*t), GFP_KERNEL);\nif (t == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_t_failed;\n}\nINIT_LIST_HEAD(&t->fd_fixups);\nbinder_stats_created(BINDER_STAT_TRANSACTION);\nspin_lock_init(&t->lock);\ntcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\nif (tcomplete == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_tcomplete_failed;\n}\nbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\nt->debug_id = t_debug_id;\nif (reply)\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_thread->pid,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nelse\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_node->debug_id,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nif (!reply && !(tr->flags & TF_ONE_WAY))\nt->from = thread;\nelse\nt->from = NULL;\nt->sender_euid = task_euid(proc->tsk);\nt->to_proc = target_proc;\nt->to_thread = target_thread;\nt->code = tr->code;\nt->flags = tr->flags;\nt->priority = task_nice(current);\nif (target_node && target_node->txn_security_ctx) {\nu32 secid;\nsize_t added_size;\nsecurity_task_getsecid(proc->tsk, &secid);\nret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\nif (ret) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_get_secctx_failed;\n}\nadded_size = ALIGN(secctx_sz, sizeof(u64));\nextra_buffers_size += added_size;\nif (extra_buffers_size < added_size) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_extra_size;\n}\n}\ntrace_binder_transaction(reply, t, target_node);\nt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\ntr->offsets_size, extra_buffers_size,\n!reply && (t->flags & TF_ONE_WAY));\nif (IS_ERR(t->buffer)) {\nreturn_error_param = PTR_ERR(t->buffer);\nreturn_error = return_error_param == -ESRCH ?\nBR_DEAD_REPLY : BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\nt->buffer = NULL;\ngoto err_binder_alloc_buf_failed;\n}\nif (secctx) {\nsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\nALIGN(tr->offsets_size, sizeof(void *)) +\nALIGN(extra_buffers_size, sizeof(void *)) -\nALIGN(secctx_sz, sizeof(u64));\nt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, buf_offset,\nsecctx, secctx_sz);\nsecurity_release_secctx(secctx, secctx_sz);\nsecctx = NULL;\n}\nt->buffer->debug_id = t->debug_id;\nt->buffer->transaction = t;\nt->buffer->target_node = target_node;\ntrace_binder_transaction_alloc_buf(t->buffer);\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer, 0,\n(const void __user *)\n(uintptr_t)tr->data.ptr.buffer,\ntr->data_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nALIGN(tr->data_size, sizeof(void *)),\n(const void __user *)\n(uintptr_t)tr->data.ptr.offsets,\ntr->offsets_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\nproc->pid, thread->pid, (u64)tr->offsets_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\nbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\nproc->pid, thread->pid,\n(u64)extra_buffers_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\noff_start_offset = ALIGN(tr->data_size, sizeof(void *));\nbuffer_offset = off_start_offset;\noff_end_offset = off_start_offset + tr->offsets_size;\nsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\nsg_buf_end_offset = sg_buf_offset + extra_buffers_size;\noff_min = 0;\nfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\nbuffer_offset += sizeof(binder_size_t)) {\nstruct binder_object_header *hdr;\nsize_t object_size;\nstruct binder_object object;\nbinder_size_t object_offset;\nbinder_alloc_copy_from_buffer(&target_proc->alloc,\n&object_offset,\nt->buffer,\nbuffer_offset,\nsizeof(object_offset));\nobject_size = binder_get_object(target_proc, t->buffer,\nobject_offset, &object);\nif (object_size == 0 || object_offset < off_min) {\nbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\nproc->pid, thread->pid,\n(u64)object_offset,\n(u64)off_min,\n(u64)t->buffer->data_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nhdr = &object.hdr;\noff_min = object_offset + object_size;\nswitch (hdr->type) {\ncase BINDER_TYPE_BINDER:\ncase BINDER_TYPE_WEAK_BINDER: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_binder(fp, t, thread);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nfp, sizeof(*fp));\n} break;\ncase BINDER_TYPE_HANDLE:\ncase BINDER_TYPE_WEAK_HANDLE: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_handle(fp, t, thread);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nfp, sizeof(*fp));\n} break;\ncase BINDER_TYPE_FD: {\nstruct binder_fd_object *fp = to_binder_fd_object(hdr);\nbinder_size_t fd_offset = object_offset +\n(uintptr_t)&fp->fd - (uintptr_t)fp;\nint ret = binder_translate_fd(fp->fd, fd_offset, t,\nthread, in_reply_to);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nfp->pad_binder = 0;\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nfp, sizeof(*fp));\n} break;\ncase BINDER_TYPE_FDA: {\nstruct binder_object ptr_object;\nbinder_size_t parent_offset;\nstruct binder_fd_array_object *fda =\nto_binder_fd_array_object(hdr);\nsize_t num_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nstruct binder_buffer_object *parent =\nbinder_validate_ptr(target_proc, t->buffer,\n&ptr_object, fda->parent,\noff_start_offset,\n&parent_offset,\nnum_valid);\nif (!parent) {\nbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nif (!binder_validate_fixup(target_proc, t->buffer,\noff_start_offset,\nparent_offset,\nfda->parent_offset,\nlast_fixup_obj_off,\nlast_fixup_min_off)) {\nbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nret = binder_translate_fd_array(fda, parent, t, thread,\nin_reply_to);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nlast_fixup_obj_off = parent_offset;\nlast_fixup_min_off =\nfda->parent_offset + sizeof(u32) * fda->num_fds;\n} break;\ncase BINDER_TYPE_PTR: {\nstruct binder_buffer_object *bp =\nto_binder_buffer_object(hdr);\nsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\nsize_t num_valid;\nif (bp->length > buf_left) {\nbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nsg_buf_offset,\n(const void __user *)\n(uintptr_t)bp->buffer,\nbp->length)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error_param = -EFAULT;\nreturn_error = BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nbp->buffer = (uintptr_t)\nt->buffer->user_data + sg_buf_offset;\nsg_buf_offset += ALIGN(bp->length, sizeof(u64));\nnum_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nret = binder_fixup_parent(t, thread, bp,\noff_start_offset,\nnum_valid,\nlast_fixup_obj_off,\nlast_fixup_min_off);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nbp, sizeof(*bp));\nlast_fixup_obj_off = object_offset;\nlast_fixup_min_off = 0;\n} break;\ndefault:\nbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\nproc->pid, thread->pid, hdr->type);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_object_type;\n}\n}\ntcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\nt->work.type = BINDER_WORK_TRANSACTION;\nif (reply) {\nbinder_enqueue_thread_work(thread, tcomplete);\nbinder_inner_proc_lock(target_proc);\nif (target_thread->is_dead) {\nbinder_inner_proc_unlock(target_proc);\ngoto err_dead_proc_or_thread;\n}\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_pop_transaction_ilocked(target_thread, in_reply_to);\nbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\nbinder_inner_proc_unlock(target_proc);\nwake_up_interruptible_sync(&target_thread->wait);\nbinder_free_transaction(in_reply_to);\n} else if (!(t->flags & TF_ONE_WAY)) {\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_inner_proc_lock(proc);\nbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\nt->need_reply = 1;\nt->from_parent = thread->transaction_stack;\nthread->transaction_stack = t;\nbinder_inner_proc_unlock(proc);\nif (!binder_proc_transaction(t, target_proc, target_thread)) {\nbinder_inner_proc_lock(proc);\nbinder_pop_transaction_ilocked(thread, t);\nbinder_inner_proc_unlock(proc);\ngoto err_dead_proc_or_thread;\n}\n} else {\nBUG_ON(target_node == NULL);\nBUG_ON(t->buffer->async_transaction != 1);\nbinder_enqueue_thread_work(thread, tcomplete);\nif (!binder_proc_transaction(t, target_proc, NULL))\ngoto err_dead_proc_or_thread;\n}\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nbinder_proc_dec_tmpref(target_proc);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nreturn;\nerr_dead_proc_or_thread:\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\nbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\nbinder_free_txn_fixups(t);\ntrace_binder_transaction_failed_buffer_release(t->buffer);\nbinder_transaction_buffer_release(target_proc, t->buffer,\nbuffer_offset, true);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\ntarget_node = NULL;\nt->buffer->transaction = NULL;\nbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\nif (secctx)\nsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\nkfree(tcomplete);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\nkfree(t);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nif (target_proc)\nbinder_proc_dec_tmpref(target_proc);\nif (target_node) {\nbinder_dec_node(target_node, 1, 0);\nbinder_dec_node_tmpref(target_node);\n}\nbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\nproc->pid, thread->pid, return_error, return_error_param,\n(u64)tr->data_size, (u64)tr->offsets_size,\nreturn_error_line);\n{\nstruct binder_transaction_log_entry *fe;\ne->return_error = return_error;\ne->return_error_param = return_error_param;\ne->return_error_line = return_error_line;\nfe = binder_transaction_log_add(&binder_transaction_log_failed);\n*fe = *e;\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nWRITE_ONCE(fe->debug_id_done, t_debug_id);\n}\nBUG_ON(thread->return_error.cmd != BR_OK);\nif (in_reply_to) {\nthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\nbinder_send_failed_reply(in_reply_to, return_error);\n} else {\nthread->return_error.cmd = return_error;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\n}\n}\n",
      "code_before_change_raw": "static void binder_transaction(struct binder_proc *proc,\nstruct binder_thread *thread,\nstruct binder_transaction_data *tr, int reply,\nbinder_size_t extra_buffers_size)\n{\nint ret;\nstruct binder_transaction *t;\nstruct binder_work *w;\nstruct binder_work *tcomplete;\nbinder_size_t buffer_offset = 0;\nbinder_size_t off_start_offset, off_end_offset;\nbinder_size_t off_min;\nbinder_size_t sg_buf_offset, sg_buf_end_offset;\nstruct binder_proc *target_proc = NULL;\nstruct binder_thread *target_thread = NULL;\nstruct binder_node *target_node = NULL;\nstruct binder_transaction *in_reply_to = NULL;\nstruct binder_transaction_log_entry *e;\nuint32_t return_error = 0;\nuint32_t return_error_param = 0;\nuint32_t return_error_line = 0;\nbinder_size_t last_fixup_obj_off = 0;\nbinder_size_t last_fixup_min_off = 0;\nstruct binder_context *context = proc->context;\nint t_debug_id = atomic_inc_return(&binder_last_id);\nchar *secctx = NULL;\nu32 secctx_sz = 0;\ne = binder_transaction_log_add(&binder_transaction_log);\ne->debug_id = t_debug_id;\ne->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\ne->from_proc = proc->pid;\ne->from_thread = thread->pid;\ne->target_handle = tr->target.handle;\ne->data_size = tr->data_size;\ne->offsets_size = tr->offsets_size;\ne->context_name = proc->context->name;\nif (reply) {\nbinder_inner_proc_lock(proc);\nin_reply_to = thread->transaction_stack;\nif (in_reply_to == NULL) {\nbinder_inner_proc_unlock(proc);\nbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_empty_call_stack;\n}\nif (in_reply_to->to_thread != thread) {\nspin_lock(&in_reply_to->lock);\nbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, in_reply_to->debug_id,\nin_reply_to->to_proc ?\nin_reply_to->to_proc->pid : 0,\nin_reply_to->to_thread ?\nin_reply_to->to_thread->pid : 0);\nspin_unlock(&in_reply_to->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ngoto err_bad_call_stack;\n}\nthread->transaction_stack = in_reply_to->to_parent;\nbinder_inner_proc_unlock(proc);\nbinder_set_nice(in_reply_to->saved_priority);\ntarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\nif (target_thread == NULL) {\n__release(&target_thread->proc->inner_lock);\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\nif (target_thread->transaction_stack != in_reply_to) {\nbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\nproc->pid, thread->pid,\ntarget_thread->transaction_stack ?\ntarget_thread->transaction_stack->debug_id : 0,\nin_reply_to->debug_id);\nbinder_inner_proc_unlock(target_thread->proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ntarget_thread = NULL;\ngoto err_dead_binder;\n}\ntarget_proc = target_thread->proc;\ntarget_proc->tmp_ref++;\nbinder_inner_proc_unlock(target_thread->proc);\n} else {\nif (tr->target.handle) {\nstruct binder_ref *ref;\nbinder_proc_lock(proc);\nref = binder_get_ref_olocked(proc, tr->target.handle,\ntrue);\nif (ref) {\ntarget_node = binder_get_node_refs_for_txn(\nref->node, &target_proc,\n&return_error);\n} else {\nbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\n}\nbinder_proc_unlock(proc);\n} else {\nmutex_lock(&context->context_mgr_node_lock);\ntarget_node = context->binder_context_mgr_node;\nif (target_node)\ntarget_node = binder_get_node_refs_for_txn(\ntarget_node, &target_proc,\n&return_error);\nelse\nreturn_error = BR_DEAD_REPLY;\nmutex_unlock(&context->context_mgr_node_lock);\nif (target_node && target_proc == proc) {\nbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\n}\nif (!target_node) {\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\ne->to_node = target_node->debug_id;\nif (security_binder_transaction(proc->tsk,\ntarget_proc->tsk) < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPERM;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\nbinder_inner_proc_lock(proc);\nw = list_first_entry_or_null(&thread->todo,\nstruct binder_work, entry);\nif (!(tr->flags & TF_ONE_WAY) && w &&\nw->type == BINDER_WORK_TRANSACTION) {\nbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\nproc->pid, thread->pid);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_todo_list;\n}\nif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\nstruct binder_transaction *tmp;\ntmp = thread->transaction_stack;\nif (tmp->to_thread != thread) {\nspin_lock(&tmp->lock);\nbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, tmp->debug_id,\ntmp->to_proc ? tmp->to_proc->pid : 0,\ntmp->to_thread ?\ntmp->to_thread->pid : 0);\nspin_unlock(&tmp->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_call_stack;\n}\nwhile (tmp) {\nstruct binder_thread *from;\nspin_lock(&tmp->lock);\nfrom = tmp->from;\nif (from && from->proc == target_proc) {\natomic_inc(&from->tmp_ref);\ntarget_thread = from;\nspin_unlock(&tmp->lock);\nbreak;\n}\nspin_unlock(&tmp->lock);\ntmp = tmp->from_parent;\n}\n}\nbinder_inner_proc_unlock(proc);\n}\nif (target_thread)\ne->to_thread = target_thread->pid;\ne->to_proc = target_proc->pid;\nt = kzalloc(sizeof(*t), GFP_KERNEL);\nif (t == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_t_failed;\n}\nINIT_LIST_HEAD(&t->fd_fixups);\nbinder_stats_created(BINDER_STAT_TRANSACTION);\nspin_lock_init(&t->lock);\ntcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\nif (tcomplete == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_tcomplete_failed;\n}\nbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\nt->debug_id = t_debug_id;\nif (reply)\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_thread->pid,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nelse\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_node->debug_id,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nif (!reply && !(tr->flags & TF_ONE_WAY))\nt->from = thread;\nelse\nt->from = NULL;\nt->sender_euid = task_euid(proc->tsk);\nt->to_proc = target_proc;\nt->to_thread = target_thread;\nt->code = tr->code;\nt->flags = tr->flags;\nt->priority = task_nice(current);\nif (target_node && target_node->txn_security_ctx) {\nu32 secid;\nsecurity_task_getsecid(proc->tsk, &secid);\nret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\nif (ret) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_get_secctx_failed;\n}\nextra_buffers_size += ALIGN(secctx_sz, sizeof(u64));\n}\ntrace_binder_transaction(reply, t, target_node);\nt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\ntr->offsets_size, extra_buffers_size,\n!reply && (t->flags & TF_ONE_WAY));\nif (IS_ERR(t->buffer)) {\nreturn_error_param = PTR_ERR(t->buffer);\nreturn_error = return_error_param == -ESRCH ?\nBR_DEAD_REPLY : BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\nt->buffer = NULL;\ngoto err_binder_alloc_buf_failed;\n}\nif (secctx) {\nsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\nALIGN(tr->offsets_size, sizeof(void *)) +\nALIGN(extra_buffers_size, sizeof(void *)) -\nALIGN(secctx_sz, sizeof(u64));\nt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, buf_offset,\nsecctx, secctx_sz);\nsecurity_release_secctx(secctx, secctx_sz);\nsecctx = NULL;\n}\nt->buffer->debug_id = t->debug_id;\nt->buffer->transaction = t;\nt->buffer->target_node = target_node;\ntrace_binder_transaction_alloc_buf(t->buffer);\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer, 0,\n(const void __user *)\n(uintptr_t)tr->data.ptr.buffer,\ntr->data_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nALIGN(tr->data_size, sizeof(void *)),\n(const void __user *)\n(uintptr_t)tr->data.ptr.offsets,\ntr->offsets_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\nproc->pid, thread->pid, (u64)tr->offsets_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\nbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\nproc->pid, thread->pid,\n(u64)extra_buffers_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\noff_start_offset = ALIGN(tr->data_size, sizeof(void *));\nbuffer_offset = off_start_offset;\noff_end_offset = off_start_offset + tr->offsets_size;\nsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\nsg_buf_end_offset = sg_buf_offset + extra_buffers_size;\noff_min = 0;\nfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\nbuffer_offset += sizeof(binder_size_t)) {\nstruct binder_object_header *hdr;\nsize_t object_size;\nstruct binder_object object;\nbinder_size_t object_offset;\nbinder_alloc_copy_from_buffer(&target_proc->alloc,\n&object_offset,\nt->buffer,\nbuffer_offset,\nsizeof(object_offset));\nobject_size = binder_get_object(target_proc, t->buffer,\nobject_offset, &object);\nif (object_size == 0 || object_offset < off_min) {\nbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\nproc->pid, thread->pid,\n(u64)object_offset,\n(u64)off_min,\n(u64)t->buffer->data_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nhdr = &object.hdr;\noff_min = object_offset + object_size;\nswitch (hdr->type) {\ncase BINDER_TYPE_BINDER:\ncase BINDER_TYPE_WEAK_BINDER: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_binder(fp, t, thread);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nfp, sizeof(*fp));\n} break;\ncase BINDER_TYPE_HANDLE:\ncase BINDER_TYPE_WEAK_HANDLE: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_handle(fp, t, thread);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nfp, sizeof(*fp));\n} break;\ncase BINDER_TYPE_FD: {\nstruct binder_fd_object *fp = to_binder_fd_object(hdr);\nbinder_size_t fd_offset = object_offset +\n(uintptr_t)&fp->fd - (uintptr_t)fp;\nint ret = binder_translate_fd(fp->fd, fd_offset, t,\nthread, in_reply_to);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nfp->pad_binder = 0;\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nfp, sizeof(*fp));\n} break;\ncase BINDER_TYPE_FDA: {\nstruct binder_object ptr_object;\nbinder_size_t parent_offset;\nstruct binder_fd_array_object *fda =\nto_binder_fd_array_object(hdr);\nsize_t num_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nstruct binder_buffer_object *parent =\nbinder_validate_ptr(target_proc, t->buffer,\n&ptr_object, fda->parent,\noff_start_offset,\n&parent_offset,\nnum_valid);\nif (!parent) {\nbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nif (!binder_validate_fixup(target_proc, t->buffer,\noff_start_offset,\nparent_offset,\nfda->parent_offset,\nlast_fixup_obj_off,\nlast_fixup_min_off)) {\nbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nret = binder_translate_fd_array(fda, parent, t, thread,\nin_reply_to);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nlast_fixup_obj_off = parent_offset;\nlast_fixup_min_off =\nfda->parent_offset + sizeof(u32) * fda->num_fds;\n} break;\ncase BINDER_TYPE_PTR: {\nstruct binder_buffer_object *bp =\nto_binder_buffer_object(hdr);\nsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\nsize_t num_valid;\nif (bp->length > buf_left) {\nbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nsg_buf_offset,\n(const void __user *)\n(uintptr_t)bp->buffer,\nbp->length)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error_param = -EFAULT;\nreturn_error = BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nbp->buffer = (uintptr_t)\nt->buffer->user_data + sg_buf_offset;\nsg_buf_offset += ALIGN(bp->length, sizeof(u64));\nnum_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nret = binder_fixup_parent(t, thread, bp,\noff_start_offset,\nnum_valid,\nlast_fixup_obj_off,\nlast_fixup_min_off);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, object_offset,\nbp, sizeof(*bp));\nlast_fixup_obj_off = object_offset;\nlast_fixup_min_off = 0;\n} break;\ndefault:\nbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\nproc->pid, thread->pid, hdr->type);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_object_type;\n}\n}\ntcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\nt->work.type = BINDER_WORK_TRANSACTION;\nif (reply) {\nbinder_enqueue_thread_work(thread, tcomplete);\nbinder_inner_proc_lock(target_proc);\nif (target_thread->is_dead) {\nbinder_inner_proc_unlock(target_proc);\ngoto err_dead_proc_or_thread;\n}\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_pop_transaction_ilocked(target_thread, in_reply_to);\nbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\nbinder_inner_proc_unlock(target_proc);\nwake_up_interruptible_sync(&target_thread->wait);\nbinder_free_transaction(in_reply_to);\n} else if (!(t->flags & TF_ONE_WAY)) {\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_inner_proc_lock(proc);\nbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\nt->need_reply = 1;\nt->from_parent = thread->transaction_stack;\nthread->transaction_stack = t;\nbinder_inner_proc_unlock(proc);\nif (!binder_proc_transaction(t, target_proc, target_thread)) {\nbinder_inner_proc_lock(proc);\nbinder_pop_transaction_ilocked(thread, t);\nbinder_inner_proc_unlock(proc);\ngoto err_dead_proc_or_thread;\n}\n} else {\nBUG_ON(target_node == NULL);\nBUG_ON(t->buffer->async_transaction != 1);\nbinder_enqueue_thread_work(thread, tcomplete);\nif (!binder_proc_transaction(t, target_proc, NULL))\ngoto err_dead_proc_or_thread;\n}\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nbinder_proc_dec_tmpref(target_proc);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nreturn;\nerr_dead_proc_or_thread:\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\nbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\nbinder_free_txn_fixups(t);\ntrace_binder_transaction_failed_buffer_release(t->buffer);\nbinder_transaction_buffer_release(target_proc, t->buffer,\nbuffer_offset, true);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\ntarget_node = NULL;\nt->buffer->transaction = NULL;\nbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nif (secctx)\nsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\nkfree(tcomplete);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\nkfree(t);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nif (target_proc)\nbinder_proc_dec_tmpref(target_proc);\nif (target_node) {\nbinder_dec_node(target_node, 1, 0);\nbinder_dec_node_tmpref(target_node);\n}\nbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\nproc->pid, thread->pid, return_error, return_error_param,\n(u64)tr->data_size, (u64)tr->offsets_size,\nreturn_error_line);\n{\nstruct binder_transaction_log_entry *fe;\ne->return_error = return_error;\ne->return_error_param = return_error_param;\ne->return_error_line = return_error_line;\nfe = binder_transaction_log_add(&binder_transaction_log_failed);\n*fe = *e;\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nWRITE_ONCE(fe->debug_id_done, t_debug_id);\n}\nBUG_ON(thread->return_error.cmd != BR_OK);\nif (in_reply_to) {\nthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\nbinder_send_failed_reply(in_reply_to, return_error);\n} else {\nthread->return_error.cmd = return_error;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\n}\n}\n"
    }
  ],
  "CVE-2019-2214": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when copying data to the buffer in the binder_transaction function.",
        "trigger_condition": "An attacker provides crafted input data that causes the buffer_offset to exceed the allocated buffer space, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly check and update sg_buf_end_offset when copying data to the buffer, allowing the buffer_offset to go beyond the allocated buffer space and potentially overwrite memory outside the buffer.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure that sg_buf_end_offset is properly calculated and checked to prevent buffer_offset from exceeding the allocated buffer space. This involves adding a check to ensure that buffer writes stay within the bounds of the allocated buffer."
      },
      "GPT_analysis": "The modification is necessary to address a possible out-of-bounds write vulnerability in the `binder_transaction` function of the code snippet. By adding the bounds check for `sg_buf_end_offset` when copying data to the buffer, we ensure that the write operation stays within the allocated buffer space. This helps prevent potential exploitation of the vulnerability for local privilege escalation without requiring additional execution privileges. The modification enhances the security of the code and mitigates the risk associated with the vulnerability.",
      "GPT_purpose": "Handle binder transactions in the Android kernel, managing communication between processes.",
      "GPT_function": "\n1. Handle binder transactions between processes.\n2. Manage transaction data and buffers.\n3. Perform security checks and validations.\n4. Enqueue and process transaction work items.\n5. Handle error cases and log transaction failures.",
      "CVE_id": "CVE-2019-2214",
      "code_before_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size;\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
      "code_after_change": "static void binder_transaction(struct binder_proc *proc,\n\t\t\t       struct binder_thread *thread,\n\t\t\t       struct binder_transaction_data *tr, int reply,\n\t\t\t       binder_size_t extra_buffers_size)\n{\n\tint ret;\n\tstruct binder_transaction *t;\n\tstruct binder_work *w;\n\tstruct binder_work *tcomplete;\n\tbinder_size_t buffer_offset = 0;\n\tbinder_size_t off_start_offset, off_end_offset;\n\tbinder_size_t off_min;\n\tbinder_size_t sg_buf_offset, sg_buf_end_offset;\n\tstruct binder_proc *target_proc = NULL;\n\tstruct binder_thread *target_thread = NULL;\n\tstruct binder_node *target_node = NULL;\n\tstruct binder_transaction *in_reply_to = NULL;\n\tstruct binder_transaction_log_entry *e;\n\tuint32_t return_error = 0;\n\tuint32_t return_error_param = 0;\n\tuint32_t return_error_line = 0;\n\tbinder_size_t last_fixup_obj_off = 0;\n\tbinder_size_t last_fixup_min_off = 0;\n\tstruct binder_context *context = proc->context;\n\tint t_debug_id = atomic_inc_return(&binder_last_id);\n\tchar *secctx = NULL;\n\tu32 secctx_sz = 0;\n\n\te = binder_transaction_log_add(&binder_transaction_log);\n\te->debug_id = t_debug_id;\n\te->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\n\te->from_proc = proc->pid;\n\te->from_thread = thread->pid;\n\te->target_handle = tr->target.handle;\n\te->data_size = tr->data_size;\n\te->offsets_size = tr->offsets_size;\n\te->context_name = proc->context->name;\n\n\tif (reply) {\n\t\tbinder_inner_proc_lock(proc);\n\t\tin_reply_to = thread->transaction_stack;\n\t\tif (in_reply_to == NULL) {\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_empty_call_stack;\n\t\t}\n\t\tif (in_reply_to->to_thread != thread) {\n\t\t\tspin_lock(&in_reply_to->lock);\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\tproc->pid, thread->pid, in_reply_to->debug_id,\n\t\t\t\tin_reply_to->to_proc ?\n\t\t\t\tin_reply_to->to_proc->pid : 0,\n\t\t\t\tin_reply_to->to_thread ?\n\t\t\t\tin_reply_to->to_thread->pid : 0);\n\t\t\tspin_unlock(&in_reply_to->lock);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\tgoto err_bad_call_stack;\n\t\t}\n\t\tthread->transaction_stack = in_reply_to->to_parent;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tbinder_set_nice(in_reply_to->saved_priority);\n\t\ttarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\n\t\tif (target_thread == NULL) {\n\t\t\t/* annotation for sparse */\n\t\t\t__release(&target_thread->proc->inner_lock);\n\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\tif (target_thread->transaction_stack != in_reply_to) {\n\t\t\tbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\n\t\t\t\tproc->pid, thread->pid,\n\t\t\t\ttarget_thread->transaction_stack ?\n\t\t\t\ttarget_thread->transaction_stack->debug_id : 0,\n\t\t\t\tin_reply_to->debug_id);\n\t\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tin_reply_to = NULL;\n\t\t\ttarget_thread = NULL;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\ttarget_proc = target_thread->proc;\n\t\ttarget_proc->tmp_ref++;\n\t\tbinder_inner_proc_unlock(target_thread->proc);\n\t} else {\n\t\tif (tr->target.handle) {\n\t\t\tstruct binder_ref *ref;\n\n\t\t\t/*\n\t\t\t * There must already be a strong ref\n\t\t\t * on this node. If so, do a strong\n\t\t\t * increment on the node to ensure it\n\t\t\t * stays alive until the transaction is\n\t\t\t * done.\n\t\t\t */\n\t\t\tbinder_proc_lock(proc);\n\t\t\tref = binder_get_ref_olocked(proc, tr->target.handle,\n\t\t\t\t\t\t     true);\n\t\t\tif (ref) {\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\tref->node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\t} else {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t}\n\t\t\tbinder_proc_unlock(proc);\n\t\t} else {\n\t\t\tmutex_lock(&context->context_mgr_node_lock);\n\t\t\ttarget_node = context->binder_context_mgr_node;\n\t\t\tif (target_node)\n\t\t\t\ttarget_node = binder_get_node_refs_for_txn(\n\t\t\t\t\t\ttarget_node, &target_proc,\n\t\t\t\t\t\t&return_error);\n\t\t\telse\n\t\t\t\treturn_error = BR_DEAD_REPLY;\n\t\t\tmutex_unlock(&context->context_mgr_node_lock);\n\t\t\tif (target_node && target_proc == proc) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_invalid_target_handle;\n\t\t\t}\n\t\t}\n\t\tif (!target_node) {\n\t\t\t/*\n\t\t\t * return_error is set above\n\t\t\t */\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_dead_binder;\n\t\t}\n\t\te->to_node = target_node->debug_id;\n\t\tif (security_binder_transaction(proc->tsk,\n\t\t\t\t\t\ttarget_proc->tsk) < 0) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPERM;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_invalid_target_handle;\n\t\t}\n\t\tbinder_inner_proc_lock(proc);\n\n\t\tw = list_first_entry_or_null(&thread->todo,\n\t\t\t\t\t     struct binder_work, entry);\n\t\tif (!(tr->flags & TF_ONE_WAY) && w &&\n\t\t    w->type == BINDER_WORK_TRANSACTION) {\n\t\t\t/*\n\t\t\t * Do not allow new outgoing transaction from a\n\t\t\t * thread that has a transaction at the head of\n\t\t\t * its todo list. Only need to check the head\n\t\t\t * because binder_select_thread_ilocked picks a\n\t\t\t * thread from proc->waiting_threads to enqueue\n\t\t\t * the transaction, and nothing is queued to the\n\t\t\t * todo list while the thread is on waiting_threads.\n\t\t\t */\n\t\t\tbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\n\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EPROTO;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_todo_list;\n\t\t}\n\n\t\tif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\n\t\t\tstruct binder_transaction *tmp;\n\n\t\t\ttmp = thread->transaction_stack;\n\t\t\tif (tmp->to_thread != thread) {\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\n\t\t\t\t\tproc->pid, thread->pid, tmp->debug_id,\n\t\t\t\t\ttmp->to_proc ? tmp->to_proc->pid : 0,\n\t\t\t\t\ttmp->to_thread ?\n\t\t\t\t\ttmp->to_thread->pid : 0);\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EPROTO;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_call_stack;\n\t\t\t}\n\t\t\twhile (tmp) {\n\t\t\t\tstruct binder_thread *from;\n\n\t\t\t\tspin_lock(&tmp->lock);\n\t\t\t\tfrom = tmp->from;\n\t\t\t\tif (from && from->proc == target_proc) {\n\t\t\t\t\tatomic_inc(&from->tmp_ref);\n\t\t\t\t\ttarget_thread = from;\n\t\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t\tspin_unlock(&tmp->lock);\n\t\t\t\ttmp = tmp->from_parent;\n\t\t\t}\n\t\t}\n\t\tbinder_inner_proc_unlock(proc);\n\t}\n\tif (target_thread)\n\t\te->to_thread = target_thread->pid;\n\te->to_proc = target_proc->pid;\n\n\t/* TODO: reuse incoming transaction for reply */\n\tt = kzalloc(sizeof(*t), GFP_KERNEL);\n\tif (t == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_t_failed;\n\t}\n\tINIT_LIST_HEAD(&t->fd_fixups);\n\tbinder_stats_created(BINDER_STAT_TRANSACTION);\n\tspin_lock_init(&t->lock);\n\n\ttcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\n\tif (tcomplete == NULL) {\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -ENOMEM;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_alloc_tcomplete_failed;\n\t}\n\tbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\n\n\tt->debug_id = t_debug_id;\n\n\tif (reply)\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_thread->pid,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\telse\n\t\tbinder_debug(BINDER_DEBUG_TRANSACTION,\n\t\t\t     \"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\n\t\t\t     proc->pid, thread->pid, t->debug_id,\n\t\t\t     target_proc->pid, target_node->debug_id,\n\t\t\t     (u64)tr->data.ptr.buffer,\n\t\t\t     (u64)tr->data.ptr.offsets,\n\t\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t\t     (u64)extra_buffers_size);\n\n\tif (!reply && !(tr->flags & TF_ONE_WAY))\n\t\tt->from = thread;\n\telse\n\t\tt->from = NULL;\n\tt->sender_euid = task_euid(proc->tsk);\n\tt->to_proc = target_proc;\n\tt->to_thread = target_thread;\n\tt->code = tr->code;\n\tt->flags = tr->flags;\n\tt->priority = task_nice(current);\n\n\tif (target_node && target_node->txn_security_ctx) {\n\t\tu32 secid;\n\t\tsize_t added_size;\n\n\t\tsecurity_task_getsecid(proc->tsk, &secid);\n\t\tret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\n\t\tif (ret) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = ret;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_get_secctx_failed;\n\t\t}\n\t\tadded_size = ALIGN(secctx_sz, sizeof(u64));\n\t\textra_buffers_size += added_size;\n\t\tif (extra_buffers_size < added_size) {\n\t\t\t/* integer overflow of extra_buffers_size */\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_extra_size;\n\t\t}\n\t}\n\n\ttrace_binder_transaction(reply, t, target_node);\n\n\tt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\n\t\ttr->offsets_size, extra_buffers_size,\n\t\t!reply && (t->flags & TF_ONE_WAY));\n\tif (IS_ERR(t->buffer)) {\n\t\t/*\n\t\t * -ESRCH indicates VMA cleared. The target is dying.\n\t\t */\n\t\treturn_error_param = PTR_ERR(t->buffer);\n\t\treturn_error = return_error_param == -ESRCH ?\n\t\t\tBR_DEAD_REPLY : BR_FAILED_REPLY;\n\t\treturn_error_line = __LINE__;\n\t\tt->buffer = NULL;\n\t\tgoto err_binder_alloc_buf_failed;\n\t}\n\tif (secctx) {\n\t\tint err;\n\t\tsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(tr->offsets_size, sizeof(void *)) +\n\t\t\t\t    ALIGN(extra_buffers_size, sizeof(void *)) -\n\t\t\t\t    ALIGN(secctx_sz, sizeof(u64));\n\n\t\tt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\n\t\terr = binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  t->buffer, buf_offset,\n\t\t\t\t\t\t  secctx, secctx_sz);\n\t\tif (err) {\n\t\t\tt->security_ctx = 0;\n\t\t\tWARN_ON(1);\n\t\t}\n\t\tsecurity_release_secctx(secctx, secctx_sz);\n\t\tsecctx = NULL;\n\t}\n\tt->buffer->debug_id = t->debug_id;\n\tt->buffer->transaction = t;\n\tt->buffer->target_node = target_node;\n\ttrace_binder_transaction_alloc_buf(t->buffer);\n\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer, 0,\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.buffer,\n\t\t\t\ttr->data_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t&target_proc->alloc,\n\t\t\t\tt->buffer,\n\t\t\t\tALIGN(tr->data_size, sizeof(void *)),\n\t\t\t\t(const void __user *)\n\t\t\t\t\t(uintptr_t)tr->data.ptr.offsets,\n\t\t\t\ttr->offsets_size)) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\tproc->pid, thread->pid);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EFAULT;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_copy_data_failed;\n\t}\n\tif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\n\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\n\t\t\t\tproc->pid, thread->pid, (u64)tr->offsets_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\tif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\n\t\tbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\n\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t  (u64)extra_buffers_size);\n\t\treturn_error = BR_FAILED_REPLY;\n\t\treturn_error_param = -EINVAL;\n\t\treturn_error_line = __LINE__;\n\t\tgoto err_bad_offset;\n\t}\n\toff_start_offset = ALIGN(tr->data_size, sizeof(void *));\n\tbuffer_offset = off_start_offset;\n\toff_end_offset = off_start_offset + tr->offsets_size;\n\tsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\n\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\n\t\tALIGN(secctx_sz, sizeof(u64));\n\toff_min = 0;\n\tfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\n\t     buffer_offset += sizeof(binder_size_t)) {\n\t\tstruct binder_object_header *hdr;\n\t\tsize_t object_size;\n\t\tstruct binder_object object;\n\t\tbinder_size_t object_offset;\n\n\t\tif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n\t\t\t\t\t\t  &object_offset,\n\t\t\t\t\t\t  t->buffer,\n\t\t\t\t\t\t  buffer_offset,\n\t\t\t\t\t\t  sizeof(object_offset))) {\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\t\tobject_size = binder_get_object(target_proc, t->buffer,\n\t\t\t\t\t\tobject_offset, &object);\n\t\tif (object_size == 0 || object_offset < off_min) {\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\n\t\t\t\t\t  proc->pid, thread->pid,\n\t\t\t\t\t  (u64)object_offset,\n\t\t\t\t\t  (u64)off_min,\n\t\t\t\t\t  (u64)t->buffer->data_size);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_offset;\n\t\t}\n\n\t\thdr = &object.hdr;\n\t\toff_min = object_offset + object_size;\n\t\tswitch (hdr->type) {\n\t\tcase BINDER_TYPE_BINDER:\n\t\tcase BINDER_TYPE_WEAK_BINDER: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_binder(fp, t, thread);\n\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_HANDLE:\n\t\tcase BINDER_TYPE_WEAK_HANDLE: {\n\t\t\tstruct flat_binder_object *fp;\n\n\t\t\tfp = to_flat_binder_object(hdr);\n\t\t\tret = binder_translate_handle(fp, t, thread);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\n\t\tcase BINDER_TYPE_FD: {\n\t\t\tstruct binder_fd_object *fp = to_binder_fd_object(hdr);\n\t\t\tbinder_size_t fd_offset = object_offset +\n\t\t\t\t(uintptr_t)&fp->fd - (uintptr_t)fp;\n\t\t\tint ret = binder_translate_fd(fp->fd, fd_offset, t,\n\t\t\t\t\t\t      thread, in_reply_to);\n\n\t\t\tfp->pad_binder = 0;\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tfp, sizeof(*fp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t} break;\n\t\tcase BINDER_TYPE_FDA: {\n\t\t\tstruct binder_object ptr_object;\n\t\t\tbinder_size_t parent_offset;\n\t\t\tstruct binder_fd_array_object *fda =\n\t\t\t\tto_binder_fd_array_object(hdr);\n\t\t\tsize_t num_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tstruct binder_buffer_object *parent =\n\t\t\t\tbinder_validate_ptr(target_proc, t->buffer,\n\t\t\t\t\t\t    &ptr_object, fda->parent,\n\t\t\t\t\t\t    off_start_offset,\n\t\t\t\t\t\t    &parent_offset,\n\t\t\t\t\t\t    num_valid);\n\t\t\tif (!parent) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tif (!binder_validate_fixup(target_proc, t->buffer,\n\t\t\t\t\t\t   off_start_offset,\n\t\t\t\t\t\t   parent_offset,\n\t\t\t\t\t\t   fda->parent_offset,\n\t\t\t\t\t\t   last_fixup_obj_off,\n\t\t\t\t\t\t   last_fixup_min_off)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_parent;\n\t\t\t}\n\t\t\tret = binder_translate_fd_array(fda, parent, t, thread,\n\t\t\t\t\t\t\tin_reply_to);\n\t\t\tif (ret < 0) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = parent_offset;\n\t\t\tlast_fixup_min_off =\n\t\t\t\tfda->parent_offset + sizeof(u32) * fda->num_fds;\n\t\t} break;\n\t\tcase BINDER_TYPE_PTR: {\n\t\t\tstruct binder_buffer_object *bp =\n\t\t\t\tto_binder_buffer_object(hdr);\n\t\t\tsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\n\t\t\tsize_t num_valid;\n\n\t\t\tif (bp->length > buf_left) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = -EINVAL;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_bad_offset;\n\t\t\t}\n\t\t\tif (binder_alloc_copy_user_to_buffer(\n\t\t\t\t\t\t&target_proc->alloc,\n\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\tsg_buf_offset,\n\t\t\t\t\t\t(const void __user *)\n\t\t\t\t\t\t\t(uintptr_t)bp->buffer,\n\t\t\t\t\t\tbp->length)) {\n\t\t\t\tbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\n\t\t\t\t\t\t  proc->pid, thread->pid);\n\t\t\t\treturn_error_param = -EFAULT;\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_copy_data_failed;\n\t\t\t}\n\t\t\t/* Fixup buffer pointer to target proc address space */\n\t\t\tbp->buffer = (uintptr_t)\n\t\t\t\tt->buffer->user_data + sg_buf_offset;\n\t\t\tsg_buf_offset += ALIGN(bp->length, sizeof(u64));\n\n\t\t\tnum_valid = (buffer_offset - off_start_offset) *\n\t\t\t\t\tsizeof(binder_size_t);\n\t\t\tret = binder_fixup_parent(t, thread, bp,\n\t\t\t\t\t\t  off_start_offset,\n\t\t\t\t\t\t  num_valid,\n\t\t\t\t\t\t  last_fixup_obj_off,\n\t\t\t\t\t\t  last_fixup_min_off);\n\t\t\tif (ret < 0 ||\n\t\t\t    binder_alloc_copy_to_buffer(&target_proc->alloc,\n\t\t\t\t\t\t\tt->buffer,\n\t\t\t\t\t\t\tobject_offset,\n\t\t\t\t\t\t\tbp, sizeof(*bp))) {\n\t\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\t\treturn_error_param = ret;\n\t\t\t\treturn_error_line = __LINE__;\n\t\t\t\tgoto err_translate_failed;\n\t\t\t}\n\t\t\tlast_fixup_obj_off = object_offset;\n\t\t\tlast_fixup_min_off = 0;\n\t\t} break;\n\t\tdefault:\n\t\t\tbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\n\t\t\t\tproc->pid, thread->pid, hdr->type);\n\t\t\treturn_error = BR_FAILED_REPLY;\n\t\t\treturn_error_param = -EINVAL;\n\t\t\treturn_error_line = __LINE__;\n\t\t\tgoto err_bad_object_type;\n\t\t}\n\t}\n\ttcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\n\tt->work.type = BINDER_WORK_TRANSACTION;\n\n\tif (reply) {\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tbinder_inner_proc_lock(target_proc);\n\t\tif (target_thread->is_dead) {\n\t\t\tbinder_inner_proc_unlock(target_proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_pop_transaction_ilocked(target_thread, in_reply_to);\n\t\tbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\n\t\tbinder_inner_proc_unlock(target_proc);\n\t\twake_up_interruptible_sync(&target_thread->wait);\n\t\tbinder_free_transaction(in_reply_to);\n\t} else if (!(t->flags & TF_ONE_WAY)) {\n\t\tBUG_ON(t->buffer->async_transaction != 0);\n\t\tbinder_inner_proc_lock(proc);\n\t\t/*\n\t\t * Defer the TRANSACTION_COMPLETE, so we don't return to\n\t\t * userspace immediately; this allows the target process to\n\t\t * immediately start processing this transaction, reducing\n\t\t * latency. We will then return the TRANSACTION_COMPLETE when\n\t\t * the target replies (or there is an error).\n\t\t */\n\t\tbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\n\t\tt->need_reply = 1;\n\t\tt->from_parent = thread->transaction_stack;\n\t\tthread->transaction_stack = t;\n\t\tbinder_inner_proc_unlock(proc);\n\t\tif (!binder_proc_transaction(t, target_proc, target_thread)) {\n\t\t\tbinder_inner_proc_lock(proc);\n\t\t\tbinder_pop_transaction_ilocked(thread, t);\n\t\t\tbinder_inner_proc_unlock(proc);\n\t\t\tgoto err_dead_proc_or_thread;\n\t\t}\n\t} else {\n\t\tBUG_ON(target_node == NULL);\n\t\tBUG_ON(t->buffer->async_transaction != 1);\n\t\tbinder_enqueue_thread_work(thread, tcomplete);\n\t\tif (!binder_proc_transaction(t, target_proc, NULL))\n\t\t\tgoto err_dead_proc_or_thread;\n\t}\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\t/*\n\t * write barrier to synchronize with initialization\n\t * of log entry\n\t */\n\tsmp_wmb();\n\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\treturn;\n\nerr_dead_proc_or_thread:\n\treturn_error = BR_DEAD_REPLY;\n\treturn_error_line = __LINE__;\n\tbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\n\tbinder_free_txn_fixups(t);\n\ttrace_binder_transaction_failed_buffer_release(t->buffer);\n\tbinder_transaction_buffer_release(target_proc, t->buffer,\n\t\t\t\t\t  buffer_offset, true);\n\tif (target_node)\n\t\tbinder_dec_node_tmpref(target_node);\n\ttarget_node = NULL;\n\tt->buffer->transaction = NULL;\n\tbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\n\tif (secctx)\n\t\tsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\n\tkfree(tcomplete);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\n\tkfree(t);\n\tbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\n\tif (target_thread)\n\t\tbinder_thread_dec_tmpref(target_thread);\n\tif (target_proc)\n\t\tbinder_proc_dec_tmpref(target_proc);\n\tif (target_node) {\n\t\tbinder_dec_node(target_node, 1, 0);\n\t\tbinder_dec_node_tmpref(target_node);\n\t}\n\n\tbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\t\t     \"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\n\t\t     proc->pid, thread->pid, return_error, return_error_param,\n\t\t     (u64)tr->data_size, (u64)tr->offsets_size,\n\t\t     return_error_line);\n\n\t{\n\t\tstruct binder_transaction_log_entry *fe;\n\n\t\te->return_error = return_error;\n\t\te->return_error_param = return_error_param;\n\t\te->return_error_line = return_error_line;\n\t\tfe = binder_transaction_log_add(&binder_transaction_log_failed);\n\t\t*fe = *e;\n\t\t/*\n\t\t * write barrier to synchronize with initialization\n\t\t * of log entry\n\t\t */\n\t\tsmp_wmb();\n\t\tWRITE_ONCE(e->debug_id_done, t_debug_id);\n\t\tWRITE_ONCE(fe->debug_id_done, t_debug_id);\n\t}\n\n\tBUG_ON(thread->return_error.cmd != BR_OK);\n\tif (in_reply_to) {\n\t\tthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t\tbinder_send_failed_reply(in_reply_to, return_error);\n\t} else {\n\t\tthread->return_error.cmd = return_error;\n\t\tbinder_enqueue_thread_work(thread, &thread->return_error.work);\n\t}\n}",
      "modified_lines": {
        "added": [
          "\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size -",
          "\t\tALIGN(secctx_sz, sizeof(u64));"
        ],
        "deleted": [
          "\tsg_buf_end_offset = sg_buf_offset + extra_buffers_size;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when copying data to the buffer in the binder_transaction function.",
      "trigger_condition": "An attacker provides crafted input data that causes the buffer_offset to exceed the allocated buffer space, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly check and update sg_buf_end_offset when copying data to the buffer, allowing the buffer_offset to go beyond the allocated buffer space and potentially overwrite memory outside the buffer.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure that sg_buf_end_offset is properly calculated and checked to prevent buffer_offset from exceeding the allocated buffer space. This involves adding a check to ensure that buffer writes stay within the bounds of the allocated buffer.",
      "id": 56,
      "code_after_change_normalized": "static void FUN1(struct binder_proc *VAR1,\nstruct binder_thread *VAR2,\nstruct binder_transaction_data *VAR3, int VAR4,\nbinder_size_t VAR5)\n{\nint VAR6;\nstruct binder_transaction *VAR7;\nstruct binder_work *VAR8;\nstruct binder_work *VAR9;\nbinder_size_t VAR10 = 0;\nbinder_size_t VAR11, VAR12;\nbinder_size_t VAR13;\nbinder_size_t VAR14, VAR15;\nstruct binder_proc *VAR16 = NULL;\nstruct binder_thread *VAR17 = NULL;\nstruct binder_node *VAR18 = NULL;\nstruct binder_transaction *VAR19 = NULL;\nstruct binder_transaction_log_entry *VAR20;\nuint32_t VAR21 = 0;\nuint32_t VAR22 = 0;\nuint32_t VAR23 = 0;\nbinder_size_t VAR24 = 0;\nbinder_size_t VAR25 = 0;\nstruct binder_context *VAR26 = VAR1->VAR26;\nint VAR27 = FUN2(&VAR28);\nchar *VAR29 = NULL;\nu32 VAR30 = 0;\nVAR20 = FUN3(&VAR31);\nVAR20->VAR32 = VAR27;\nVAR20->VAR33 = VAR4 ? 2 : !!(VAR3->VAR34 & VAR35);\nVAR20->VAR36 = VAR1->VAR37;\nVAR20->VAR38 = VAR2->VAR37;\nVAR20->VAR39 = VAR3->VAR40.VAR41;\nVAR20->VAR42 = VAR3->VAR42;\nVAR20->VAR43 = VAR3->VAR43;\nVAR20->VAR44 = VAR1->VAR26->VAR45;\nif (VAR4) {\nFUN4(VAR1);\nVAR19 = VAR2->VAR46;\nif (VAR19 == NULL) {\nFUN5(VAR1);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR50;\n}\nif (VAR19->VAR51 != VAR2) {\nFUN7(&VAR19->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR19->VAR32,\nVAR19->VAR53 ?\nVAR19->VAR53->VAR37 : 0,\nVAR19->VAR51 ?\nVAR19->VAR51->VAR37 : 0);\nFUN8(&VAR19->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\ngoto VAR54;\n}\nVAR2->VAR46 = VAR19->VAR55;\nFUN5(VAR1);\nFUN9(VAR19->VAR56);\nVAR17 = FUN10(VAR19);\nif (VAR17 == NULL) {\nFUN11(&VAR17->VAR1->VAR57);\nVAR21 = VAR58;\nVAR23 = VAR49;\ngoto VAR59;\n}\nif (VAR17->VAR46 != VAR19) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\nVAR17->VAR46 ?\nVAR17->VAR46->VAR32 : 0,\nVAR19->VAR32);\nFUN5(VAR17->VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\nVAR17 = NULL;\ngoto VAR59;\n}\nVAR16 = VAR17->VAR1;\nVAR16->VAR60++;\nFUN5(VAR17->VAR1);\n} else {\nif (VAR3->VAR40.VAR41) {\nstruct binder_ref *VAR61;\nFUN12(VAR1);\nVAR61 = FUN13(VAR1, VAR3->VAR40.VAR41,\ntrue);\nif (VAR61) {\nVAR18 = FUN14(\nVAR61->VAR62, &VAR16,\n&VAR21);\n} else {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\n}\nFUN15(VAR1);\n} else {\nFUN16(&VAR26->VAR63);\nVAR18 = VAR26->VAR64;\nif (VAR18)\nVAR18 = FUN14(\nVAR18, &VAR16,\n&VAR21);\nelse\nVAR21 = VAR58;\nFUN17(&VAR26->VAR63);\nif (VAR18 && VAR16 == VAR1) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR66;\n}\n}\nif (!VAR18) {\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR59;\n}\nVAR20->VAR67 = VAR18->VAR32;\nif (FUN18(VAR1->VAR68,\nVAR16->VAR68) < 0) {\nVAR21 = VAR47;\nVAR22 = -VAR69;\nVAR23 = VAR49;\ngoto VAR66;\n}\nFUN4(VAR1);\nVAR8 = FUN19(&VAR2->VAR70,\nstruct VAR71, VAR72);\nif (!(VAR3->VAR34 & VAR35) && VAR8 &&\nVAR8->VAR73 == VAR74) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR75;\n}\nif (!(VAR3->VAR34 & VAR35) && VAR2->VAR46) {\nstruct binder_transaction *VAR76;\nVAR76 = VAR2->VAR46;\nif (VAR76->VAR51 != VAR2) {\nFUN7(&VAR76->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR76->VAR32,\nVAR76->VAR53 ? VAR76->VAR53->VAR37 : 0,\nVAR76->VAR51 ?\nVAR76->VAR51->VAR37 : 0);\nFUN8(&VAR76->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR54;\n}\nwhile (VAR76) {\nstruct binder_thread *VAR77;\nFUN7(&VAR76->VAR52);\nVAR77 = VAR76->VAR77;\nif (VAR77 && VAR77->VAR1 == VAR16) {\nFUN20(&VAR77->VAR60);\nVAR17 = VAR77;\nFUN8(&VAR76->VAR52);\nbreak;\n}\nFUN8(&VAR76->VAR52);\nVAR76 = VAR76->VAR78;\n}\n}\nFUN5(VAR1);\n}\nif (VAR17)\nVAR20->VAR51 = VAR17->VAR37;\nVAR20->VAR53 = VAR16->VAR37;\nVAR7 = FUN21(sizeof(*VAR7), VAR79);\nif (VAR7 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR81;\n}\nFUN22(&VAR7->VAR82);\nFUN23(VAR83);\nFUN24(&VAR7->VAR52);\nVAR9 = FUN21(sizeof(*VAR9), VAR79);\nif (VAR9 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR84;\n}\nFUN23(VAR85);\nVAR7->VAR32 = VAR27;\nif (VAR4)\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR17->VAR37,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nelse\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR18->VAR32,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nif (!VAR4 && !(VAR3->VAR34 & VAR35))\nVAR7->VAR77 = VAR2;\nelse\nVAR7->VAR77 = NULL;\nVAR7->VAR92 = FUN26(VAR1->VAR68);\nVAR7->VAR53 = VAR16;\nVAR7->VAR51 = VAR17;\nVAR7->VAR93 = VAR3->VAR93;\nVAR7->VAR34 = VAR3->VAR34;\nVAR7->VAR94 = FUN27(VAR95);\nif (VAR18 && VAR18->VAR96) {\nu32 VAR97;\nsize_t VAR98;\nFUN28(VAR1->VAR68, &VAR97);\nVAR6 = FUN29(VAR97, &VAR29, &VAR30);\nif (VAR6) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR99;\n}\nVAR98 = FUN30(VAR30, sizeof(VAR87));\nVAR5 += VAR98;\nif (VAR5 < VAR98) {\nVAR21 = VAR47;\nVAR22 = VAR65;\nVAR23 = VAR49;\ngoto VAR100;\n}\n}\nFUN31(VAR4, VAR7, VAR18);\nVAR7->VAR90 = FUN32(&VAR16->VAR101, VAR3->VAR42,\nVAR3->VAR43, VAR5,\n!VAR4 && (VAR7->VAR34 & VAR35));\nif (FUN33(VAR7->VAR90)) {\nVAR22 = FUN34(VAR7->VAR90);\nVAR21 = VAR22 == -VAR102 ?\nVAR58 : VAR47;\nVAR23 = VAR49;\nVAR7->VAR90 = NULL;\ngoto VAR103;\n}\nif (VAR29) {\nint VAR104;\nsize_t VAR105 = FUN30(VAR3->VAR42, sizeof(void *)) +\nFUN30(VAR3->VAR43, sizeof(void *)) +\nFUN30(VAR5, sizeof(void *)) -\nFUN30(VAR30, sizeof(VAR87));\nVAR7->VAR106 = (VAR107)VAR7->VAR90->VAR108 + VAR105;\nVAR104 = FUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR105,\nVAR29, VAR30);\nif (VAR104) {\nVAR7->VAR106 = 0;\nFUN36(1);\n}\nFUN37(VAR29, VAR30);\nVAR29 = NULL;\n}\nVAR7->VAR90->VAR32 = VAR7->VAR32;\nVAR7->VAR90->VAR109 = VAR7;\nVAR7->VAR90->VAR18 = VAR18;\nFUN38(VAR7->VAR90);\nif (FUN39(\n&VAR16->VAR101,\nVAR7->VAR90, 0,\n(const void VAR110 *)\n(VAR107)VAR3->VAR88.VAR89.VAR90,\nVAR3->VAR42)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR111;\nVAR23 = VAR49;\ngoto VAR112;\n}\nif (FUN39(\n&VAR16->VAR101,\nVAR7->VAR90,\nFUN30(VAR3->VAR42, sizeof(void *)),\n(const void VAR110 *)\n(VAR107)VAR3->VAR88.VAR89.VAR91,\nVAR3->VAR43)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR111;\nVAR23 = VAR49;\ngoto VAR112;\n}\nif (!FUN40(VAR3->VAR43, sizeof(VAR113))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, (VAR87)VAR3->VAR43);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nif (!FUN40(VAR5, sizeof(VAR87))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR5);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nVAR11 = FUN30(VAR3->VAR42, sizeof(void *));\nVAR10 = VAR11;\nVAR12 = VAR11 + VAR3->VAR43;\nVAR14 = FUN30(VAR12, sizeof(void *));\nVAR15 = VAR14 + VAR5 -\nFUN30(VAR30, sizeof(VAR87));\nVAR13 = 0;\nfor (VAR10 = VAR11; VAR10 < VAR12;\nVAR10 += sizeof(VAR113)) {\nstruct binder_object_header *VAR115;\nsize_t VAR116;\nstruct binder_object VAR117;\nbinder_size_t VAR118;\nif (FUN41(&VAR16->VAR101,\n&VAR118,\nVAR7->VAR90,\nVAR10,\nsizeof(VAR118))) {\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nVAR116 = FUN42(VAR16, VAR7->VAR90,\nVAR118, &VAR117);\nif (VAR116 == 0 || VAR118 < VAR13) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR118,\n(VAR87)VAR13,\n(VAR87)VAR7->VAR90->VAR42);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nVAR115 = &VAR117.VAR115;\nVAR13 = VAR118 + VAR116;\nswitch (VAR115->VAR73) {\ncase VAR119:\ncase VAR120: {\nstruct flat_binder_object *VAR121;\nVAR121 = FUN43(VAR115);\nVAR6 = FUN44(VAR121, VAR7, VAR2);\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR121, sizeof(*VAR121))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\n} break;\ncase VAR123:\ncase VAR124: {\nstruct flat_binder_object *VAR121;\nVAR121 = FUN43(VAR115);\nVAR6 = FUN45(VAR121, VAR7, VAR2);\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR121, sizeof(*VAR121))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\n} break;\ncase VAR125: {\nstruct binder_fd_object *VAR121 = FUN46(VAR115);\nbinder_size_t VAR126 = VAR118 +\n(VAR107)&VAR121->VAR127 - (VAR107)VAR121;\nint VAR6 = FUN47(VAR121->VAR127, VAR126, VAR7,\nVAR2, VAR19);\nVAR121->VAR128 = 0;\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR121, sizeof(*VAR121))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\n} break;\ncase VAR129: {\nstruct binder_object VAR130;\nbinder_size_t VAR131;\nstruct binder_fd_array_object *VAR132 =\nFUN48(VAR115);\nsize_t VAR133 = (VAR10 - VAR11) *\nsizeof(VAR113);\nstruct binder_buffer_object *VAR134 =\nFUN49(VAR16, VAR7->VAR90,\n&VAR130, VAR132->VAR134,\nVAR11,\n&VAR131,\nVAR133);\nif (!VAR134) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR135;\n}\nif (!FUN50(VAR16, VAR7->VAR90,\nVAR11,\nVAR131,\nVAR132->VAR131,\nVAR24,\nVAR25)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR135;\n}\nVAR6 = FUN51(VAR132, VAR134, VAR7, VAR2,\nVAR19);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\nVAR24 = VAR131;\nVAR25 =\nVAR132->VAR131 + sizeof(VAR136) * VAR132->VAR137;\n} break;\ncase VAR138: {\nstruct binder_buffer_object *VAR139 =\nFUN52(VAR115);\nsize_t VAR140 = VAR15 - VAR14;\nsize_t VAR133;\nif (VAR139->VAR141 > VAR140) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nif (FUN39(\n&VAR16->VAR101,\nVAR7->VAR90,\nVAR14,\n(const void VAR110 *)\n(VAR107)VAR139->VAR90,\nVAR139->VAR141)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR22 = -VAR111;\nVAR21 = VAR47;\nVAR23 = VAR49;\ngoto VAR112;\n}\nVAR139->VAR90 = (VAR107)\nVAR7->VAR90->VAR108 + VAR14;\nVAR14 += FUN30(VAR139->VAR141, sizeof(VAR87));\nVAR133 = (VAR10 - VAR11) *\nsizeof(VAR113);\nVAR6 = FUN53(VAR7, VAR2, VAR139,\nVAR11,\nVAR133,\nVAR24,\nVAR25);\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR139, sizeof(*VAR139))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\nVAR24 = VAR118;\nVAR25 = 0;\n} break;\ndefault:\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR115->VAR73);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR142;\n}\n}\nVAR9->VAR73 = VAR143;\nVAR7->VAR144.VAR73 = VAR74;\nif (VAR4) {\nFUN54(VAR2, VAR9);\nFUN4(VAR16);\nif (VAR17->VAR145) {\nFUN5(VAR16);\ngoto VAR146;\n}\nFUN55(VAR7->VAR90->VAR147 != 0);\nFUN56(VAR17, VAR19);\nFUN57(VAR17, &VAR7->VAR144);\nFUN5(VAR16);\nFUN58(&VAR17->VAR148);\nFUN59(VAR19);\n} else if (!(VAR7->VAR34 & VAR35)) {\nFUN55(VAR7->VAR90->VAR147 != 0);\nFUN4(VAR1);\nFUN60(VAR2, VAR9);\nVAR7->VAR149 = 1;\nVAR7->VAR78 = VAR2->VAR46;\nVAR2->VAR46 = VAR7;\nFUN5(VAR1);\nif (!FUN61(VAR7, VAR16, VAR17)) {\nFUN4(VAR1);\nFUN56(VAR2, VAR7);\nFUN5(VAR1);\ngoto VAR146;\n}\n} else {\nFUN55(VAR18 == NULL);\nFUN55(VAR7->VAR90->VAR147 != 1);\nFUN54(VAR2, VAR9);\nif (!FUN61(VAR7, VAR16, NULL))\ngoto VAR146;\n}\nif (VAR17)\nFUN62(VAR17);\nFUN63(VAR16);\nif (VAR18)\nFUN64(VAR18);\nFUN65();\nFUN66(VAR20->VAR150, VAR27);\nreturn;\nVAR146:\nVAR21 = VAR58;\nVAR23 = VAR49;\nFUN67(VAR1, VAR9);\nVAR122:\nVAR142:\nVAR114:\nVAR135:\nVAR112:\nFUN68(VAR7);\nFUN69(VAR7->VAR90);\nFUN70(VAR16, VAR7->VAR90,\nVAR10, true);\nif (VAR18)\nFUN64(VAR18);\nVAR18 = NULL;\nVAR7->VAR90->VAR109 = NULL;\nFUN71(&VAR16->VAR101, VAR7->VAR90);\nVAR103:\nVAR100:\nif (VAR29)\nFUN37(VAR29, VAR30);\nVAR99:\nFUN72(VAR9);\nFUN73(VAR85);\nVAR84:\nFUN72(VAR7);\nFUN73(VAR83);\nVAR81:\nVAR75:\nVAR54:\nVAR50:\nVAR59:\nVAR66:\nif (VAR17)\nFUN62(VAR17);\nif (VAR16)\nFUN63(VAR16);\nif (VAR18) {\nFUN74(VAR18, 1, 0);\nFUN64(VAR18);\n}\nFUN25(VAR151,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR21, VAR22,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\nVAR23);\n{\nstruct binder_transaction_log_entry *VAR152;\nVAR20->VAR21 = VAR21;\nVAR20->VAR22 = VAR22;\nVAR20->VAR23 = VAR23;\nVAR152 = FUN3(&VAR153);\n*VAR152 = *VAR20;\nFUN65();\nFUN66(VAR20->VAR150, VAR27);\nFUN66(VAR152->VAR150, VAR27);\n}\nFUN55(VAR2->VAR21.VAR154 != VAR155);\nif (VAR19) {\nVAR2->VAR21.VAR154 = VAR156;\nFUN54(VAR2, &VAR2->VAR21.VAR144);\nFUN75(VAR19, VAR21);\n} else {\nVAR2->VAR21.VAR154 = VAR21;\nFUN54(VAR2, &VAR2->VAR21.VAR144);\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct binder_proc *VAR1,\nstruct binder_thread *VAR2,\nstruct binder_transaction_data *VAR3, int VAR4,\nbinder_size_t VAR5)\n{\nint VAR6;\nstruct binder_transaction *VAR7;\nstruct binder_work *VAR8;\nstruct binder_work *VAR9;\nbinder_size_t VAR10 = 0;\nbinder_size_t VAR11, VAR12;\nbinder_size_t VAR13;\nbinder_size_t VAR14, VAR15;\nstruct binder_proc *VAR16 = NULL;\nstruct binder_thread *VAR17 = NULL;\nstruct binder_node *VAR18 = NULL;\nstruct binder_transaction *VAR19 = NULL;\nstruct binder_transaction_log_entry *VAR20;\nuint32_t VAR21 = 0;\nuint32_t VAR22 = 0;\nuint32_t VAR23 = 0;\nbinder_size_t VAR24 = 0;\nbinder_size_t VAR25 = 0;\nstruct binder_context *VAR26 = VAR1->VAR26;\nint VAR27 = FUN2(&VAR28);\nchar *VAR29 = NULL;\nu32 VAR30 = 0;\nVAR20 = FUN3(&VAR31);\nVAR20->VAR32 = VAR27;\nVAR20->VAR33 = VAR4 ? 2 : !!(VAR3->VAR34 & VAR35);\nVAR20->VAR36 = VAR1->VAR37;\nVAR20->VAR38 = VAR2->VAR37;\nVAR20->VAR39 = VAR3->VAR40.VAR41;\nVAR20->VAR42 = VAR3->VAR42;\nVAR20->VAR43 = VAR3->VAR43;\nVAR20->VAR44 = VAR1->VAR26->VAR45;\nif (VAR4) {\nFUN4(VAR1);\nVAR19 = VAR2->VAR46;\nif (VAR19 == NULL) {\nFUN5(VAR1);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR50;\n}\nif (VAR19->VAR51 != VAR2) {\nFUN7(&VAR19->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR19->VAR32,\nVAR19->VAR53 ?\nVAR19->VAR53->VAR37 : 0,\nVAR19->VAR51 ?\nVAR19->VAR51->VAR37 : 0);\nFUN8(&VAR19->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\ngoto VAR54;\n}\nVAR2->VAR46 = VAR19->VAR55;\nFUN5(VAR1);\nFUN9(VAR19->VAR56);\nVAR17 = FUN10(VAR19);\nif (VAR17 == NULL) {\nFUN11(&VAR17->VAR1->VAR57);\nVAR21 = VAR58;\nVAR23 = VAR49;\ngoto VAR59;\n}\nif (VAR17->VAR46 != VAR19) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\nVAR17->VAR46 ?\nVAR17->VAR46->VAR32 : 0,\nVAR19->VAR32);\nFUN5(VAR17->VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\nVAR19 = NULL;\nVAR17 = NULL;\ngoto VAR59;\n}\nVAR16 = VAR17->VAR1;\nVAR16->VAR60++;\nFUN5(VAR17->VAR1);\n} else {\nif (VAR3->VAR40.VAR41) {\nstruct binder_ref *VAR61;\nFUN12(VAR1);\nVAR61 = FUN13(VAR1, VAR3->VAR40.VAR41,\ntrue);\nif (VAR61) {\nVAR18 = FUN14(\nVAR61->VAR62, &VAR16,\n&VAR21);\n} else {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\n}\nFUN15(VAR1);\n} else {\nFUN16(&VAR26->VAR63);\nVAR18 = VAR26->VAR64;\nif (VAR18)\nVAR18 = FUN14(\nVAR18, &VAR16,\n&VAR21);\nelse\nVAR21 = VAR58;\nFUN17(&VAR26->VAR63);\nif (VAR18 && VAR16 == VAR1) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR66;\n}\n}\nif (!VAR18) {\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR59;\n}\nVAR20->VAR67 = VAR18->VAR32;\nif (FUN18(VAR1->VAR68,\nVAR16->VAR68) < 0) {\nVAR21 = VAR47;\nVAR22 = -VAR69;\nVAR23 = VAR49;\ngoto VAR66;\n}\nFUN4(VAR1);\nVAR8 = FUN19(&VAR2->VAR70,\nstruct VAR71, VAR72);\nif (!(VAR3->VAR34 & VAR35) && VAR8 &&\nVAR8->VAR73 == VAR74) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR75;\n}\nif (!(VAR3->VAR34 & VAR35) && VAR2->VAR46) {\nstruct binder_transaction *VAR76;\nVAR76 = VAR2->VAR46;\nif (VAR76->VAR51 != VAR2) {\nFUN7(&VAR76->VAR52);\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR76->VAR32,\nVAR76->VAR53 ? VAR76->VAR53->VAR37 : 0,\nVAR76->VAR51 ?\nVAR76->VAR51->VAR37 : 0);\nFUN8(&VAR76->VAR52);\nFUN5(VAR1);\nVAR21 = VAR47;\nVAR22 = -VAR48;\nVAR23 = VAR49;\ngoto VAR54;\n}\nwhile (VAR76) {\nstruct binder_thread *VAR77;\nFUN7(&VAR76->VAR52);\nVAR77 = VAR76->VAR77;\nif (VAR77 && VAR77->VAR1 == VAR16) {\nFUN20(&VAR77->VAR60);\nVAR17 = VAR77;\nFUN8(&VAR76->VAR52);\nbreak;\n}\nFUN8(&VAR76->VAR52);\nVAR76 = VAR76->VAR78;\n}\n}\nFUN5(VAR1);\n}\nif (VAR17)\nVAR20->VAR51 = VAR17->VAR37;\nVAR20->VAR53 = VAR16->VAR37;\nVAR7 = FUN21(sizeof(*VAR7), VAR79);\nif (VAR7 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR81;\n}\nFUN22(&VAR7->VAR82);\nFUN23(VAR83);\nFUN24(&VAR7->VAR52);\nVAR9 = FUN21(sizeof(*VAR9), VAR79);\nif (VAR9 == NULL) {\nVAR21 = VAR47;\nVAR22 = -VAR80;\nVAR23 = VAR49;\ngoto VAR84;\n}\nFUN23(VAR85);\nVAR7->VAR32 = VAR27;\nif (VAR4)\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR17->VAR37,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nelse\nFUN25(VAR86,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR7->VAR32,\nVAR16->VAR37, VAR18->VAR32,\n(VAR87)VAR3->VAR88.VAR89.VAR90,\n(VAR87)VAR3->VAR88.VAR89.VAR91,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\n(VAR87)VAR5);\nif (!VAR4 && !(VAR3->VAR34 & VAR35))\nVAR7->VAR77 = VAR2;\nelse\nVAR7->VAR77 = NULL;\nVAR7->VAR92 = FUN26(VAR1->VAR68);\nVAR7->VAR53 = VAR16;\nVAR7->VAR51 = VAR17;\nVAR7->VAR93 = VAR3->VAR93;\nVAR7->VAR34 = VAR3->VAR34;\nVAR7->VAR94 = FUN27(VAR95);\nif (VAR18 && VAR18->VAR96) {\nu32 VAR97;\nsize_t VAR98;\nFUN28(VAR1->VAR68, &VAR97);\nVAR6 = FUN29(VAR97, &VAR29, &VAR30);\nif (VAR6) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR99;\n}\nVAR98 = FUN30(VAR30, sizeof(VAR87));\nVAR5 += VAR98;\nif (VAR5 < VAR98) {\nVAR21 = VAR47;\nVAR22 = VAR65;\nVAR23 = VAR49;\ngoto VAR100;\n}\n}\nFUN31(VAR4, VAR7, VAR18);\nVAR7->VAR90 = FUN32(&VAR16->VAR101, VAR3->VAR42,\nVAR3->VAR43, VAR5,\n!VAR4 && (VAR7->VAR34 & VAR35));\nif (FUN33(VAR7->VAR90)) {\nVAR22 = FUN34(VAR7->VAR90);\nVAR21 = VAR22 == -VAR102 ?\nVAR58 : VAR47;\nVAR23 = VAR49;\nVAR7->VAR90 = NULL;\ngoto VAR103;\n}\nif (VAR29) {\nint VAR104;\nsize_t VAR105 = FUN30(VAR3->VAR42, sizeof(void *)) +\nFUN30(VAR3->VAR43, sizeof(void *)) +\nFUN30(VAR5, sizeof(void *)) -\nFUN30(VAR30, sizeof(VAR87));\nVAR7->VAR106 = (VAR107)VAR7->VAR90->VAR108 + VAR105;\nVAR104 = FUN35(&VAR16->VAR101,\nVAR7->VAR90, VAR105,\nVAR29, VAR30);\nif (VAR104) {\nVAR7->VAR106 = 0;\nFUN36(1);\n}\nFUN37(VAR29, VAR30);\nVAR29 = NULL;\n}\nVAR7->VAR90->VAR32 = VAR7->VAR32;\nVAR7->VAR90->VAR109 = VAR7;\nVAR7->VAR90->VAR18 = VAR18;\nFUN38(VAR7->VAR90);\nif (FUN39(\n&VAR16->VAR101,\nVAR7->VAR90, 0,\n(const void VAR110 *)\n(VAR107)VAR3->VAR88.VAR89.VAR90,\nVAR3->VAR42)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR111;\nVAR23 = VAR49;\ngoto VAR112;\n}\nif (FUN39(\n&VAR16->VAR101,\nVAR7->VAR90,\nFUN30(VAR3->VAR42, sizeof(void *)),\n(const void VAR110 *)\n(VAR107)VAR3->VAR88.VAR89.VAR91,\nVAR3->VAR43)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR111;\nVAR23 = VAR49;\ngoto VAR112;\n}\nif (!FUN40(VAR3->VAR43, sizeof(VAR113))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, (VAR87)VAR3->VAR43);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nif (!FUN40(VAR5, sizeof(VAR87))) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR5);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nVAR11 = FUN30(VAR3->VAR42, sizeof(void *));\nVAR10 = VAR11;\nVAR12 = VAR11 + VAR3->VAR43;\nVAR14 = FUN30(VAR12, sizeof(void *));\nVAR15 = VAR14 + VAR5;\nVAR13 = 0;\nfor (VAR10 = VAR11; VAR10 < VAR12;\nVAR10 += sizeof(VAR113)) {\nstruct binder_object_header *VAR115;\nsize_t VAR116;\nstruct binder_object VAR117;\nbinder_size_t VAR118;\nif (FUN41(&VAR16->VAR101,\n&VAR118,\nVAR7->VAR90,\nVAR10,\nsizeof(VAR118))) {\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nVAR116 = FUN42(VAR16, VAR7->VAR90,\nVAR118, &VAR117);\nif (VAR116 == 0 || VAR118 < VAR13) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37,\n(VAR87)VAR118,\n(VAR87)VAR13,\n(VAR87)VAR7->VAR90->VAR42);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nVAR115 = &VAR117.VAR115;\nVAR13 = VAR118 + VAR116;\nswitch (VAR115->VAR73) {\ncase VAR119:\ncase VAR120: {\nstruct flat_binder_object *VAR121;\nVAR121 = FUN43(VAR115);\nVAR6 = FUN44(VAR121, VAR7, VAR2);\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR121, sizeof(*VAR121))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\n} break;\ncase VAR123:\ncase VAR124: {\nstruct flat_binder_object *VAR121;\nVAR121 = FUN43(VAR115);\nVAR6 = FUN45(VAR121, VAR7, VAR2);\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR121, sizeof(*VAR121))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\n} break;\ncase VAR125: {\nstruct binder_fd_object *VAR121 = FUN46(VAR115);\nbinder_size_t VAR126 = VAR118 +\n(VAR107)&VAR121->VAR127 - (VAR107)VAR121;\nint VAR6 = FUN47(VAR121->VAR127, VAR126, VAR7,\nVAR2, VAR19);\nVAR121->VAR128 = 0;\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR121, sizeof(*VAR121))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\n} break;\ncase VAR129: {\nstruct binder_object VAR130;\nbinder_size_t VAR131;\nstruct binder_fd_array_object *VAR132 =\nFUN48(VAR115);\nsize_t VAR133 = (VAR10 - VAR11) *\nsizeof(VAR113);\nstruct binder_buffer_object *VAR134 =\nFUN49(VAR16, VAR7->VAR90,\n&VAR130, VAR132->VAR134,\nVAR11,\n&VAR131,\nVAR133);\nif (!VAR134) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR135;\n}\nif (!FUN50(VAR16, VAR7->VAR90,\nVAR11,\nVAR131,\nVAR132->VAR131,\nVAR24,\nVAR25)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR135;\n}\nVAR6 = FUN51(VAR132, VAR134, VAR7, VAR2,\nVAR19);\nif (VAR6 < 0) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\nVAR24 = VAR131;\nVAR25 =\nVAR132->VAR131 + sizeof(VAR136) * VAR132->VAR137;\n} break;\ncase VAR138: {\nstruct binder_buffer_object *VAR139 =\nFUN52(VAR115);\nsize_t VAR140 = VAR15 - VAR14;\nsize_t VAR133;\nif (VAR139->VAR141 > VAR140) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR114;\n}\nif (FUN39(\n&VAR16->VAR101,\nVAR7->VAR90,\nVAR14,\n(const void VAR110 *)\n(VAR107)VAR139->VAR90,\nVAR139->VAR141)) {\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37);\nVAR22 = -VAR111;\nVAR21 = VAR47;\nVAR23 = VAR49;\ngoto VAR112;\n}\nVAR139->VAR90 = (VAR107)\nVAR7->VAR90->VAR108 + VAR14;\nVAR14 += FUN30(VAR139->VAR141, sizeof(VAR87));\nVAR133 = (VAR10 - VAR11) *\nsizeof(VAR113);\nVAR6 = FUN53(VAR7, VAR2, VAR139,\nVAR11,\nVAR133,\nVAR24,\nVAR25);\nif (VAR6 < 0 ||\nFUN35(&VAR16->VAR101,\nVAR7->VAR90,\nVAR118,\nVAR139, sizeof(*VAR139))) {\nVAR21 = VAR47;\nVAR22 = VAR6;\nVAR23 = VAR49;\ngoto VAR122;\n}\nVAR24 = VAR118;\nVAR25 = 0;\n} break;\ndefault:\nFUN6(\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR115->VAR73);\nVAR21 = VAR47;\nVAR22 = -VAR65;\nVAR23 = VAR49;\ngoto VAR142;\n}\n}\nVAR9->VAR73 = VAR143;\nVAR7->VAR144.VAR73 = VAR74;\nif (VAR4) {\nFUN54(VAR2, VAR9);\nFUN4(VAR16);\nif (VAR17->VAR145) {\nFUN5(VAR16);\ngoto VAR146;\n}\nFUN55(VAR7->VAR90->VAR147 != 0);\nFUN56(VAR17, VAR19);\nFUN57(VAR17, &VAR7->VAR144);\nFUN5(VAR16);\nFUN58(&VAR17->VAR148);\nFUN59(VAR19);\n} else if (!(VAR7->VAR34 & VAR35)) {\nFUN55(VAR7->VAR90->VAR147 != 0);\nFUN4(VAR1);\nFUN60(VAR2, VAR9);\nVAR7->VAR149 = 1;\nVAR7->VAR78 = VAR2->VAR46;\nVAR2->VAR46 = VAR7;\nFUN5(VAR1);\nif (!FUN61(VAR7, VAR16, VAR17)) {\nFUN4(VAR1);\nFUN56(VAR2, VAR7);\nFUN5(VAR1);\ngoto VAR146;\n}\n} else {\nFUN55(VAR18 == NULL);\nFUN55(VAR7->VAR90->VAR147 != 1);\nFUN54(VAR2, VAR9);\nif (!FUN61(VAR7, VAR16, NULL))\ngoto VAR146;\n}\nif (VAR17)\nFUN62(VAR17);\nFUN63(VAR16);\nif (VAR18)\nFUN64(VAR18);\nFUN65();\nFUN66(VAR20->VAR150, VAR27);\nreturn;\nVAR146:\nVAR21 = VAR58;\nVAR23 = VAR49;\nFUN67(VAR1, VAR9);\nVAR122:\nVAR142:\nVAR114:\nVAR135:\nVAR112:\nFUN68(VAR7);\nFUN69(VAR7->VAR90);\nFUN70(VAR16, VAR7->VAR90,\nVAR10, true);\nif (VAR18)\nFUN64(VAR18);\nVAR18 = NULL;\nVAR7->VAR90->VAR109 = NULL;\nFUN71(&VAR16->VAR101, VAR7->VAR90);\nVAR103:\nVAR100:\nif (VAR29)\nFUN37(VAR29, VAR30);\nVAR99:\nFUN72(VAR9);\nFUN73(VAR85);\nVAR84:\nFUN72(VAR7);\nFUN73(VAR83);\nVAR81:\nVAR75:\nVAR54:\nVAR50:\nVAR59:\nVAR66:\nif (VAR17)\nFUN62(VAR17);\nif (VAR16)\nFUN63(VAR16);\nif (VAR18) {\nFUN74(VAR18, 1, 0);\nFUN64(VAR18);\n}\nFUN25(VAR151,\n\"STR\",\nVAR1->VAR37, VAR2->VAR37, VAR21, VAR22,\n(VAR87)VAR3->VAR42, (VAR87)VAR3->VAR43,\nVAR23);\n{\nstruct binder_transaction_log_entry *VAR152;\nVAR20->VAR21 = VAR21;\nVAR20->VAR22 = VAR22;\nVAR20->VAR23 = VAR23;\nVAR152 = FUN3(&VAR153);\n*VAR152 = *VAR20;\nFUN65();\nFUN66(VAR20->VAR150, VAR27);\nFUN66(VAR152->VAR150, VAR27);\n}\nFUN55(VAR2->VAR21.VAR154 != VAR155);\nif (VAR19) {\nVAR2->VAR21.VAR154 = VAR156;\nFUN54(VAR2, &VAR2->VAR21.VAR144);\nFUN75(VAR19, VAR21);\n} else {\nVAR2->VAR21.VAR154 = VAR21;\nFUN54(VAR2, &VAR2->VAR21.VAR144);\n}\n}\n",
      "code_after_change_raw": "static void binder_transaction(struct binder_proc *proc,\nstruct binder_thread *thread,\nstruct binder_transaction_data *tr, int reply,\nbinder_size_t extra_buffers_size)\n{\nint ret;\nstruct binder_transaction *t;\nstruct binder_work *w;\nstruct binder_work *tcomplete;\nbinder_size_t buffer_offset = 0;\nbinder_size_t off_start_offset, off_end_offset;\nbinder_size_t off_min;\nbinder_size_t sg_buf_offset, sg_buf_end_offset;\nstruct binder_proc *target_proc = NULL;\nstruct binder_thread *target_thread = NULL;\nstruct binder_node *target_node = NULL;\nstruct binder_transaction *in_reply_to = NULL;\nstruct binder_transaction_log_entry *e;\nuint32_t return_error = 0;\nuint32_t return_error_param = 0;\nuint32_t return_error_line = 0;\nbinder_size_t last_fixup_obj_off = 0;\nbinder_size_t last_fixup_min_off = 0;\nstruct binder_context *context = proc->context;\nint t_debug_id = atomic_inc_return(&binder_last_id);\nchar *secctx = NULL;\nu32 secctx_sz = 0;\ne = binder_transaction_log_add(&binder_transaction_log);\ne->debug_id = t_debug_id;\ne->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\ne->from_proc = proc->pid;\ne->from_thread = thread->pid;\ne->target_handle = tr->target.handle;\ne->data_size = tr->data_size;\ne->offsets_size = tr->offsets_size;\ne->context_name = proc->context->name;\nif (reply) {\nbinder_inner_proc_lock(proc);\nin_reply_to = thread->transaction_stack;\nif (in_reply_to == NULL) {\nbinder_inner_proc_unlock(proc);\nbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_empty_call_stack;\n}\nif (in_reply_to->to_thread != thread) {\nspin_lock(&in_reply_to->lock);\nbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, in_reply_to->debug_id,\nin_reply_to->to_proc ?\nin_reply_to->to_proc->pid : 0,\nin_reply_to->to_thread ?\nin_reply_to->to_thread->pid : 0);\nspin_unlock(&in_reply_to->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ngoto err_bad_call_stack;\n}\nthread->transaction_stack = in_reply_to->to_parent;\nbinder_inner_proc_unlock(proc);\nbinder_set_nice(in_reply_to->saved_priority);\ntarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\nif (target_thread == NULL) {\n__release(&target_thread->proc->inner_lock);\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\nif (target_thread->transaction_stack != in_reply_to) {\nbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\nproc->pid, thread->pid,\ntarget_thread->transaction_stack ?\ntarget_thread->transaction_stack->debug_id : 0,\nin_reply_to->debug_id);\nbinder_inner_proc_unlock(target_thread->proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ntarget_thread = NULL;\ngoto err_dead_binder;\n}\ntarget_proc = target_thread->proc;\ntarget_proc->tmp_ref++;\nbinder_inner_proc_unlock(target_thread->proc);\n} else {\nif (tr->target.handle) {\nstruct binder_ref *ref;\nbinder_proc_lock(proc);\nref = binder_get_ref_olocked(proc, tr->target.handle,\ntrue);\nif (ref) {\ntarget_node = binder_get_node_refs_for_txn(\nref->node, &target_proc,\n&return_error);\n} else {\nbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\n}\nbinder_proc_unlock(proc);\n} else {\nmutex_lock(&context->context_mgr_node_lock);\ntarget_node = context->binder_context_mgr_node;\nif (target_node)\ntarget_node = binder_get_node_refs_for_txn(\ntarget_node, &target_proc,\n&return_error);\nelse\nreturn_error = BR_DEAD_REPLY;\nmutex_unlock(&context->context_mgr_node_lock);\nif (target_node && target_proc == proc) {\nbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\n}\nif (!target_node) {\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\ne->to_node = target_node->debug_id;\nif (security_binder_transaction(proc->tsk,\ntarget_proc->tsk) < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPERM;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\nbinder_inner_proc_lock(proc);\nw = list_first_entry_or_null(&thread->todo,\nstruct binder_work, entry);\nif (!(tr->flags & TF_ONE_WAY) && w &&\nw->type == BINDER_WORK_TRANSACTION) {\nbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\nproc->pid, thread->pid);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_todo_list;\n}\nif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\nstruct binder_transaction *tmp;\ntmp = thread->transaction_stack;\nif (tmp->to_thread != thread) {\nspin_lock(&tmp->lock);\nbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, tmp->debug_id,\ntmp->to_proc ? tmp->to_proc->pid : 0,\ntmp->to_thread ?\ntmp->to_thread->pid : 0);\nspin_unlock(&tmp->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_call_stack;\n}\nwhile (tmp) {\nstruct binder_thread *from;\nspin_lock(&tmp->lock);\nfrom = tmp->from;\nif (from && from->proc == target_proc) {\natomic_inc(&from->tmp_ref);\ntarget_thread = from;\nspin_unlock(&tmp->lock);\nbreak;\n}\nspin_unlock(&tmp->lock);\ntmp = tmp->from_parent;\n}\n}\nbinder_inner_proc_unlock(proc);\n}\nif (target_thread)\ne->to_thread = target_thread->pid;\ne->to_proc = target_proc->pid;\nt = kzalloc(sizeof(*t), GFP_KERNEL);\nif (t == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_t_failed;\n}\nINIT_LIST_HEAD(&t->fd_fixups);\nbinder_stats_created(BINDER_STAT_TRANSACTION);\nspin_lock_init(&t->lock);\ntcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\nif (tcomplete == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_tcomplete_failed;\n}\nbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\nt->debug_id = t_debug_id;\nif (reply)\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_thread->pid,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nelse\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_node->debug_id,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nif (!reply && !(tr->flags & TF_ONE_WAY))\nt->from = thread;\nelse\nt->from = NULL;\nt->sender_euid = task_euid(proc->tsk);\nt->to_proc = target_proc;\nt->to_thread = target_thread;\nt->code = tr->code;\nt->flags = tr->flags;\nt->priority = task_nice(current);\nif (target_node && target_node->txn_security_ctx) {\nu32 secid;\nsize_t added_size;\nsecurity_task_getsecid(proc->tsk, &secid);\nret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\nif (ret) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_get_secctx_failed;\n}\nadded_size = ALIGN(secctx_sz, sizeof(u64));\nextra_buffers_size += added_size;\nif (extra_buffers_size < added_size) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_extra_size;\n}\n}\ntrace_binder_transaction(reply, t, target_node);\nt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\ntr->offsets_size, extra_buffers_size,\n!reply && (t->flags & TF_ONE_WAY));\nif (IS_ERR(t->buffer)) {\nreturn_error_param = PTR_ERR(t->buffer);\nreturn_error = return_error_param == -ESRCH ?\nBR_DEAD_REPLY : BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\nt->buffer = NULL;\ngoto err_binder_alloc_buf_failed;\n}\nif (secctx) {\nint err;\nsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\nALIGN(tr->offsets_size, sizeof(void *)) +\nALIGN(extra_buffers_size, sizeof(void *)) -\nALIGN(secctx_sz, sizeof(u64));\nt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\nerr = binder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, buf_offset,\nsecctx, secctx_sz);\nif (err) {\nt->security_ctx = 0;\nWARN_ON(1);\n}\nsecurity_release_secctx(secctx, secctx_sz);\nsecctx = NULL;\n}\nt->buffer->debug_id = t->debug_id;\nt->buffer->transaction = t;\nt->buffer->target_node = target_node;\ntrace_binder_transaction_alloc_buf(t->buffer);\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer, 0,\n(const void __user *)\n(uintptr_t)tr->data.ptr.buffer,\ntr->data_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nALIGN(tr->data_size, sizeof(void *)),\n(const void __user *)\n(uintptr_t)tr->data.ptr.offsets,\ntr->offsets_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\nproc->pid, thread->pid, (u64)tr->offsets_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\nbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\nproc->pid, thread->pid,\n(u64)extra_buffers_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\noff_start_offset = ALIGN(tr->data_size, sizeof(void *));\nbuffer_offset = off_start_offset;\noff_end_offset = off_start_offset + tr->offsets_size;\nsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\nsg_buf_end_offset = sg_buf_offset + extra_buffers_size -\nALIGN(secctx_sz, sizeof(u64));\noff_min = 0;\nfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\nbuffer_offset += sizeof(binder_size_t)) {\nstruct binder_object_header *hdr;\nsize_t object_size;\nstruct binder_object object;\nbinder_size_t object_offset;\nif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n&object_offset,\nt->buffer,\nbuffer_offset,\nsizeof(object_offset))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nobject_size = binder_get_object(target_proc, t->buffer,\nobject_offset, &object);\nif (object_size == 0 || object_offset < off_min) {\nbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\nproc->pid, thread->pid,\n(u64)object_offset,\n(u64)off_min,\n(u64)t->buffer->data_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nhdr = &object.hdr;\noff_min = object_offset + object_size;\nswitch (hdr->type) {\ncase BINDER_TYPE_BINDER:\ncase BINDER_TYPE_WEAK_BINDER: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_binder(fp, t, thread);\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nfp, sizeof(*fp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\n} break;\ncase BINDER_TYPE_HANDLE:\ncase BINDER_TYPE_WEAK_HANDLE: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_handle(fp, t, thread);\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nfp, sizeof(*fp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\n} break;\ncase BINDER_TYPE_FD: {\nstruct binder_fd_object *fp = to_binder_fd_object(hdr);\nbinder_size_t fd_offset = object_offset +\n(uintptr_t)&fp->fd - (uintptr_t)fp;\nint ret = binder_translate_fd(fp->fd, fd_offset, t,\nthread, in_reply_to);\nfp->pad_binder = 0;\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nfp, sizeof(*fp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\n} break;\ncase BINDER_TYPE_FDA: {\nstruct binder_object ptr_object;\nbinder_size_t parent_offset;\nstruct binder_fd_array_object *fda =\nto_binder_fd_array_object(hdr);\nsize_t num_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nstruct binder_buffer_object *parent =\nbinder_validate_ptr(target_proc, t->buffer,\n&ptr_object, fda->parent,\noff_start_offset,\n&parent_offset,\nnum_valid);\nif (!parent) {\nbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nif (!binder_validate_fixup(target_proc, t->buffer,\noff_start_offset,\nparent_offset,\nfda->parent_offset,\nlast_fixup_obj_off,\nlast_fixup_min_off)) {\nbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nret = binder_translate_fd_array(fda, parent, t, thread,\nin_reply_to);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nlast_fixup_obj_off = parent_offset;\nlast_fixup_min_off =\nfda->parent_offset + sizeof(u32) * fda->num_fds;\n} break;\ncase BINDER_TYPE_PTR: {\nstruct binder_buffer_object *bp =\nto_binder_buffer_object(hdr);\nsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\nsize_t num_valid;\nif (bp->length > buf_left) {\nbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nsg_buf_offset,\n(const void __user *)\n(uintptr_t)bp->buffer,\nbp->length)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error_param = -EFAULT;\nreturn_error = BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nbp->buffer = (uintptr_t)\nt->buffer->user_data + sg_buf_offset;\nsg_buf_offset += ALIGN(bp->length, sizeof(u64));\nnum_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nret = binder_fixup_parent(t, thread, bp,\noff_start_offset,\nnum_valid,\nlast_fixup_obj_off,\nlast_fixup_min_off);\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nbp, sizeof(*bp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nlast_fixup_obj_off = object_offset;\nlast_fixup_min_off = 0;\n} break;\ndefault:\nbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\nproc->pid, thread->pid, hdr->type);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_object_type;\n}\n}\ntcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\nt->work.type = BINDER_WORK_TRANSACTION;\nif (reply) {\nbinder_enqueue_thread_work(thread, tcomplete);\nbinder_inner_proc_lock(target_proc);\nif (target_thread->is_dead) {\nbinder_inner_proc_unlock(target_proc);\ngoto err_dead_proc_or_thread;\n}\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_pop_transaction_ilocked(target_thread, in_reply_to);\nbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\nbinder_inner_proc_unlock(target_proc);\nwake_up_interruptible_sync(&target_thread->wait);\nbinder_free_transaction(in_reply_to);\n} else if (!(t->flags & TF_ONE_WAY)) {\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_inner_proc_lock(proc);\nbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\nt->need_reply = 1;\nt->from_parent = thread->transaction_stack;\nthread->transaction_stack = t;\nbinder_inner_proc_unlock(proc);\nif (!binder_proc_transaction(t, target_proc, target_thread)) {\nbinder_inner_proc_lock(proc);\nbinder_pop_transaction_ilocked(thread, t);\nbinder_inner_proc_unlock(proc);\ngoto err_dead_proc_or_thread;\n}\n} else {\nBUG_ON(target_node == NULL);\nBUG_ON(t->buffer->async_transaction != 1);\nbinder_enqueue_thread_work(thread, tcomplete);\nif (!binder_proc_transaction(t, target_proc, NULL))\ngoto err_dead_proc_or_thread;\n}\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nbinder_proc_dec_tmpref(target_proc);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nreturn;\nerr_dead_proc_or_thread:\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\nbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\nbinder_free_txn_fixups(t);\ntrace_binder_transaction_failed_buffer_release(t->buffer);\nbinder_transaction_buffer_release(target_proc, t->buffer,\nbuffer_offset, true);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\ntarget_node = NULL;\nt->buffer->transaction = NULL;\nbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\nif (secctx)\nsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\nkfree(tcomplete);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\nkfree(t);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nif (target_proc)\nbinder_proc_dec_tmpref(target_proc);\nif (target_node) {\nbinder_dec_node(target_node, 1, 0);\nbinder_dec_node_tmpref(target_node);\n}\nbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\nproc->pid, thread->pid, return_error, return_error_param,\n(u64)tr->data_size, (u64)tr->offsets_size,\nreturn_error_line);\n{\nstruct binder_transaction_log_entry *fe;\ne->return_error = return_error;\ne->return_error_param = return_error_param;\ne->return_error_line = return_error_line;\nfe = binder_transaction_log_add(&binder_transaction_log_failed);\n*fe = *e;\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nWRITE_ONCE(fe->debug_id_done, t_debug_id);\n}\nBUG_ON(thread->return_error.cmd != BR_OK);\nif (in_reply_to) {\nthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\nbinder_send_failed_reply(in_reply_to, return_error);\n} else {\nthread->return_error.cmd = return_error;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\n}\n}\n",
      "code_before_change_raw": "static void binder_transaction(struct binder_proc *proc,\nstruct binder_thread *thread,\nstruct binder_transaction_data *tr, int reply,\nbinder_size_t extra_buffers_size)\n{\nint ret;\nstruct binder_transaction *t;\nstruct binder_work *w;\nstruct binder_work *tcomplete;\nbinder_size_t buffer_offset = 0;\nbinder_size_t off_start_offset, off_end_offset;\nbinder_size_t off_min;\nbinder_size_t sg_buf_offset, sg_buf_end_offset;\nstruct binder_proc *target_proc = NULL;\nstruct binder_thread *target_thread = NULL;\nstruct binder_node *target_node = NULL;\nstruct binder_transaction *in_reply_to = NULL;\nstruct binder_transaction_log_entry *e;\nuint32_t return_error = 0;\nuint32_t return_error_param = 0;\nuint32_t return_error_line = 0;\nbinder_size_t last_fixup_obj_off = 0;\nbinder_size_t last_fixup_min_off = 0;\nstruct binder_context *context = proc->context;\nint t_debug_id = atomic_inc_return(&binder_last_id);\nchar *secctx = NULL;\nu32 secctx_sz = 0;\ne = binder_transaction_log_add(&binder_transaction_log);\ne->debug_id = t_debug_id;\ne->call_type = reply ? 2 : !!(tr->flags & TF_ONE_WAY);\ne->from_proc = proc->pid;\ne->from_thread = thread->pid;\ne->target_handle = tr->target.handle;\ne->data_size = tr->data_size;\ne->offsets_size = tr->offsets_size;\ne->context_name = proc->context->name;\nif (reply) {\nbinder_inner_proc_lock(proc);\nin_reply_to = thread->transaction_stack;\nif (in_reply_to == NULL) {\nbinder_inner_proc_unlock(proc);\nbinder_user_error(\"%d:%d got reply transaction with no transaction stack\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_empty_call_stack;\n}\nif (in_reply_to->to_thread != thread) {\nspin_lock(&in_reply_to->lock);\nbinder_user_error(\"%d:%d got reply transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, in_reply_to->debug_id,\nin_reply_to->to_proc ?\nin_reply_to->to_proc->pid : 0,\nin_reply_to->to_thread ?\nin_reply_to->to_thread->pid : 0);\nspin_unlock(&in_reply_to->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ngoto err_bad_call_stack;\n}\nthread->transaction_stack = in_reply_to->to_parent;\nbinder_inner_proc_unlock(proc);\nbinder_set_nice(in_reply_to->saved_priority);\ntarget_thread = binder_get_txn_from_and_acq_inner(in_reply_to);\nif (target_thread == NULL) {\n__release(&target_thread->proc->inner_lock);\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\nif (target_thread->transaction_stack != in_reply_to) {\nbinder_user_error(\"%d:%d got reply transaction with bad target transaction stack %d, expected %d\\n\",\nproc->pid, thread->pid,\ntarget_thread->transaction_stack ?\ntarget_thread->transaction_stack->debug_id : 0,\nin_reply_to->debug_id);\nbinder_inner_proc_unlock(target_thread->proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\nin_reply_to = NULL;\ntarget_thread = NULL;\ngoto err_dead_binder;\n}\ntarget_proc = target_thread->proc;\ntarget_proc->tmp_ref++;\nbinder_inner_proc_unlock(target_thread->proc);\n} else {\nif (tr->target.handle) {\nstruct binder_ref *ref;\nbinder_proc_lock(proc);\nref = binder_get_ref_olocked(proc, tr->target.handle,\ntrue);\nif (ref) {\ntarget_node = binder_get_node_refs_for_txn(\nref->node, &target_proc,\n&return_error);\n} else {\nbinder_user_error(\"%d:%d got transaction to invalid handle\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\n}\nbinder_proc_unlock(proc);\n} else {\nmutex_lock(&context->context_mgr_node_lock);\ntarget_node = context->binder_context_mgr_node;\nif (target_node)\ntarget_node = binder_get_node_refs_for_txn(\ntarget_node, &target_proc,\n&return_error);\nelse\nreturn_error = BR_DEAD_REPLY;\nmutex_unlock(&context->context_mgr_node_lock);\nif (target_node && target_proc == proc) {\nbinder_user_error(\"%d:%d got transaction to context manager from process owning it\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\n}\nif (!target_node) {\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_dead_binder;\n}\ne->to_node = target_node->debug_id;\nif (security_binder_transaction(proc->tsk,\ntarget_proc->tsk) < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPERM;\nreturn_error_line = __LINE__;\ngoto err_invalid_target_handle;\n}\nbinder_inner_proc_lock(proc);\nw = list_first_entry_or_null(&thread->todo,\nstruct binder_work, entry);\nif (!(tr->flags & TF_ONE_WAY) && w &&\nw->type == BINDER_WORK_TRANSACTION) {\nbinder_user_error(\"%d:%d new transaction not allowed when there is a transaction on thread todo\\n\",\nproc->pid, thread->pid);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_todo_list;\n}\nif (!(tr->flags & TF_ONE_WAY) && thread->transaction_stack) {\nstruct binder_transaction *tmp;\ntmp = thread->transaction_stack;\nif (tmp->to_thread != thread) {\nspin_lock(&tmp->lock);\nbinder_user_error(\"%d:%d got new transaction with bad transaction stack, transaction %d has target %d:%d\\n\",\nproc->pid, thread->pid, tmp->debug_id,\ntmp->to_proc ? tmp->to_proc->pid : 0,\ntmp->to_thread ?\ntmp->to_thread->pid : 0);\nspin_unlock(&tmp->lock);\nbinder_inner_proc_unlock(proc);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EPROTO;\nreturn_error_line = __LINE__;\ngoto err_bad_call_stack;\n}\nwhile (tmp) {\nstruct binder_thread *from;\nspin_lock(&tmp->lock);\nfrom = tmp->from;\nif (from && from->proc == target_proc) {\natomic_inc(&from->tmp_ref);\ntarget_thread = from;\nspin_unlock(&tmp->lock);\nbreak;\n}\nspin_unlock(&tmp->lock);\ntmp = tmp->from_parent;\n}\n}\nbinder_inner_proc_unlock(proc);\n}\nif (target_thread)\ne->to_thread = target_thread->pid;\ne->to_proc = target_proc->pid;\nt = kzalloc(sizeof(*t), GFP_KERNEL);\nif (t == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_t_failed;\n}\nINIT_LIST_HEAD(&t->fd_fixups);\nbinder_stats_created(BINDER_STAT_TRANSACTION);\nspin_lock_init(&t->lock);\ntcomplete = kzalloc(sizeof(*tcomplete), GFP_KERNEL);\nif (tcomplete == NULL) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -ENOMEM;\nreturn_error_line = __LINE__;\ngoto err_alloc_tcomplete_failed;\n}\nbinder_stats_created(BINDER_STAT_TRANSACTION_COMPLETE);\nt->debug_id = t_debug_id;\nif (reply)\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_REPLY %d -> %d:%d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_thread->pid,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nelse\nbinder_debug(BINDER_DEBUG_TRANSACTION,\n\"%d:%d BC_TRANSACTION %d -> %d - node %d, data %016llx-%016llx size %lld-%lld-%lld\\n\",\nproc->pid, thread->pid, t->debug_id,\ntarget_proc->pid, target_node->debug_id,\n(u64)tr->data.ptr.buffer,\n(u64)tr->data.ptr.offsets,\n(u64)tr->data_size, (u64)tr->offsets_size,\n(u64)extra_buffers_size);\nif (!reply && !(tr->flags & TF_ONE_WAY))\nt->from = thread;\nelse\nt->from = NULL;\nt->sender_euid = task_euid(proc->tsk);\nt->to_proc = target_proc;\nt->to_thread = target_thread;\nt->code = tr->code;\nt->flags = tr->flags;\nt->priority = task_nice(current);\nif (target_node && target_node->txn_security_ctx) {\nu32 secid;\nsize_t added_size;\nsecurity_task_getsecid(proc->tsk, &secid);\nret = security_secid_to_secctx(secid, &secctx, &secctx_sz);\nif (ret) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_get_secctx_failed;\n}\nadded_size = ALIGN(secctx_sz, sizeof(u64));\nextra_buffers_size += added_size;\nif (extra_buffers_size < added_size) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_extra_size;\n}\n}\ntrace_binder_transaction(reply, t, target_node);\nt->buffer = binder_alloc_new_buf(&target_proc->alloc, tr->data_size,\ntr->offsets_size, extra_buffers_size,\n!reply && (t->flags & TF_ONE_WAY));\nif (IS_ERR(t->buffer)) {\nreturn_error_param = PTR_ERR(t->buffer);\nreturn_error = return_error_param == -ESRCH ?\nBR_DEAD_REPLY : BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\nt->buffer = NULL;\ngoto err_binder_alloc_buf_failed;\n}\nif (secctx) {\nint err;\nsize_t buf_offset = ALIGN(tr->data_size, sizeof(void *)) +\nALIGN(tr->offsets_size, sizeof(void *)) +\nALIGN(extra_buffers_size, sizeof(void *)) -\nALIGN(secctx_sz, sizeof(u64));\nt->security_ctx = (uintptr_t)t->buffer->user_data + buf_offset;\nerr = binder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer, buf_offset,\nsecctx, secctx_sz);\nif (err) {\nt->security_ctx = 0;\nWARN_ON(1);\n}\nsecurity_release_secctx(secctx, secctx_sz);\nsecctx = NULL;\n}\nt->buffer->debug_id = t->debug_id;\nt->buffer->transaction = t;\nt->buffer->target_node = target_node;\ntrace_binder_transaction_alloc_buf(t->buffer);\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer, 0,\n(const void __user *)\n(uintptr_t)tr->data.ptr.buffer,\ntr->data_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid data ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nALIGN(tr->data_size, sizeof(void *)),\n(const void __user *)\n(uintptr_t)tr->data.ptr.offsets,\ntr->offsets_size)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EFAULT;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nif (!IS_ALIGNED(tr->offsets_size, sizeof(binder_size_t))) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets size, %lld\\n\",\nproc->pid, thread->pid, (u64)tr->offsets_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (!IS_ALIGNED(extra_buffers_size, sizeof(u64))) {\nbinder_user_error(\"%d:%d got transaction with unaligned buffers size, %lld\\n\",\nproc->pid, thread->pid,\n(u64)extra_buffers_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\noff_start_offset = ALIGN(tr->data_size, sizeof(void *));\nbuffer_offset = off_start_offset;\noff_end_offset = off_start_offset + tr->offsets_size;\nsg_buf_offset = ALIGN(off_end_offset, sizeof(void *));\nsg_buf_end_offset = sg_buf_offset + extra_buffers_size;\noff_min = 0;\nfor (buffer_offset = off_start_offset; buffer_offset < off_end_offset;\nbuffer_offset += sizeof(binder_size_t)) {\nstruct binder_object_header *hdr;\nsize_t object_size;\nstruct binder_object object;\nbinder_size_t object_offset;\nif (binder_alloc_copy_from_buffer(&target_proc->alloc,\n&object_offset,\nt->buffer,\nbuffer_offset,\nsizeof(object_offset))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nobject_size = binder_get_object(target_proc, t->buffer,\nobject_offset, &object);\nif (object_size == 0 || object_offset < off_min) {\nbinder_user_error(\"%d:%d got transaction with invalid offset (%lld, min %lld max %lld) or object.\\n\",\nproc->pid, thread->pid,\n(u64)object_offset,\n(u64)off_min,\n(u64)t->buffer->data_size);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nhdr = &object.hdr;\noff_min = object_offset + object_size;\nswitch (hdr->type) {\ncase BINDER_TYPE_BINDER:\ncase BINDER_TYPE_WEAK_BINDER: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_binder(fp, t, thread);\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nfp, sizeof(*fp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\n} break;\ncase BINDER_TYPE_HANDLE:\ncase BINDER_TYPE_WEAK_HANDLE: {\nstruct flat_binder_object *fp;\nfp = to_flat_binder_object(hdr);\nret = binder_translate_handle(fp, t, thread);\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nfp, sizeof(*fp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\n} break;\ncase BINDER_TYPE_FD: {\nstruct binder_fd_object *fp = to_binder_fd_object(hdr);\nbinder_size_t fd_offset = object_offset +\n(uintptr_t)&fp->fd - (uintptr_t)fp;\nint ret = binder_translate_fd(fp->fd, fd_offset, t,\nthread, in_reply_to);\nfp->pad_binder = 0;\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nfp, sizeof(*fp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\n} break;\ncase BINDER_TYPE_FDA: {\nstruct binder_object ptr_object;\nbinder_size_t parent_offset;\nstruct binder_fd_array_object *fda =\nto_binder_fd_array_object(hdr);\nsize_t num_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nstruct binder_buffer_object *parent =\nbinder_validate_ptr(target_proc, t->buffer,\n&ptr_object, fda->parent,\noff_start_offset,\n&parent_offset,\nnum_valid);\nif (!parent) {\nbinder_user_error(\"%d:%d got transaction with invalid parent offset or type\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nif (!binder_validate_fixup(target_proc, t->buffer,\noff_start_offset,\nparent_offset,\nfda->parent_offset,\nlast_fixup_obj_off,\nlast_fixup_min_off)) {\nbinder_user_error(\"%d:%d got transaction with out-of-order buffer fixup\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_parent;\n}\nret = binder_translate_fd_array(fda, parent, t, thread,\nin_reply_to);\nif (ret < 0) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nlast_fixup_obj_off = parent_offset;\nlast_fixup_min_off =\nfda->parent_offset + sizeof(u32) * fda->num_fds;\n} break;\ncase BINDER_TYPE_PTR: {\nstruct binder_buffer_object *bp =\nto_binder_buffer_object(hdr);\nsize_t buf_left = sg_buf_end_offset - sg_buf_offset;\nsize_t num_valid;\nif (bp->length > buf_left) {\nbinder_user_error(\"%d:%d got transaction with too large buffer\\n\",\nproc->pid, thread->pid);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_offset;\n}\nif (binder_alloc_copy_user_to_buffer(\n&target_proc->alloc,\nt->buffer,\nsg_buf_offset,\n(const void __user *)\n(uintptr_t)bp->buffer,\nbp->length)) {\nbinder_user_error(\"%d:%d got transaction with invalid offsets ptr\\n\",\nproc->pid, thread->pid);\nreturn_error_param = -EFAULT;\nreturn_error = BR_FAILED_REPLY;\nreturn_error_line = __LINE__;\ngoto err_copy_data_failed;\n}\nbp->buffer = (uintptr_t)\nt->buffer->user_data + sg_buf_offset;\nsg_buf_offset += ALIGN(bp->length, sizeof(u64));\nnum_valid = (buffer_offset - off_start_offset) *\nsizeof(binder_size_t);\nret = binder_fixup_parent(t, thread, bp,\noff_start_offset,\nnum_valid,\nlast_fixup_obj_off,\nlast_fixup_min_off);\nif (ret < 0 ||\nbinder_alloc_copy_to_buffer(&target_proc->alloc,\nt->buffer,\nobject_offset,\nbp, sizeof(*bp))) {\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = ret;\nreturn_error_line = __LINE__;\ngoto err_translate_failed;\n}\nlast_fixup_obj_off = object_offset;\nlast_fixup_min_off = 0;\n} break;\ndefault:\nbinder_user_error(\"%d:%d got transaction with invalid object type, %x\\n\",\nproc->pid, thread->pid, hdr->type);\nreturn_error = BR_FAILED_REPLY;\nreturn_error_param = -EINVAL;\nreturn_error_line = __LINE__;\ngoto err_bad_object_type;\n}\n}\ntcomplete->type = BINDER_WORK_TRANSACTION_COMPLETE;\nt->work.type = BINDER_WORK_TRANSACTION;\nif (reply) {\nbinder_enqueue_thread_work(thread, tcomplete);\nbinder_inner_proc_lock(target_proc);\nif (target_thread->is_dead) {\nbinder_inner_proc_unlock(target_proc);\ngoto err_dead_proc_or_thread;\n}\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_pop_transaction_ilocked(target_thread, in_reply_to);\nbinder_enqueue_thread_work_ilocked(target_thread, &t->work);\nbinder_inner_proc_unlock(target_proc);\nwake_up_interruptible_sync(&target_thread->wait);\nbinder_free_transaction(in_reply_to);\n} else if (!(t->flags & TF_ONE_WAY)) {\nBUG_ON(t->buffer->async_transaction != 0);\nbinder_inner_proc_lock(proc);\nbinder_enqueue_deferred_thread_work_ilocked(thread, tcomplete);\nt->need_reply = 1;\nt->from_parent = thread->transaction_stack;\nthread->transaction_stack = t;\nbinder_inner_proc_unlock(proc);\nif (!binder_proc_transaction(t, target_proc, target_thread)) {\nbinder_inner_proc_lock(proc);\nbinder_pop_transaction_ilocked(thread, t);\nbinder_inner_proc_unlock(proc);\ngoto err_dead_proc_or_thread;\n}\n} else {\nBUG_ON(target_node == NULL);\nBUG_ON(t->buffer->async_transaction != 1);\nbinder_enqueue_thread_work(thread, tcomplete);\nif (!binder_proc_transaction(t, target_proc, NULL))\ngoto err_dead_proc_or_thread;\n}\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nbinder_proc_dec_tmpref(target_proc);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nreturn;\nerr_dead_proc_or_thread:\nreturn_error = BR_DEAD_REPLY;\nreturn_error_line = __LINE__;\nbinder_dequeue_work(proc, tcomplete);\nerr_translate_failed:\nerr_bad_object_type:\nerr_bad_offset:\nerr_bad_parent:\nerr_copy_data_failed:\nbinder_free_txn_fixups(t);\ntrace_binder_transaction_failed_buffer_release(t->buffer);\nbinder_transaction_buffer_release(target_proc, t->buffer,\nbuffer_offset, true);\nif (target_node)\nbinder_dec_node_tmpref(target_node);\ntarget_node = NULL;\nt->buffer->transaction = NULL;\nbinder_alloc_free_buf(&target_proc->alloc, t->buffer);\nerr_binder_alloc_buf_failed:\nerr_bad_extra_size:\nif (secctx)\nsecurity_release_secctx(secctx, secctx_sz);\nerr_get_secctx_failed:\nkfree(tcomplete);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION_COMPLETE);\nerr_alloc_tcomplete_failed:\nkfree(t);\nbinder_stats_deleted(BINDER_STAT_TRANSACTION);\nerr_alloc_t_failed:\nerr_bad_todo_list:\nerr_bad_call_stack:\nerr_empty_call_stack:\nerr_dead_binder:\nerr_invalid_target_handle:\nif (target_thread)\nbinder_thread_dec_tmpref(target_thread);\nif (target_proc)\nbinder_proc_dec_tmpref(target_proc);\nif (target_node) {\nbinder_dec_node(target_node, 1, 0);\nbinder_dec_node_tmpref(target_node);\n}\nbinder_debug(BINDER_DEBUG_FAILED_TRANSACTION,\n\"%d:%d transaction failed %d/%d, size %lld-%lld line %d\\n\",\nproc->pid, thread->pid, return_error, return_error_param,\n(u64)tr->data_size, (u64)tr->offsets_size,\nreturn_error_line);\n{\nstruct binder_transaction_log_entry *fe;\ne->return_error = return_error;\ne->return_error_param = return_error_param;\ne->return_error_line = return_error_line;\nfe = binder_transaction_log_add(&binder_transaction_log_failed);\n*fe = *e;\nsmp_wmb();\nWRITE_ONCE(e->debug_id_done, t_debug_id);\nWRITE_ONCE(fe->debug_id_done, t_debug_id);\n}\nBUG_ON(thread->return_error.cmd != BR_OK);\nif (in_reply_to) {\nthread->return_error.cmd = BR_TRANSACTION_COMPLETE;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\nbinder_send_failed_reply(in_reply_to, return_error);\n} else {\nthread->return_error.cmd = return_error;\nbinder_enqueue_thread_work(thread, &thread->return_error.work);\n}\n}\n"
    }
  ],
  "CVE-2019-3701": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation checks on the modified CAN frame data length code (can_dlc) in the CAN frame modification rules.",
        "trigger_condition": "A privileged user with CAP_NET_ADMIN capabilities creates a CAN frame modification rule that sets the can_dlc field to a value higher than the available data size of the CAN frame.",
        "specific_code_behavior_causing_vulnerability": "The code does not include checks to ensure that the modified can_dlc value does not exceed the available space in the CAN frame. This can lead to a situation where the tail of the skb can be rewritten beyond its allocated memory space, potentially causing a system crash."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add checks to verify that the modified can_dlc value does not exceed the available space in the CAN frame. If the can_dlc value is found to be larger than the available space, the code should handle the situation by deleting the frame due to misconfiguration and freeing the associated memory to prevent system instability or crashes.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2019-3701 present in the original code snippet. The vulnerability arises from the fact that the CAN frame modification rules allow bitwise logical operations that can be applied to the `can_dlc` field, which represents the data length code of the CAN frame. A privileged user with CAP_NET_ADMIN capabilities can create a CAN frame modification rule that sets the `can_dlc` field to a higher value than the available data size of the CAN frame. \n\nThis vulnerability can lead to a situation where the tail of the `skb` (socket buffer) can be rewritten beyond its allocated memory space, potentially causing a system crash. The modification introduced in the code snippet addresses this vulnerability by adding checks to ensure that the modified `can_dlc` value does not exceed the available space in the CAN frame. If the `can_dlc` value is found to be larger than the available space, the modified code snippet handles the situation by deleting the frame due to misconfiguration and freeing the associated memory to prevent a system crash.\n\nBy adding these checks and handling the scenario where the `can_dlc` value exceeds the available space, the modified code snippet effectively mitigates the vulnerability and prevents the exploitation that could lead to system instability or crashes.",
      "GPT_purpose": "Receive and process CAN frames in a CAN gateway, applying modifications and forwarding them to the appropriate network device.",
      "GPT_function": "\n1. Receive CAN frames and process them based on specified rules.\n2. Clone the received skb and modify it if necessary.\n3. Apply modification functions to the CAN frame data.\n4. Update checksums if the CAN frame has been modified.\n5. Adjust the hop counter and other parameters of the skb.\n6. Send the modified skb to the specified netdevice.",
      "CVE_id": "CVE-2019-3701",
      "code_before_change": "static void can_can_gw_rcv(struct sk_buff *skb, void *data)\n{\n\tstruct cgw_job *gwj = (struct cgw_job *)data;\n\tstruct can_frame *cf;\n\tstruct sk_buff *nskb;\n\tint modidx = 0;\n\n\t/*\n\t * Do not handle CAN frames routed more than 'max_hops' times.\n\t * In general we should never catch this delimiter which is intended\n\t * to cover a misconfiguration protection (e.g. circular CAN routes).\n\t *\n\t * The Controller Area Network controllers only accept CAN frames with\n\t * correct CRCs - which are not visible in the controller registers.\n\t * According to skbuff.h documentation the csum_start element for IP\n\t * checksums is undefined/unused when ip_summed == CHECKSUM_UNNECESSARY.\n\t * Only CAN skbs can be processed here which already have this property.\n\t */\n\n#define cgw_hops(skb) ((skb)->csum_start)\n\n\tBUG_ON(skb->ip_summed != CHECKSUM_UNNECESSARY);\n\n\tif (cgw_hops(skb) >= max_hops) {\n\t\t/* indicate deleted frames due to misconfiguration */\n\t\tgwj->deleted_frames++;\n\t\treturn;\n\t}\n\n\tif (!(gwj->dst.dev->flags & IFF_UP)) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\n\t/* is sending the skb back to the incoming interface not allowed? */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_IIF_TX_OK) &&\n\t    can_skb_prv(skb)->ifindex == gwj->dst.dev->ifindex)\n\t\treturn;\n\n\t/*\n\t * clone the given skb, which has not been done in can_rcv()\n\t *\n\t * When there is at least one modification function activated,\n\t * we need to copy the skb as we want to modify skb->data.\n\t */\n\tif (gwj->mod.modfunc[0])\n\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (!nskb) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\n\t/* put the incremented hop counter in the cloned skb */\n\tcgw_hops(nskb) = cgw_hops(skb) + 1;\n\n\t/* first processing of this CAN frame -> adjust to private hop limit */\n\tif (gwj->limit_hops && cgw_hops(nskb) == 1)\n\t\tcgw_hops(nskb) = max_hops - gwj->limit_hops + 1;\n\n\tnskb->dev = gwj->dst.dev;\n\n\t/* pointer to modifiable CAN frame */\n\tcf = (struct can_frame *)nskb->data;\n\n\t/* perform preprocessed modification functions if there are any */\n\twhile (modidx < MAX_MODFUNCTIONS && gwj->mod.modfunc[modidx])\n\t\t(*gwj->mod.modfunc[modidx++])(cf, &gwj->mod);\n\n\t/* check for checksum updates when the CAN frame has been modified */\n\tif (modidx) {\n\t\tif (gwj->mod.csumfunc.crc8)\n\t\t\t(*gwj->mod.csumfunc.crc8)(cf, &gwj->mod.csum.crc8);\n\n\t\tif (gwj->mod.csumfunc.xor)\n\t\t\t(*gwj->mod.csumfunc.xor)(cf, &gwj->mod.csum.xor);\n\t}\n\n\t/* clear the skb timestamp if not configured the other way */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_SRC_TSTAMP))\n\t\tnskb->tstamp = 0;\n\n\t/* send to netdevice */\n\tif (can_send(nskb, gwj->flags & CGW_FLAGS_CAN_ECHO))\n\t\tgwj->dropped_frames++;\n\telse\n\t\tgwj->handled_frames++;\n}",
      "code_after_change": "static void can_can_gw_rcv(struct sk_buff *skb, void *data)\n{\n\tstruct cgw_job *gwj = (struct cgw_job *)data;\n\tstruct can_frame *cf;\n\tstruct sk_buff *nskb;\n\tint modidx = 0;\n\n\t/*\n\t * Do not handle CAN frames routed more than 'max_hops' times.\n\t * In general we should never catch this delimiter which is intended\n\t * to cover a misconfiguration protection (e.g. circular CAN routes).\n\t *\n\t * The Controller Area Network controllers only accept CAN frames with\n\t * correct CRCs - which are not visible in the controller registers.\n\t * According to skbuff.h documentation the csum_start element for IP\n\t * checksums is undefined/unused when ip_summed == CHECKSUM_UNNECESSARY.\n\t * Only CAN skbs can be processed here which already have this property.\n\t */\n\n#define cgw_hops(skb) ((skb)->csum_start)\n\n\tBUG_ON(skb->ip_summed != CHECKSUM_UNNECESSARY);\n\n\tif (cgw_hops(skb) >= max_hops) {\n\t\t/* indicate deleted frames due to misconfiguration */\n\t\tgwj->deleted_frames++;\n\t\treturn;\n\t}\n\n\tif (!(gwj->dst.dev->flags & IFF_UP)) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\n\t/* is sending the skb back to the incoming interface not allowed? */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_IIF_TX_OK) &&\n\t    can_skb_prv(skb)->ifindex == gwj->dst.dev->ifindex)\n\t\treturn;\n\n\t/*\n\t * clone the given skb, which has not been done in can_rcv()\n\t *\n\t * When there is at least one modification function activated,\n\t * we need to copy the skb as we want to modify skb->data.\n\t */\n\tif (gwj->mod.modfunc[0])\n\t\tnskb = skb_copy(skb, GFP_ATOMIC);\n\telse\n\t\tnskb = skb_clone(skb, GFP_ATOMIC);\n\n\tif (!nskb) {\n\t\tgwj->dropped_frames++;\n\t\treturn;\n\t}\n\n\t/* put the incremented hop counter in the cloned skb */\n\tcgw_hops(nskb) = cgw_hops(skb) + 1;\n\n\t/* first processing of this CAN frame -> adjust to private hop limit */\n\tif (gwj->limit_hops && cgw_hops(nskb) == 1)\n\t\tcgw_hops(nskb) = max_hops - gwj->limit_hops + 1;\n\n\tnskb->dev = gwj->dst.dev;\n\n\t/* pointer to modifiable CAN frame */\n\tcf = (struct can_frame *)nskb->data;\n\n\t/* perform preprocessed modification functions if there are any */\n\twhile (modidx < MAX_MODFUNCTIONS && gwj->mod.modfunc[modidx])\n\t\t(*gwj->mod.modfunc[modidx++])(cf, &gwj->mod);\n\n\t/* Has the CAN frame been modified? */\n\tif (modidx) {\n\t\t/* get available space for the processed CAN frame type */\n\t\tint max_len = nskb->len - offsetof(struct can_frame, data);\n\n\t\t/* dlc may have changed, make sure it fits to the CAN frame */\n\t\tif (cf->can_dlc > max_len)\n\t\t\tgoto out_delete;\n\n\t\t/* check for checksum updates in classic CAN length only */\n\t\tif (gwj->mod.csumfunc.crc8) {\n\t\t\tif (cf->can_dlc > 8)\n\t\t\t\tgoto out_delete;\n\n\t\t\t(*gwj->mod.csumfunc.crc8)(cf, &gwj->mod.csum.crc8);\n\t\t}\n\n\t\tif (gwj->mod.csumfunc.xor) {\n\t\t\tif (cf->can_dlc > 8)\n\t\t\t\tgoto out_delete;\n\n\t\t\t(*gwj->mod.csumfunc.xor)(cf, &gwj->mod.csum.xor);\n\t\t}\n\t}\n\n\t/* clear the skb timestamp if not configured the other way */\n\tif (!(gwj->flags & CGW_FLAGS_CAN_SRC_TSTAMP))\n\t\tnskb->tstamp = 0;\n\n\t/* send to netdevice */\n\tif (can_send(nskb, gwj->flags & CGW_FLAGS_CAN_ECHO))\n\t\tgwj->dropped_frames++;\n\telse\n\t\tgwj->handled_frames++;\n\n\treturn;\n\n out_delete:\n\t/* delete frame due to misconfiguration */\n\tgwj->deleted_frames++;\n\tkfree_skb(nskb);\n\treturn;\n}",
      "modified_lines": {
        "added": [
          "\t/* Has the CAN frame been modified? */",
          "\t\t/* get available space for the processed CAN frame type */",
          "\t\tint max_len = nskb->len - offsetof(struct can_frame, data);",
          "",
          "\t\t/* dlc may have changed, make sure it fits to the CAN frame */",
          "\t\tif (cf->can_dlc > max_len)",
          "\t\t\tgoto out_delete;",
          "",
          "\t\t/* check for checksum updates in classic CAN length only */",
          "\t\tif (gwj->mod.csumfunc.crc8) {",
          "\t\t\tif (cf->can_dlc > 8)",
          "\t\t\t\tgoto out_delete;",
          "",
          "\t\t}",
          "\t\tif (gwj->mod.csumfunc.xor) {",
          "\t\t\tif (cf->can_dlc > 8)",
          "\t\t\t\tgoto out_delete;",
          "",
          "\t\t}",
          "",
          "\treturn;",
          "",
          " out_delete:",
          "\t/* delete frame due to misconfiguration */",
          "\tgwj->deleted_frames++;",
          "\tkfree_skb(nskb);",
          "\treturn;"
        ],
        "deleted": [
          "\t/* check for checksum updates when the CAN frame has been modified */",
          "\t\tif (gwj->mod.csumfunc.crc8)",
          "\t\tif (gwj->mod.csumfunc.xor)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation checks on the modified CAN frame data length code (can_dlc) in the CAN frame modification rules.",
      "trigger_condition": "A privileged user with CAP_NET_ADMIN capabilities creates a CAN frame modification rule that sets the can_dlc field to a value higher than the available data size of the CAN frame.",
      "specific_code_behavior_causing_vulnerability": "The code does not include checks to ensure that the modified can_dlc value does not exceed the available space in the CAN frame. This can lead to a situation where the tail of the skb can be rewritten beyond its allocated memory space, potentially causing a system crash.",
      "id": 57,
      "code_after_change_normalized": "static void FUN1(struct sk_buff *VAR1, void *VAR2)\n{\nstruct VAR4 *VAR3 = (struct VAR4 *)VAR2;\nstruct can_frame *VAR5;\nstruct sk_buff *VAR6;\nint VAR7 = 0;\n#define FUN2(VAR1) ((VAR1)->VAR8)\nFUN3(VAR1->VAR9 != VAR10);\nif (FUN2(VAR1) >= VAR11) {\nVAR3->VAR12++;\nreturn;\n}\nif (!(VAR3->VAR13.VAR14->VAR15 & VAR16)) {\nVAR3->VAR17++;\nreturn;\n}\nif (!(VAR3->VAR15 & VAR18) &&\nFUN4(VAR1)->VAR19 == VAR3->VAR13.VAR14->VAR19)\nreturn;\nif (VAR3->VAR20.VAR21[0])\nVAR6 = FUN5(VAR1, VAR22);\nelse\nVAR6 = FUN6(VAR1, VAR22);\nif (!VAR6) {\nVAR3->VAR17++;\nreturn;\n}\nFUN2(VAR6) = FUN2(VAR1) + 1;\nif (VAR3->VAR23 && FUN2(VAR6) == 1)\nFUN2(VAR6) = VAR11 - VAR3->VAR23 + 1;\nVAR6->VAR14 = VAR3->VAR13.VAR14;\nVAR5 = (struct VAR24 *)VAR6->VAR2;\nwhile (VAR7 < VAR25 && VAR3->VAR20.VAR21[VAR7])\n(*VAR3->VAR20.VAR21[VAR7++])(VAR5, &VAR3->VAR20);\nif (VAR7) {\nint VAR26 = VAR6->VAR27 - FUN7(struct VAR24, VAR2);\nif (VAR5->VAR28 > VAR26)\ngoto VAR29;\nif (VAR3->VAR20.VAR30.VAR31) {\nif (VAR5->VAR28 > 8)\ngoto VAR29;\n(*VAR3->VAR20.VAR30.VAR31)(VAR5, &VAR3->VAR20.VAR32.VAR31);\n}\nif (VAR3->VAR20.VAR30.xor) {\nif (VAR5->VAR28 > 8)\ngoto VAR29;\n(*VAR3->VAR20.VAR30.xor)(VAR5, &VAR3->VAR20.VAR32.xor);\n}\n}\nif (!(VAR3->VAR15 & VAR33))\nVAR6->VAR34 = 0;\nif (FUN8(VAR6, VAR3->VAR15 & VAR35))\nVAR3->VAR17++;\nelse\nVAR3->VAR36++;\nreturn;\nVAR29:\nVAR3->VAR12++;\nFUN9(VAR6);\nreturn;\n}\n",
      "code_before_change_normalized": "static void FUN1(struct sk_buff *VAR1, void *VAR2)\n{\nstruct VAR4 *VAR3 = (struct VAR4 *)VAR2;\nstruct can_frame *VAR5;\nstruct sk_buff *VAR6;\nint VAR7 = 0;\n#define FUN2(VAR1) ((VAR1)->VAR8)\nFUN3(VAR1->VAR9 != VAR10);\nif (FUN2(VAR1) >= VAR11) {\nVAR3->VAR12++;\nreturn;\n}\nif (!(VAR3->VAR13.VAR14->VAR15 & VAR16)) {\nVAR3->VAR17++;\nreturn;\n}\nif (!(VAR3->VAR15 & VAR18) &&\nFUN4(VAR1)->VAR19 == VAR3->VAR13.VAR14->VAR19)\nreturn;\nif (VAR3->VAR20.VAR21[0])\nVAR6 = FUN5(VAR1, VAR22);\nelse\nVAR6 = FUN6(VAR1, VAR22);\nif (!VAR6) {\nVAR3->VAR17++;\nreturn;\n}\nFUN2(VAR6) = FUN2(VAR1) + 1;\nif (VAR3->VAR23 && FUN2(VAR6) == 1)\nFUN2(VAR6) = VAR11 - VAR3->VAR23 + 1;\nVAR6->VAR14 = VAR3->VAR13.VAR14;\nVAR5 = (struct VAR24 *)VAR6->VAR2;\nwhile (VAR7 < VAR25 && VAR3->VAR20.VAR21[VAR7])\n(*VAR3->VAR20.VAR21[VAR7++])(VAR5, &VAR3->VAR20);\nif (VAR7) {\nif (VAR3->VAR20.VAR26.VAR27)\n(*VAR3->VAR20.VAR26.VAR27)(VAR5, &VAR3->VAR20.VAR28.VAR27);\nif (VAR3->VAR20.VAR26.xor)\n(*VAR3->VAR20.VAR26.xor)(VAR5, &VAR3->VAR20.VAR28.xor);\n}\nif (!(VAR3->VAR15 & VAR29))\nVAR6->VAR30 = 0;\nif (FUN7(VAR6, VAR3->VAR15 & VAR31))\nVAR3->VAR17++;\nelse\nVAR3->VAR32++;\n}\n",
      "code_after_change_raw": "static void can_can_gw_rcv(struct sk_buff *skb, void *data)\n{\nstruct cgw_job *gwj = (struct cgw_job *)data;\nstruct can_frame *cf;\nstruct sk_buff *nskb;\nint modidx = 0;\n#define cgw_hops(skb) ((skb)->csum_start)\nBUG_ON(skb->ip_summed != CHECKSUM_UNNECESSARY);\nif (cgw_hops(skb) >= max_hops) {\ngwj->deleted_frames++;\nreturn;\n}\nif (!(gwj->dst.dev->flags & IFF_UP)) {\ngwj->dropped_frames++;\nreturn;\n}\nif (!(gwj->flags & CGW_FLAGS_CAN_IIF_TX_OK) &&\ncan_skb_prv(skb)->ifindex == gwj->dst.dev->ifindex)\nreturn;\nif (gwj->mod.modfunc[0])\nnskb = skb_copy(skb, GFP_ATOMIC);\nelse\nnskb = skb_clone(skb, GFP_ATOMIC);\nif (!nskb) {\ngwj->dropped_frames++;\nreturn;\n}\ncgw_hops(nskb) = cgw_hops(skb) + 1;\nif (gwj->limit_hops && cgw_hops(nskb) == 1)\ncgw_hops(nskb) = max_hops - gwj->limit_hops + 1;\nnskb->dev = gwj->dst.dev;\ncf = (struct can_frame *)nskb->data;\nwhile (modidx < MAX_MODFUNCTIONS && gwj->mod.modfunc[modidx])\n(*gwj->mod.modfunc[modidx++])(cf, &gwj->mod);\nif (modidx) {\nint max_len = nskb->len - offsetof(struct can_frame, data);\nif (cf->can_dlc > max_len)\ngoto out_delete;\nif (gwj->mod.csumfunc.crc8) {\nif (cf->can_dlc > 8)\ngoto out_delete;\n(*gwj->mod.csumfunc.crc8)(cf, &gwj->mod.csum.crc8);\n}\nif (gwj->mod.csumfunc.xor) {\nif (cf->can_dlc > 8)\ngoto out_delete;\n(*gwj->mod.csumfunc.xor)(cf, &gwj->mod.csum.xor);\n}\n}\nif (!(gwj->flags & CGW_FLAGS_CAN_SRC_TSTAMP))\nnskb->tstamp = 0;\nif (can_send(nskb, gwj->flags & CGW_FLAGS_CAN_ECHO))\ngwj->dropped_frames++;\nelse\ngwj->handled_frames++;\nreturn;\nout_delete:\ngwj->deleted_frames++;\nkfree_skb(nskb);\nreturn;\n}\n",
      "code_before_change_raw": "static void can_can_gw_rcv(struct sk_buff *skb, void *data)\n{\nstruct cgw_job *gwj = (struct cgw_job *)data;\nstruct can_frame *cf;\nstruct sk_buff *nskb;\nint modidx = 0;\n#define cgw_hops(skb) ((skb)->csum_start)\nBUG_ON(skb->ip_summed != CHECKSUM_UNNECESSARY);\nif (cgw_hops(skb) >= max_hops) {\ngwj->deleted_frames++;\nreturn;\n}\nif (!(gwj->dst.dev->flags & IFF_UP)) {\ngwj->dropped_frames++;\nreturn;\n}\nif (!(gwj->flags & CGW_FLAGS_CAN_IIF_TX_OK) &&\ncan_skb_prv(skb)->ifindex == gwj->dst.dev->ifindex)\nreturn;\nif (gwj->mod.modfunc[0])\nnskb = skb_copy(skb, GFP_ATOMIC);\nelse\nnskb = skb_clone(skb, GFP_ATOMIC);\nif (!nskb) {\ngwj->dropped_frames++;\nreturn;\n}\ncgw_hops(nskb) = cgw_hops(skb) + 1;\nif (gwj->limit_hops && cgw_hops(nskb) == 1)\ncgw_hops(nskb) = max_hops - gwj->limit_hops + 1;\nnskb->dev = gwj->dst.dev;\ncf = (struct can_frame *)nskb->data;\nwhile (modidx < MAX_MODFUNCTIONS && gwj->mod.modfunc[modidx])\n(*gwj->mod.modfunc[modidx++])(cf, &gwj->mod);\nif (modidx) {\nif (gwj->mod.csumfunc.crc8)\n(*gwj->mod.csumfunc.crc8)(cf, &gwj->mod.csum.crc8);\nif (gwj->mod.csumfunc.xor)\n(*gwj->mod.csumfunc.xor)(cf, &gwj->mod.csum.xor);\n}\nif (!(gwj->flags & CGW_FLAGS_CAN_SRC_TSTAMP))\nnskb->tstamp = 0;\nif (can_send(nskb, gwj->flags & CGW_FLAGS_CAN_ECHO))\ngwj->dropped_frames++;\nelse\ngwj->handled_frames++;\n}\n"
    }
  ],
  "CVE-2019-8956": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of safe list iteration in the SCTP_SENDALL process, specifically when handling the SCTP_SENDALL flag.",
        "trigger_condition": "The code does not use a safe list iteration method, leading to a use-after-free scenario when elements are removed during the iteration.",
        "specific_code_behavior_causing_vulnerability": "The code iterates over a list of associations without using a safe list iteration method, which can result in accessing memory that has been freed if elements are removed during the iteration. This behavior can lead to memory corruption and a use-after-free vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to use a safe list iteration method, such as list_for_each_entry_safe, when iterating over the list of associations in the SCTP_SENDALL process. This ensures that the code handles the removal of elements during iteration properly and prevents accessing freed memory, thus addressing the use-after-free vulnerability. In this case, the solution involves replacing the unsafe list iteration with list_for_each_entry_safe to ensure safe traversal of the list of associations.",
      "GPT_analysis": "The modification is necessary to fix the use-after-free vulnerability in the \"sctp_sendmsg()\" function. In the original code snippet, the list iteration in the SCTP_SENDALL process does not use the safe version of the list_for_each_entry macro. This can lead to a use-after-free scenario where memory that has been freed is accessed, potentially causing memory corruption.\n\nBy using list_for_each_entry_safe instead of list_for_each_entry, the code ensures that the list iteration is safe even if elements are removed during the iteration. This prevents accessing memory that has been freed and helps to avoid the use-after-free vulnerability, thus improving the overall security of the function.",
      "GPT_purpose": "Send an SCTP message with specified flags and data to the appropriate association or associations.",
      "GPT_function": "\n1. Parse and get snd_info.\n2. Get daddr from msg.\n3. Process SCTP_SENDALL.\n4. Get and check or create asoc.\n5. Update snd_info with the asoc.\n6. Send msg to the asoc.",
      "CVE_id": "CVE-2019-8956",
      "code_before_change": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\n\tstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\n\tstruct sctp_transport *transport = NULL;\n\tstruct sctp_sndrcvinfo _sinfo, *sinfo;\n\tstruct sctp_association *asoc;\n\tstruct sctp_cmsgs cmsgs;\n\tunion sctp_addr *daddr;\n\tbool new = false;\n\t__u16 sflags;\n\tint err;\n\n\t/* Parse and get snd_info */\n\terr = sctp_sendmsg_parse(sk, &cmsgs, &_sinfo, msg, msg_len);\n\tif (err)\n\t\tgoto out;\n\n\tsinfo  = &_sinfo;\n\tsflags = sinfo->sinfo_flags;\n\n\t/* Get daddr from msg */\n\tdaddr = sctp_sendmsg_get_daddr(sk, msg, &cmsgs);\n\tif (IS_ERR(daddr)) {\n\t\terr = PTR_ERR(daddr);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* SCTP_SENDALL process */\n\tif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\n\t\tlist_for_each_entry(asoc, &ep->asocs, asocs) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err == 0)\n\t\t\t\tcontinue;\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t\t\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len,\n\t\t\t\t\t\t   NULL, sinfo);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tiov_iter_revert(&msg->msg_iter, err);\n\t\t}\n\n\t\tgoto out_unlock;\n\t}\n\n\t/* Get and check or create asoc */\n\tif (daddr) {\n\t\tasoc = sctp_endpoint_lookup_assoc(ep, daddr, &transport);\n\t\tif (asoc) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\terr = sctp_sendmsg_new_asoc(sk, sflags, &cmsgs, daddr,\n\t\t\t\t\t\t    &transport);\n\t\t\tif (err)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tasoc = transport->asoc;\n\t\t\tnew = true;\n\t\t}\n\n\t\tif (!sctp_style(sk, TCP) && !(sflags & SCTP_ADDR_OVER))\n\t\t\ttransport = NULL;\n\t} else {\n\t\tasoc = sctp_id2assoc(sk, sinfo->sinfo_assoc_id);\n\t\tif (!asoc) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg, msg_len);\n\t\tif (err <= 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Update snd_info with the asoc */\n\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t/* Send msg to the asoc */\n\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len, transport, sinfo);\n\tif (err < 0 && err != -ESRCH && new)\n\t\tsctp_association_free(asoc);\n\nout_unlock:\n\trelease_sock(sk);\nout:\n\treturn sctp_error(sk, msg->msg_flags, err);\n}",
      "code_after_change": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\n\tstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\n\tstruct sctp_transport *transport = NULL;\n\tstruct sctp_sndrcvinfo _sinfo, *sinfo;\n\tstruct sctp_association *asoc, *tmp;\n\tstruct sctp_cmsgs cmsgs;\n\tunion sctp_addr *daddr;\n\tbool new = false;\n\t__u16 sflags;\n\tint err;\n\n\t/* Parse and get snd_info */\n\terr = sctp_sendmsg_parse(sk, &cmsgs, &_sinfo, msg, msg_len);\n\tif (err)\n\t\tgoto out;\n\n\tsinfo  = &_sinfo;\n\tsflags = sinfo->sinfo_flags;\n\n\t/* Get daddr from msg */\n\tdaddr = sctp_sendmsg_get_daddr(sk, msg, &cmsgs);\n\tif (IS_ERR(daddr)) {\n\t\terr = PTR_ERR(daddr);\n\t\tgoto out;\n\t}\n\n\tlock_sock(sk);\n\n\t/* SCTP_SENDALL process */\n\tif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\n\t\tlist_for_each_entry_safe(asoc, tmp, &ep->asocs, asocs) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err == 0)\n\t\t\t\tcontinue;\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t\t\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len,\n\t\t\t\t\t\t   NULL, sinfo);\n\t\t\tif (err < 0)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tiov_iter_revert(&msg->msg_iter, err);\n\t\t}\n\n\t\tgoto out_unlock;\n\t}\n\n\t/* Get and check or create asoc */\n\tif (daddr) {\n\t\tasoc = sctp_endpoint_lookup_assoc(ep, daddr, &transport);\n\t\tif (asoc) {\n\t\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\n\t\t\t\t\t\t\tmsg_len);\n\t\t\tif (err <= 0)\n\t\t\t\tgoto out_unlock;\n\t\t} else {\n\t\t\terr = sctp_sendmsg_new_asoc(sk, sflags, &cmsgs, daddr,\n\t\t\t\t\t\t    &transport);\n\t\t\tif (err)\n\t\t\t\tgoto out_unlock;\n\n\t\t\tasoc = transport->asoc;\n\t\t\tnew = true;\n\t\t}\n\n\t\tif (!sctp_style(sk, TCP) && !(sflags & SCTP_ADDR_OVER))\n\t\t\ttransport = NULL;\n\t} else {\n\t\tasoc = sctp_id2assoc(sk, sinfo->sinfo_assoc_id);\n\t\tif (!asoc) {\n\t\t\terr = -EPIPE;\n\t\t\tgoto out_unlock;\n\t\t}\n\n\t\terr = sctp_sendmsg_check_sflags(asoc, sflags, msg, msg_len);\n\t\tif (err <= 0)\n\t\t\tgoto out_unlock;\n\t}\n\n\t/* Update snd_info with the asoc */\n\tsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\n\n\t/* Send msg to the asoc */\n\terr = sctp_sendmsg_to_asoc(asoc, msg, msg_len, transport, sinfo);\n\tif (err < 0 && err != -ESRCH && new)\n\t\tsctp_association_free(asoc);\n\nout_unlock:\n\trelease_sock(sk);\nout:\n\treturn sctp_error(sk, msg->msg_flags, err);\n}",
      "modified_lines": {
        "added": [
          "\tstruct sctp_association *asoc, *tmp;",
          "\t\tlist_for_each_entry_safe(asoc, tmp, &ep->asocs, asocs) {"
        ],
        "deleted": [
          "\tstruct sctp_association *asoc;",
          "\t\tlist_for_each_entry(asoc, &ep->asocs, asocs) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of safe list iteration in the SCTP_SENDALL process, specifically when handling the SCTP_SENDALL flag.",
      "trigger_condition": "The code does not use a safe list iteration method, leading to a use-after-free scenario when elements are removed during the iteration.",
      "specific_code_behavior_causing_vulnerability": "The code iterates over a list of associations without using a safe list iteration method, which can result in accessing memory that has been freed if elements are removed during the iteration. This behavior can lead to memory corruption and a use-after-free vulnerability.",
      "id": 58,
      "code_after_change_normalized": "static int FUN1(struct sock *VAR1, struct msghdr *VAR2, size_t VAR3)\n{\nstruct sctp_endpoint *VAR4 = FUN2(VAR1)->VAR4;\nstruct sctp_transport *VAR5 = NULL;\nstruct sctp_sndrcvinfo VAR6, *VAR7;\nstruct sctp_association *VAR8, *VAR9;\nstruct sctp_cmsgs VAR10;\nunion sctp_addr *VAR11;\nbool new = false;\n__u16 VAR12;\nint VAR13;\nVAR13 = FUN3(VAR1, &VAR10, &VAR6, VAR2, VAR3);\nif (VAR13)\ngoto VAR14;\nVAR7  = &VAR6;\nVAR12 = VAR7->VAR15;\nVAR11 = FUN4(VAR1, VAR2, &VAR10);\nif (FUN5(VAR11)) {\nVAR13 = FUN6(VAR11);\ngoto VAR14;\n}\nFUN7(VAR1);\nif ((VAR12 & VAR16) && FUN8(VAR1, VAR17)) {\nFUN9(VAR8, VAR9, &VAR4->VAR18, VAR18) {\nVAR13 = FUN10(VAR8, VAR12, VAR2,\nVAR3);\nif (VAR13 == 0)\ncontinue;\nif (VAR13 < 0)\ngoto VAR19;\nFUN11(VAR8, VAR7, &VAR10);\nVAR13 = FUN12(VAR8, VAR2, VAR3,\nNULL, VAR7);\nif (VAR13 < 0)\ngoto VAR19;\nFUN13(&VAR2->VAR20, VAR13);\n}\ngoto VAR19;\n}\nif (VAR11) {\nVAR8 = FUN14(VAR4, VAR11, &VAR5);\nif (VAR8) {\nVAR13 = FUN10(VAR8, VAR12, VAR2,\nVAR3);\nif (VAR13 <= 0)\ngoto VAR19;\n} else {\nVAR13 = FUN15(VAR1, VAR12, &VAR10, VAR11,\n&VAR5);\nif (VAR13)\ngoto VAR19;\nVAR8 = VAR5->VAR8;\nnew = true;\n}\nif (!FUN8(VAR1, VAR21) && !(VAR12 & VAR22))\nVAR5 = NULL;\n} else {\nVAR8 = FUN16(VAR1, VAR7->VAR23);\nif (!VAR8) {\nVAR13 = -VAR24;\ngoto VAR19;\n}\nVAR13 = FUN10(VAR8, VAR12, VAR2, VAR3);\nif (VAR13 <= 0)\ngoto VAR19;\n}\nFUN11(VAR8, VAR7, &VAR10);\nVAR13 = FUN12(VAR8, VAR2, VAR3, VAR5, VAR7);\nif (VAR13 < 0 && VAR13 != -VAR25 && new)\nFUN17(VAR8);\nVAR19:\nFUN18(VAR1);\nVAR14:\nreturn FUN19(VAR1, VAR2->VAR26, VAR13);\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sock *VAR1, struct msghdr *VAR2, size_t VAR3)\n{\nstruct sctp_endpoint *VAR4 = FUN2(VAR1)->VAR4;\nstruct sctp_transport *VAR5 = NULL;\nstruct sctp_sndrcvinfo VAR6, *VAR7;\nstruct sctp_association *VAR8;\nstruct sctp_cmsgs VAR9;\nunion sctp_addr *VAR10;\nbool new = false;\n__u16 VAR11;\nint VAR12;\nVAR12 = FUN3(VAR1, &VAR9, &VAR6, VAR2, VAR3);\nif (VAR12)\ngoto VAR13;\nVAR7  = &VAR6;\nVAR11 = VAR7->VAR14;\nVAR10 = FUN4(VAR1, VAR2, &VAR9);\nif (FUN5(VAR10)) {\nVAR12 = FUN6(VAR10);\ngoto VAR13;\n}\nFUN7(VAR1);\nif ((VAR11 & VAR15) && FUN8(VAR1, VAR16)) {\nFUN9(VAR8, &VAR4->VAR17, VAR17) {\nVAR12 = FUN10(VAR8, VAR11, VAR2,\nVAR3);\nif (VAR12 == 0)\ncontinue;\nif (VAR12 < 0)\ngoto VAR18;\nFUN11(VAR8, VAR7, &VAR9);\nVAR12 = FUN12(VAR8, VAR2, VAR3,\nNULL, VAR7);\nif (VAR12 < 0)\ngoto VAR18;\nFUN13(&VAR2->VAR19, VAR12);\n}\ngoto VAR18;\n}\nif (VAR10) {\nVAR8 = FUN14(VAR4, VAR10, &VAR5);\nif (VAR8) {\nVAR12 = FUN10(VAR8, VAR11, VAR2,\nVAR3);\nif (VAR12 <= 0)\ngoto VAR18;\n} else {\nVAR12 = FUN15(VAR1, VAR11, &VAR9, VAR10,\n&VAR5);\nif (VAR12)\ngoto VAR18;\nVAR8 = VAR5->VAR8;\nnew = true;\n}\nif (!FUN8(VAR1, VAR20) && !(VAR11 & VAR21))\nVAR5 = NULL;\n} else {\nVAR8 = FUN16(VAR1, VAR7->VAR22);\nif (!VAR8) {\nVAR12 = -VAR23;\ngoto VAR18;\n}\nVAR12 = FUN10(VAR8, VAR11, VAR2, VAR3);\nif (VAR12 <= 0)\ngoto VAR18;\n}\nFUN11(VAR8, VAR7, &VAR9);\nVAR12 = FUN12(VAR8, VAR2, VAR3, VAR5, VAR7);\nif (VAR12 < 0 && VAR12 != -VAR24 && new)\nFUN17(VAR8);\nVAR18:\nFUN18(VAR1);\nVAR13:\nreturn FUN19(VAR1, VAR2->VAR25, VAR12);\n}\n",
      "code_after_change_raw": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\nstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\nstruct sctp_transport *transport = NULL;\nstruct sctp_sndrcvinfo _sinfo, *sinfo;\nstruct sctp_association *asoc, *tmp;\nstruct sctp_cmsgs cmsgs;\nunion sctp_addr *daddr;\nbool new = false;\n__u16 sflags;\nint err;\nerr = sctp_sendmsg_parse(sk, &cmsgs, &_sinfo, msg, msg_len);\nif (err)\ngoto out;\nsinfo  = &_sinfo;\nsflags = sinfo->sinfo_flags;\ndaddr = sctp_sendmsg_get_daddr(sk, msg, &cmsgs);\nif (IS_ERR(daddr)) {\nerr = PTR_ERR(daddr);\ngoto out;\n}\nlock_sock(sk);\nif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\nlist_for_each_entry_safe(asoc, tmp, &ep->asocs, asocs) {\nerr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\nmsg_len);\nif (err == 0)\ncontinue;\nif (err < 0)\ngoto out_unlock;\nsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\nerr = sctp_sendmsg_to_asoc(asoc, msg, msg_len,\nNULL, sinfo);\nif (err < 0)\ngoto out_unlock;\niov_iter_revert(&msg->msg_iter, err);\n}\ngoto out_unlock;\n}\nif (daddr) {\nasoc = sctp_endpoint_lookup_assoc(ep, daddr, &transport);\nif (asoc) {\nerr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\nmsg_len);\nif (err <= 0)\ngoto out_unlock;\n} else {\nerr = sctp_sendmsg_new_asoc(sk, sflags, &cmsgs, daddr,\n&transport);\nif (err)\ngoto out_unlock;\nasoc = transport->asoc;\nnew = true;\n}\nif (!sctp_style(sk, TCP) && !(sflags & SCTP_ADDR_OVER))\ntransport = NULL;\n} else {\nasoc = sctp_id2assoc(sk, sinfo->sinfo_assoc_id);\nif (!asoc) {\nerr = -EPIPE;\ngoto out_unlock;\n}\nerr = sctp_sendmsg_check_sflags(asoc, sflags, msg, msg_len);\nif (err <= 0)\ngoto out_unlock;\n}\nsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\nerr = sctp_sendmsg_to_asoc(asoc, msg, msg_len, transport, sinfo);\nif (err < 0 && err != -ESRCH && new)\nsctp_association_free(asoc);\nout_unlock:\nrelease_sock(sk);\nout:\nreturn sctp_error(sk, msg->msg_flags, err);\n}\n",
      "code_before_change_raw": "static int sctp_sendmsg(struct sock *sk, struct msghdr *msg, size_t msg_len)\n{\nstruct sctp_endpoint *ep = sctp_sk(sk)->ep;\nstruct sctp_transport *transport = NULL;\nstruct sctp_sndrcvinfo _sinfo, *sinfo;\nstruct sctp_association *asoc;\nstruct sctp_cmsgs cmsgs;\nunion sctp_addr *daddr;\nbool new = false;\n__u16 sflags;\nint err;\nerr = sctp_sendmsg_parse(sk, &cmsgs, &_sinfo, msg, msg_len);\nif (err)\ngoto out;\nsinfo  = &_sinfo;\nsflags = sinfo->sinfo_flags;\ndaddr = sctp_sendmsg_get_daddr(sk, msg, &cmsgs);\nif (IS_ERR(daddr)) {\nerr = PTR_ERR(daddr);\ngoto out;\n}\nlock_sock(sk);\nif ((sflags & SCTP_SENDALL) && sctp_style(sk, UDP)) {\nlist_for_each_entry(asoc, &ep->asocs, asocs) {\nerr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\nmsg_len);\nif (err == 0)\ncontinue;\nif (err < 0)\ngoto out_unlock;\nsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\nerr = sctp_sendmsg_to_asoc(asoc, msg, msg_len,\nNULL, sinfo);\nif (err < 0)\ngoto out_unlock;\niov_iter_revert(&msg->msg_iter, err);\n}\ngoto out_unlock;\n}\nif (daddr) {\nasoc = sctp_endpoint_lookup_assoc(ep, daddr, &transport);\nif (asoc) {\nerr = sctp_sendmsg_check_sflags(asoc, sflags, msg,\nmsg_len);\nif (err <= 0)\ngoto out_unlock;\n} else {\nerr = sctp_sendmsg_new_asoc(sk, sflags, &cmsgs, daddr,\n&transport);\nif (err)\ngoto out_unlock;\nasoc = transport->asoc;\nnew = true;\n}\nif (!sctp_style(sk, TCP) && !(sflags & SCTP_ADDR_OVER))\ntransport = NULL;\n} else {\nasoc = sctp_id2assoc(sk, sinfo->sinfo_assoc_id);\nif (!asoc) {\nerr = -EPIPE;\ngoto out_unlock;\n}\nerr = sctp_sendmsg_check_sflags(asoc, sflags, msg, msg_len);\nif (err <= 0)\ngoto out_unlock;\n}\nsctp_sendmsg_update_sinfo(asoc, sinfo, &cmsgs);\nerr = sctp_sendmsg_to_asoc(asoc, msg, msg_len, transport, sinfo);\nif (err < 0 && err != -ESRCH && new)\nsctp_association_free(asoc);\nout_unlock:\nrelease_sock(sk);\nout:\nreturn sctp_error(sk, msg->msg_flags, err);\n}\n"
    }
  ],
  "CVE-2019-9162": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper length validation for data passed to the snmp_version function.",
        "trigger_condition": "Passing data with a length greater than 1 to the snmp_version function triggers an out-of-bounds read operation.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the length of the data being passed to the snmp_version function, allowing for potential out-of-bounds read operations if the length is greater than 1."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check for the length of the data being passed to the snmp_version function. By ensuring that the data length is exactly 1, the code prevents out-of-bounds read operations and enhances security. In this case, the solution involves adding a line to check if the datalen is not equal to 1 before further processing the data.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-9162 by adding a check for the length of the data being passed to the `snmp_version` function. \n\nIn the original code snippet, there is no check on the length of the `data` being passed, which can lead to an out-of-bounds read operation if the length of the data is greater than 1. This vulnerability can be exploited to cause an OOPS (Out Of Print Space) or local privilege escalation.\n\nBy adding the check `if (datalen != 1) return -EINVAL;`, we ensure that the `data` being passed to the function has a valid length of 1. This helps prevent out-of-bounds read and write operations, making the code more secure and preventing potential exploits related to the vulnerability.",
      "GPT_purpose": "This function checks the SNMP version and returns an error if the version is greater than 1.",
      "GPT_function": "\n1. Check if the value pointed to by 'data' is greater than 1.\n2. Return -ENOTSUPP if the condition is met.\n3. Return 1 if the condition is not met.",
      "CVE_id": "CVE-2019-9162",
      "code_before_change": "int snmp_version(void *context, size_t hdrlen, unsigned char tag,\n\t\t const void *data, size_t datalen)\n{\n\tif (*(unsigned char *)data > 1)\n\t\treturn -ENOTSUPP;\n\treturn 1;\n}",
      "code_after_change": "int snmp_version(void *context, size_t hdrlen, unsigned char tag,\n\t\t const void *data, size_t datalen)\n{\n\tif (datalen != 1)\n\t\treturn -EINVAL;\n\tif (*(unsigned char *)data > 1)\n\t\treturn -ENOTSUPP;\n\treturn 1;\n}",
      "modified_lines": {
        "added": [
          "\tif (datalen != 1)",
          "\t\treturn -EINVAL;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper length validation for data passed to the snmp_version function.",
      "trigger_condition": "Passing data with a length greater than 1 to the snmp_version function triggers an out-of-bounds read operation.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the length of the data being passed to the snmp_version function, allowing for potential out-of-bounds read operations if the length is greater than 1.",
      "id": 59,
      "code_after_change_normalized": "int FUN1(void *VAR1, size_t VAR2, unsigned char VAR3,\nconst void *VAR4, size_t VAR5)\n{\nif (VAR5 != 1)\nreturn -VAR6;\nif (*(unsigned char *)VAR4 > 1)\nreturn -VAR7;\nreturn 1;\n}\n",
      "code_before_change_normalized": "int FUN1(void *VAR1, size_t VAR2, unsigned char VAR3,\nconst void *VAR4, size_t VAR5)\n{\nif (*(unsigned char *)VAR4 > 1)\nreturn -VAR6;\nreturn 1;\n}\n",
      "code_after_change_raw": "int snmp_version(void *context, size_t hdrlen, unsigned char tag,\nconst void *data, size_t datalen)\n{\nif (datalen != 1)\nreturn -EINVAL;\nif (*(unsigned char *)data > 1)\nreturn -ENOTSUPP;\nreturn 1;\n}\n",
      "code_before_change_raw": "int snmp_version(void *context, size_t hdrlen, unsigned char tag,\nconst void *data, size_t datalen)\n{\nif (*(unsigned char *)data > 1)\nreturn -ENOTSUPP;\nreturn 1;\n}\n"
    }
  ],
  "CVE-2019-9454": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for block size in the I2C_SMBUS_I2C_BLOCK_DATA case.",
        "trigger_condition": "When the block size is not properly validated and an out-of-bounds write operation is performed based on the unvalidated size.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the block size before accessing the data array in the I2C_SMBUS_I2C_BLOCK_DATA case, potentially allowing an out-of-bounds write operation if the block size exceeds the maximum allowed value."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add proper validation for the block size in the I2C_SMBUS_I2C_BLOCK_DATA case. This validation should ensure that the block size does not exceed the maximum allowed value before accessing the data array. By validating the block size, the code can prevent out-of-bounds writes and potential memory corruption issues.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-9454, which involves a possible out-of-bounds write due to memory corruption in the i2c driver of the Android kernel. By updating the code as shown, specifically by adding the check for the block size in the `I2C_SMBUS_I2C_BLOCK_DATA` case, we can prevent the out-of-bounds write vulnerability from being exploited. This modification ensures that the block size is properly validated before accessing the data, thus mitigating the risk of memory corruption and potential privilege escalation.",
      "GPT_purpose": "Perform I2C SMBus transactions with support for various transaction types and data sizes.",
      "GPT_function": "\n1. Generate a series of messages for I2C communication.\n2. Handle different types of I2C transactions based on the size parameter.\n3. Compute and check PEC (Packet Error Checking) for I2C messages.\n4. Extract and store data from the received I2C messages based on the transaction type.",
      "CVE_id": "CVE-2019-9454",
      "code_before_change": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\n\t\t\t\t   unsigned short flags,\n\t\t\t\t   char read_write, u8 command, int size,\n\t\t\t\t   union i2c_smbus_data *data)\n{\n\t/* So we need to generate a series of msgs. In the case of writing, we\n\t  need to use only one message; when reading, we need two. We initialize\n\t  most things with sane defaults, to keep the code below somewhat\n\t  simpler. */\n\tunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\n\tunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\n\tint num = read_write == I2C_SMBUS_READ ? 2 : 1;\n\tint i;\n\tu8 partial_pec = 0;\n\tint status;\n\tstruct i2c_msg msg[2] = {\n\t\t{\n\t\t\t.addr = addr,\n\t\t\t.flags = flags,\n\t\t\t.len = 1,\n\t\t\t.buf = msgbuf0,\n\t\t}, {\n\t\t\t.addr = addr,\n\t\t\t.flags = flags | I2C_M_RD,\n\t\t\t.len = 0,\n\t\t\t.buf = msgbuf1,\n\t\t},\n\t};\n\n\tmsgbuf0[0] = command;\n\tswitch (size) {\n\tcase I2C_SMBUS_QUICK:\n\t\tmsg[0].len = 0;\n\t\t/* Special case: The read/write field is used as data */\n\t\tmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\n\t\t\t\t\tI2C_M_RD : 0);\n\t\tnum = 1;\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\t/* Special case: only a read! */\n\t\t\tmsg[0].flags = I2C_M_RD | flags;\n\t\t\tnum = 1;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 1;\n\t\telse {\n\t\t\tmsg[0].len = 2;\n\t\t\tmsgbuf0[1] = data->byte;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_WORD_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 2;\n\t\telse {\n\t\t\tmsg[0].len = 3;\n\t\t\tmsgbuf0[1] = data->word & 0xff;\n\t\t\tmsgbuf0[2] = data->word >> 8;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_PROC_CALL:\n\t\tnum = 2; /* Special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tmsg[0].len = 3;\n\t\tmsg[1].len = 2;\n\t\tmsgbuf0[1] = data->word & 0xff;\n\t\tmsgbuf0[2] = data->word >> 8;\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t\t   the underlying bus driver */\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 2;\n\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\n\t\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\t\tdata->block[0]);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\tnum = 2; /* Another special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\tdata->block[0]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmsg[0].len = data->block[0] + 2;\n\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t   the underlying bus driver */\n\t\tbreak;\n\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].len = data->block[0];\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 1;\n\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {\n\t\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\t\tdata->block[0]);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tfor (i = 1; i <= data->block[0]; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i];\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\ti = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n\t\t\t\t      && size != I2C_SMBUS_I2C_BLOCK_DATA);\n\tif (i) {\n\t\t/* Compute PEC if first message is a write */\n\t\tif (!(msg[0].flags & I2C_M_RD)) {\n\t\t\tif (num == 1) /* Write only */\n\t\t\t\ti2c_smbus_add_pec(&msg[0]);\n\t\t\telse /* Write followed by read */\n\t\t\t\tpartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n\t\t}\n\t\t/* Ask for PEC if last message is a read */\n\t\tif (msg[num-1].flags & I2C_M_RD)\n\t\t\tmsg[num-1].len++;\n\t}\n\n\tstatus = i2c_transfer(adapter, msg, num);\n\tif (status < 0)\n\t\treturn status;\n\n\t/* Check PEC if last message is a read */\n\tif (i && (msg[num-1].flags & I2C_M_RD)) {\n\t\tstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\n\t\tif (status < 0)\n\t\t\treturn status;\n\t}\n\n\tif (read_write == I2C_SMBUS_READ)\n\t\tswitch (size) {\n\t\tcase I2C_SMBUS_BYTE:\n\t\t\tdata->byte = msgbuf0[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BYTE_DATA:\n\t\t\tdata->byte = msgbuf1[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_WORD_DATA:\n\t\tcase I2C_SMBUS_PROC_CALL:\n\t\t\tdata->word = msgbuf1[0] | (msgbuf1[1] << 8);\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\t\tfor (i = 0; i < data->block[0]; i++)\n\t\t\t\tdata->block[i+1] = msgbuf1[i];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\t\tfor (i = 0; i < msgbuf1[0] + 1; i++)\n\t\t\t\tdata->block[i] = msgbuf1[i];\n\t\t\tbreak;\n\t\t}\n\treturn 0;\n}",
      "code_after_change": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\n\t\t\t\t   unsigned short flags,\n\t\t\t\t   char read_write, u8 command, int size,\n\t\t\t\t   union i2c_smbus_data *data)\n{\n\t/* So we need to generate a series of msgs. In the case of writing, we\n\t  need to use only one message; when reading, we need two. We initialize\n\t  most things with sane defaults, to keep the code below somewhat\n\t  simpler. */\n\tunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\n\tunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\n\tint num = read_write == I2C_SMBUS_READ ? 2 : 1;\n\tint i;\n\tu8 partial_pec = 0;\n\tint status;\n\tstruct i2c_msg msg[2] = {\n\t\t{\n\t\t\t.addr = addr,\n\t\t\t.flags = flags,\n\t\t\t.len = 1,\n\t\t\t.buf = msgbuf0,\n\t\t}, {\n\t\t\t.addr = addr,\n\t\t\t.flags = flags | I2C_M_RD,\n\t\t\t.len = 0,\n\t\t\t.buf = msgbuf1,\n\t\t},\n\t};\n\n\tmsgbuf0[0] = command;\n\tswitch (size) {\n\tcase I2C_SMBUS_QUICK:\n\t\tmsg[0].len = 0;\n\t\t/* Special case: The read/write field is used as data */\n\t\tmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\n\t\t\t\t\tI2C_M_RD : 0);\n\t\tnum = 1;\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\t/* Special case: only a read! */\n\t\t\tmsg[0].flags = I2C_M_RD | flags;\n\t\t\tnum = 1;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BYTE_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 1;\n\t\telse {\n\t\t\tmsg[0].len = 2;\n\t\t\tmsgbuf0[1] = data->byte;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_WORD_DATA:\n\t\tif (read_write == I2C_SMBUS_READ)\n\t\t\tmsg[1].len = 2;\n\t\telse {\n\t\t\tmsg[0].len = 3;\n\t\t\tmsgbuf0[1] = data->word & 0xff;\n\t\t\tmsgbuf0[2] = data->word >> 8;\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_PROC_CALL:\n\t\tnum = 2; /* Special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tmsg[0].len = 3;\n\t\tmsg[1].len = 2;\n\t\tmsgbuf0[1] = data->word & 0xff;\n\t\tmsgbuf0[2] = data->word >> 8;\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t\t   the underlying bus driver */\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 2;\n\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\n\t\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\t\tdata->block[0]);\n\t\t\t\treturn -EINVAL;\n\t\t\t}\n\t\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\t}\n\t\tbreak;\n\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\tnum = 2; /* Another special case */\n\t\tread_write = I2C_SMBUS_READ;\n\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\t\t\tdev_err(&adapter->dev,\n\t\t\t\t\"Invalid block write size %d\\n\",\n\t\t\t\tdata->block[0]);\n\t\t\treturn -EINVAL;\n\t\t}\n\t\tmsg[0].len = data->block[0] + 2;\n\t\tfor (i = 1; i < msg[0].len; i++)\n\t\t\tmsgbuf0[i] = data->block[i-1];\n\t\tmsg[1].flags |= I2C_M_RECV_LEN;\n\t\tmsg[1].len = 1; /* block length will be added by\n\t\t\t\t   the underlying bus driver */\n\t\tbreak;\n\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\n\t\t\tdev_err(&adapter->dev, \"Invalid block %s size %d\\n\",\n\t\t\t\tread_write == I2C_SMBUS_READ ? \"read\" : \"write\",\n\t\t\t\tdata->block[0]);\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tif (read_write == I2C_SMBUS_READ) {\n\t\t\tmsg[1].len = data->block[0];\n\t\t} else {\n\t\t\tmsg[0].len = data->block[0] + 1;\n\t\t\tfor (i = 1; i <= data->block[0]; i++)\n\t\t\t\tmsgbuf0[i] = data->block[i];\n\t\t}\n\t\tbreak;\n\tdefault:\n\t\tdev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\n\t\treturn -EOPNOTSUPP;\n\t}\n\n\ti = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n\t\t\t\t      && size != I2C_SMBUS_I2C_BLOCK_DATA);\n\tif (i) {\n\t\t/* Compute PEC if first message is a write */\n\t\tif (!(msg[0].flags & I2C_M_RD)) {\n\t\t\tif (num == 1) /* Write only */\n\t\t\t\ti2c_smbus_add_pec(&msg[0]);\n\t\t\telse /* Write followed by read */\n\t\t\t\tpartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n\t\t}\n\t\t/* Ask for PEC if last message is a read */\n\t\tif (msg[num-1].flags & I2C_M_RD)\n\t\t\tmsg[num-1].len++;\n\t}\n\n\tstatus = i2c_transfer(adapter, msg, num);\n\tif (status < 0)\n\t\treturn status;\n\n\t/* Check PEC if last message is a read */\n\tif (i && (msg[num-1].flags & I2C_M_RD)) {\n\t\tstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\n\t\tif (status < 0)\n\t\t\treturn status;\n\t}\n\n\tif (read_write == I2C_SMBUS_READ)\n\t\tswitch (size) {\n\t\tcase I2C_SMBUS_BYTE:\n\t\t\tdata->byte = msgbuf0[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BYTE_DATA:\n\t\t\tdata->byte = msgbuf1[0];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_WORD_DATA:\n\t\tcase I2C_SMBUS_PROC_CALL:\n\t\t\tdata->word = msgbuf1[0] | (msgbuf1[1] << 8);\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_I2C_BLOCK_DATA:\n\t\t\tfor (i = 0; i < data->block[0]; i++)\n\t\t\t\tdata->block[i+1] = msgbuf1[i];\n\t\t\tbreak;\n\t\tcase I2C_SMBUS_BLOCK_DATA:\n\t\tcase I2C_SMBUS_BLOCK_PROC_CALL:\n\t\t\tfor (i = 0; i < msgbuf1[0] + 1; i++)\n\t\t\t\tdata->block[i] = msgbuf1[i];\n\t\t\tbreak;\n\t\t}\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\tif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {",
          "\t\t\tdev_err(&adapter->dev, \"Invalid block %s size %d\\n\",",
          "\t\t\t\tread_write == I2C_SMBUS_READ ? \"read\" : \"write\",",
          "\t\t\t\tdata->block[0]);",
          "\t\t\treturn -EINVAL;",
          "\t\t}",
          ""
        ],
        "deleted": [
          "\t\t\tif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {",
          "\t\t\t\tdev_err(&adapter->dev,",
          "\t\t\t\t\t\"Invalid block write size %d\\n\",",
          "\t\t\t\t\tdata->block[0]);",
          "\t\t\t\treturn -EINVAL;",
          "\t\t\t}"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for block size in the I2C_SMBUS_I2C_BLOCK_DATA case.",
      "trigger_condition": "When the block size is not properly validated and an out-of-bounds write operation is performed based on the unvalidated size.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the block size before accessing the data array in the I2C_SMBUS_I2C_BLOCK_DATA case, potentially allowing an out-of-bounds write operation if the block size exceeds the maximum allowed value.",
      "id": 60,
      "code_after_change_normalized": "static s32 FUN1(struct i2c_adapter *VAR1, u16 VAR2,\nunsigned short VAR3,\nchar VAR4, u8 VAR5, int VAR6,\nunion i2c_smbus_data *VAR7)\n{\nunsigned char VAR8[VAR9+3];\nunsigned char VAR10[VAR9+2];\nint VAR11 = VAR4 == VAR12 ? 2 : 1;\nint VAR13;\nu8 VAR14 = 0;\nint VAR15;\nstruct i2c_msg VAR16[2] = {\n{\n.VAR2 = VAR2,\n.VAR3 = VAR3,\n.VAR17 = 1,\n.VAR18 = VAR8,\n}, {\n.VAR2 = VAR2,\n.VAR3 = VAR3 | VAR19,\n.VAR17 = 0,\n.VAR18 = VAR10,\n},\n};\nVAR8[0] = VAR5;\nswitch (VAR6) {\ncase VAR20:\nVAR16[0].VAR17 = 0;\nVAR16[0].VAR3 = VAR3 | (VAR4 == VAR12 ?\nVAR19 : 0);\nVAR11 = 1;\nbreak;\ncase VAR21:\nif (VAR4 == VAR12) {\nVAR16[0].VAR3 = VAR19 | VAR3;\nVAR11 = 1;\n}\nbreak;\ncase VAR22:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 1;\nelse {\nVAR16[0].VAR17 = 2;\nVAR8[1] = VAR7->VAR23;\n}\nbreak;\ncase VAR24:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 2;\nelse {\nVAR16[0].VAR17 = 3;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\n}\nbreak;\ncase VAR27:\nVAR11 = 2; \nVAR4 = VAR12;\nVAR16[0].VAR17 = 3;\nVAR16[1].VAR17 = 2;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\nbreak;\ncase VAR28:\nif (VAR4 == VAR12) {\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nif (VAR16[0].VAR17 > VAR9 + 2) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\n}\nbreak;\ncase VAR33:\nVAR11 = 2; \nVAR4 = VAR12;\nif (VAR7->VAR30[0] > VAR9) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \nbreak;\ncase VAR34:\nif (VAR7->VAR30[0] > VAR9) {\nFUN2(&VAR1->VAR31, \"STR\",\nVAR4 == VAR12 ? \"STR\" : \"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nif (VAR4 == VAR12) {\nVAR16[1].VAR17 = VAR7->VAR30[0];\n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 1;\nfor (VAR13 = 1; VAR13 <= VAR7->VAR30[0]; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13];\n}\nbreak;\ndefault:\nFUN2(&VAR1->VAR31, \"STR\", VAR6);\nreturn -VAR35;\n}\nVAR13 = ((VAR3 & VAR36) && VAR6 != VAR20\n&& VAR6 != VAR34);\nif (VAR13) {\nif (!(VAR16[0].VAR3 & VAR19)) {\nif (VAR11 == 1) \nFUN3(&VAR16[0]);\nelse \nVAR14 = FUN4(0, &VAR16[0]);\n}\nif (VAR16[VAR11-1].VAR3 & VAR19)\nVAR16[VAR11-1].VAR17++;\n}\nVAR15 = FUN5(VAR1, VAR16, VAR11);\nif (VAR15 < 0)\nreturn VAR15;\nif (VAR13 && (VAR16[VAR11-1].VAR3 & VAR19)) {\nVAR15 = FUN6(VAR14, &VAR16[VAR11-1]);\nif (VAR15 < 0)\nreturn VAR15;\n}\nif (VAR4 == VAR12)\nswitch (VAR6) {\ncase VAR21:\nVAR7->VAR23 = VAR8[0];\nbreak;\ncase VAR22:\nVAR7->VAR23 = VAR10[0];\nbreak;\ncase VAR24:\ncase VAR27:\nVAR7->VAR25 = VAR10[0] | (VAR10[1] << 8);\nbreak;\ncase VAR34:\nfor (VAR13 = 0; VAR13 < VAR7->VAR30[0]; VAR13++)\nVAR7->VAR30[VAR13+1] = VAR10[VAR13];\nbreak;\ncase VAR28:\ncase VAR33:\nfor (VAR13 = 0; VAR13 < VAR10[0] + 1; VAR13++)\nVAR7->VAR30[VAR13] = VAR10[VAR13];\nbreak;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static s32 FUN1(struct i2c_adapter *VAR1, u16 VAR2,\nunsigned short VAR3,\nchar VAR4, u8 VAR5, int VAR6,\nunion i2c_smbus_data *VAR7)\n{\nunsigned char VAR8[VAR9+3];\nunsigned char VAR10[VAR9+2];\nint VAR11 = VAR4 == VAR12 ? 2 : 1;\nint VAR13;\nu8 VAR14 = 0;\nint VAR15;\nstruct i2c_msg VAR16[2] = {\n{\n.VAR2 = VAR2,\n.VAR3 = VAR3,\n.VAR17 = 1,\n.VAR18 = VAR8,\n}, {\n.VAR2 = VAR2,\n.VAR3 = VAR3 | VAR19,\n.VAR17 = 0,\n.VAR18 = VAR10,\n},\n};\nVAR8[0] = VAR5;\nswitch (VAR6) {\ncase VAR20:\nVAR16[0].VAR17 = 0;\nVAR16[0].VAR3 = VAR3 | (VAR4 == VAR12 ?\nVAR19 : 0);\nVAR11 = 1;\nbreak;\ncase VAR21:\nif (VAR4 == VAR12) {\nVAR16[0].VAR3 = VAR19 | VAR3;\nVAR11 = 1;\n}\nbreak;\ncase VAR22:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 1;\nelse {\nVAR16[0].VAR17 = 2;\nVAR8[1] = VAR7->VAR23;\n}\nbreak;\ncase VAR24:\nif (VAR4 == VAR12)\nVAR16[1].VAR17 = 2;\nelse {\nVAR16[0].VAR17 = 3;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\n}\nbreak;\ncase VAR27:\nVAR11 = 2; \nVAR4 = VAR12;\nVAR16[0].VAR17 = 3;\nVAR16[1].VAR17 = 2;\nVAR8[1] = VAR7->VAR25 & VAR26;\nVAR8[2] = VAR7->VAR25 >> 8;\nbreak;\ncase VAR28:\nif (VAR4 == VAR12) {\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nif (VAR16[0].VAR17 > VAR9 + 2) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\n}\nbreak;\ncase VAR33:\nVAR11 = 2; \nVAR4 = VAR12;\nif (VAR7->VAR30[0] > VAR9) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nVAR16[0].VAR17 = VAR7->VAR30[0] + 2;\nfor (VAR13 = 1; VAR13 < VAR16[0].VAR17; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13-1];\nVAR16[1].VAR3 |= VAR29;\nVAR16[1].VAR17 = 1; \nbreak;\ncase VAR34:\nif (VAR4 == VAR12) {\nVAR16[1].VAR17 = VAR7->VAR30[0];\n} else {\nVAR16[0].VAR17 = VAR7->VAR30[0] + 1;\nif (VAR16[0].VAR17 > VAR9 + 1) {\nFUN2(&VAR1->VAR31,\n\"STR\",\nVAR7->VAR30[0]);\nreturn -VAR32;\n}\nfor (VAR13 = 1; VAR13 <= VAR7->VAR30[0]; VAR13++)\nVAR8[VAR13] = VAR7->VAR30[VAR13];\n}\nbreak;\ndefault:\nFUN2(&VAR1->VAR31, \"STR\", VAR6);\nreturn -VAR35;\n}\nVAR13 = ((VAR3 & VAR36) && VAR6 != VAR20\n&& VAR6 != VAR34);\nif (VAR13) {\nif (!(VAR16[0].VAR3 & VAR19)) {\nif (VAR11 == 1) \nFUN3(&VAR16[0]);\nelse \nVAR14 = FUN4(0, &VAR16[0]);\n}\nif (VAR16[VAR11-1].VAR3 & VAR19)\nVAR16[VAR11-1].VAR17++;\n}\nVAR15 = FUN5(VAR1, VAR16, VAR11);\nif (VAR15 < 0)\nreturn VAR15;\nif (VAR13 && (VAR16[VAR11-1].VAR3 & VAR19)) {\nVAR15 = FUN6(VAR14, &VAR16[VAR11-1]);\nif (VAR15 < 0)\nreturn VAR15;\n}\nif (VAR4 == VAR12)\nswitch (VAR6) {\ncase VAR21:\nVAR7->VAR23 = VAR8[0];\nbreak;\ncase VAR22:\nVAR7->VAR23 = VAR10[0];\nbreak;\ncase VAR24:\ncase VAR27:\nVAR7->VAR25 = VAR10[0] | (VAR10[1] << 8);\nbreak;\ncase VAR34:\nfor (VAR13 = 0; VAR13 < VAR7->VAR30[0]; VAR13++)\nVAR7->VAR30[VAR13+1] = VAR10[VAR13];\nbreak;\ncase VAR28:\ncase VAR33:\nfor (VAR13 = 0; VAR13 < VAR10[0] + 1; VAR13++)\nVAR7->VAR30[VAR13] = VAR10[VAR13];\nbreak;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\nunsigned short flags,\nchar read_write, u8 command, int size,\nunion i2c_smbus_data *data)\n{\nunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\nunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\nint num = read_write == I2C_SMBUS_READ ? 2 : 1;\nint i;\nu8 partial_pec = 0;\nint status;\nstruct i2c_msg msg[2] = {\n{\n.addr = addr,\n.flags = flags,\n.len = 1,\n.buf = msgbuf0,\n}, {\n.addr = addr,\n.flags = flags | I2C_M_RD,\n.len = 0,\n.buf = msgbuf1,\n},\n};\nmsgbuf0[0] = command;\nswitch (size) {\ncase I2C_SMBUS_QUICK:\nmsg[0].len = 0;\nmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\nI2C_M_RD : 0);\nnum = 1;\nbreak;\ncase I2C_SMBUS_BYTE:\nif (read_write == I2C_SMBUS_READ) {\nmsg[0].flags = I2C_M_RD | flags;\nnum = 1;\n}\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 1;\nelse {\nmsg[0].len = 2;\nmsgbuf0[1] = data->byte;\n}\nbreak;\ncase I2C_SMBUS_WORD_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 2;\nelse {\nmsg[0].len = 3;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\n}\nbreak;\ncase I2C_SMBUS_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nmsg[0].len = 3;\nmsg[1].len = 2;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \n} else {\nmsg[0].len = data->block[0] + 2;\nif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\n}\nbreak;\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nmsg[0].len = data->block[0] + 2;\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\ndev_err(&adapter->dev, \"Invalid block %s size %d\\n\",\nread_write == I2C_SMBUS_READ ? \"read\" : \"write\",\ndata->block[0]);\nreturn -EINVAL;\n}\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].len = data->block[0];\n} else {\nmsg[0].len = data->block[0] + 1;\nfor (i = 1; i <= data->block[0]; i++)\nmsgbuf0[i] = data->block[i];\n}\nbreak;\ndefault:\ndev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\nreturn -EOPNOTSUPP;\n}\ni = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n&& size != I2C_SMBUS_I2C_BLOCK_DATA);\nif (i) {\nif (!(msg[0].flags & I2C_M_RD)) {\nif (num == 1) \ni2c_smbus_add_pec(&msg[0]);\nelse \npartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n}\nif (msg[num-1].flags & I2C_M_RD)\nmsg[num-1].len++;\n}\nstatus = i2c_transfer(adapter, msg, num);\nif (status < 0)\nreturn status;\nif (i && (msg[num-1].flags & I2C_M_RD)) {\nstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\nif (status < 0)\nreturn status;\n}\nif (read_write == I2C_SMBUS_READ)\nswitch (size) {\ncase I2C_SMBUS_BYTE:\ndata->byte = msgbuf0[0];\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\ndata->byte = msgbuf1[0];\nbreak;\ncase I2C_SMBUS_WORD_DATA:\ncase I2C_SMBUS_PROC_CALL:\ndata->word = msgbuf1[0] | (msgbuf1[1] << 8);\nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nfor (i = 0; i < data->block[0]; i++)\ndata->block[i+1] = msgbuf1[i];\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nfor (i = 0; i < msgbuf1[0] + 1; i++)\ndata->block[i] = msgbuf1[i];\nbreak;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static s32 i2c_smbus_xfer_emulated(struct i2c_adapter *adapter, u16 addr,\nunsigned short flags,\nchar read_write, u8 command, int size,\nunion i2c_smbus_data *data)\n{\nunsigned char msgbuf0[I2C_SMBUS_BLOCK_MAX+3];\nunsigned char msgbuf1[I2C_SMBUS_BLOCK_MAX+2];\nint num = read_write == I2C_SMBUS_READ ? 2 : 1;\nint i;\nu8 partial_pec = 0;\nint status;\nstruct i2c_msg msg[2] = {\n{\n.addr = addr,\n.flags = flags,\n.len = 1,\n.buf = msgbuf0,\n}, {\n.addr = addr,\n.flags = flags | I2C_M_RD,\n.len = 0,\n.buf = msgbuf1,\n},\n};\nmsgbuf0[0] = command;\nswitch (size) {\ncase I2C_SMBUS_QUICK:\nmsg[0].len = 0;\nmsg[0].flags = flags | (read_write == I2C_SMBUS_READ ?\nI2C_M_RD : 0);\nnum = 1;\nbreak;\ncase I2C_SMBUS_BYTE:\nif (read_write == I2C_SMBUS_READ) {\nmsg[0].flags = I2C_M_RD | flags;\nnum = 1;\n}\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 1;\nelse {\nmsg[0].len = 2;\nmsgbuf0[1] = data->byte;\n}\nbreak;\ncase I2C_SMBUS_WORD_DATA:\nif (read_write == I2C_SMBUS_READ)\nmsg[1].len = 2;\nelse {\nmsg[0].len = 3;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\n}\nbreak;\ncase I2C_SMBUS_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nmsg[0].len = 3;\nmsg[1].len = 2;\nmsgbuf0[1] = data->word & 0xff;\nmsgbuf0[2] = data->word >> 8;\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \n} else {\nmsg[0].len = data->block[0] + 2;\nif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 2) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\n}\nbreak;\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nnum = 2; \nread_write = I2C_SMBUS_READ;\nif (data->block[0] > I2C_SMBUS_BLOCK_MAX) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nmsg[0].len = data->block[0] + 2;\nfor (i = 1; i < msg[0].len; i++)\nmsgbuf0[i] = data->block[i-1];\nmsg[1].flags |= I2C_M_RECV_LEN;\nmsg[1].len = 1; \nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nif (read_write == I2C_SMBUS_READ) {\nmsg[1].len = data->block[0];\n} else {\nmsg[0].len = data->block[0] + 1;\nif (msg[0].len > I2C_SMBUS_BLOCK_MAX + 1) {\ndev_err(&adapter->dev,\n\"Invalid block write size %d\\n\",\ndata->block[0]);\nreturn -EINVAL;\n}\nfor (i = 1; i <= data->block[0]; i++)\nmsgbuf0[i] = data->block[i];\n}\nbreak;\ndefault:\ndev_err(&adapter->dev, \"Unsupported transaction %d\\n\", size);\nreturn -EOPNOTSUPP;\n}\ni = ((flags & I2C_CLIENT_PEC) && size != I2C_SMBUS_QUICK\n&& size != I2C_SMBUS_I2C_BLOCK_DATA);\nif (i) {\nif (!(msg[0].flags & I2C_M_RD)) {\nif (num == 1) \ni2c_smbus_add_pec(&msg[0]);\nelse \npartial_pec = i2c_smbus_msg_pec(0, &msg[0]);\n}\nif (msg[num-1].flags & I2C_M_RD)\nmsg[num-1].len++;\n}\nstatus = i2c_transfer(adapter, msg, num);\nif (status < 0)\nreturn status;\nif (i && (msg[num-1].flags & I2C_M_RD)) {\nstatus = i2c_smbus_check_pec(partial_pec, &msg[num-1]);\nif (status < 0)\nreturn status;\n}\nif (read_write == I2C_SMBUS_READ)\nswitch (size) {\ncase I2C_SMBUS_BYTE:\ndata->byte = msgbuf0[0];\nbreak;\ncase I2C_SMBUS_BYTE_DATA:\ndata->byte = msgbuf1[0];\nbreak;\ncase I2C_SMBUS_WORD_DATA:\ncase I2C_SMBUS_PROC_CALL:\ndata->word = msgbuf1[0] | (msgbuf1[1] << 8);\nbreak;\ncase I2C_SMBUS_I2C_BLOCK_DATA:\nfor (i = 0; i < data->block[0]; i++)\ndata->block[i+1] = msgbuf1[i];\nbreak;\ncase I2C_SMBUS_BLOCK_DATA:\ncase I2C_SMBUS_BLOCK_PROC_CALL:\nfor (i = 0; i < msgbuf1[0] + 1; i++)\ndata->block[i] = msgbuf1[i];\nbreak;\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2019-9456": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of bounds checking before copying data from a kernel buffer to user space buffer.",
        "trigger_condition": "An attacker can exploit this vulnerability by providing crafted input data to trigger an out-of-bounds write during the data copy operation.",
        "specific_code_behavior_causing_vulnerability": "The original code snippet copies data from a kernel buffer to a user space buffer without verifying the bounds of the data being copied, potentially leading to a buffer overflow vulnerability."
      },
      "solution": "To mitigate the vulnerability, the code was modified to introduce a check before reading and processing the data to ensure there is no pending data to be sent to the user space buffer. Additionally, a new function was added to handle the data copy operation with proper bounds checking to prevent out-of-bounds writes. These changes enhance the security of the code by ensuring safe data copying to the user space buffer.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2019-9456, which is a possible out-of-bounds (OOB) write due to a missing bounds check in the original code snippet. \n\nIn the original code, the function `mon_text_read_u` directly copies data from `rp->printf_buf` to the user space buffer `buf` without checking the bounds of the data being copied. This can lead to a buffer overflow vulnerability, allowing an attacker to write beyond the allocated memory region and potentially execute arbitrary code or escalate privileges.\n\nThe modification introduces a check `if (rp->printf_togo == 0)` before reading and processing the data. This check ensures that the data is only read and processed when there is no pending data to be sent to the user space buffer. Additionally, the modified code introduces a new function `mon_text_copy_to_user` to handle the data copy operation, which includes proper bounds checking to prevent OOB writes.\n\nBy making these changes, the modified code ensures that data is copied safely to the user space buffer, mitigating the risk of the OOB write vulnerability and enhancing the overall security of the code.",
      "GPT_purpose": "Read USB monitor events and transfer the data to user space, potentially leading to an out-of-bounds write vulnerability.",
      "GPT_function": "\n1. Read data from a USB monitor device.\n2. Process the read data based on event type and transfer type.\n3. Copy the processed data to a user buffer.",
      "CVE_id": "CVE-2019-9456",
      "code_before_change": "static ssize_t mon_text_read_u(struct file *file, char __user *buf,\n\t\t\t\tsize_t nbytes, loff_t *ppos)\n{\n\tstruct mon_reader_text *rp = file->private_data;\n\tstruct mon_event_text *ep;\n\tstruct mon_text_ptr ptr;\n\n\tep = mon_text_read_wait(rp, file);\n\tif (IS_ERR(ep))\n\t\treturn PTR_ERR(ep);\n\tmutex_lock(&rp->printf_lock);\n\tptr.cnt = 0;\n\tptr.pbuf = rp->printf_buf;\n\tptr.limit = rp->printf_size;\n\n\tmon_text_read_head_u(rp, &ptr, ep);\n\tif (ep->type == 'E') {\n\t\tmon_text_read_statset(rp, &ptr, ep);\n\t} else if (ep->xfertype == USB_ENDPOINT_XFER_ISOC) {\n\t\tmon_text_read_isostat(rp, &ptr, ep);\n\t\tmon_text_read_isodesc(rp, &ptr, ep);\n\t} else if (ep->xfertype == USB_ENDPOINT_XFER_INT) {\n\t\tmon_text_read_intstat(rp, &ptr, ep);\n\t} else {\n\t\tmon_text_read_statset(rp, &ptr, ep);\n\t}\n\tptr.cnt += snprintf(ptr.pbuf + ptr.cnt, ptr.limit - ptr.cnt,\n\t    \" %d\", ep->length);\n\tmon_text_read_data(rp, &ptr, ep);\n\n\tif (copy_to_user(buf, rp->printf_buf, ptr.cnt))\n\t\tptr.cnt = -EFAULT;\n\tmutex_unlock(&rp->printf_lock);\n\tkmem_cache_free(rp->e_slab, ep);\n\treturn ptr.cnt;\n}",
      "code_after_change": "static ssize_t mon_text_read_u(struct file *file, char __user *buf,\n    size_t nbytes, loff_t *ppos)\n{\n\tstruct mon_reader_text *rp = file->private_data;\n\tstruct mon_event_text *ep;\n\tstruct mon_text_ptr ptr;\n\tssize_t ret;\n\n\tmutex_lock(&rp->printf_lock);\n\n\tif (rp->printf_togo == 0) {\n\n\t\tep = mon_text_read_wait(rp, file);\n\t\tif (IS_ERR(ep)) {\n\t\t\tmutex_unlock(&rp->printf_lock);\n\t\t\treturn PTR_ERR(ep);\n\t\t}\n\t\tptr.cnt = 0;\n\t\tptr.pbuf = rp->printf_buf;\n\t\tptr.limit = rp->printf_size;\n\n\t\tmon_text_read_head_u(rp, &ptr, ep);\n\t\tif (ep->type == 'E') {\n\t\t\tmon_text_read_statset(rp, &ptr, ep);\n\t\t} else if (ep->xfertype == USB_ENDPOINT_XFER_ISOC) {\n\t\t\tmon_text_read_isostat(rp, &ptr, ep);\n\t\t\tmon_text_read_isodesc(rp, &ptr, ep);\n\t\t} else if (ep->xfertype == USB_ENDPOINT_XFER_INT) {\n\t\t\tmon_text_read_intstat(rp, &ptr, ep);\n\t\t} else {\n\t\t\tmon_text_read_statset(rp, &ptr, ep);\n\t\t}\n\t\tptr.cnt += snprintf(ptr.pbuf + ptr.cnt, ptr.limit - ptr.cnt,\n\t\t    \" %d\", ep->length);\n\t\tmon_text_read_data(rp, &ptr, ep);\n\n\t\trp->printf_togo = ptr.cnt;\n\t\trp->printf_offset = 0;\n\n\t\tkmem_cache_free(rp->e_slab, ep);\n\t}\n\n\tret = mon_text_copy_to_user(rp, buf, nbytes);\n\tmutex_unlock(&rp->printf_lock);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "    size_t nbytes, loff_t *ppos)",
          "\tssize_t ret;",
          "\tif (rp->printf_togo == 0) {",
          "",
          "\t\tep = mon_text_read_wait(rp, file);",
          "\t\tif (IS_ERR(ep)) {",
          "\t\t\tmutex_unlock(&rp->printf_lock);",
          "\t\t\treturn PTR_ERR(ep);",
          "\t\t}",
          "\t\tptr.cnt = 0;",
          "\t\tptr.pbuf = rp->printf_buf;",
          "\t\tptr.limit = rp->printf_size;",
          "",
          "\t\tmon_text_read_head_u(rp, &ptr, ep);",
          "\t\tif (ep->type == 'E') {",
          "\t\t\tmon_text_read_statset(rp, &ptr, ep);",
          "\t\t} else if (ep->xfertype == USB_ENDPOINT_XFER_ISOC) {",
          "\t\t\tmon_text_read_isostat(rp, &ptr, ep);",
          "\t\t\tmon_text_read_isodesc(rp, &ptr, ep);",
          "\t\t} else if (ep->xfertype == USB_ENDPOINT_XFER_INT) {",
          "\t\t\tmon_text_read_intstat(rp, &ptr, ep);",
          "\t\t} else {",
          "\t\t\tmon_text_read_statset(rp, &ptr, ep);",
          "\t\t}",
          "\t\tptr.cnt += snprintf(ptr.pbuf + ptr.cnt, ptr.limit - ptr.cnt,",
          "\t\t    \" %d\", ep->length);",
          "\t\tmon_text_read_data(rp, &ptr, ep);",
          "",
          "\t\trp->printf_togo = ptr.cnt;",
          "\t\trp->printf_offset = 0;",
          "",
          "\t\tkmem_cache_free(rp->e_slab, ep);",
          "\tret = mon_text_copy_to_user(rp, buf, nbytes);",
          "\treturn ret;"
        ],
        "deleted": [
          "\t\t\t\tsize_t nbytes, loff_t *ppos)",
          "\tep = mon_text_read_wait(rp, file);",
          "\tif (IS_ERR(ep))",
          "\t\treturn PTR_ERR(ep);",
          "\tptr.cnt = 0;",
          "\tptr.pbuf = rp->printf_buf;",
          "\tptr.limit = rp->printf_size;",
          "\tmon_text_read_head_u(rp, &ptr, ep);",
          "\tif (ep->type == 'E') {",
          "\t\tmon_text_read_statset(rp, &ptr, ep);",
          "\t} else if (ep->xfertype == USB_ENDPOINT_XFER_ISOC) {",
          "\t\tmon_text_read_isostat(rp, &ptr, ep);",
          "\t\tmon_text_read_isodesc(rp, &ptr, ep);",
          "\t} else if (ep->xfertype == USB_ENDPOINT_XFER_INT) {",
          "\t\tmon_text_read_intstat(rp, &ptr, ep);",
          "\t} else {",
          "\t\tmon_text_read_statset(rp, &ptr, ep);",
          "\tptr.cnt += snprintf(ptr.pbuf + ptr.cnt, ptr.limit - ptr.cnt,",
          "\t    \" %d\", ep->length);",
          "\tmon_text_read_data(rp, &ptr, ep);",
          "\tif (copy_to_user(buf, rp->printf_buf, ptr.cnt))",
          "\t\tptr.cnt = -EFAULT;",
          "\tkmem_cache_free(rp->e_slab, ep);",
          "\treturn ptr.cnt;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of bounds checking before copying data from a kernel buffer to user space buffer.",
      "trigger_condition": "An attacker can exploit this vulnerability by providing crafted input data to trigger an out-of-bounds write during the data copy operation.",
      "specific_code_behavior_causing_vulnerability": "The original code snippet copies data from a kernel buffer to a user space buffer without verifying the bounds of the data being copied, potentially leading to a buffer overflow vulnerability.",
      "id": 61,
      "code_after_change_normalized": "static ssize_t FUN1(struct VAR1 *VAR1, char __user *VAR2,\nsize_t VAR3, loff_t *VAR4)\n{\nstruct mon_reader_text *VAR5 = VAR1->VAR6;\nstruct mon_event_text *VAR7;\nstruct mon_text_ptr VAR8;\nssize_t VAR9;\nFUN2(&VAR5->VAR10);\nif (VAR5->VAR11 == 0) {\nVAR7 = FUN3(VAR5, VAR1);\nif (FUN4(VAR7)) {\nFUN5(&VAR5->VAR10);\nreturn FUN6(VAR7);\n}\nVAR8.VAR12 = 0;\nVAR8.VAR13 = VAR5->VAR14;\nVAR8.VAR15 = VAR5->VAR16;\nFUN7(VAR5, &VAR8, VAR7);\nif (VAR7->VAR17 == ) {\nFUN8(VAR5, &VAR8, VAR7);\n} else if (VAR7->VAR18 == VAR19) {\nFUN9(VAR5, &VAR8, VAR7);\nFUN10(VAR5, &VAR8, VAR7);\n} else if (VAR7->VAR18 == VAR20) {\nFUN11(VAR5, &VAR8, VAR7);\n} else {\nFUN8(VAR5, &VAR8, VAR7);\n}\nVAR8.VAR12 += FUN12(VAR8.VAR13 + VAR8.VAR12, VAR8.VAR15 - VAR8.VAR12,\n\"STR\", VAR7->VAR21);\nFUN13(VAR5, &VAR8, VAR7);\nVAR5->VAR11 = VAR8.VAR12;\nVAR5->VAR22 = 0;\nFUN14(VAR5->VAR23, VAR7);\n}\nVAR9 = FUN15(VAR5, VAR2, VAR3);\nFUN5(&VAR5->VAR10);\nreturn VAR9;\n}\n",
      "code_before_change_normalized": "static ssize_t FUN1(struct VAR1 *VAR1, char __user *VAR2,\nsize_t VAR3, loff_t *VAR4)\n{\nstruct mon_reader_text *VAR5 = VAR1->VAR6;\nstruct mon_event_text *VAR7;\nstruct mon_text_ptr VAR8;\nVAR7 = FUN2(VAR5, VAR1);\nif (FUN3(VAR7))\nreturn FUN4(VAR7);\nFUN5(&VAR5->VAR9);\nVAR8.VAR10 = 0;\nVAR8.VAR11 = VAR5->VAR12;\nVAR8.VAR13 = VAR5->VAR14;\nFUN6(VAR5, &VAR8, VAR7);\nif (VAR7->VAR15 == ) {\nFUN7(VAR5, &VAR8, VAR7);\n} else if (VAR7->VAR16 == VAR17) {\nFUN8(VAR5, &VAR8, VAR7);\nFUN9(VAR5, &VAR8, VAR7);\n} else if (VAR7->VAR16 == VAR18) {\nFUN10(VAR5, &VAR8, VAR7);\n} else {\nFUN7(VAR5, &VAR8, VAR7);\n}\nVAR8.VAR10 += FUN11(VAR8.VAR11 + VAR8.VAR10, VAR8.VAR13 - VAR8.VAR10,\n\"STR\", VAR7->VAR19);\nFUN12(VAR5, &VAR8, VAR7);\nif (FUN13(VAR2, VAR5->VAR12, VAR8.VAR10))\nVAR8.VAR10 = -VAR20;\nFUN14(&VAR5->VAR9);\nFUN15(VAR5->VAR21, VAR7);\nreturn VAR8.VAR10;\n}\n",
      "code_after_change_raw": "static ssize_t mon_text_read_u(struct file *file, char __user *buf,\nsize_t nbytes, loff_t *ppos)\n{\nstruct mon_reader_text *rp = file->private_data;\nstruct mon_event_text *ep;\nstruct mon_text_ptr ptr;\nssize_t ret;\nmutex_lock(&rp->printf_lock);\nif (rp->printf_togo == 0) {\nep = mon_text_read_wait(rp, file);\nif (IS_ERR(ep)) {\nmutex_unlock(&rp->printf_lock);\nreturn PTR_ERR(ep);\n}\nptr.cnt = 0;\nptr.pbuf = rp->printf_buf;\nptr.limit = rp->printf_size;\nmon_text_read_head_u(rp, &ptr, ep);\nif (ep->type == 'E') {\nmon_text_read_statset(rp, &ptr, ep);\n} else if (ep->xfertype == USB_ENDPOINT_XFER_ISOC) {\nmon_text_read_isostat(rp, &ptr, ep);\nmon_text_read_isodesc(rp, &ptr, ep);\n} else if (ep->xfertype == USB_ENDPOINT_XFER_INT) {\nmon_text_read_intstat(rp, &ptr, ep);\n} else {\nmon_text_read_statset(rp, &ptr, ep);\n}\nptr.cnt += snprintf(ptr.pbuf + ptr.cnt, ptr.limit - ptr.cnt,\n\" %d\", ep->length);\nmon_text_read_data(rp, &ptr, ep);\nrp->printf_togo = ptr.cnt;\nrp->printf_offset = 0;\nkmem_cache_free(rp->e_slab, ep);\n}\nret = mon_text_copy_to_user(rp, buf, nbytes);\nmutex_unlock(&rp->printf_lock);\nreturn ret;\n}\n",
      "code_before_change_raw": "static ssize_t mon_text_read_u(struct file *file, char __user *buf,\nsize_t nbytes, loff_t *ppos)\n{\nstruct mon_reader_text *rp = file->private_data;\nstruct mon_event_text *ep;\nstruct mon_text_ptr ptr;\nep = mon_text_read_wait(rp, file);\nif (IS_ERR(ep))\nreturn PTR_ERR(ep);\nmutex_lock(&rp->printf_lock);\nptr.cnt = 0;\nptr.pbuf = rp->printf_buf;\nptr.limit = rp->printf_size;\nmon_text_read_head_u(rp, &ptr, ep);\nif (ep->type == 'E') {\nmon_text_read_statset(rp, &ptr, ep);\n} else if (ep->xfertype == USB_ENDPOINT_XFER_ISOC) {\nmon_text_read_isostat(rp, &ptr, ep);\nmon_text_read_isodesc(rp, &ptr, ep);\n} else if (ep->xfertype == USB_ENDPOINT_XFER_INT) {\nmon_text_read_intstat(rp, &ptr, ep);\n} else {\nmon_text_read_statset(rp, &ptr, ep);\n}\nptr.cnt += snprintf(ptr.pbuf + ptr.cnt, ptr.limit - ptr.cnt,\n\" %d\", ep->length);\nmon_text_read_data(rp, &ptr, ep);\nif (copy_to_user(buf, rp->printf_buf, ptr.cnt))\nptr.cnt = -EFAULT;\nmutex_unlock(&rp->printf_lock);\nkmem_cache_free(rp->e_slab, ep);\nreturn ptr.cnt;\n}\n"
    }
  ],
  "CVE-2019-9500": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of network information during event processing in the WiFi driver.",
        "trigger_condition": "Receiving a malicious event frame related to Wake-up on Wireless LAN functionality, leading to a heap buffer overflow in the brcmf_wowl_nd_results function.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly check and limit the length of the SSID field in the received network information, allowing a potential buffer overflow when copying the SSID data into the destination buffer."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the length of the SSID field does not exceed the maximum allowed length before copying it into the destination buffer. By limiting the size of the copied data to the allocated buffer size, the code prevents a buffer overflow and enhances security.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2019-9500, which involves a heap buffer overflow in the brcmf_wowl_nd_results function of the Broadcom brcmfmac WiFi driver. \n\nThe vulnerability occurs when handling event frames related to Wake-up on Wireless LAN functionality. By sending a malicious event frame, an attacker can trigger a heap buffer overflow, potentially leading to arbitrary code execution or denial-of-service conditions.\n\nThe specific modification made in the code snippet is to ensure that the length of the SSID (Service Set Identifier) field in the received network information is properly checked and limited to prevent a buffer overflow. By adding the check `if (netinfo->SSID_len > IEEE80211_MAX_SSID_LEN) netinfo->SSID_len = IEEE80211_MAX_SSID_LEN;`, the code ensures that the SSID length does not exceed the maximum allowed length (IEEE80211_MAX_SSID_LEN) before copying it into the destination buffer.\n\nThis modification helps prevent the buffer overflow by restricting the size of the copied data to the allocated buffer size, thereby mitigating the vulnerability and enhancing the security of the code.",
      "GPT_purpose": "Handle Wake-up on Wireless LAN (WoWLAN) network detection results in the Broadcom brcmfmac WiFi driver.",
      "GPT_function": "\n1. Check event data size and return if too small.\n2. Handle PFN NET LOST event and return if detected.\n3. Copy network information from event data to configuration.\n4. Set network detect information parameters.\n5. Signal completion of network detect data reception.",
      "CVE_id": "CVE-2019-9500",
      "code_before_change": "static s32\nbrcmf_wowl_nd_results(struct brcmf_if *ifp, const struct brcmf_event_msg *e,\n\t\t      void *data)\n{\n\tstruct brcmf_cfg80211_info *cfg = ifp->drvr->config;\n\tstruct wiphy *wiphy = cfg_to_wiphy(cfg);\n\tstruct brcmf_pno_scanresults_le *pfn_result;\n\tstruct brcmf_pno_net_info_le *netinfo;\n\n\tbrcmf_dbg(SCAN, \"Enter\\n\");\n\n\tif (e->datalen < (sizeof(*pfn_result) + sizeof(*netinfo))) {\n\t\tbrcmf_dbg(SCAN, \"Event data to small. Ignore\\n\");\n\t\treturn 0;\n\t}\n\n\tpfn_result = (struct brcmf_pno_scanresults_le *)data;\n\n\tif (e->event_code == BRCMF_E_PFN_NET_LOST) {\n\t\tbrcmf_dbg(SCAN, \"PFN NET LOST event. Ignore\\n\");\n\t\treturn 0;\n\t}\n\n\tif (le32_to_cpu(pfn_result->count) < 1) {\n\t\tbphy_err(wiphy, \"Invalid result count, expected 1 (%d)\\n\",\n\t\t\t le32_to_cpu(pfn_result->count));\n\t\treturn -EINVAL;\n\t}\n\n\tnetinfo = brcmf_get_netinfo_array(pfn_result);\n\tmemcpy(cfg->wowl.nd->ssid.ssid, netinfo->SSID, netinfo->SSID_len);\n\tcfg->wowl.nd->ssid.ssid_len = netinfo->SSID_len;\n\tcfg->wowl.nd->n_channels = 1;\n\tcfg->wowl.nd->channels[0] =\n\t\tieee80211_channel_to_frequency(netinfo->channel,\n\t\t\tnetinfo->channel <= CH_MAX_2G_CHANNEL ?\n\t\t\t\t\tNL80211_BAND_2GHZ : NL80211_BAND_5GHZ);\n\tcfg->wowl.nd_info->n_matches = 1;\n\tcfg->wowl.nd_info->matches[0] = cfg->wowl.nd;\n\n\t/* Inform (the resume task) that the net detect information was recvd */\n\tcfg->wowl.nd_data_completed = true;\n\twake_up(&cfg->wowl.nd_data_wait);\n\n\treturn 0;\n}",
      "code_after_change": "static s32\nbrcmf_wowl_nd_results(struct brcmf_if *ifp, const struct brcmf_event_msg *e,\n\t\t      void *data)\n{\n\tstruct brcmf_cfg80211_info *cfg = ifp->drvr->config;\n\tstruct wiphy *wiphy = cfg_to_wiphy(cfg);\n\tstruct brcmf_pno_scanresults_le *pfn_result;\n\tstruct brcmf_pno_net_info_le *netinfo;\n\n\tbrcmf_dbg(SCAN, \"Enter\\n\");\n\n\tif (e->datalen < (sizeof(*pfn_result) + sizeof(*netinfo))) {\n\t\tbrcmf_dbg(SCAN, \"Event data to small. Ignore\\n\");\n\t\treturn 0;\n\t}\n\n\tpfn_result = (struct brcmf_pno_scanresults_le *)data;\n\n\tif (e->event_code == BRCMF_E_PFN_NET_LOST) {\n\t\tbrcmf_dbg(SCAN, \"PFN NET LOST event. Ignore\\n\");\n\t\treturn 0;\n\t}\n\n\tif (le32_to_cpu(pfn_result->count) < 1) {\n\t\tbphy_err(wiphy, \"Invalid result count, expected 1 (%d)\\n\",\n\t\t\t le32_to_cpu(pfn_result->count));\n\t\treturn -EINVAL;\n\t}\n\n\tnetinfo = brcmf_get_netinfo_array(pfn_result);\n\tif (netinfo->SSID_len > IEEE80211_MAX_SSID_LEN)\n\t\tnetinfo->SSID_len = IEEE80211_MAX_SSID_LEN;\n\tmemcpy(cfg->wowl.nd->ssid.ssid, netinfo->SSID, netinfo->SSID_len);\n\tcfg->wowl.nd->ssid.ssid_len = netinfo->SSID_len;\n\tcfg->wowl.nd->n_channels = 1;\n\tcfg->wowl.nd->channels[0] =\n\t\tieee80211_channel_to_frequency(netinfo->channel,\n\t\t\tnetinfo->channel <= CH_MAX_2G_CHANNEL ?\n\t\t\t\t\tNL80211_BAND_2GHZ : NL80211_BAND_5GHZ);\n\tcfg->wowl.nd_info->n_matches = 1;\n\tcfg->wowl.nd_info->matches[0] = cfg->wowl.nd;\n\n\t/* Inform (the resume task) that the net detect information was recvd */\n\tcfg->wowl.nd_data_completed = true;\n\twake_up(&cfg->wowl.nd_data_wait);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tif (netinfo->SSID_len > IEEE80211_MAX_SSID_LEN)",
          "\t\tnetinfo->SSID_len = IEEE80211_MAX_SSID_LEN;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Improper handling of network information during event processing in the WiFi driver.",
      "trigger_condition": "Receiving a malicious event frame related to Wake-up on Wireless LAN functionality, leading to a heap buffer overflow in the brcmf_wowl_nd_results function.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly check and limit the length of the SSID field in the received network information, allowing a potential buffer overflow when copying the SSID data into the destination buffer.",
      "id": 62,
      "code_after_change_normalized": "static VAR1\nFUN1(struct brcmf_if *VAR2, const struct brcmf_event_msg *VAR3,\nvoid *VAR4)\n{\nstruct brcmf_cfg80211_info *VAR5 = VAR2->VAR6->VAR7;\nstruct VAR8 *VAR8 = FUN2(VAR5);\nstruct brcmf_pno_scanresults_le *VAR9;\nstruct brcmf_pno_net_info_le *VAR10;\nFUN3(VAR11, \"STR\");\nif (VAR3->VAR12 < (sizeof(*VAR9) + sizeof(*VAR10))) {\nFUN3(VAR11, \"STR\");\nreturn 0;\n}\nVAR9 = (struct VAR13 *)VAR4;\nif (VAR3->VAR14 == VAR15) {\nFUN3(VAR11, \"STR\");\nreturn 0;\n}\nif (FUN4(VAR9->VAR16) < 1) {\nFUN5(VAR8, \"STR\",\nFUN4(VAR9->VAR16));\nreturn -VAR17;\n}\nVAR10 = FUN6(VAR9);\nif (VAR10->VAR18 > VAR19)\nVAR10->VAR18 = VAR19;\nFUN7(VAR5->VAR20.VAR21->VAR22.VAR22, VAR10->VAR23, VAR10->VAR18);\nVAR5->VAR20.VAR21->VAR22.VAR24 = VAR10->VAR18;\nVAR5->VAR20.VAR21->VAR25 = 1;\nVAR5->VAR20.VAR21->VAR26[0] =\nFUN8(VAR10->VAR27,\nVAR10->VAR27 <= VAR28 ?\nVAR29 : VAR30);\nVAR5->VAR20.VAR31->VAR32 = 1;\nVAR5->VAR20.VAR31->VAR33[0] = VAR5->VAR20.VAR21;\nVAR5->VAR20.VAR34 = true;\nFUN9(&VAR5->VAR20.VAR35);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static VAR1\nFUN1(struct brcmf_if *VAR2, const struct brcmf_event_msg *VAR3,\nvoid *VAR4)\n{\nstruct brcmf_cfg80211_info *VAR5 = VAR2->VAR6->VAR7;\nstruct VAR8 *VAR8 = FUN2(VAR5);\nstruct brcmf_pno_scanresults_le *VAR9;\nstruct brcmf_pno_net_info_le *VAR10;\nFUN3(VAR11, \"STR\");\nif (VAR3->VAR12 < (sizeof(*VAR9) + sizeof(*VAR10))) {\nFUN3(VAR11, \"STR\");\nreturn 0;\n}\nVAR9 = (struct VAR13 *)VAR4;\nif (VAR3->VAR14 == VAR15) {\nFUN3(VAR11, \"STR\");\nreturn 0;\n}\nif (FUN4(VAR9->VAR16) < 1) {\nFUN5(VAR8, \"STR\",\nFUN4(VAR9->VAR16));\nreturn -VAR17;\n}\nVAR10 = FUN6(VAR9);\nFUN7(VAR5->VAR18.VAR19->VAR20.VAR20, VAR10->VAR21, VAR10->VAR22);\nVAR5->VAR18.VAR19->VAR20.VAR23 = VAR10->VAR22;\nVAR5->VAR18.VAR19->VAR24 = 1;\nVAR5->VAR18.VAR19->VAR25[0] =\nFUN8(VAR10->VAR26,\nVAR10->VAR26 <= VAR27 ?\nVAR28 : VAR29);\nVAR5->VAR18.VAR30->VAR31 = 1;\nVAR5->VAR18.VAR30->VAR32[0] = VAR5->VAR18.VAR19;\nVAR5->VAR18.VAR33 = true;\nFUN9(&VAR5->VAR18.VAR34);\nreturn 0;\n}\n",
      "code_after_change_raw": "static s32\nbrcmf_wowl_nd_results(struct brcmf_if *ifp, const struct brcmf_event_msg *e,\nvoid *data)\n{\nstruct brcmf_cfg80211_info *cfg = ifp->drvr->config;\nstruct wiphy *wiphy = cfg_to_wiphy(cfg);\nstruct brcmf_pno_scanresults_le *pfn_result;\nstruct brcmf_pno_net_info_le *netinfo;\nbrcmf_dbg(SCAN, \"Enter\\n\");\nif (e->datalen < (sizeof(*pfn_result) + sizeof(*netinfo))) {\nbrcmf_dbg(SCAN, \"Event data to small. Ignore\\n\");\nreturn 0;\n}\npfn_result = (struct brcmf_pno_scanresults_le *)data;\nif (e->event_code == BRCMF_E_PFN_NET_LOST) {\nbrcmf_dbg(SCAN, \"PFN NET LOST event. Ignore\\n\");\nreturn 0;\n}\nif (le32_to_cpu(pfn_result->count) < 1) {\nbphy_err(wiphy, \"Invalid result count, expected 1 (%d)\\n\",\nle32_to_cpu(pfn_result->count));\nreturn -EINVAL;\n}\nnetinfo = brcmf_get_netinfo_array(pfn_result);\nif (netinfo->SSID_len > IEEE80211_MAX_SSID_LEN)\nnetinfo->SSID_len = IEEE80211_MAX_SSID_LEN;\nmemcpy(cfg->wowl.nd->ssid.ssid, netinfo->SSID, netinfo->SSID_len);\ncfg->wowl.nd->ssid.ssid_len = netinfo->SSID_len;\ncfg->wowl.nd->n_channels = 1;\ncfg->wowl.nd->channels[0] =\nieee80211_channel_to_frequency(netinfo->channel,\nnetinfo->channel <= CH_MAX_2G_CHANNEL ?\nNL80211_BAND_2GHZ : NL80211_BAND_5GHZ);\ncfg->wowl.nd_info->n_matches = 1;\ncfg->wowl.nd_info->matches[0] = cfg->wowl.nd;\ncfg->wowl.nd_data_completed = true;\nwake_up(&cfg->wowl.nd_data_wait);\nreturn 0;\n}\n",
      "code_before_change_raw": "static s32\nbrcmf_wowl_nd_results(struct brcmf_if *ifp, const struct brcmf_event_msg *e,\nvoid *data)\n{\nstruct brcmf_cfg80211_info *cfg = ifp->drvr->config;\nstruct wiphy *wiphy = cfg_to_wiphy(cfg);\nstruct brcmf_pno_scanresults_le *pfn_result;\nstruct brcmf_pno_net_info_le *netinfo;\nbrcmf_dbg(SCAN, \"Enter\\n\");\nif (e->datalen < (sizeof(*pfn_result) + sizeof(*netinfo))) {\nbrcmf_dbg(SCAN, \"Event data to small. Ignore\\n\");\nreturn 0;\n}\npfn_result = (struct brcmf_pno_scanresults_le *)data;\nif (e->event_code == BRCMF_E_PFN_NET_LOST) {\nbrcmf_dbg(SCAN, \"PFN NET LOST event. Ignore\\n\");\nreturn 0;\n}\nif (le32_to_cpu(pfn_result->count) < 1) {\nbphy_err(wiphy, \"Invalid result count, expected 1 (%d)\\n\",\nle32_to_cpu(pfn_result->count));\nreturn -EINVAL;\n}\nnetinfo = brcmf_get_netinfo_array(pfn_result);\nmemcpy(cfg->wowl.nd->ssid.ssid, netinfo->SSID, netinfo->SSID_len);\ncfg->wowl.nd->ssid.ssid_len = netinfo->SSID_len;\ncfg->wowl.nd->n_channels = 1;\ncfg->wowl.nd->channels[0] =\nieee80211_channel_to_frequency(netinfo->channel,\nnetinfo->channel <= CH_MAX_2G_CHANNEL ?\nNL80211_BAND_2GHZ : NL80211_BAND_5GHZ);\ncfg->wowl.nd_info->n_matches = 1;\ncfg->wowl.nd_info->matches[0] = cfg->wowl.nd;\ncfg->wowl.nd_data_completed = true;\nwake_up(&cfg->wowl.nd_data_wait);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2020-0066": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate allocation strategy for the socket buffer (skb) size in the netlink driver.",
        "trigger_condition": "A race condition occurs during the allocation of the skb, potentially leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly adjust the allocation size of the skb based on the minimum dump allocation size and the maximum receive message length, which can result in high-order allocations and potential out-of-bounds writes."
      },
      "solution": "To mitigate the vulnerability, it is necessary to enhance the allocation strategy for the skb in the netlink driver. This involves adjusting the allocation size based on the minimum dump allocation size and the maximum receive message length to prevent high-order allocations and reduce the risk of out-of-bounds writes. Additionally, trimming the skb to the allocated size and updating the allocation logic to handle allocation failures effectively are crucial steps in fixing the vulnerability.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2020-0066 in the netlink driver. The vulnerability involves a possible out-of-bounds write due to a race condition, which could potentially lead to local escalation of privilege with system execution privileges required.\n\nThe modification focuses on improving the allocation strategy for the skb (socket buffer) in the netlink_dump function. By adjusting the allocation size based on the minimum dump allocation size (alloc_min_size) and the maximum receive message length (nlk->max_recvmsg_len), the code aims to prevent high-order allocations and reduce the risk of out-of-bounds writes.\n\nSpecifically, the modification introduces the following changes:\n1. Introduces a new variable alloc_min_size to store the maximum value between cb->min_dump_alloc and NLMSG_GOODSIZE.\n2. Adjusts the allocation size based on alloc_min_size and nlk->max_recvmsg_len to ensure that the skb is allocated with an appropriate size to handle the dump operation efficiently.\n3. Trims the skb to the allocated size to prevent excess space that could lead to potential vulnerabilities.\n4. Updates the allocation logic to handle cases where the initial allocation attempt fails, ensuring that the skb is allocated with a suitable size to avoid potential issues.\n\nOverall, these modifications aim to enhance the allocation strategy for the skb in the netlink_dump function, reducing the likelihood of out-of-bounds writes and improving the overall security of the netlink driver.",
      "GPT_purpose": "This function is responsible for dumping netlink messages from a socket.",
      "GPT_function": "\n1. Allocate a socket buffer for netlink communication.\n2. Check for conditions and errors during the buffer allocation process.\n3. Perform actions based on the length of data received in the buffer.\n4. Handle sending and processing of the buffer data.\n5. Release resources and clean up after processing.",
      "CVE_id": "CVE-2020-0066",
      "code_before_change": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tcb = &nlk->cb;\n\talloc_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (!netlink_rx_is_mmaped(sk) &&\n\t    atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tif (alloc_size < nlk->max_recvmsg_len) {\n\t\tskb = netlink_alloc_skb(sk,\n\t\t\t\t\tnlk->max_recvmsg_len,\n\t\t\t\t\tnlk->portid,\n\t\t\t\t\tGFP_KERNEL |\n\t\t\t\t\t__GFP_NOWARN |\n\t\t\t\t\t__GFP_NORETRY);\n\t\t/* available room should be exact amount to avoid MSG_TRUNC */\n\t\tif (skb)\n\t\t\tskb_reserve(skb, skb_tailroom(skb) -\n\t\t\t\t\t nlk->max_recvmsg_len);\n\t}\n\tif (!skb)\n\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n\t\t\t\t\tGFP_KERNEL);\n\tif (!skb)\n\t\tgoto errout_skb;\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
      "code_after_change": "static int netlink_dump(struct sock *sk)\n{\n\tstruct netlink_sock *nlk = nlk_sk(sk);\n\tstruct netlink_callback *cb;\n\tstruct sk_buff *skb = NULL;\n\tstruct nlmsghdr *nlh;\n\tint len, err = -ENOBUFS;\n\tint alloc_min_size;\n\tint alloc_size;\n\n\tmutex_lock(nlk->cb_mutex);\n\tif (!nlk->cb_running) {\n\t\terr = -EINVAL;\n\t\tgoto errout_skb;\n\t}\n\n\tif (!netlink_rx_is_mmaped(sk) &&\n\t    atomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\n\t\tgoto errout_skb;\n\n\t/* NLMSG_GOODSIZE is small to avoid high order allocations being\n\t * required, but it makes sense to _attempt_ a 16K bytes allocation\n\t * to reduce number of system calls on dump operations, if user\n\t * ever provided a big enough buffer.\n\t */\n\tcb = &nlk->cb;\n\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\n\n\tif (alloc_min_size < nlk->max_recvmsg_len) {\n\t\talloc_size = nlk->max_recvmsg_len;\n\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n\t\t\t\t\tGFP_KERNEL |\n\t\t\t\t\t__GFP_NOWARN |\n\t\t\t\t\t__GFP_NORETRY);\n\t}\n\tif (!skb) {\n\t\talloc_size = alloc_min_size;\n\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\n\t\t\t\t\tGFP_KERNEL);\n\t}\n\tif (!skb)\n\t\tgoto errout_skb;\n\n\t/* Trim skb to allocated size. User is expected to provide buffer as\n\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at\n\t * netlink_recvmsg())). dump will pack as many smaller messages as\n\t * could fit within the allocated skb. skb is typically allocated\n\t * with larger space than required (could be as much as near 2x the\n\t * requested size with align to next power of 2 approach). Allowing\n\t * dump to use the excess space makes it difficult for a user to have a\n\t * reasonable static buffer based on the expected largest dump of a\n\t * single netdev. The outcome is MSG_TRUNC error.\n\t */\n\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);\n\tnetlink_skb_set_owner_r(skb, sk);\n\n\tlen = cb->dump(skb, cb);\n\n\tif (len > 0) {\n\t\tmutex_unlock(nlk->cb_mutex);\n\n\t\tif (sk_filter(sk, skb))\n\t\t\tkfree_skb(skb);\n\t\telse\n\t\t\t__netlink_sendskb(sk, skb);\n\t\treturn 0;\n\t}\n\n\tnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\n\tif (!nlh)\n\t\tgoto errout_skb;\n\n\tnl_dump_check_consistent(cb, nlh);\n\n\tmemcpy(nlmsg_data(nlh), &len, sizeof(len));\n\n\tif (sk_filter(sk, skb))\n\t\tkfree_skb(skb);\n\telse\n\t\t__netlink_sendskb(sk, skb);\n\n\tif (cb->done)\n\t\tcb->done(cb);\n\n\tnlk->cb_running = false;\n\tmutex_unlock(nlk->cb_mutex);\n\tmodule_put(cb->module);\n\tconsume_skb(cb->skb);\n\treturn 0;\n\nerrout_skb:\n\tmutex_unlock(nlk->cb_mutex);\n\tkfree_skb(skb);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tint alloc_min_size;",
          "\tcb = &nlk->cb;",
          "\talloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);",
          "",
          "\tif (alloc_min_size < nlk->max_recvmsg_len) {",
          "\t\talloc_size = nlk->max_recvmsg_len;",
          "\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,",
          "\t}",
          "\tif (!skb) {",
          "\t\talloc_size = alloc_min_size;",
          "\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,",
          "\t\t\t\t\tGFP_KERNEL);",
          "",
          "\t/* Trim skb to allocated size. User is expected to provide buffer as",
          "\t * large as max(min_dump_alloc, 16KiB (mac_recvmsg_len capped at",
          "\t * netlink_recvmsg())). dump will pack as many smaller messages as",
          "\t * could fit within the allocated skb. skb is typically allocated",
          "\t * with larger space than required (could be as much as near 2x the",
          "\t * requested size with align to next power of 2 approach). Allowing",
          "\t * dump to use the excess space makes it difficult for a user to have a",
          "\t * reasonable static buffer based on the expected largest dump of a",
          "\t * single netdev. The outcome is MSG_TRUNC error.",
          "\t */",
          "\tskb_reserve(skb, skb_tailroom(skb) - alloc_size);"
        ],
        "deleted": [
          "",
          "\tcb = &nlk->cb;",
          "\talloc_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);",
          "\tif (alloc_size < nlk->max_recvmsg_len) {",
          "\t\tskb = netlink_alloc_skb(sk,",
          "\t\t\t\t\tnlk->max_recvmsg_len,",
          "\t\t\t\t\tnlk->portid,",
          "\t\t/* available room should be exact amount to avoid MSG_TRUNC */",
          "\t\tif (skb)",
          "\t\t\tskb_reserve(skb, skb_tailroom(skb) -",
          "\t\t\t\t\t nlk->max_recvmsg_len);",
          "\t\tskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,",
          "\t\t\t\t\tGFP_KERNEL);",
          "\tif (!skb)"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate allocation strategy for the socket buffer (skb) size in the netlink driver.",
      "trigger_condition": "A race condition occurs during the allocation of the skb, potentially leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly adjust the allocation size of the skb based on the minimum dump allocation size and the maximum receive message length, which can result in high-order allocations and potential out-of-bounds writes.",
      "id": 63,
      "code_after_change_normalized": "static int FUN1(struct sock *VAR1)\n{\nstruct netlink_sock *VAR2 = FUN2(VAR1);\nstruct netlink_callback *VAR3;\nstruct sk_buff *VAR4 = NULL;\nstruct nlmsghdr *VAR5;\nint VAR6, VAR7 = -VAR8;\nint VAR9;\nint VAR10;\nFUN3(VAR2->VAR11);\nif (!VAR2->VAR12) {\nVAR7 = -VAR13;\ngoto VAR14;\n}\nif (!FUN4(VAR1) &&\nFUN5(&VAR1->VAR15) >= VAR1->VAR16)\ngoto VAR14;\nVAR3 = &VAR2->VAR3;\nVAR9 = FUN6(int, VAR3->VAR17, VAR18);\nif (VAR9 < VAR2->VAR19) {\nVAR10 = VAR2->VAR19;\nVAR4 = FUN7(VAR1, VAR10, VAR2->VAR20,\nVAR21 |\nVAR22 |\nVAR23);\n}\nif (!VAR4) {\nVAR10 = VAR9;\nVAR4 = FUN7(VAR1, VAR10, VAR2->VAR20,\nVAR21);\n}\nif (!VAR4)\ngoto VAR14;\nFUN8(VAR4, FUN9(VAR4) - VAR10);\nFUN10(VAR4, VAR1);\nVAR6 = VAR3->FUN11(VAR4, VAR3);\nif (VAR6 > 0) {\nFUN12(VAR2->VAR11);\nif (FUN13(VAR1, VAR4))\nFUN14(VAR4);\nelse\nFUN15(VAR1, VAR4);\nreturn 0;\n}\nVAR5 = FUN16(VAR4, VAR3, VAR24, sizeof(VAR6), VAR25);\nif (!VAR5)\ngoto VAR14;\nFUN17(VAR3, VAR5);\nFUN18(FUN19(VAR5), &VAR6, sizeof(VAR6));\nif (FUN13(VAR1, VAR4))\nFUN14(VAR4);\nelse\nFUN15(VAR1, VAR4);\nif (VAR3->VAR26)\nVAR3->FUN20(VAR3);\nVAR2->VAR12 = false;\nFUN12(VAR2->VAR11);\nFUN21(VAR3->VAR27);\nFUN22(VAR3->VAR4);\nreturn 0;\nVAR14:\nFUN12(VAR2->VAR11);\nFUN14(VAR4);\nreturn VAR7;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sock *VAR1)\n{\nstruct netlink_sock *VAR2 = FUN2(VAR1);\nstruct netlink_callback *VAR3;\nstruct sk_buff *VAR4 = NULL;\nstruct nlmsghdr *VAR5;\nint VAR6, VAR7 = -VAR8;\nint VAR9;\nFUN3(VAR2->VAR10);\nif (!VAR2->VAR11) {\nVAR7 = -VAR12;\ngoto VAR13;\n}\nVAR3 = &VAR2->VAR3;\nVAR9 = FUN4(int, VAR3->VAR14, VAR15);\nif (!FUN5(VAR1) &&\nFUN6(&VAR1->VAR16) >= VAR1->VAR17)\ngoto VAR13;\nif (VAR9 < VAR2->VAR18) {\nVAR4 = FUN7(VAR1,\nVAR2->VAR18,\nVAR2->VAR19,\nVAR20 |\nVAR21 |\nVAR22);\nif (VAR4)\nFUN8(VAR4, FUN9(VAR4) -\nVAR2->VAR18);\n}\nif (!VAR4)\nVAR4 = FUN7(VAR1, VAR9, VAR2->VAR19,\nVAR20);\nif (!VAR4)\ngoto VAR13;\nFUN10(VAR4, VAR1);\nVAR6 = VAR3->FUN11(VAR4, VAR3);\nif (VAR6 > 0) {\nFUN12(VAR2->VAR10);\nif (FUN13(VAR1, VAR4))\nFUN14(VAR4);\nelse\nFUN15(VAR1, VAR4);\nreturn 0;\n}\nVAR5 = FUN16(VAR4, VAR3, VAR23, sizeof(VAR6), VAR24);\nif (!VAR5)\ngoto VAR13;\nFUN17(VAR3, VAR5);\nFUN18(FUN19(VAR5), &VAR6, sizeof(VAR6));\nif (FUN13(VAR1, VAR4))\nFUN14(VAR4);\nelse\nFUN15(VAR1, VAR4);\nif (VAR3->VAR25)\nVAR3->FUN20(VAR3);\nVAR2->VAR11 = false;\nFUN12(VAR2->VAR10);\nFUN21(VAR3->VAR26);\nFUN22(VAR3->VAR4);\nreturn 0;\nVAR13:\nFUN12(VAR2->VAR10);\nFUN14(VAR4);\nreturn VAR7;\n}\n",
      "code_after_change_raw": "static int netlink_dump(struct sock *sk)\n{\nstruct netlink_sock *nlk = nlk_sk(sk);\nstruct netlink_callback *cb;\nstruct sk_buff *skb = NULL;\nstruct nlmsghdr *nlh;\nint len, err = -ENOBUFS;\nint alloc_min_size;\nint alloc_size;\nmutex_lock(nlk->cb_mutex);\nif (!nlk->cb_running) {\nerr = -EINVAL;\ngoto errout_skb;\n}\nif (!netlink_rx_is_mmaped(sk) &&\natomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\ngoto errout_skb;\ncb = &nlk->cb;\nalloc_min_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\nif (alloc_min_size < nlk->max_recvmsg_len) {\nalloc_size = nlk->max_recvmsg_len;\nskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\nGFP_KERNEL |\n__GFP_NOWARN |\n__GFP_NORETRY);\n}\nif (!skb) {\nalloc_size = alloc_min_size;\nskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\nGFP_KERNEL);\n}\nif (!skb)\ngoto errout_skb;\nskb_reserve(skb, skb_tailroom(skb) - alloc_size);\nnetlink_skb_set_owner_r(skb, sk);\nlen = cb->dump(skb, cb);\nif (len > 0) {\nmutex_unlock(nlk->cb_mutex);\nif (sk_filter(sk, skb))\nkfree_skb(skb);\nelse\n__netlink_sendskb(sk, skb);\nreturn 0;\n}\nnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\nif (!nlh)\ngoto errout_skb;\nnl_dump_check_consistent(cb, nlh);\nmemcpy(nlmsg_data(nlh), &len, sizeof(len));\nif (sk_filter(sk, skb))\nkfree_skb(skb);\nelse\n__netlink_sendskb(sk, skb);\nif (cb->done)\ncb->done(cb);\nnlk->cb_running = false;\nmutex_unlock(nlk->cb_mutex);\nmodule_put(cb->module);\nconsume_skb(cb->skb);\nreturn 0;\nerrout_skb:\nmutex_unlock(nlk->cb_mutex);\nkfree_skb(skb);\nreturn err;\n}\n",
      "code_before_change_raw": "static int netlink_dump(struct sock *sk)\n{\nstruct netlink_sock *nlk = nlk_sk(sk);\nstruct netlink_callback *cb;\nstruct sk_buff *skb = NULL;\nstruct nlmsghdr *nlh;\nint len, err = -ENOBUFS;\nint alloc_size;\nmutex_lock(nlk->cb_mutex);\nif (!nlk->cb_running) {\nerr = -EINVAL;\ngoto errout_skb;\n}\ncb = &nlk->cb;\nalloc_size = max_t(int, cb->min_dump_alloc, NLMSG_GOODSIZE);\nif (!netlink_rx_is_mmaped(sk) &&\natomic_read(&sk->sk_rmem_alloc) >= sk->sk_rcvbuf)\ngoto errout_skb;\nif (alloc_size < nlk->max_recvmsg_len) {\nskb = netlink_alloc_skb(sk,\nnlk->max_recvmsg_len,\nnlk->portid,\nGFP_KERNEL |\n__GFP_NOWARN |\n__GFP_NORETRY);\nif (skb)\nskb_reserve(skb, skb_tailroom(skb) -\nnlk->max_recvmsg_len);\n}\nif (!skb)\nskb = netlink_alloc_skb(sk, alloc_size, nlk->portid,\nGFP_KERNEL);\nif (!skb)\ngoto errout_skb;\nnetlink_skb_set_owner_r(skb, sk);\nlen = cb->dump(skb, cb);\nif (len > 0) {\nmutex_unlock(nlk->cb_mutex);\nif (sk_filter(sk, skb))\nkfree_skb(skb);\nelse\n__netlink_sendskb(sk, skb);\nreturn 0;\n}\nnlh = nlmsg_put_answer(skb, cb, NLMSG_DONE, sizeof(len), NLM_F_MULTI);\nif (!nlh)\ngoto errout_skb;\nnl_dump_check_consistent(cb, nlh);\nmemcpy(nlmsg_data(nlh), &len, sizeof(len));\nif (sk_filter(sk, skb))\nkfree_skb(skb);\nelse\n__netlink_sendskb(sk, skb);\nif (cb->done)\ncb->done(cb);\nnlk->cb_running = false;\nmutex_unlock(nlk->cb_mutex);\nmodule_put(cb->module);\nconsume_skb(cb->skb);\nreturn 0;\nerrout_skb:\nmutex_unlock(nlk->cb_mutex);\nkfree_skb(skb);\nreturn err;\n}\n"
    }
  ],
  "CVE-2020-0429": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for session deletion in the code, leading to a use-after-free vulnerability.",
        "trigger_condition": "Session deletion is not properly synchronized, allowing the session to be accessed after it has been deleted, leading to memory corruption.",
        "specific_code_behavior_causing_vulnerability": "The code deletes a session from the hash list without ensuring that the session is not accessed again, potentially leading to a use-after-free scenario.",
        "specific_solution_to_fix_vulnerability": "To mitigate the vulnerability, it is necessary to mark the session as 'dead' before any further operations are performed on it. This ensures that the session is not accessed after deletion, preventing memory corruption and potential exploitation. In this case, the solution involves adding a check to mark the session as 'dead' and skip further processing if it has already been marked as such."
      },
      "GPT_analysis": "The modification is necessary to prevent a use-after-free vulnerability in the code. In the original code snippet, the session is deleted from the hash list using `hlist_del_init(&session->hlist)`. However, there is a possibility that the session could be accessed again after it has been deleted, leading to memory corruption.\n\nBy adding the check `if (test_and_set_bit(0, &session->dead)) goto again;`, the code ensures that the session is marked as \"dead\" before any further operations are performed on it. This prevents the use-after-free scenario by checking if the session has already been marked as dead before attempting to access it again. If the session is already marked as dead, the code will skip further processing and move on to the next session.\n\nThis modification helps to mitigate the vulnerability described in CVE-2020-0429 by preventing memory corruption and potential exploitation for local privilege escalation.",
      "GPT_purpose": "Close all sessions associated with a given L2TP tunnel.",
      "GPT_function": "\n1. Close all sessions in an L2TP tunnel.\n2. Iterate through the session hash table to close each session.\n3. Perform necessary cleanup actions for each session.",
      "CVE_id": "CVE-2020-0429",
      "code_before_change": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\n\tint hash;\n\tstruct hlist_node *walk;\n\tstruct hlist_node *tmp;\n\tstruct l2tp_session *session;\n\n\tBUG_ON(tunnel == NULL);\n\n\tl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\n\t\t  tunnel->name);\n\n\twrite_lock_bh(&tunnel->hlist_lock);\n\ttunnel->acpt_newsess = false;\n\tfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\n\t\thlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\n\t\t\tsession = hlist_entry(walk, struct l2tp_session, hlist);\n\n\t\t\tl2tp_info(session, L2TP_MSG_CONTROL,\n\t\t\t\t  \"%s: closing session\\n\", session->name);\n\n\t\t\thlist_del_init(&session->hlist);\n\n\t\t\tif (session->ref != NULL)\n\t\t\t\t(*session->ref)(session);\n\n\t\t\twrite_unlock_bh(&tunnel->hlist_lock);\n\n\t\t\t__l2tp_session_unhash(session);\n\t\t\tl2tp_session_queue_purge(session);\n\n\t\t\tif (session->session_close != NULL)\n\t\t\t\t(*session->session_close)(session);\n\n\t\t\tif (session->deref != NULL)\n\t\t\t\t(*session->deref)(session);\n\n\t\t\tl2tp_session_dec_refcount(session);\n\n\t\t\twrite_lock_bh(&tunnel->hlist_lock);\n\n\t\t\t/* Now restart from the beginning of this hash\n\t\t\t * chain.  We always remove a session from the\n\t\t\t * list so we are guaranteed to make forward\n\t\t\t * progress.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\twrite_unlock_bh(&tunnel->hlist_lock);\n}",
      "code_after_change": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\n\tint hash;\n\tstruct hlist_node *walk;\n\tstruct hlist_node *tmp;\n\tstruct l2tp_session *session;\n\n\tBUG_ON(tunnel == NULL);\n\n\tl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\n\t\t  tunnel->name);\n\n\twrite_lock_bh(&tunnel->hlist_lock);\n\ttunnel->acpt_newsess = false;\n\tfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\n\t\thlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\n\t\t\tsession = hlist_entry(walk, struct l2tp_session, hlist);\n\n\t\t\tl2tp_info(session, L2TP_MSG_CONTROL,\n\t\t\t\t  \"%s: closing session\\n\", session->name);\n\n\t\t\thlist_del_init(&session->hlist);\n\n\t\t\tif (test_and_set_bit(0, &session->dead))\n\t\t\t\tgoto again;\n\n\t\t\tif (session->ref != NULL)\n\t\t\t\t(*session->ref)(session);\n\n\t\t\twrite_unlock_bh(&tunnel->hlist_lock);\n\n\t\t\t__l2tp_session_unhash(session);\n\t\t\tl2tp_session_queue_purge(session);\n\n\t\t\tif (session->session_close != NULL)\n\t\t\t\t(*session->session_close)(session);\n\n\t\t\tif (session->deref != NULL)\n\t\t\t\t(*session->deref)(session);\n\n\t\t\tl2tp_session_dec_refcount(session);\n\n\t\t\twrite_lock_bh(&tunnel->hlist_lock);\n\n\t\t\t/* Now restart from the beginning of this hash\n\t\t\t * chain.  We always remove a session from the\n\t\t\t * list so we are guaranteed to make forward\n\t\t\t * progress.\n\t\t\t */\n\t\t\tgoto again;\n\t\t}\n\t}\n\twrite_unlock_bh(&tunnel->hlist_lock);\n}",
      "modified_lines": {
        "added": [
          "",
          "\t\t\tif (test_and_set_bit(0, &session->dead))",
          "\t\t\t\tgoto again;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper handling for session deletion in the code, leading to a use-after-free vulnerability.",
      "trigger_condition": "Session deletion is not properly synchronized, allowing the session to be accessed after it has been deleted, leading to memory corruption.",
      "specific_code_behavior_causing_vulnerability": "The code deletes a session from the hash list without ensuring that the session is not accessed again, potentially leading to a use-after-free scenario.",
      "id": 64,
      "code_after_change_normalized": "void FUN1(struct l2tp_tunnel *VAR1)\n{\nint VAR2;\nstruct hlist_node *VAR3;\nstruct hlist_node *VAR4;\nstruct l2tp_session *VAR5;\nFUN2(VAR1 == NULL);\nFUN3(VAR1, VAR6, \"STR\",\nVAR1->VAR7);\nFUN4(&VAR1->VAR8);\nVAR1->VAR9 = false;\nfor (VAR2 = 0; VAR2 < VAR10; VAR2++) {\nVAR11:\nFUN5(VAR3, VAR4, &VAR1->VAR12[VAR2]) {\nVAR5 = FUN6(VAR3, struct VAR13, VAR14);\nFUN3(VAR5, VAR6,\n\"STR\", VAR5->VAR7);\nFUN7(&VAR5->VAR14);\nif (FUN8(0, &VAR5->VAR15))\ngoto VAR11;\nif (VAR5->VAR16 != NULL)\n(*VAR5->VAR16)(VAR5);\nFUN9(&VAR1->VAR8);\nFUN10(VAR5);\nFUN11(VAR5);\nif (VAR5->VAR17 != NULL)\n(*VAR5->VAR17)(VAR5);\nif (VAR5->VAR18 != NULL)\n(*VAR5->VAR18)(VAR5);\nFUN12(VAR5);\nFUN4(&VAR1->VAR8);\ngoto VAR11;\n}\n}\nFUN9(&VAR1->VAR8);\n}\n",
      "code_before_change_normalized": "void FUN1(struct l2tp_tunnel *VAR1)\n{\nint VAR2;\nstruct hlist_node *VAR3;\nstruct hlist_node *VAR4;\nstruct l2tp_session *VAR5;\nFUN2(VAR1 == NULL);\nFUN3(VAR1, VAR6, \"STR\",\nVAR1->VAR7);\nFUN4(&VAR1->VAR8);\nVAR1->VAR9 = false;\nfor (VAR2 = 0; VAR2 < VAR10; VAR2++) {\nVAR11:\nFUN5(VAR3, VAR4, &VAR1->VAR12[VAR2]) {\nVAR5 = FUN6(VAR3, struct VAR13, VAR14);\nFUN3(VAR5, VAR6,\n\"STR\", VAR5->VAR7);\nFUN7(&VAR5->VAR14);\nif (VAR5->VAR15 != NULL)\n(*VAR5->VAR15)(VAR5);\nFUN8(&VAR1->VAR8);\nFUN9(VAR5);\nFUN10(VAR5);\nif (VAR5->VAR16 != NULL)\n(*VAR5->VAR16)(VAR5);\nif (VAR5->VAR17 != NULL)\n(*VAR5->VAR17)(VAR5);\nFUN11(VAR5);\nFUN4(&VAR1->VAR8);\ngoto VAR11;\n}\n}\nFUN8(&VAR1->VAR8);\n}\n",
      "code_after_change_raw": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\nint hash;\nstruct hlist_node *walk;\nstruct hlist_node *tmp;\nstruct l2tp_session *session;\nBUG_ON(tunnel == NULL);\nl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\ntunnel->name);\nwrite_lock_bh(&tunnel->hlist_lock);\ntunnel->acpt_newsess = false;\nfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\nhlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\nsession = hlist_entry(walk, struct l2tp_session, hlist);\nl2tp_info(session, L2TP_MSG_CONTROL,\n\"%s: closing session\\n\", session->name);\nhlist_del_init(&session->hlist);\nif (test_and_set_bit(0, &session->dead))\ngoto again;\nif (session->ref != NULL)\n(*session->ref)(session);\nwrite_unlock_bh(&tunnel->hlist_lock);\n__l2tp_session_unhash(session);\nl2tp_session_queue_purge(session);\nif (session->session_close != NULL)\n(*session->session_close)(session);\nif (session->deref != NULL)\n(*session->deref)(session);\nl2tp_session_dec_refcount(session);\nwrite_lock_bh(&tunnel->hlist_lock);\ngoto again;\n}\n}\nwrite_unlock_bh(&tunnel->hlist_lock);\n}\n",
      "code_before_change_raw": "void l2tp_tunnel_closeall(struct l2tp_tunnel *tunnel)\n{\nint hash;\nstruct hlist_node *walk;\nstruct hlist_node *tmp;\nstruct l2tp_session *session;\nBUG_ON(tunnel == NULL);\nl2tp_info(tunnel, L2TP_MSG_CONTROL, \"%s: closing all sessions...\\n\",\ntunnel->name);\nwrite_lock_bh(&tunnel->hlist_lock);\ntunnel->acpt_newsess = false;\nfor (hash = 0; hash < L2TP_HASH_SIZE; hash++) {\nagain:\nhlist_for_each_safe(walk, tmp, &tunnel->session_hlist[hash]) {\nsession = hlist_entry(walk, struct l2tp_session, hlist);\nl2tp_info(session, L2TP_MSG_CONTROL,\n\"%s: closing session\\n\", session->name);\nhlist_del_init(&session->hlist);\nif (session->ref != NULL)\n(*session->ref)(session);\nwrite_unlock_bh(&tunnel->hlist_lock);\n__l2tp_session_unhash(session);\nl2tp_session_queue_purge(session);\nif (session->session_close != NULL)\n(*session->session_close)(session);\nif (session->deref != NULL)\n(*session->deref)(session);\nl2tp_session_dec_refcount(session);\nwrite_lock_bh(&tunnel->hlist_lock);\ngoto again;\n}\n}\nwrite_unlock_bh(&tunnel->hlist_lock);\n}\n"
    }
  ],
  "CVE-2020-0432": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for packet size calculations, leading to a potential integer overflow.",
        "trigger_condition": "When calculating the length of a packet header combined with the length of the incoming packet, an integer overflow occurs if the packet length is close to the maximum value that can be represented by an unsigned integer.",
        "specific_code_behavior_causing_vulnerability": "The code calculates the total length of the packet header and the incoming packet without checking for potential integer overflow. This can result in the calculated length being smaller than the actual packet length, leading to a buffer overflow during data copying."
      },
      "solution": "To mitigate the vulnerability, it is necessary to validate the calculated packet length to ensure it does not overflow and is greater than or equal to the actual packet length. In this case, the solution involves adding a check to verify that the calculated length is not smaller than the actual packet length before proceeding with data copying. This check prevents the possibility of a buffer overflow due to integer overflow.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2020-0432, which involves a possible out-of-bounds write due to an integer overflow in the original code. \n\nIn the original code snippet, the calculation of `mep_len` as `skb->len + MEP_HDR_LEN` could potentially lead to an integer overflow if `skb->len` is close to the maximum value that can be represented by an unsigned integer. This integer overflow can result in `mep_len` being smaller than `skb->len`, leading to a buffer overflow when writing data to `buff`.\n\nBy adding the check `if (mep_len < skb->len)` in the modified code, we ensure that `mep_len` is not smaller than `skb->len`, preventing the possibility of an out-of-bounds write due to integer overflow. This check helps to mitigate the vulnerability and ensures that the code behaves correctly even when dealing with large packet sizes.",
      "GPT_purpose": "Copy data from a network packet to a memory buffer, potentially vulnerable to an out of bounds write due to an integer overflow.",
      "GPT_function": "\n1. Convert an skb buffer to a mbo buffer.\n2. Check if the mbo buffer is large enough to accommodate the skb buffer.\n3. Copy data from the skb buffer to the mbo buffer.",
      "CVE_id": "CVE-2020-0432",
      "code_before_change": "static int skb_to_mep(const struct sk_buff *skb, struct mbo *mbo)\n{\n\tu8 *buff = mbo->virt_address;\n\tunsigned int mep_len = skb->len + MEP_HDR_LEN;\n\n\tif (mbo->buffer_length < mep_len) {\n\t\tpr_err(\"drop: too small buffer! (%d for %d)\\n\",\n\t\t       mbo->buffer_length, mep_len);\n\t\treturn -EINVAL;\n\t}\n\n\t*buff++ = HB(mep_len - 2);\n\t*buff++ = LB(mep_len - 2);\n\n\t*buff++ = PMHL;\n\t*buff++ = (PMS_FIFONO_MEP << PMS_FIFONO_SHIFT) | PMS_MSGTYPE_DATA;\n\t*buff++ = (MEP_DEF_RETRY << PMS_RETRY_SHIFT) | PMS_DEF_PRIO;\n\t*buff++ = 0;\n\t*buff++ = 0;\n\t*buff++ = 0;\n\n\tmemcpy(buff, skb->data, skb->len);\n\tmbo->buffer_length = mep_len;\n\treturn 0;\n}",
      "code_after_change": "static int skb_to_mep(const struct sk_buff *skb, struct mbo *mbo)\n{\n\tu8 *buff = mbo->virt_address;\n\tunsigned int mep_len = skb->len + MEP_HDR_LEN;\n\n\tif (mep_len < skb->len) {\n\t\tpr_err(\"drop: too large packet! (%u)\\n\", skb->len);\n\t\treturn -EINVAL;\n\t}\n\n\tif (mbo->buffer_length < mep_len) {\n\t\tpr_err(\"drop: too small buffer! (%d for %d)\\n\",\n\t\t       mbo->buffer_length, mep_len);\n\t\treturn -EINVAL;\n\t}\n\n\t*buff++ = HB(mep_len - 2);\n\t*buff++ = LB(mep_len - 2);\n\n\t*buff++ = PMHL;\n\t*buff++ = (PMS_FIFONO_MEP << PMS_FIFONO_SHIFT) | PMS_MSGTYPE_DATA;\n\t*buff++ = (MEP_DEF_RETRY << PMS_RETRY_SHIFT) | PMS_DEF_PRIO;\n\t*buff++ = 0;\n\t*buff++ = 0;\n\t*buff++ = 0;\n\n\tmemcpy(buff, skb->data, skb->len);\n\tmbo->buffer_length = mep_len;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (mep_len < skb->len) {",
          "\t\tpr_err(\"drop: too large packet! (%u)\\n\", skb->len);",
          "\t\treturn -EINVAL;",
          "\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation for packet size calculations, leading to a potential integer overflow.",
      "trigger_condition": "When calculating the length of a packet header combined with the length of the incoming packet, an integer overflow occurs if the packet length is close to the maximum value that can be represented by an unsigned integer.",
      "specific_code_behavior_causing_vulnerability": "The code calculates the total length of the packet header and the incoming packet without checking for potential integer overflow. This can result in the calculated length being smaller than the actual packet length, leading to a buffer overflow during data copying.",
      "id": 65,
      "code_after_change_normalized": "static int FUN1(const struct sk_buff *VAR1, struct VAR2 *VAR2)\n{\nu8 *VAR3 = VAR2->VAR4;\nunsigned int VAR5 = VAR1->VAR6 + VAR7;\nif (VAR5 < VAR1->VAR6) {\nFUN2(\"STR\", VAR1->VAR6);\nreturn -VAR8;\n}\nif (VAR2->VAR9 < VAR5) {\nFUN2(\"STR\",\nVAR2->VAR9, VAR5);\nreturn -VAR8;\n}\n*VAR3++ = FUN3(VAR5 - 2);\n*VAR3++ = FUN4(VAR5 - 2);\n*VAR3++ = VAR10;\n*VAR3++ = (VAR11 << VAR12) | VAR13;\n*VAR3++ = (VAR14 << VAR15) | VAR16;\n*VAR3++ = 0;\n*VAR3++ = 0;\n*VAR3++ = 0;\nFUN5(VAR3, VAR1->VAR17, VAR1->VAR6);\nVAR2->VAR9 = VAR5;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(const struct sk_buff *VAR1, struct VAR2 *VAR2)\n{\nu8 *VAR3 = VAR2->VAR4;\nunsigned int VAR5 = VAR1->VAR6 + VAR7;\nif (VAR2->VAR8 < VAR5) {\nFUN2(\"STR\",\nVAR2->VAR8, VAR5);\nreturn -VAR9;\n}\n*VAR3++ = FUN3(VAR5 - 2);\n*VAR3++ = FUN4(VAR5 - 2);\n*VAR3++ = VAR10;\n*VAR3++ = (VAR11 << VAR12) | VAR13;\n*VAR3++ = (VAR14 << VAR15) | VAR16;\n*VAR3++ = 0;\n*VAR3++ = 0;\n*VAR3++ = 0;\nFUN5(VAR3, VAR1->VAR17, VAR1->VAR6);\nVAR2->VAR8 = VAR5;\nreturn 0;\n}\n",
      "code_after_change_raw": "static int skb_to_mep(const struct sk_buff *skb, struct mbo *mbo)\n{\nu8 *buff = mbo->virt_address;\nunsigned int mep_len = skb->len + MEP_HDR_LEN;\nif (mep_len < skb->len) {\npr_err(\"drop: too large packet! (%u)\\n\", skb->len);\nreturn -EINVAL;\n}\nif (mbo->buffer_length < mep_len) {\npr_err(\"drop: too small buffer! (%d for %d)\\n\",\nmbo->buffer_length, mep_len);\nreturn -EINVAL;\n}\n*buff++ = HB(mep_len - 2);\n*buff++ = LB(mep_len - 2);\n*buff++ = PMHL;\n*buff++ = (PMS_FIFONO_MEP << PMS_FIFONO_SHIFT) | PMS_MSGTYPE_DATA;\n*buff++ = (MEP_DEF_RETRY << PMS_RETRY_SHIFT) | PMS_DEF_PRIO;\n*buff++ = 0;\n*buff++ = 0;\n*buff++ = 0;\nmemcpy(buff, skb->data, skb->len);\nmbo->buffer_length = mep_len;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int skb_to_mep(const struct sk_buff *skb, struct mbo *mbo)\n{\nu8 *buff = mbo->virt_address;\nunsigned int mep_len = skb->len + MEP_HDR_LEN;\nif (mbo->buffer_length < mep_len) {\npr_err(\"drop: too small buffer! (%d for %d)\\n\",\nmbo->buffer_length, mep_len);\nreturn -EINVAL;\n}\n*buff++ = HB(mep_len - 2);\n*buff++ = LB(mep_len - 2);\n*buff++ = PMHL;\n*buff++ = (PMS_FIFONO_MEP << PMS_FIFONO_SHIFT) | PMS_MSGTYPE_DATA;\n*buff++ = (MEP_DEF_RETRY << PMS_RETRY_SHIFT) | PMS_DEF_PRIO;\n*buff++ = 0;\n*buff++ = 0;\n*buff++ = 0;\nmemcpy(buff, skb->data, skb->len);\nmbo->buffer_length = mep_len;\nreturn 0;\n}\n"
    }
  ],
  "CVE-2020-0465": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking for memory access in the code.",
        "trigger_condition": "The code attempts to clear a bit in a bit array without checking if the bit array pointer is valid, leading to a possible out of bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not verify the validity of the bit array pointer before attempting to clear a bit in the array. This can result in accessing memory out of bounds, potentially leading to a security vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to verify the validity of the bit array pointer before attempting to clear a bit in the array. This ensures that the code only accesses memory within the bounds of the array and reduces the risk of out of bounds writes. In this case, the solution involves adding the check 'if (*bit)' before calling the clear_bit function.",
      "GPT_analysis": "The modification is necessary to prevent a possible out of bounds write vulnerability in the code. By adding the check `if (*bit)`, the code ensures that the `clear_bit` function is only called if the `bit` pointer is not NULL. This check helps prevent accessing memory that is out of bounds, which could lead to a security vulnerability. By making this modification, the code becomes more robust and secure, reducing the risk of exploitation and potential privilege escalation.",
      "GPT_purpose": "Clear a specific bit in the HID usage map.",
      "GPT_function": "\n1. `hid_map_usage_clear` is a function that maps a HID usage to an input device and then clears a specific bit in the bit array associated with the usage.\n2. It takes parameters including a `struct hid_input`, a `struct hid_usage`, a pointer to an unsigned long array, a pointer to an integer, a type value, and a usage code.\n3. The function calls `hid_map_usage` to map the usage and then clears a specific bit in the bit array using `clear_bit`.",
      "CVE_id": "CVE-2020-0465",
      "code_before_change": "static inline void hid_map_usage_clear(struct hid_input *hidinput,\n\t\tstruct hid_usage *usage, unsigned long **bit, int *max,\n\t\t__u8 type, __u16 c)\n{\n\thid_map_usage(hidinput, usage, bit, max, type, c);\n\tclear_bit(c, *bit);\n}",
      "code_after_change": "static inline void hid_map_usage_clear(struct hid_input *hidinput,\n\t\tstruct hid_usage *usage, unsigned long **bit, int *max,\n\t\t__u8 type, __u16 c)\n{\n\thid_map_usage(hidinput, usage, bit, max, type, c);\n\tif (*bit)\n\t\tclear_bit(usage->code, *bit);\n}",
      "modified_lines": {
        "added": [
          "\tif (*bit)",
          "\t\tclear_bit(usage->code, *bit);"
        ],
        "deleted": [
          "\tclear_bit(c, *bit);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking for memory access in the code.",
      "trigger_condition": "The code attempts to clear a bit in a bit array without checking if the bit array pointer is valid, leading to a possible out of bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not verify the validity of the bit array pointer before attempting to clear a bit in the array. This can result in accessing memory out of bounds, potentially leading to a security vulnerability.",
      "id": 66,
      "code_after_change_normalized": "static inline void FUN1(struct hid_input *VAR1,\nstruct hid_usage *VAR2, unsigned long **VAR3, int *VAR4,\n__u8 VAR5, __u16 VAR6)\n{\nFUN2(VAR1, VAR2, VAR3, VAR4, VAR5, VAR6);\nif (*VAR3)\nFUN3(VAR2->VAR7, *VAR3);\n}\n",
      "code_before_change_normalized": "static inline void FUN1(struct hid_input *VAR1,\nstruct hid_usage *VAR2, unsigned long **VAR3, int *VAR4,\n__u8 VAR5, __u16 VAR6)\n{\nFUN2(VAR1, VAR2, VAR3, VAR4, VAR5, VAR6);\nFUN3(VAR6, *VAR3);\n}\n",
      "code_after_change_raw": "static inline void hid_map_usage_clear(struct hid_input *hidinput,\nstruct hid_usage *usage, unsigned long **bit, int *max,\n__u8 type, __u16 c)\n{\nhid_map_usage(hidinput, usage, bit, max, type, c);\nif (*bit)\nclear_bit(usage->code, *bit);\n}\n",
      "code_before_change_raw": "static inline void hid_map_usage_clear(struct hid_input *hidinput,\nstruct hid_usage *usage, unsigned long **bit, int *max,\n__u8 type, __u16 c)\n{\nhid_map_usage(hidinput, usage, bit, max, type, c);\nclear_bit(c, *bit);\n}\n"
    }
  ],
  "CVE-2020-10742": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when accessing elements in an array of pages.",
        "trigger_condition": "Direct IO write operation leads to an index buffer overflow, causing the NFS client to crash.",
        "specific_code_behavior_causing_vulnerability": "The code does not perform proper bounds checking before accessing elements in the 'pages' array, potentially allowing access beyond the allocated buffer size. This can result in a kernel panic or other issues if the index overflows after memory allocation.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking when accessing elements in the array to prevent buffer overflows. In this case, the solution involves removing the check for 'pages[i] == NULL' before accessing 'pages[i]' and replacing the memory deallocation function with 'kvfree(pages)' to correctly free the allocated memory."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-10742, which involves an index buffer overflow during Direct IO write that can lead to the NFS client crashing. By removing the check for `pages[i] == NULL` before accessing `pages[i]`, the code is vulnerable to accessing memory beyond the allocated buffer, potentially causing a kernel panic or other issues.\n\nBy replacing the call to `OBD_FREE_LARGE(pages, npages * sizeof(*pages));` with `kvfree(pages);`, the modified code correctly frees the memory allocated for the `pages` array using the appropriate kernel memory allocation function. This change ensures that memory is deallocated correctly and helps prevent potential buffer overflow vulnerabilities.",
      "GPT_purpose": "Freeing user pages and releasing page cache references.",
      "GPT_function": "\n1. Freeing user pages allocated in memory.\n2. Setting pages as dirty if specified.\n3. Releasing the page cache for each page.",
      "CVE_id": "CVE-2020-10742",
      "code_before_change": "static void ll_free_user_pages(struct page **pages, int npages, int do_dirty)\n{\n\tint i;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tif (pages[i] == NULL)\n\t\t\tbreak;\n\t\tif (do_dirty)\n\t\t\tset_page_dirty_lock(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\n\tOBD_FREE_LARGE(pages, npages * sizeof(*pages));\n}",
      "code_after_change": "static void ll_free_user_pages(struct page **pages, int npages, int do_dirty)\n{\n\tint i;\n\n\tfor (i = 0; i < npages; i++) {\n\t\tif (do_dirty)\n\t\t\tset_page_dirty_lock(pages[i]);\n\t\tpage_cache_release(pages[i]);\n\t}\n\tkvfree(pages);\n}",
      "modified_lines": {
        "added": [
          "\tkvfree(pages);"
        ],
        "deleted": [
          "\t\tif (pages[i] == NULL)",
          "\t\t\tbreak;",
          "",
          "\tOBD_FREE_LARGE(pages, npages * sizeof(*pages));"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when accessing elements in an array of pages.",
      "trigger_condition": "Direct IO write operation leads to an index buffer overflow, causing the NFS client to crash.",
      "specific_code_behavior_causing_vulnerability": "The code does not perform proper bounds checking before accessing elements in the 'pages' array, potentially allowing access beyond the allocated buffer size. This can result in a kernel panic or other issues if the index overflows after memory allocation.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking when accessing elements in the array to prevent buffer overflows. In this case, the solution involves removing the check for 'pages[i] == NULL' before accessing 'pages[i]' and replacing the memory deallocation function with 'kvfree(pages)' to correctly free the allocated memory.",
      "id": 67,
      "code_after_change_normalized": "static void FUN1(struct page **VAR1, int VAR2, int VAR3)\n{\nint VAR4;\nfor (VAR4 = 0; VAR4 < VAR2; VAR4++) {\nif (VAR3)\nFUN2(VAR1[VAR4]);\nFUN3(VAR1[VAR4]);\n}\nFUN4(VAR1);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct page **VAR1, int VAR2, int VAR3)\n{\nint VAR4;\nfor (VAR4 = 0; VAR4 < VAR2; VAR4++) {\nif (VAR1[VAR4] == NULL)\nbreak;\nif (VAR3)\nFUN2(VAR1[VAR4]);\nFUN3(VAR1[VAR4]);\n}\nFUN4(VAR1, VAR2 * sizeof(*VAR1));\n}\n",
      "code_after_change_raw": "static void ll_free_user_pages(struct page **pages, int npages, int do_dirty)\n{\nint i;\nfor (i = 0; i < npages; i++) {\nif (do_dirty)\nset_page_dirty_lock(pages[i]);\npage_cache_release(pages[i]);\n}\nkvfree(pages);\n}\n",
      "code_before_change_raw": "static void ll_free_user_pages(struct page **pages, int npages, int do_dirty)\n{\nint i;\nfor (i = 0; i < npages; i++) {\nif (pages[i] == NULL)\nbreak;\nif (do_dirty)\nset_page_dirty_lock(pages[i]);\npage_cache_release(pages[i]);\n}\nOBD_FREE_LARGE(pages, npages * sizeof(*pages));\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate validation of input parameters in a Direct IO read function.",
        "trigger_condition": "An index buffer overflow occurs during Direct IO write, potentially leading to a kernel panic.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly handle the 'uio' parameter in the function call to nfs_direct_read_schedule_iovec, which can result in an index buffer overflow and subsequent kernel panic."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the unnecessary 'uio' parameter from the function call to nfs_direct_read_schedule_iovec. This action prevents the index buffer overflow issue and reduces the risk of a kernel panic. By ensuring that only essential parameters are passed to the function, the code can avoid the vulnerability associated with the index buffer overflow.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-10742, which involves an index buffer overflow during Direct IO write that can lead to the NFS client crashing. By removing the unnecessary parameter `uio` from the `nfs_direct_read_schedule_iovec` function call in the modified code snippet, we prevent the potential index buffer overflow issue and mitigate the risk of a kernel panic caused by reaching out of the index after memory allocation by kmalloc. This modification helps ensure data confidentiality and system availability by fixing the vulnerability in the code.",
      "GPT_purpose": "Perform direct read operations for NFS files.",
      "GPT_function": "\n1. Perform direct read operation on an NFS file.\n2. Allocate memory for a direct request structure.\n3. Schedule IO operation for direct read.\n4. Wait for the completion of the direct read operation.\n5. Release resources and return the result.",
      "CVE_id": "CVE-2020-10742",
      "code_before_change": "ssize_t nfs_file_direct_read(struct kiocb *iocb, struct iov_iter *iter,\n\t\t\t\tloff_t pos, bool uio)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct nfs_direct_req *dreq;\n\tstruct nfs_lock_context *l_ctx;\n\tssize_t result = -EINVAL;\n\tsize_t count = iov_iter_count(iter);\n\tnfs_add_stats(mapping->host, NFSIOS_DIRECTREADBYTES, count);\n\n\tdfprintk(FILE, \"NFS: direct read(%pD2, %zd@%Ld)\\n\",\n\t\tfile, count, (long long) pos);\n\n\tresult = 0;\n\tif (!count)\n\t\tgoto out;\n\n\tmutex_lock(&inode->i_mutex);\n\tresult = nfs_sync_mapping(mapping);\n\tif (result)\n\t\tgoto out_unlock;\n\n\ttask_io_account_read(count);\n\n\tresult = -ENOMEM;\n\tdreq = nfs_direct_req_alloc();\n\tif (dreq == NULL)\n\t\tgoto out_unlock;\n\n\tdreq->inode = inode;\n\tdreq->bytes_left = count;\n\tdreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\n\tl_ctx = nfs_get_lock_context(dreq->ctx);\n\tif (IS_ERR(l_ctx)) {\n\t\tresult = PTR_ERR(l_ctx);\n\t\tgoto out_release;\n\t}\n\tdreq->l_ctx = l_ctx;\n\tif (!is_sync_kiocb(iocb))\n\t\tdreq->iocb = iocb;\n\n\tNFS_I(inode)->read_io += count;\n\tresult = nfs_direct_read_schedule_iovec(dreq, iter, pos, uio);\n\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (!result) {\n\t\tresult = nfs_direct_wait(dreq);\n\t\tif (result > 0)\n\t\t\tiocb->ki_pos = pos + result;\n\t}\n\n\tnfs_direct_req_release(dreq);\n\treturn result;\n\nout_release:\n\tnfs_direct_req_release(dreq);\nout_unlock:\n\tmutex_unlock(&inode->i_mutex);\nout:\n\treturn result;\n}",
      "code_after_change": "ssize_t nfs_file_direct_read(struct kiocb *iocb, struct iov_iter *iter,\n\t\t\t\tloff_t pos, bool uio)\n{\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct nfs_direct_req *dreq;\n\tstruct nfs_lock_context *l_ctx;\n\tssize_t result = -EINVAL;\n\tsize_t count = iov_iter_count(iter);\n\tnfs_add_stats(mapping->host, NFSIOS_DIRECTREADBYTES, count);\n\n\tdfprintk(FILE, \"NFS: direct read(%pD2, %zd@%Ld)\\n\",\n\t\tfile, count, (long long) pos);\n\n\tresult = 0;\n\tif (!count)\n\t\tgoto out;\n\n\tmutex_lock(&inode->i_mutex);\n\tresult = nfs_sync_mapping(mapping);\n\tif (result)\n\t\tgoto out_unlock;\n\n\ttask_io_account_read(count);\n\n\tresult = -ENOMEM;\n\tdreq = nfs_direct_req_alloc();\n\tif (dreq == NULL)\n\t\tgoto out_unlock;\n\n\tdreq->inode = inode;\n\tdreq->bytes_left = count;\n\tdreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\n\tl_ctx = nfs_get_lock_context(dreq->ctx);\n\tif (IS_ERR(l_ctx)) {\n\t\tresult = PTR_ERR(l_ctx);\n\t\tgoto out_release;\n\t}\n\tdreq->l_ctx = l_ctx;\n\tif (!is_sync_kiocb(iocb))\n\t\tdreq->iocb = iocb;\n\n\tNFS_I(inode)->read_io += count;\n\tresult = nfs_direct_read_schedule_iovec(dreq, iter, pos);\n\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (!result) {\n\t\tresult = nfs_direct_wait(dreq);\n\t\tif (result > 0)\n\t\t\tiocb->ki_pos = pos + result;\n\t}\n\n\tnfs_direct_req_release(dreq);\n\treturn result;\n\nout_release:\n\tnfs_direct_req_release(dreq);\nout_unlock:\n\tmutex_unlock(&inode->i_mutex);\nout:\n\treturn result;\n}",
      "modified_lines": {
        "added": [
          "\tresult = nfs_direct_read_schedule_iovec(dreq, iter, pos);"
        ],
        "deleted": [
          "\tresult = nfs_direct_read_schedule_iovec(dreq, iter, pos, uio);"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate validation of input parameters in a Direct IO read function.",
      "trigger_condition": "An index buffer overflow occurs during Direct IO write, potentially leading to a kernel panic.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly handle the 'uio' parameter in the function call to nfs_direct_read_schedule_iovec, which can result in an index buffer overflow and subsequent kernel panic.",
      "id": 68,
      "code_after_change_normalized": "ssize_t FUN1(struct kiocb *VAR1, struct iov_iter *VAR2,\nloff_t VAR3, bool VAR4)\n{\nstruct VAR5 *VAR5 = VAR1->VAR6;\nstruct address_space *VAR7 = VAR5->VAR8;\nstruct VAR9 *VAR9 = VAR7->VAR10;\nstruct nfs_direct_req *VAR11;\nstruct nfs_lock_context *VAR12;\nssize_t VAR13 = -VAR14;\nsize_t VAR15 = FUN2(VAR2);\nFUN3(VAR7->VAR10, VAR16, VAR15);\nFUN4(VAR17, \"STR\",\nVAR5, VAR15, (long long) VAR3);\nVAR13 = 0;\nif (!VAR15)\ngoto VAR18;\nFUN5(&VAR9->VAR19);\nVAR13 = FUN6(VAR7);\nif (VAR13)\ngoto VAR20;\nFUN7(VAR15);\nVAR13 = -VAR21;\nVAR11 = FUN8();\nif (VAR11 == NULL)\ngoto VAR20;\nVAR11->VAR9 = VAR9;\nVAR11->VAR22 = VAR15;\nVAR11->VAR23 = FUN9(FUN10(VAR1->VAR6));\nVAR12 = FUN11(VAR11->VAR23);\nif (FUN12(VAR12)) {\nVAR13 = FUN13(VAR12);\ngoto VAR24;\n}\nVAR11->VAR12 = VAR12;\nif (!FUN14(VAR1))\nVAR11->VAR1 = VAR1;\nFUN15(VAR9)->VAR25 += VAR15;\nVAR13 = FUN16(VAR11, VAR2, VAR3);\nFUN17(&VAR9->VAR19);\nif (!VAR13) {\nVAR13 = FUN18(VAR11);\nif (VAR13 > 0)\nVAR1->VAR26 = VAR3 + VAR13;\n}\nFUN19(VAR11);\nreturn VAR13;\nVAR24:\nFUN19(VAR11);\nVAR20:\nFUN17(&VAR9->VAR19);\nVAR18:\nreturn VAR13;\n}\n",
      "code_before_change_normalized": "ssize_t FUN1(struct kiocb *VAR1, struct iov_iter *VAR2,\nloff_t VAR3, bool VAR4)\n{\nstruct VAR5 *VAR5 = VAR1->VAR6;\nstruct address_space *VAR7 = VAR5->VAR8;\nstruct VAR9 *VAR9 = VAR7->VAR10;\nstruct nfs_direct_req *VAR11;\nstruct nfs_lock_context *VAR12;\nssize_t VAR13 = -VAR14;\nsize_t VAR15 = FUN2(VAR2);\nFUN3(VAR7->VAR10, VAR16, VAR15);\nFUN4(VAR17, \"STR\",\nVAR5, VAR15, (long long) VAR3);\nVAR13 = 0;\nif (!VAR15)\ngoto VAR18;\nFUN5(&VAR9->VAR19);\nVAR13 = FUN6(VAR7);\nif (VAR13)\ngoto VAR20;\nFUN7(VAR15);\nVAR13 = -VAR21;\nVAR11 = FUN8();\nif (VAR11 == NULL)\ngoto VAR20;\nVAR11->VAR9 = VAR9;\nVAR11->VAR22 = VAR15;\nVAR11->VAR23 = FUN9(FUN10(VAR1->VAR6));\nVAR12 = FUN11(VAR11->VAR23);\nif (FUN12(VAR12)) {\nVAR13 = FUN13(VAR12);\ngoto VAR24;\n}\nVAR11->VAR12 = VAR12;\nif (!FUN14(VAR1))\nVAR11->VAR1 = VAR1;\nFUN15(VAR9)->VAR25 += VAR15;\nVAR13 = FUN16(VAR11, VAR2, VAR3, VAR4);\nFUN17(&VAR9->VAR19);\nif (!VAR13) {\nVAR13 = FUN18(VAR11);\nif (VAR13 > 0)\nVAR1->VAR26 = VAR3 + VAR13;\n}\nFUN19(VAR11);\nreturn VAR13;\nVAR24:\nFUN19(VAR11);\nVAR20:\nFUN17(&VAR9->VAR19);\nVAR18:\nreturn VAR13;\n}\n",
      "code_after_change_raw": "ssize_t nfs_file_direct_read(struct kiocb *iocb, struct iov_iter *iter,\nloff_t pos, bool uio)\n{\nstruct file *file = iocb->ki_filp;\nstruct address_space *mapping = file->f_mapping;\nstruct inode *inode = mapping->host;\nstruct nfs_direct_req *dreq;\nstruct nfs_lock_context *l_ctx;\nssize_t result = -EINVAL;\nsize_t count = iov_iter_count(iter);\nnfs_add_stats(mapping->host, NFSIOS_DIRECTREADBYTES, count);\ndfprintk(FILE, \"NFS: direct read(%pD2, %zd@%Ld)\\n\",\nfile, count, (long long) pos);\nresult = 0;\nif (!count)\ngoto out;\nmutex_lock(&inode->i_mutex);\nresult = nfs_sync_mapping(mapping);\nif (result)\ngoto out_unlock;\ntask_io_account_read(count);\nresult = -ENOMEM;\ndreq = nfs_direct_req_alloc();\nif (dreq == NULL)\ngoto out_unlock;\ndreq->inode = inode;\ndreq->bytes_left = count;\ndreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\nl_ctx = nfs_get_lock_context(dreq->ctx);\nif (IS_ERR(l_ctx)) {\nresult = PTR_ERR(l_ctx);\ngoto out_release;\n}\ndreq->l_ctx = l_ctx;\nif (!is_sync_kiocb(iocb))\ndreq->iocb = iocb;\nNFS_I(inode)->read_io += count;\nresult = nfs_direct_read_schedule_iovec(dreq, iter, pos);\nmutex_unlock(&inode->i_mutex);\nif (!result) {\nresult = nfs_direct_wait(dreq);\nif (result > 0)\niocb->ki_pos = pos + result;\n}\nnfs_direct_req_release(dreq);\nreturn result;\nout_release:\nnfs_direct_req_release(dreq);\nout_unlock:\nmutex_unlock(&inode->i_mutex);\nout:\nreturn result;\n}\n",
      "code_before_change_raw": "ssize_t nfs_file_direct_read(struct kiocb *iocb, struct iov_iter *iter,\nloff_t pos, bool uio)\n{\nstruct file *file = iocb->ki_filp;\nstruct address_space *mapping = file->f_mapping;\nstruct inode *inode = mapping->host;\nstruct nfs_direct_req *dreq;\nstruct nfs_lock_context *l_ctx;\nssize_t result = -EINVAL;\nsize_t count = iov_iter_count(iter);\nnfs_add_stats(mapping->host, NFSIOS_DIRECTREADBYTES, count);\ndfprintk(FILE, \"NFS: direct read(%pD2, %zd@%Ld)\\n\",\nfile, count, (long long) pos);\nresult = 0;\nif (!count)\ngoto out;\nmutex_lock(&inode->i_mutex);\nresult = nfs_sync_mapping(mapping);\nif (result)\ngoto out_unlock;\ntask_io_account_read(count);\nresult = -ENOMEM;\ndreq = nfs_direct_req_alloc();\nif (dreq == NULL)\ngoto out_unlock;\ndreq->inode = inode;\ndreq->bytes_left = count;\ndreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\nl_ctx = nfs_get_lock_context(dreq->ctx);\nif (IS_ERR(l_ctx)) {\nresult = PTR_ERR(l_ctx);\ngoto out_release;\n}\ndreq->l_ctx = l_ctx;\nif (!is_sync_kiocb(iocb))\ndreq->iocb = iocb;\nNFS_I(inode)->read_io += count;\nresult = nfs_direct_read_schedule_iovec(dreq, iter, pos, uio);\nmutex_unlock(&inode->i_mutex);\nif (!result) {\nresult = nfs_direct_wait(dreq);\nif (result > 0)\niocb->ki_pos = pos + result;\n}\nnfs_direct_req_release(dreq);\nreturn result;\nout_release:\nnfs_direct_req_release(dreq);\nout_unlock:\nmutex_unlock(&inode->i_mutex);\nout:\nreturn result;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of index buffer during Direct IO write process.",
        "trigger_condition": "An index buffer overflow occurs when writing data directly to the NFS client, potentially leading to a kernel panic.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly handle the index buffer size during Direct IO write, which can result in a buffer overflow and subsequent kernel panic. This vulnerability arises from a memory allocation issue that can cause the index to reach beyond the allocated memory, leading to a crash."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking and handling of the index buffer size during Direct IO write operations. Specifically, the code should be modified to prevent the index from exceeding the allocated memory, thus avoiding the buffer overflow and potential kernel panic. In this case, the solution involves removing the unused parameter 'uio' from the function call to prevent the index buffer from reaching beyond the allocated memory.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2020-10742, which involves an index buffer overflow during Direct IO write leading to the NFS client crashing. The vulnerability occurs due to a potential memory allocation issue that can result in a kernel panic.\n\nThe specific modification made in the code snippet is related to the function `nfs_direct_write_schedule_iovec(dreq, iter, pos, uio)`. The modification involves removing the `uio` parameter from the function call, as it is not used within the function. By removing the unused parameter, the code is made more secure and less prone to vulnerabilities.\n\nBy making this modification, the code is improved in terms of security and stability, reducing the risk of a kernel panic and enhancing the overall reliability of the NFS client functionality.",
      "GPT_purpose": "Perform direct write operations for NFS files.",
      "GPT_function": "\n1. Perform direct write operation for NFS files.\n2. Update statistics for direct written bytes.\n3. Perform generic write checks.\n4. Lock the inode mutex.\n5. Synchronize the NFS mapping.\n6. Invalidate inode pages within a specified range.\n7. Account for write operations.\n8. Allocate memory for a NFS direct request.\n9. Obtain NFS open context and lock context.\n10. Schedule IO operations for direct write.\n11. Wait for completion of direct write operation.\n12. Update file size if necessary.\n13. Release resources and return the result.",
      "CVE_id": "CVE-2020-10742",
      "code_before_change": "ssize_t nfs_file_direct_write(struct kiocb *iocb, struct iov_iter *iter,\n\t\t\t\tloff_t pos, bool uio)\n{\n\tssize_t result = -EINVAL;\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct nfs_direct_req *dreq;\n\tstruct nfs_lock_context *l_ctx;\n\tloff_t end;\n\tsize_t count = iov_iter_count(iter);\n\tend = (pos + count - 1) >> PAGE_CACHE_SHIFT;\n\n\tnfs_add_stats(mapping->host, NFSIOS_DIRECTWRITTENBYTES, count);\n\n\tdfprintk(FILE, \"NFS: direct write(%pD2, %zd@%Ld)\\n\",\n\t\tfile, count, (long long) pos);\n\n\tresult = generic_write_checks(file, &pos, &count, 0);\n\tif (result)\n\t\tgoto out;\n\n\tresult = -EINVAL;\n\tif ((ssize_t) count < 0)\n\t\tgoto out;\n\tresult = 0;\n\tif (!count)\n\t\tgoto out;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tresult = nfs_sync_mapping(mapping);\n\tif (result)\n\t\tgoto out_unlock;\n\n\tif (mapping->nrpages) {\n\t\tresult = invalidate_inode_pages2_range(mapping,\n\t\t\t\t\tpos >> PAGE_CACHE_SHIFT, end);\n\t\tif (result)\n\t\t\tgoto out_unlock;\n\t}\n\n\ttask_io_account_write(count);\n\n\tresult = -ENOMEM;\n\tdreq = nfs_direct_req_alloc();\n\tif (!dreq)\n\t\tgoto out_unlock;\n\n\tdreq->inode = inode;\n\tdreq->bytes_left = count;\n\tdreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\n\tl_ctx = nfs_get_lock_context(dreq->ctx);\n\tif (IS_ERR(l_ctx)) {\n\t\tresult = PTR_ERR(l_ctx);\n\t\tgoto out_release;\n\t}\n\tdreq->l_ctx = l_ctx;\n\tif (!is_sync_kiocb(iocb))\n\t\tdreq->iocb = iocb;\n\n\tresult = nfs_direct_write_schedule_iovec(dreq, iter, pos, uio);\n\n\tif (mapping->nrpages) {\n\t\tinvalidate_inode_pages2_range(mapping,\n\t\t\t\t\t      pos >> PAGE_CACHE_SHIFT, end);\n\t}\n\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (!result) {\n\t\tresult = nfs_direct_wait(dreq);\n\t\tif (result > 0) {\n\t\t\tstruct inode *inode = mapping->host;\n\n\t\t\tiocb->ki_pos = pos + result;\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tif (i_size_read(inode) < iocb->ki_pos)\n\t\t\t\ti_size_write(inode, iocb->ki_pos);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\t}\n\tnfs_direct_req_release(dreq);\n\treturn result;\n\nout_release:\n\tnfs_direct_req_release(dreq);\nout_unlock:\n\tmutex_unlock(&inode->i_mutex);\nout:\n\treturn result;\n}",
      "code_after_change": "ssize_t nfs_file_direct_write(struct kiocb *iocb, struct iov_iter *iter,\n\t\t\t\tloff_t pos, bool uio)\n{\n\tssize_t result = -EINVAL;\n\tstruct file *file = iocb->ki_filp;\n\tstruct address_space *mapping = file->f_mapping;\n\tstruct inode *inode = mapping->host;\n\tstruct nfs_direct_req *dreq;\n\tstruct nfs_lock_context *l_ctx;\n\tloff_t end;\n\tsize_t count = iov_iter_count(iter);\n\tend = (pos + count - 1) >> PAGE_CACHE_SHIFT;\n\n\tnfs_add_stats(mapping->host, NFSIOS_DIRECTWRITTENBYTES, count);\n\n\tdfprintk(FILE, \"NFS: direct write(%pD2, %zd@%Ld)\\n\",\n\t\tfile, count, (long long) pos);\n\n\tresult = generic_write_checks(file, &pos, &count, 0);\n\tif (result)\n\t\tgoto out;\n\n\tresult = -EINVAL;\n\tif ((ssize_t) count < 0)\n\t\tgoto out;\n\tresult = 0;\n\tif (!count)\n\t\tgoto out;\n\n\tmutex_lock(&inode->i_mutex);\n\n\tresult = nfs_sync_mapping(mapping);\n\tif (result)\n\t\tgoto out_unlock;\n\n\tif (mapping->nrpages) {\n\t\tresult = invalidate_inode_pages2_range(mapping,\n\t\t\t\t\tpos >> PAGE_CACHE_SHIFT, end);\n\t\tif (result)\n\t\t\tgoto out_unlock;\n\t}\n\n\ttask_io_account_write(count);\n\n\tresult = -ENOMEM;\n\tdreq = nfs_direct_req_alloc();\n\tif (!dreq)\n\t\tgoto out_unlock;\n\n\tdreq->inode = inode;\n\tdreq->bytes_left = count;\n\tdreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\n\tl_ctx = nfs_get_lock_context(dreq->ctx);\n\tif (IS_ERR(l_ctx)) {\n\t\tresult = PTR_ERR(l_ctx);\n\t\tgoto out_release;\n\t}\n\tdreq->l_ctx = l_ctx;\n\tif (!is_sync_kiocb(iocb))\n\t\tdreq->iocb = iocb;\n\n\tresult = nfs_direct_write_schedule_iovec(dreq, iter, pos);\n\n\tif (mapping->nrpages) {\n\t\tinvalidate_inode_pages2_range(mapping,\n\t\t\t\t\t      pos >> PAGE_CACHE_SHIFT, end);\n\t}\n\n\tmutex_unlock(&inode->i_mutex);\n\n\tif (!result) {\n\t\tresult = nfs_direct_wait(dreq);\n\t\tif (result > 0) {\n\t\t\tstruct inode *inode = mapping->host;\n\n\t\t\tiocb->ki_pos = pos + result;\n\t\t\tspin_lock(&inode->i_lock);\n\t\t\tif (i_size_read(inode) < iocb->ki_pos)\n\t\t\t\ti_size_write(inode, iocb->ki_pos);\n\t\t\tspin_unlock(&inode->i_lock);\n\t\t}\n\t}\n\tnfs_direct_req_release(dreq);\n\treturn result;\n\nout_release:\n\tnfs_direct_req_release(dreq);\nout_unlock:\n\tmutex_unlock(&inode->i_mutex);\nout:\n\treturn result;\n}",
      "modified_lines": {
        "added": [
          "\tresult = nfs_direct_write_schedule_iovec(dreq, iter, pos);"
        ],
        "deleted": [
          "\tresult = nfs_direct_write_schedule_iovec(dreq, iter, pos, uio);"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of index buffer during Direct IO write process.",
      "trigger_condition": "An index buffer overflow occurs when writing data directly to the NFS client, potentially leading to a kernel panic.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly handle the index buffer size during Direct IO write, which can result in a buffer overflow and subsequent kernel panic. This vulnerability arises from a memory allocation issue that can cause the index to reach beyond the allocated memory, leading to a crash.",
      "id": 69,
      "code_after_change_normalized": "ssize_t FUN1(struct kiocb *VAR1, struct iov_iter *VAR2,\nloff_t VAR3, bool VAR4)\n{\nssize_t VAR5 = -VAR6;\nstruct VAR7 *VAR7 = VAR1->VAR8;\nstruct address_space *VAR9 = VAR7->VAR10;\nstruct VAR11 *VAR11 = VAR9->VAR12;\nstruct nfs_direct_req *VAR13;\nstruct nfs_lock_context *VAR14;\nloff_t VAR15;\nsize_t VAR16 = FUN2(VAR2);\nVAR15 = (VAR3 + VAR16 - 1) >> VAR17;\nFUN3(VAR9->VAR12, VAR18, VAR16);\nFUN4(VAR19, \"STR\",\nVAR7, VAR16, (long long) VAR3);\nVAR5 = FUN5(VAR7, &VAR3, &VAR16, 0);\nif (VAR5)\ngoto VAR20;\nVAR5 = -VAR6;\nif ((VAR21) VAR16 < 0)\ngoto VAR20;\nVAR5 = 0;\nif (!VAR16)\ngoto VAR20;\nFUN6(&VAR11->VAR22);\nVAR5 = FUN7(VAR9);\nif (VAR5)\ngoto VAR23;\nif (VAR9->VAR24) {\nVAR5 = FUN8(VAR9,\nVAR3 >> VAR17, VAR15);\nif (VAR5)\ngoto VAR23;\n}\nFUN9(VAR16);\nVAR5 = -VAR25;\nVAR13 = FUN10();\nif (!VAR13)\ngoto VAR23;\nVAR13->VAR11 = VAR11;\nVAR13->VAR26 = VAR16;\nVAR13->VAR27 = FUN11(FUN12(VAR1->VAR8));\nVAR14 = FUN13(VAR13->VAR27);\nif (FUN14(VAR14)) {\nVAR5 = FUN15(VAR14);\ngoto VAR28;\n}\nVAR13->VAR14 = VAR14;\nif (!FUN16(VAR1))\nVAR13->VAR1 = VAR1;\nVAR5 = FUN17(VAR13, VAR2, VAR3);\nif (VAR9->VAR24) {\nFUN8(VAR9,\nVAR3 >> VAR17, VAR15);\n}\nFUN18(&VAR11->VAR22);\nif (!VAR5) {\nVAR5 = FUN19(VAR13);\nif (VAR5 > 0) {\nstruct VAR11 *VAR11 = VAR9->VAR12;\nVAR1->VAR29 = VAR3 + VAR5;\nFUN20(&VAR11->VAR30);\nif (FUN21(VAR11) < VAR1->VAR29)\nFUN22(VAR11, VAR1->VAR29);\nFUN23(&VAR11->VAR30);\n}\n}\nFUN24(VAR13);\nreturn VAR5;\nVAR28:\nFUN24(VAR13);\nVAR23:\nFUN18(&VAR11->VAR22);\nVAR20:\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "ssize_t FUN1(struct kiocb *VAR1, struct iov_iter *VAR2,\nloff_t VAR3, bool VAR4)\n{\nssize_t VAR5 = -VAR6;\nstruct VAR7 *VAR7 = VAR1->VAR8;\nstruct address_space *VAR9 = VAR7->VAR10;\nstruct VAR11 *VAR11 = VAR9->VAR12;\nstruct nfs_direct_req *VAR13;\nstruct nfs_lock_context *VAR14;\nloff_t VAR15;\nsize_t VAR16 = FUN2(VAR2);\nVAR15 = (VAR3 + VAR16 - 1) >> VAR17;\nFUN3(VAR9->VAR12, VAR18, VAR16);\nFUN4(VAR19, \"STR\",\nVAR7, VAR16, (long long) VAR3);\nVAR5 = FUN5(VAR7, &VAR3, &VAR16, 0);\nif (VAR5)\ngoto VAR20;\nVAR5 = -VAR6;\nif ((VAR21) VAR16 < 0)\ngoto VAR20;\nVAR5 = 0;\nif (!VAR16)\ngoto VAR20;\nFUN6(&VAR11->VAR22);\nVAR5 = FUN7(VAR9);\nif (VAR5)\ngoto VAR23;\nif (VAR9->VAR24) {\nVAR5 = FUN8(VAR9,\nVAR3 >> VAR17, VAR15);\nif (VAR5)\ngoto VAR23;\n}\nFUN9(VAR16);\nVAR5 = -VAR25;\nVAR13 = FUN10();\nif (!VAR13)\ngoto VAR23;\nVAR13->VAR11 = VAR11;\nVAR13->VAR26 = VAR16;\nVAR13->VAR27 = FUN11(FUN12(VAR1->VAR8));\nVAR14 = FUN13(VAR13->VAR27);\nif (FUN14(VAR14)) {\nVAR5 = FUN15(VAR14);\ngoto VAR28;\n}\nVAR13->VAR14 = VAR14;\nif (!FUN16(VAR1))\nVAR13->VAR1 = VAR1;\nVAR5 = FUN17(VAR13, VAR2, VAR3, VAR4);\nif (VAR9->VAR24) {\nFUN8(VAR9,\nVAR3 >> VAR17, VAR15);\n}\nFUN18(&VAR11->VAR22);\nif (!VAR5) {\nVAR5 = FUN19(VAR13);\nif (VAR5 > 0) {\nstruct VAR11 *VAR11 = VAR9->VAR12;\nVAR1->VAR29 = VAR3 + VAR5;\nFUN20(&VAR11->VAR30);\nif (FUN21(VAR11) < VAR1->VAR29)\nFUN22(VAR11, VAR1->VAR29);\nFUN23(&VAR11->VAR30);\n}\n}\nFUN24(VAR13);\nreturn VAR5;\nVAR28:\nFUN24(VAR13);\nVAR23:\nFUN18(&VAR11->VAR22);\nVAR20:\nreturn VAR5;\n}\n",
      "code_after_change_raw": "ssize_t nfs_file_direct_write(struct kiocb *iocb, struct iov_iter *iter,\nloff_t pos, bool uio)\n{\nssize_t result = -EINVAL;\nstruct file *file = iocb->ki_filp;\nstruct address_space *mapping = file->f_mapping;\nstruct inode *inode = mapping->host;\nstruct nfs_direct_req *dreq;\nstruct nfs_lock_context *l_ctx;\nloff_t end;\nsize_t count = iov_iter_count(iter);\nend = (pos + count - 1) >> PAGE_CACHE_SHIFT;\nnfs_add_stats(mapping->host, NFSIOS_DIRECTWRITTENBYTES, count);\ndfprintk(FILE, \"NFS: direct write(%pD2, %zd@%Ld)\\n\",\nfile, count, (long long) pos);\nresult = generic_write_checks(file, &pos, &count, 0);\nif (result)\ngoto out;\nresult = -EINVAL;\nif ((ssize_t) count < 0)\ngoto out;\nresult = 0;\nif (!count)\ngoto out;\nmutex_lock(&inode->i_mutex);\nresult = nfs_sync_mapping(mapping);\nif (result)\ngoto out_unlock;\nif (mapping->nrpages) {\nresult = invalidate_inode_pages2_range(mapping,\npos >> PAGE_CACHE_SHIFT, end);\nif (result)\ngoto out_unlock;\n}\ntask_io_account_write(count);\nresult = -ENOMEM;\ndreq = nfs_direct_req_alloc();\nif (!dreq)\ngoto out_unlock;\ndreq->inode = inode;\ndreq->bytes_left = count;\ndreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\nl_ctx = nfs_get_lock_context(dreq->ctx);\nif (IS_ERR(l_ctx)) {\nresult = PTR_ERR(l_ctx);\ngoto out_release;\n}\ndreq->l_ctx = l_ctx;\nif (!is_sync_kiocb(iocb))\ndreq->iocb = iocb;\nresult = nfs_direct_write_schedule_iovec(dreq, iter, pos);\nif (mapping->nrpages) {\ninvalidate_inode_pages2_range(mapping,\npos >> PAGE_CACHE_SHIFT, end);\n}\nmutex_unlock(&inode->i_mutex);\nif (!result) {\nresult = nfs_direct_wait(dreq);\nif (result > 0) {\nstruct inode *inode = mapping->host;\niocb->ki_pos = pos + result;\nspin_lock(&inode->i_lock);\nif (i_size_read(inode) < iocb->ki_pos)\ni_size_write(inode, iocb->ki_pos);\nspin_unlock(&inode->i_lock);\n}\n}\nnfs_direct_req_release(dreq);\nreturn result;\nout_release:\nnfs_direct_req_release(dreq);\nout_unlock:\nmutex_unlock(&inode->i_mutex);\nout:\nreturn result;\n}\n",
      "code_before_change_raw": "ssize_t nfs_file_direct_write(struct kiocb *iocb, struct iov_iter *iter,\nloff_t pos, bool uio)\n{\nssize_t result = -EINVAL;\nstruct file *file = iocb->ki_filp;\nstruct address_space *mapping = file->f_mapping;\nstruct inode *inode = mapping->host;\nstruct nfs_direct_req *dreq;\nstruct nfs_lock_context *l_ctx;\nloff_t end;\nsize_t count = iov_iter_count(iter);\nend = (pos + count - 1) >> PAGE_CACHE_SHIFT;\nnfs_add_stats(mapping->host, NFSIOS_DIRECTWRITTENBYTES, count);\ndfprintk(FILE, \"NFS: direct write(%pD2, %zd@%Ld)\\n\",\nfile, count, (long long) pos);\nresult = generic_write_checks(file, &pos, &count, 0);\nif (result)\ngoto out;\nresult = -EINVAL;\nif ((ssize_t) count < 0)\ngoto out;\nresult = 0;\nif (!count)\ngoto out;\nmutex_lock(&inode->i_mutex);\nresult = nfs_sync_mapping(mapping);\nif (result)\ngoto out_unlock;\nif (mapping->nrpages) {\nresult = invalidate_inode_pages2_range(mapping,\npos >> PAGE_CACHE_SHIFT, end);\nif (result)\ngoto out_unlock;\n}\ntask_io_account_write(count);\nresult = -ENOMEM;\ndreq = nfs_direct_req_alloc();\nif (!dreq)\ngoto out_unlock;\ndreq->inode = inode;\ndreq->bytes_left = count;\ndreq->ctx = get_nfs_open_context(nfs_file_open_context(iocb->ki_filp));\nl_ctx = nfs_get_lock_context(dreq->ctx);\nif (IS_ERR(l_ctx)) {\nresult = PTR_ERR(l_ctx);\ngoto out_release;\n}\ndreq->l_ctx = l_ctx;\nif (!is_sync_kiocb(iocb))\ndreq->iocb = iocb;\nresult = nfs_direct_write_schedule_iovec(dreq, iter, pos, uio);\nif (mapping->nrpages) {\ninvalidate_inode_pages2_range(mapping,\npos >> PAGE_CACHE_SHIFT, end);\n}\nmutex_unlock(&inode->i_mutex);\nif (!result) {\nresult = nfs_direct_wait(dreq);\nif (result > 0) {\nstruct inode *inode = mapping->host;\niocb->ki_pos = pos + result;\nspin_lock(&inode->i_lock);\nif (i_size_read(inode) < iocb->ki_pos)\ni_size_write(inode, iocb->ki_pos);\nspin_unlock(&inode->i_lock);\n}\n}\nnfs_direct_req_release(dreq);\nreturn result;\nout_release:\nnfs_direct_req_release(dreq);\nout_unlock:\nmutex_unlock(&inode->i_mutex);\nout:\nreturn result;\n}\n"
    }
  ],
  "CVE-2020-10942": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation of the sk_family field in the get_raw_socket function.",
        "trigger_condition": "An attacker can exploit this vulnerability by crafting system calls to trigger kernel stack corruption.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the sk_family field, allowing an attacker to manipulate the field and potentially corrupt the kernel stack."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check for the sk_family field in the get_raw_socket function. By ensuring that the socket family is correct before proceeding, the code can prevent unauthorized manipulation of the field and enhance the security of the system. In this case, the solution involves adding the check 'if (sock->sk->sk_family != AF_PACKET)' to validate the socket family.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-10942, which involves a lack of validation of the sk_family field in the get_raw_socket function. By adding the check `if (sock->sk->sk_family != AF_PACKET)`, the code now verifies that the socket family is AF_PACKET before proceeding. This validation helps prevent potential attacks that could trigger kernel stack corruption via crafted system calls.\n\nBy including this additional check, the code ensures that only sockets with the correct family (AF_PACKET) are allowed to proceed, thereby mitigating the vulnerability and enhancing the security of the system.",
      "GPT_purpose": "Obtain a raw socket from a file descriptor and perform parameter checking.",
      "GPT_function": "\n1. Retrieves a raw socket based on the provided file descriptor.\n2. Checks if the socket is not NULL and is of type SOCK_RAW.\n3. Retrieves the socket name and validates the address family as AF_PACKET.",
      "CVE_id": "CVE-2020-10942",
      "code_before_change": "static struct socket *get_raw_socket(int fd)\n{\n\tstruct {\n\t\tstruct sockaddr_ll sa;\n\t\tchar  buf[MAX_ADDR_LEN];\n\t} uaddr;\n\tint r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\t/* Parameter checking */\n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tr = sock->ops->getname(sock, (struct sockaddr *)&uaddr.sa, 0);\n\tif (r < 0)\n\t\tgoto err;\n\n\tif (uaddr.sa.sll_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:\n\tsockfd_put(sock);\n\treturn ERR_PTR(r);\n}",
      "code_after_change": "static struct socket *get_raw_socket(int fd)\n{\n\tint r;\n\tstruct socket *sock = sockfd_lookup(fd, &r);\n\n\tif (!sock)\n\t\treturn ERR_PTR(-ENOTSOCK);\n\n\t/* Parameter checking */\n\tif (sock->sk->sk_type != SOCK_RAW) {\n\t\tr = -ESOCKTNOSUPPORT;\n\t\tgoto err;\n\t}\n\n\tif (sock->sk->sk_family != AF_PACKET) {\n\t\tr = -EPFNOSUPPORT;\n\t\tgoto err;\n\t}\n\treturn sock;\nerr:\n\tsockfd_put(sock);\n\treturn ERR_PTR(r);\n}",
      "modified_lines": {
        "added": [
          "\tif (sock->sk->sk_family != AF_PACKET) {"
        ],
        "deleted": [
          "\tstruct {",
          "\t\tstruct sockaddr_ll sa;",
          "\t\tchar  buf[MAX_ADDR_LEN];",
          "\t} uaddr;",
          "\tr = sock->ops->getname(sock, (struct sockaddr *)&uaddr.sa, 0);",
          "\tif (r < 0)",
          "\t\tgoto err;",
          "",
          "\tif (uaddr.sa.sll_family != AF_PACKET) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation of the sk_family field in the get_raw_socket function.",
      "trigger_condition": "An attacker can exploit this vulnerability by crafting system calls to trigger kernel stack corruption.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the sk_family field, allowing an attacker to manipulate the field and potentially corrupt the kernel stack.",
      "id": 70,
      "code_after_change_normalized": "static struct socket *FUN1(int VAR1)\n{\nint VAR2;\nstruct socket *VAR3 = FUN2(VAR1, &VAR2);\nif (!VAR3)\nreturn FUN3(-VAR4);\nif (VAR3->VAR5->VAR6 != VAR7) {\nVAR2 = -VAR8;\ngoto VAR9;\n}\nif (VAR3->VAR5->VAR10 != VAR11) {\nVAR2 = -VAR12;\ngoto VAR9;\n}\nreturn VAR3;\nVAR9:\nFUN4(VAR3);\nreturn FUN3(VAR2);\n}\n",
      "code_before_change_normalized": "static struct socket *FUN1(int VAR1)\n{\nstruct {\nstruct sockaddr_ll VAR2;\nchar  VAR3[VAR4];\n} VAR5;\nint VAR6;\nstruct socket *VAR7 = FUN2(VAR1, &VAR6);\nif (!VAR7)\nreturn FUN3(-VAR8);\nif (VAR7->VAR9->VAR10 != VAR11) {\nVAR6 = -VAR12;\ngoto VAR13;\n}\nVAR6 = VAR7->VAR14->FUN4(VAR7, (struct VAR15 *)&VAR5.VAR2, 0);\nif (VAR6 < 0)\ngoto VAR13;\nif (VAR5.VAR2.VAR16 != VAR17) {\nVAR6 = -VAR18;\ngoto VAR13;\n}\nreturn VAR7;\nVAR13:\nFUN5(VAR7);\nreturn FUN3(VAR6);\n}\n",
      "code_after_change_raw": "static struct socket *get_raw_socket(int fd)\n{\nint r;\nstruct socket *sock = sockfd_lookup(fd, &r);\nif (!sock)\nreturn ERR_PTR(-ENOTSOCK);\nif (sock->sk->sk_type != SOCK_RAW) {\nr = -ESOCKTNOSUPPORT;\ngoto err;\n}\nif (sock->sk->sk_family != AF_PACKET) {\nr = -EPFNOSUPPORT;\ngoto err;\n}\nreturn sock;\nerr:\nsockfd_put(sock);\nreturn ERR_PTR(r);\n}\n",
      "code_before_change_raw": "static struct socket *get_raw_socket(int fd)\n{\nstruct {\nstruct sockaddr_ll sa;\nchar  buf[MAX_ADDR_LEN];\n} uaddr;\nint r;\nstruct socket *sock = sockfd_lookup(fd, &r);\nif (!sock)\nreturn ERR_PTR(-ENOTSOCK);\nif (sock->sk->sk_type != SOCK_RAW) {\nr = -ESOCKTNOSUPPORT;\ngoto err;\n}\nr = sock->ops->getname(sock, (struct sockaddr *)&uaddr.sa, 0);\nif (r < 0)\ngoto err;\nif (uaddr.sa.sll_family != AF_PACKET) {\nr = -EPFNOSUPPORT;\ngoto err;\n}\nreturn sock;\nerr:\nsockfd_put(sock);\nreturn ERR_PTR(r);\n}\n"
    }
  ],
  "CVE-2020-11565": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for an empty nodelist during mount option parsing.",
        "trigger_condition": "An empty nodelist is not properly handled in specific scenarios, leading to a stack-based out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not adequately check for an empty nodelist in the MPOL_PREFERRED case, which can result in a stack-based out-of-bounds write if the nodelist (or nodes) is empty at a critical point."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that an empty nodelist is properly validated before proceeding with operations that could lead to a stack-based out-of-bounds write. In this case, the solution involves modifying the code to explicitly check for an empty nodelist in the MPOL_PREFERRED case before continuing with the execution. By adding this check, the code can prevent the vulnerability from being exploited by ensuring that nodelist (or nodes) cannot be empty in situations where it could lead to a security issue.",
      "GPT_analysis": "The modification made to the code snippet is necessary to address the vulnerability CVE-2020-11565, which involves a stack-based out-of-bounds write due to mishandling an empty nodelist during mount option parsing in the mpol_parse_str function.\n\nThe vulnerability occurs when the code does not properly handle an empty nodelist in certain scenarios, potentially leading to a stack-based out-of-bounds write. By modifying the code as shown in the corrected version, specifically in the MPOL_PREFERRED case, we ensure that the code checks for an empty nodelist before proceeding. This modification helps prevent the vulnerability by ensuring that nodelist (or nodes) cannot be empty in situations where it could lead to a security issue.\n\nIn summary, the modification is necessary to improve the handling of empty nodelists and prevent the stack-based out-of-bounds write vulnerability from being exploited.",
      "GPT_purpose": "Parse a string representing a memory policy and create a new mempolicy structure based on the parsed information.",
      "GPT_function": "\n1. Parsing a string to extract memory policy information.\n2. Handling different memory policy modes and flags.\n3. Creating a new memory policy based on the parsed information.\n4. Saving nodes for specific contexts and mount options.\n5. Restoring the string for error message display.",
      "CVE_id": "CVE-2020-11565",
      "code_before_change": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t/* terminate mode string */\n\n\tif (nodelist) {\n\t\t/* NUL-terminate mode or flags string */\n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*\n\t\t * Default to online nodes with memory if no nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t/*\n\t\t * Don't allow a nodelist;  mpol_new() checks flags\n\t\t */\n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tmode = MPOL_PREFERRED;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t/*\n\t\t * Insist on a empty nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_BIND:\n\t\t/*\n\t\t * Insist on a nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t/*\n\t\t * Currently, we only support two mutually exclusive\n\t\t * mode flags.\n\t\t */\n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t/*\n\t * Save nodes for mpol_to_str() to show the tmpfs mount options\n\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.\n\t */\n\tif (mode != MPOL_PREFERRED)\n\t\tnew->v.nodes = nodes;\n\telse if (nodelist)\n\t\tnew->v.preferred_node = first_node(nodes);\n\telse\n\t\tnew->flags |= MPOL_F_LOCAL;\n\n\t/*\n\t * Save nodes for contextualization: this will be used to \"clone\"\n\t * the mempolicy in a specific context [cpuset] at a later time.\n\t */\n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t/* Restore string for error message */\n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}",
      "code_after_change": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\n\tstruct mempolicy *new = NULL;\n\tunsigned short mode_flags;\n\tnodemask_t nodes;\n\tchar *nodelist = strchr(str, ':');\n\tchar *flags = strchr(str, '=');\n\tint err = 1, mode;\n\n\tif (flags)\n\t\t*flags++ = '\\0';\t/* terminate mode string */\n\n\tif (nodelist) {\n\t\t/* NUL-terminate mode or flags string */\n\t\t*nodelist++ = '\\0';\n\t\tif (nodelist_parse(nodelist, nodes))\n\t\t\tgoto out;\n\t\tif (!nodes_subset(nodes, node_states[N_MEMORY]))\n\t\t\tgoto out;\n\t} else\n\t\tnodes_clear(nodes);\n\n\tmode = match_string(policy_modes, MPOL_MAX, str);\n\tif (mode < 0)\n\t\tgoto out;\n\n\tswitch (mode) {\n\tcase MPOL_PREFERRED:\n\t\t/*\n\t\t * Insist on a nodelist of one node only, although later\n\t\t * we use first_node(nodes) to grab a single node, so here\n\t\t * nodelist (or nodes) cannot be empty.\n\t\t */\n\t\tif (nodelist) {\n\t\t\tchar *rest = nodelist;\n\t\t\twhile (isdigit(*rest))\n\t\t\t\trest++;\n\t\t\tif (*rest)\n\t\t\t\tgoto out;\n\t\t\tif (nodes_empty(nodes))\n\t\t\t\tgoto out;\n\t\t}\n\t\tbreak;\n\tcase MPOL_INTERLEAVE:\n\t\t/*\n\t\t * Default to online nodes with memory if no nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tnodes = node_states[N_MEMORY];\n\t\tbreak;\n\tcase MPOL_LOCAL:\n\t\t/*\n\t\t * Don't allow a nodelist;  mpol_new() checks flags\n\t\t */\n\t\tif (nodelist)\n\t\t\tgoto out;\n\t\tmode = MPOL_PREFERRED;\n\t\tbreak;\n\tcase MPOL_DEFAULT:\n\t\t/*\n\t\t * Insist on a empty nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\terr = 0;\n\t\tgoto out;\n\tcase MPOL_BIND:\n\t\t/*\n\t\t * Insist on a nodelist\n\t\t */\n\t\tif (!nodelist)\n\t\t\tgoto out;\n\t}\n\n\tmode_flags = 0;\n\tif (flags) {\n\t\t/*\n\t\t * Currently, we only support two mutually exclusive\n\t\t * mode flags.\n\t\t */\n\t\tif (!strcmp(flags, \"static\"))\n\t\t\tmode_flags |= MPOL_F_STATIC_NODES;\n\t\telse if (!strcmp(flags, \"relative\"))\n\t\t\tmode_flags |= MPOL_F_RELATIVE_NODES;\n\t\telse\n\t\t\tgoto out;\n\t}\n\n\tnew = mpol_new(mode, mode_flags, &nodes);\n\tif (IS_ERR(new))\n\t\tgoto out;\n\n\t/*\n\t * Save nodes for mpol_to_str() to show the tmpfs mount options\n\t * for /proc/mounts, /proc/pid/mounts and /proc/pid/mountinfo.\n\t */\n\tif (mode != MPOL_PREFERRED)\n\t\tnew->v.nodes = nodes;\n\telse if (nodelist)\n\t\tnew->v.preferred_node = first_node(nodes);\n\telse\n\t\tnew->flags |= MPOL_F_LOCAL;\n\n\t/*\n\t * Save nodes for contextualization: this will be used to \"clone\"\n\t * the mempolicy in a specific context [cpuset] at a later time.\n\t */\n\tnew->w.user_nodemask = nodes;\n\n\terr = 0;\n\nout:\n\t/* Restore string for error message */\n\tif (nodelist)\n\t\t*--nodelist = ':';\n\tif (flags)\n\t\t*--flags = '=';\n\tif (!err)\n\t\t*mpol = new;\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\t\t * Insist on a nodelist of one node only, although later",
          "\t\t * we use first_node(nodes) to grab a single node, so here",
          "\t\t * nodelist (or nodes) cannot be empty.",
          "\t\t\t\tgoto out;",
          "\t\t\tif (nodes_empty(nodes))"
        ],
        "deleted": [
          "\t\t * Insist on a nodelist of one node only"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for an empty nodelist during mount option parsing.",
      "trigger_condition": "An empty nodelist is not properly handled in specific scenarios, leading to a stack-based out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not adequately check for an empty nodelist in the MPOL_PREFERRED case, which can result in a stack-based out-of-bounds write if the nodelist (or nodes) is empty at a critical point.",
      "id": 71,
      "code_after_change_normalized": "int FUN1(char *VAR1, struct mempolicy **VAR2)\n{\nstruct mempolicy *new = NULL;\nunsigned short VAR3;\nnodemask_t VAR4;\nchar *VAR5 = FUN2(VAR1, );\nchar *VAR6 = FUN2(VAR1, );\nint VAR7 = 1, VAR8;\nif (VAR6)\n*VAR6++ = ;\t\nif (VAR5) {\n*VAR5++ = ;\nif (FUN3(VAR5, VAR4))\ngoto VAR9;\nif (!FUN4(VAR4, VAR10[VAR11]))\ngoto VAR9;\n} else\nFUN5(VAR4);\nVAR8 = FUN6(VAR12, VAR13, VAR1);\nif (VAR8 < 0)\ngoto VAR9;\nswitch (VAR8) {\ncase VAR14:\nif (VAR5) {\nchar *VAR15 = VAR5;\nwhile (FUN7(*VAR15))\nVAR15++;\nif (*VAR15)\ngoto VAR9;\nif (FUN8(VAR4))\ngoto VAR9;\n}\nbreak;\ncase VAR16:\nif (!VAR5)\nVAR4 = VAR10[VAR11];\nbreak;\ncase VAR17:\nif (VAR5)\ngoto VAR9;\nVAR8 = VAR14;\nbreak;\ncase VAR18:\nif (!VAR5)\nVAR7 = 0;\ngoto VAR9;\ncase VAR19:\nif (!VAR5)\ngoto VAR9;\n}\nVAR3 = 0;\nif (VAR6) {\nif (!FUN9(VAR6, \"STR\"))\nVAR3 |= VAR20;\nelse if (!FUN9(VAR6, \"STR\"))\nVAR3 |= VAR21;\nelse\ngoto VAR9;\n}\nnew = FUN10(VAR8, VAR3, &VAR4);\nif (FUN11(new))\ngoto VAR9;\nif (VAR8 != VAR14)\nnew->VAR22.VAR4 = VAR4;\nelse if (VAR5)\nnew->VAR22.VAR23 = FUN12(VAR4);\nelse\nnew->VAR6 |= VAR24;\nnew->VAR25.VAR26 = VAR4;\nVAR7 = 0;\nVAR9:\nif (VAR5)\n*--VAR5 = ;\nif (VAR6)\n*--VAR6 = ;\nif (!VAR7)\n*VAR2 = new;\nreturn VAR7;\n}\n",
      "code_before_change_normalized": "int FUN1(char *VAR1, struct mempolicy **VAR2)\n{\nstruct mempolicy *new = NULL;\nunsigned short VAR3;\nnodemask_t VAR4;\nchar *VAR5 = FUN2(VAR1, );\nchar *VAR6 = FUN2(VAR1, );\nint VAR7 = 1, VAR8;\nif (VAR6)\n*VAR6++ = ;\t\nif (VAR5) {\n*VAR5++ = ;\nif (FUN3(VAR5, VAR4))\ngoto VAR9;\nif (!FUN4(VAR4, VAR10[VAR11]))\ngoto VAR9;\n} else\nFUN5(VAR4);\nVAR8 = FUN6(VAR12, VAR13, VAR1);\nif (VAR8 < 0)\ngoto VAR9;\nswitch (VAR8) {\ncase VAR14:\nif (VAR5) {\nchar *VAR15 = VAR5;\nwhile (FUN7(*VAR15))\nVAR15++;\nif (*VAR15)\ngoto VAR9;\n}\nbreak;\ncase VAR16:\nif (!VAR5)\nVAR4 = VAR10[VAR11];\nbreak;\ncase VAR17:\nif (VAR5)\ngoto VAR9;\nVAR8 = VAR14;\nbreak;\ncase VAR18:\nif (!VAR5)\nVAR7 = 0;\ngoto VAR9;\ncase VAR19:\nif (!VAR5)\ngoto VAR9;\n}\nVAR3 = 0;\nif (VAR6) {\nif (!FUN8(VAR6, \"STR\"))\nVAR3 |= VAR20;\nelse if (!FUN8(VAR6, \"STR\"))\nVAR3 |= VAR21;\nelse\ngoto VAR9;\n}\nnew = FUN9(VAR8, VAR3, &VAR4);\nif (FUN10(new))\ngoto VAR9;\nif (VAR8 != VAR14)\nnew->VAR22.VAR4 = VAR4;\nelse if (VAR5)\nnew->VAR22.VAR23 = FUN11(VAR4);\nelse\nnew->VAR6 |= VAR24;\nnew->VAR25.VAR26 = VAR4;\nVAR7 = 0;\nVAR9:\nif (VAR5)\n*--VAR5 = ;\nif (VAR6)\n*--VAR6 = ;\nif (!VAR7)\n*VAR2 = new;\nreturn VAR7;\n}\n",
      "code_after_change_raw": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\nstruct mempolicy *new = NULL;\nunsigned short mode_flags;\nnodemask_t nodes;\nchar *nodelist = strchr(str, ':');\nchar *flags = strchr(str, '=');\nint err = 1, mode;\nif (flags)\n*flags++ = '\\0';\t\nif (nodelist) {\n*nodelist++ = '\\0';\nif (nodelist_parse(nodelist, nodes))\ngoto out;\nif (!nodes_subset(nodes, node_states[N_MEMORY]))\ngoto out;\n} else\nnodes_clear(nodes);\nmode = match_string(policy_modes, MPOL_MAX, str);\nif (mode < 0)\ngoto out;\nswitch (mode) {\ncase MPOL_PREFERRED:\nif (nodelist) {\nchar *rest = nodelist;\nwhile (isdigit(*rest))\nrest++;\nif (*rest)\ngoto out;\nif (nodes_empty(nodes))\ngoto out;\n}\nbreak;\ncase MPOL_INTERLEAVE:\nif (!nodelist)\nnodes = node_states[N_MEMORY];\nbreak;\ncase MPOL_LOCAL:\nif (nodelist)\ngoto out;\nmode = MPOL_PREFERRED;\nbreak;\ncase MPOL_DEFAULT:\nif (!nodelist)\nerr = 0;\ngoto out;\ncase MPOL_BIND:\nif (!nodelist)\ngoto out;\n}\nmode_flags = 0;\nif (flags) {\nif (!strcmp(flags, \"static\"))\nmode_flags |= MPOL_F_STATIC_NODES;\nelse if (!strcmp(flags, \"relative\"))\nmode_flags |= MPOL_F_RELATIVE_NODES;\nelse\ngoto out;\n}\nnew = mpol_new(mode, mode_flags, &nodes);\nif (IS_ERR(new))\ngoto out;\nif (mode != MPOL_PREFERRED)\nnew->v.nodes = nodes;\nelse if (nodelist)\nnew->v.preferred_node = first_node(nodes);\nelse\nnew->flags |= MPOL_F_LOCAL;\nnew->w.user_nodemask = nodes;\nerr = 0;\nout:\nif (nodelist)\n*--nodelist = ':';\nif (flags)\n*--flags = '=';\nif (!err)\n*mpol = new;\nreturn err;\n}\n",
      "code_before_change_raw": "int mpol_parse_str(char *str, struct mempolicy **mpol)\n{\nstruct mempolicy *new = NULL;\nunsigned short mode_flags;\nnodemask_t nodes;\nchar *nodelist = strchr(str, ':');\nchar *flags = strchr(str, '=');\nint err = 1, mode;\nif (flags)\n*flags++ = '\\0';\t\nif (nodelist) {\n*nodelist++ = '\\0';\nif (nodelist_parse(nodelist, nodes))\ngoto out;\nif (!nodes_subset(nodes, node_states[N_MEMORY]))\ngoto out;\n} else\nnodes_clear(nodes);\nmode = match_string(policy_modes, MPOL_MAX, str);\nif (mode < 0)\ngoto out;\nswitch (mode) {\ncase MPOL_PREFERRED:\nif (nodelist) {\nchar *rest = nodelist;\nwhile (isdigit(*rest))\nrest++;\nif (*rest)\ngoto out;\n}\nbreak;\ncase MPOL_INTERLEAVE:\nif (!nodelist)\nnodes = node_states[N_MEMORY];\nbreak;\ncase MPOL_LOCAL:\nif (nodelist)\ngoto out;\nmode = MPOL_PREFERRED;\nbreak;\ncase MPOL_DEFAULT:\nif (!nodelist)\nerr = 0;\ngoto out;\ncase MPOL_BIND:\nif (!nodelist)\ngoto out;\n}\nmode_flags = 0;\nif (flags) {\nif (!strcmp(flags, \"static\"))\nmode_flags |= MPOL_F_STATIC_NODES;\nelse if (!strcmp(flags, \"relative\"))\nmode_flags |= MPOL_F_RELATIVE_NODES;\nelse\ngoto out;\n}\nnew = mpol_new(mode, mode_flags, &nodes);\nif (IS_ERR(new))\ngoto out;\nif (mode != MPOL_PREFERRED)\nnew->v.nodes = nodes;\nelse if (nodelist)\nnew->v.preferred_node = first_node(nodes);\nelse\nnew->flags |= MPOL_F_LOCAL;\nnew->w.user_nodemask = nodes;\nerr = 0;\nout:\nif (nodelist)\n*--nodelist = ':';\nif (flags)\n*--flags = '=';\nif (!err)\n*mpol = new;\nreturn err;\n}\n"
    }
  ],
  "CVE-2020-12653": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper input validation and length checking for data being copied into a buffer.",
        "trigger_condition": "An attacker provides malicious input data with a length exceeding the buffer size, causing a buffer overflow during the memcpy operation.",
        "specific_code_behavior_causing_vulnerability": "The code blindly copies data into a buffer without verifying that the length of the data does not exceed the maximum allowed buffer size. This lack of input validation opens up the possibility of a buffer overflow vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a length check before copying data into the buffer. By verifying that the length of the data does not exceed the maximum allowed buffer size, the code can prevent buffer overflow vulnerabilities. In this case, the solution involves adding a check to ensure that the length of the data being copied does not exceed the specified maximum length. If the length exceeds this limit, appropriate error handling should be implemented to prevent buffer overflows.",
      "GPT_analysis": "The modification is necessary to prevent a buffer overflow vulnerability in the mwifiex_cmd_append_vsie_tlv() function. The vulnerability occurs due to an incorrect memcpy operation that does not check the length of the data being copied into the buffer. By adding the check `if (le16_to_cpu(vs_param_set->header.len) > MWIFIEX_MAX_VSIE_LEN)`, the code ensures that the length of the data being copied does not exceed the maximum allowed length `MWIFIEX_MAX_VSIE_LEN`. If the length exceeds this limit, an error message is logged, and the loop is terminated to prevent a buffer overflow.\n\nBy adding this check, the code becomes more secure and prevents potential exploitation of the vulnerability, which could lead to privilege escalation or denial of service.",
      "GPT_purpose": "Append selected vendor specific IE as TLV to a command in the mwifiex driver.",
      "GPT_function": "\n1. Append vendor specific IE as TLV to the command.\n2. Traverse through the saved vendor specific IE array.\n3. Update the header type and length of the vendor specific IE.",
      "CVE_id": "CVE-2020-12653",
      "code_before_change": "int\nmwifiex_cmd_append_vsie_tlv(struct mwifiex_private *priv,\n\t\t\t    u16 vsie_mask, u8 **buffer)\n{\n\tint id, ret_len = 0;\n\tstruct mwifiex_ie_types_vendor_param_set *vs_param_set;\n\n\tif (!buffer)\n\t\treturn 0;\n\tif (!(*buffer))\n\t\treturn 0;\n\n\t/*\n\t * Traverse through the saved vendor specific IE array and append\n\t * the selected(scan/assoc/adhoc) IE as TLV to the command\n\t */\n\tfor (id = 0; id < MWIFIEX_MAX_VSIE_NUM; id++) {\n\t\tif (priv->vs_ie[id].mask & vsie_mask) {\n\t\t\tvs_param_set =\n\t\t\t\t(struct mwifiex_ie_types_vendor_param_set *)\n\t\t\t\t*buffer;\n\t\t\tvs_param_set->header.type =\n\t\t\t\tcpu_to_le16(TLV_TYPE_PASSTHROUGH);\n\t\t\tvs_param_set->header.len =\n\t\t\t\tcpu_to_le16((((u16) priv->vs_ie[id].ie[1])\n\t\t\t\t& 0x00FF) + 2);\n\t\t\tmemcpy(vs_param_set->ie, priv->vs_ie[id].ie,\n\t\t\t       le16_to_cpu(vs_param_set->header.len));\n\t\t\t*buffer += le16_to_cpu(vs_param_set->header.len) +\n\t\t\t\t   sizeof(struct mwifiex_ie_types_header);\n\t\t\tret_len += le16_to_cpu(vs_param_set->header.len) +\n\t\t\t\t   sizeof(struct mwifiex_ie_types_header);\n\t\t}\n\t}\n\treturn ret_len;\n}",
      "code_after_change": "int\nmwifiex_cmd_append_vsie_tlv(struct mwifiex_private *priv,\n\t\t\t    u16 vsie_mask, u8 **buffer)\n{\n\tint id, ret_len = 0;\n\tstruct mwifiex_ie_types_vendor_param_set *vs_param_set;\n\n\tif (!buffer)\n\t\treturn 0;\n\tif (!(*buffer))\n\t\treturn 0;\n\n\t/*\n\t * Traverse through the saved vendor specific IE array and append\n\t * the selected(scan/assoc/adhoc) IE as TLV to the command\n\t */\n\tfor (id = 0; id < MWIFIEX_MAX_VSIE_NUM; id++) {\n\t\tif (priv->vs_ie[id].mask & vsie_mask) {\n\t\t\tvs_param_set =\n\t\t\t\t(struct mwifiex_ie_types_vendor_param_set *)\n\t\t\t\t*buffer;\n\t\t\tvs_param_set->header.type =\n\t\t\t\tcpu_to_le16(TLV_TYPE_PASSTHROUGH);\n\t\t\tvs_param_set->header.len =\n\t\t\t\tcpu_to_le16((((u16) priv->vs_ie[id].ie[1])\n\t\t\t\t& 0x00FF) + 2);\n\t\t\tif (le16_to_cpu(vs_param_set->header.len) >\n\t\t\t\tMWIFIEX_MAX_VSIE_LEN) {\n\t\t\t\tmwifiex_dbg(priv->adapter, ERROR,\n\t\t\t\t\t    \"Invalid param length!\\n\");\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tmemcpy(vs_param_set->ie, priv->vs_ie[id].ie,\n\t\t\t       le16_to_cpu(vs_param_set->header.len));\n\t\t\t*buffer += le16_to_cpu(vs_param_set->header.len) +\n\t\t\t\t   sizeof(struct mwifiex_ie_types_header);\n\t\t\tret_len += le16_to_cpu(vs_param_set->header.len) +\n\t\t\t\t   sizeof(struct mwifiex_ie_types_header);\n\t\t}\n\t}\n\treturn ret_len;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tif (le16_to_cpu(vs_param_set->header.len) >",
          "\t\t\t\tMWIFIEX_MAX_VSIE_LEN) {",
          "\t\t\t\tmwifiex_dbg(priv->adapter, ERROR,",
          "\t\t\t\t\t    \"Invalid param length!\\n\");",
          "\t\t\t\tbreak;",
          "\t\t\t}",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper input validation and length checking for data being copied into a buffer.",
      "trigger_condition": "An attacker provides malicious input data with a length exceeding the buffer size, causing a buffer overflow during the memcpy operation.",
      "specific_code_behavior_causing_vulnerability": "The code blindly copies data into a buffer without verifying that the length of the data does not exceed the maximum allowed buffer size. This lack of input validation opens up the possibility of a buffer overflow vulnerability.",
      "id": 72,
      "code_after_change_normalized": "int\nFUN1(struct mwifiex_private *VAR1,\nu16 VAR2, u8 **VAR3)\n{\nint VAR4, VAR5 = 0;\nstruct mwifiex_ie_types_vendor_param_set *VAR6;\nif (!VAR3)\nreturn 0;\nif (!(*VAR3))\nreturn 0;\nfor (VAR4 = 0; VAR4 < VAR7; VAR4++) {\nif (VAR1->VAR8[VAR4].VAR9 & VAR2) {\nVAR6 =\n(struct VAR10 *)\n*VAR3;\nVAR6->VAR11.VAR12 =\nFUN2(VAR13);\nVAR6->VAR11.VAR14 =\nFUN2((((VAR15) VAR1->VAR8[VAR4].VAR16[1])\n& VAR17) + 2);\nif (FUN3(VAR6->VAR11.VAR14) >\nVAR18) {\nFUN4(VAR1->VAR19, VAR20,\n\"STR\");\nbreak;\n}\nFUN5(VAR6->VAR16, VAR1->VAR8[VAR4].VAR16,\nFUN3(VAR6->VAR11.VAR14));\n*VAR3 += FUN3(VAR6->VAR11.VAR14) +\nsizeof(struct VAR21);\nVAR5 += FUN3(VAR6->VAR11.VAR14) +\nsizeof(struct VAR21);\n}\n}\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "int\nFUN1(struct mwifiex_private *VAR1,\nu16 VAR2, u8 **VAR3)\n{\nint VAR4, VAR5 = 0;\nstruct mwifiex_ie_types_vendor_param_set *VAR6;\nif (!VAR3)\nreturn 0;\nif (!(*VAR3))\nreturn 0;\nfor (VAR4 = 0; VAR4 < VAR7; VAR4++) {\nif (VAR1->VAR8[VAR4].VAR9 & VAR2) {\nVAR6 =\n(struct VAR10 *)\n*VAR3;\nVAR6->VAR11.VAR12 =\nFUN2(VAR13);\nVAR6->VAR11.VAR14 =\nFUN2((((VAR15) VAR1->VAR8[VAR4].VAR16[1])\n& VAR17) + 2);\nFUN3(VAR6->VAR16, VAR1->VAR8[VAR4].VAR16,\nFUN4(VAR6->VAR11.VAR14));\n*VAR3 += FUN4(VAR6->VAR11.VAR14) +\nsizeof(struct VAR18);\nVAR5 += FUN4(VAR6->VAR11.VAR14) +\nsizeof(struct VAR18);\n}\n}\nreturn VAR5;\n}\n",
      "code_after_change_raw": "int\nmwifiex_cmd_append_vsie_tlv(struct mwifiex_private *priv,\nu16 vsie_mask, u8 **buffer)\n{\nint id, ret_len = 0;\nstruct mwifiex_ie_types_vendor_param_set *vs_param_set;\nif (!buffer)\nreturn 0;\nif (!(*buffer))\nreturn 0;\nfor (id = 0; id < MWIFIEX_MAX_VSIE_NUM; id++) {\nif (priv->vs_ie[id].mask & vsie_mask) {\nvs_param_set =\n(struct mwifiex_ie_types_vendor_param_set *)\n*buffer;\nvs_param_set->header.type =\ncpu_to_le16(TLV_TYPE_PASSTHROUGH);\nvs_param_set->header.len =\ncpu_to_le16((((u16) priv->vs_ie[id].ie[1])\n& 0x00FF) + 2);\nif (le16_to_cpu(vs_param_set->header.len) >\nMWIFIEX_MAX_VSIE_LEN) {\nmwifiex_dbg(priv->adapter, ERROR,\n\"Invalid param length!\\n\");\nbreak;\n}\nmemcpy(vs_param_set->ie, priv->vs_ie[id].ie,\nle16_to_cpu(vs_param_set->header.len));\n*buffer += le16_to_cpu(vs_param_set->header.len) +\nsizeof(struct mwifiex_ie_types_header);\nret_len += le16_to_cpu(vs_param_set->header.len) +\nsizeof(struct mwifiex_ie_types_header);\n}\n}\nreturn ret_len;\n}\n",
      "code_before_change_raw": "int\nmwifiex_cmd_append_vsie_tlv(struct mwifiex_private *priv,\nu16 vsie_mask, u8 **buffer)\n{\nint id, ret_len = 0;\nstruct mwifiex_ie_types_vendor_param_set *vs_param_set;\nif (!buffer)\nreturn 0;\nif (!(*buffer))\nreturn 0;\nfor (id = 0; id < MWIFIEX_MAX_VSIE_NUM; id++) {\nif (priv->vs_ie[id].mask & vsie_mask) {\nvs_param_set =\n(struct mwifiex_ie_types_vendor_param_set *)\n*buffer;\nvs_param_set->header.type =\ncpu_to_le16(TLV_TYPE_PASSTHROUGH);\nvs_param_set->header.len =\ncpu_to_le16((((u16) priv->vs_ie[id].ie[1])\n& 0x00FF) + 2);\nmemcpy(vs_param_set->ie, priv->vs_ie[id].ie,\nle16_to_cpu(vs_param_set->header.len));\n*buffer += le16_to_cpu(vs_param_set->header.len) +\nsizeof(struct mwifiex_ie_types_header);\nret_len += le16_to_cpu(vs_param_set->header.len) +\nsizeof(struct mwifiex_ie_types_header);\n}\n}\nreturn ret_len;\n}\n"
    }
  ],
  "CVE-2020-12654": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of size validation for the data being copied in a memcpy operation, allowing potential buffer overflow.",
        "trigger_condition": "The code performs a memcpy operation without verifying that the length of the data being copied does not exceed the size of the destination buffer.",
        "specific_code_behavior_causing_vulnerability": "The vulnerable code snippet lacks a check to ensure that the length of the data being copied in the memcpy operation is within the bounds of the destination buffer. This oversight can lead to a heap-based buffer overflow if the length of the data exceeds the size of the destination buffer."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a size check before the memcpy operation to ensure that the length of the data being copied does not exceed the size of the destination buffer. By verifying the size of the data to be copied, the code can prevent potential buffer overflow exploits. In this case, the solution involves adding a conditional check to break out of the operation if the length of the data exceeds the size of the destination buffer, thus preventing a heap-based buffer overflow.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-12654, which involves a heap-based buffer overflow caused by an incorrect `memcpy` operation in the `mwifiex_ret_wmm_get_status` function. \n\nIn the vulnerable code snippet, the `memcpy` operation is copying data from `wmm_param_ie` to `priv->curr_bss_params.bss_descriptor.wmm_ie` without checking if the size of the destination buffer is sufficient to hold the copied data. This can lead to a buffer overflow if the length of the data being copied exceeds the size of the destination buffer.\n\nThe modification adds a check before the `memcpy` operation to ensure that the length of the data being copied does not exceed the size of the destination buffer. If the length exceeds the buffer size, the code breaks out of the switch case, preventing the buffer overflow.\n\nBy adding this check, the code mitigates the vulnerability by ensuring that the `memcpy` operation does not write beyond the bounds of the destination buffer, thus preventing potential heap-based buffer overflow exploits.",
      "GPT_purpose": "Retrieve WMM (Wi-Fi Multimedia) status information from a response command and update the WMM queue status and parameters accordingly in a Marvell wireless driver.",
      "GPT_function": "\n1. Retrieve WMM status information from a response packet.\n2. Process TLVs (Type-Length-Value) in the response packet.\n3. Handle different TLV types including WMM queue status and vendor-specific information.\n4. Update WMM queue status information in the driver's data structure.\n5. Copy WMM parameter set count information to the driver's data structure.\n6. Call functions to set up queue priorities and AC (Access Category) downgrade.",
      "CVE_id": "CVE-2020-12654",
      "code_before_change": "int mwifiex_ret_wmm_get_status(struct mwifiex_private *priv,\n\t\t\t       const struct host_cmd_ds_command *resp)\n{\n\tu8 *curr = (u8 *) &resp->params.get_wmm_status;\n\tuint16_t resp_len = le16_to_cpu(resp->size), tlv_len;\n\tint mask = IEEE80211_WMM_IE_AP_QOSINFO_PARAM_SET_CNT_MASK;\n\tbool valid = true;\n\n\tstruct mwifiex_ie_types_data *tlv_hdr;\n\tstruct mwifiex_ie_types_wmm_queue_status *tlv_wmm_qstatus;\n\tstruct ieee_types_wmm_parameter *wmm_param_ie = NULL;\n\tstruct mwifiex_wmm_ac_status *ac_status;\n\n\tmwifiex_dbg(priv->adapter, INFO,\n\t\t    \"info: WMM: WMM_GET_STATUS cmdresp received: %d\\n\",\n\t\t    resp_len);\n\n\twhile ((resp_len >= sizeof(tlv_hdr->header)) && valid) {\n\t\ttlv_hdr = (struct mwifiex_ie_types_data *) curr;\n\t\ttlv_len = le16_to_cpu(tlv_hdr->header.len);\n\n\t\tif (resp_len < tlv_len + sizeof(tlv_hdr->header))\n\t\t\tbreak;\n\n\t\tswitch (le16_to_cpu(tlv_hdr->header.type)) {\n\t\tcase TLV_TYPE_WMMQSTATUS:\n\t\t\ttlv_wmm_qstatus =\n\t\t\t\t(struct mwifiex_ie_types_wmm_queue_status *)\n\t\t\t\ttlv_hdr;\n\t\t\tmwifiex_dbg(priv->adapter, CMD,\n\t\t\t\t    \"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\t\t\t\t    \"QSTATUS TLV: %d, %d, %d\\n\",\n\t\t\t\t    tlv_wmm_qstatus->queue_index,\n\t\t\t\t    tlv_wmm_qstatus->flow_required,\n\t\t\t\t    tlv_wmm_qstatus->disabled);\n\n\t\t\tac_status = &priv->wmm.ac_status[tlv_wmm_qstatus->\n\t\t\t\t\t\t\t queue_index];\n\t\t\tac_status->disabled = tlv_wmm_qstatus->disabled;\n\t\t\tac_status->flow_required =\n\t\t\t\t\t\ttlv_wmm_qstatus->flow_required;\n\t\t\tac_status->flow_created = tlv_wmm_qstatus->flow_created;\n\t\t\tbreak;\n\n\t\tcase WLAN_EID_VENDOR_SPECIFIC:\n\t\t\t/*\n\t\t\t * Point the regular IEEE IE 2 bytes into the Marvell IE\n\t\t\t *   and setup the IEEE IE type and length byte fields\n\t\t\t */\n\n\t\t\twmm_param_ie =\n\t\t\t\t(struct ieee_types_wmm_parameter *) (curr +\n\t\t\t\t\t\t\t\t    2);\n\t\t\twmm_param_ie->vend_hdr.len = (u8) tlv_len;\n\t\t\twmm_param_ie->vend_hdr.element_id =\n\t\t\t\t\t\tWLAN_EID_VENDOR_SPECIFIC;\n\n\t\t\tmwifiex_dbg(priv->adapter, CMD,\n\t\t\t\t    \"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\t\t\t\t    \"WMM Parameter Set Count: %d\\n\",\n\t\t\t\t    wmm_param_ie->qos_info_bitmap & mask);\n\n\t\t\tmemcpy((u8 *) &priv->curr_bss_params.bss_descriptor.\n\t\t\t       wmm_ie, wmm_param_ie,\n\t\t\t       wmm_param_ie->vend_hdr.len + 2);\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tvalid = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tcurr += (tlv_len + sizeof(tlv_hdr->header));\n\t\tresp_len -= (tlv_len + sizeof(tlv_hdr->header));\n\t}\n\n\tmwifiex_wmm_setup_queue_priorities(priv, wmm_param_ie);\n\tmwifiex_wmm_setup_ac_downgrade(priv);\n\n\treturn 0;\n}",
      "code_after_change": "int mwifiex_ret_wmm_get_status(struct mwifiex_private *priv,\n\t\t\t       const struct host_cmd_ds_command *resp)\n{\n\tu8 *curr = (u8 *) &resp->params.get_wmm_status;\n\tuint16_t resp_len = le16_to_cpu(resp->size), tlv_len;\n\tint mask = IEEE80211_WMM_IE_AP_QOSINFO_PARAM_SET_CNT_MASK;\n\tbool valid = true;\n\n\tstruct mwifiex_ie_types_data *tlv_hdr;\n\tstruct mwifiex_ie_types_wmm_queue_status *tlv_wmm_qstatus;\n\tstruct ieee_types_wmm_parameter *wmm_param_ie = NULL;\n\tstruct mwifiex_wmm_ac_status *ac_status;\n\n\tmwifiex_dbg(priv->adapter, INFO,\n\t\t    \"info: WMM: WMM_GET_STATUS cmdresp received: %d\\n\",\n\t\t    resp_len);\n\n\twhile ((resp_len >= sizeof(tlv_hdr->header)) && valid) {\n\t\ttlv_hdr = (struct mwifiex_ie_types_data *) curr;\n\t\ttlv_len = le16_to_cpu(tlv_hdr->header.len);\n\n\t\tif (resp_len < tlv_len + sizeof(tlv_hdr->header))\n\t\t\tbreak;\n\n\t\tswitch (le16_to_cpu(tlv_hdr->header.type)) {\n\t\tcase TLV_TYPE_WMMQSTATUS:\n\t\t\ttlv_wmm_qstatus =\n\t\t\t\t(struct mwifiex_ie_types_wmm_queue_status *)\n\t\t\t\ttlv_hdr;\n\t\t\tmwifiex_dbg(priv->adapter, CMD,\n\t\t\t\t    \"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\t\t\t\t    \"QSTATUS TLV: %d, %d, %d\\n\",\n\t\t\t\t    tlv_wmm_qstatus->queue_index,\n\t\t\t\t    tlv_wmm_qstatus->flow_required,\n\t\t\t\t    tlv_wmm_qstatus->disabled);\n\n\t\t\tac_status = &priv->wmm.ac_status[tlv_wmm_qstatus->\n\t\t\t\t\t\t\t queue_index];\n\t\t\tac_status->disabled = tlv_wmm_qstatus->disabled;\n\t\t\tac_status->flow_required =\n\t\t\t\t\t\ttlv_wmm_qstatus->flow_required;\n\t\t\tac_status->flow_created = tlv_wmm_qstatus->flow_created;\n\t\t\tbreak;\n\n\t\tcase WLAN_EID_VENDOR_SPECIFIC:\n\t\t\t/*\n\t\t\t * Point the regular IEEE IE 2 bytes into the Marvell IE\n\t\t\t *   and setup the IEEE IE type and length byte fields\n\t\t\t */\n\n\t\t\twmm_param_ie =\n\t\t\t\t(struct ieee_types_wmm_parameter *) (curr +\n\t\t\t\t\t\t\t\t    2);\n\t\t\twmm_param_ie->vend_hdr.len = (u8) tlv_len;\n\t\t\twmm_param_ie->vend_hdr.element_id =\n\t\t\t\t\t\tWLAN_EID_VENDOR_SPECIFIC;\n\n\t\t\tmwifiex_dbg(priv->adapter, CMD,\n\t\t\t\t    \"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\t\t\t\t    \"WMM Parameter Set Count: %d\\n\",\n\t\t\t\t    wmm_param_ie->qos_info_bitmap & mask);\n\n\t\t\tif (wmm_param_ie->vend_hdr.len + 2 >\n\t\t\t\tsizeof(struct ieee_types_wmm_parameter))\n\t\t\t\tbreak;\n\n\t\t\tmemcpy((u8 *) &priv->curr_bss_params.bss_descriptor.\n\t\t\t       wmm_ie, wmm_param_ie,\n\t\t\t       wmm_param_ie->vend_hdr.len + 2);\n\n\t\t\tbreak;\n\n\t\tdefault:\n\t\t\tvalid = false;\n\t\t\tbreak;\n\t\t}\n\n\t\tcurr += (tlv_len + sizeof(tlv_hdr->header));\n\t\tresp_len -= (tlv_len + sizeof(tlv_hdr->header));\n\t}\n\n\tmwifiex_wmm_setup_queue_priorities(priv, wmm_param_ie);\n\tmwifiex_wmm_setup_ac_downgrade(priv);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tif (wmm_param_ie->vend_hdr.len + 2 >",
          "\t\t\t\tsizeof(struct ieee_types_wmm_parameter))",
          "\t\t\t\tbreak;",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of size validation for the data being copied in a memcpy operation, allowing potential buffer overflow.",
      "trigger_condition": "The code performs a memcpy operation without verifying that the length of the data being copied does not exceed the size of the destination buffer.",
      "specific_code_behavior_causing_vulnerability": "The vulnerable code snippet lacks a check to ensure that the length of the data being copied in the memcpy operation is within the bounds of the destination buffer. This oversight can lead to a heap-based buffer overflow if the length of the data exceeds the size of the destination buffer.",
      "id": 73,
      "code_after_change_normalized": "int FUN1(struct mwifiex_private *VAR1,\nconst struct host_cmd_ds_command *VAR2)\n{\nVAR4 *VAR3 = (VAR4 *) &VAR2->VAR5.VAR6;\nuint16_t VAR7 = FUN2(VAR2->VAR8), VAR9;\nint VAR10 = VAR11;\nbool VAR12 = true;\nstruct mwifiex_ie_types_data *VAR13;\nstruct mwifiex_ie_types_wmm_queue_status *VAR14;\nstruct ieee_types_wmm_parameter *VAR15 = NULL;\nstruct mwifiex_wmm_ac_status *VAR16;\nFUN3(VAR1->VAR17, VAR18,\n\"STR\",\nVAR7);\nwhile ((VAR7 >= sizeof(VAR13->VAR19)) && VAR12) {\nVAR13 = (struct VAR20 *) VAR3;\nVAR9 = FUN2(VAR13->VAR19.VAR21);\nif (VAR7 < VAR9 + sizeof(VAR13->VAR19))\nbreak;\nswitch (FUN2(VAR13->VAR19.VAR22)) {\ncase VAR23:\nVAR14 =\n(struct VAR24 *)\nVAR13;\nFUN3(VAR1->VAR17, VAR25,\n\"STR\"\n\"STR\",\nVAR14->VAR26,\nVAR14->VAR27,\nVAR14->VAR28);\nVAR16 = &VAR1->VAR29.VAR16[VAR14->\nVAR26];\nVAR16->VAR28 = VAR14->VAR28;\nVAR16->VAR27 =\nVAR14->VAR27;\nVAR16->VAR30 = VAR14->VAR30;\nbreak;\ncase VAR31:\nVAR15 =\n(struct VAR32 *) (VAR3 +\n2);\nVAR15->VAR33.VAR21 = (VAR4) VAR9;\nVAR15->VAR33.VAR34 =\nVAR31;\nFUN3(VAR1->VAR17, VAR25,\n\"STR\"\n\"STR\",\nVAR15->VAR35 & VAR10);\nif (VAR15->VAR33.VAR21 + 2 >\nsizeof(struct VAR32))\nbreak;\nFUN4((VAR4 *) &VAR1->VAR36.VAR37.\nVAR38, VAR15,\nVAR15->VAR33.VAR21 + 2);\nbreak;\ndefault:\nVAR12 = false;\nbreak;\n}\nVAR3 += (VAR9 + sizeof(VAR13->VAR19));\nVAR7 -= (VAR9 + sizeof(VAR13->VAR19));\n}\nFUN5(VAR1, VAR15);\nFUN6(VAR1);\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct mwifiex_private *VAR1,\nconst struct host_cmd_ds_command *VAR2)\n{\nVAR4 *VAR3 = (VAR4 *) &VAR2->VAR5.VAR6;\nuint16_t VAR7 = FUN2(VAR2->VAR8), VAR9;\nint VAR10 = VAR11;\nbool VAR12 = true;\nstruct mwifiex_ie_types_data *VAR13;\nstruct mwifiex_ie_types_wmm_queue_status *VAR14;\nstruct ieee_types_wmm_parameter *VAR15 = NULL;\nstruct mwifiex_wmm_ac_status *VAR16;\nFUN3(VAR1->VAR17, VAR18,\n\"STR\",\nVAR7);\nwhile ((VAR7 >= sizeof(VAR13->VAR19)) && VAR12) {\nVAR13 = (struct VAR20 *) VAR3;\nVAR9 = FUN2(VAR13->VAR19.VAR21);\nif (VAR7 < VAR9 + sizeof(VAR13->VAR19))\nbreak;\nswitch (FUN2(VAR13->VAR19.VAR22)) {\ncase VAR23:\nVAR14 =\n(struct VAR24 *)\nVAR13;\nFUN3(VAR1->VAR17, VAR25,\n\"STR\"\n\"STR\",\nVAR14->VAR26,\nVAR14->VAR27,\nVAR14->VAR28);\nVAR16 = &VAR1->VAR29.VAR16[VAR14->\nVAR26];\nVAR16->VAR28 = VAR14->VAR28;\nVAR16->VAR27 =\nVAR14->VAR27;\nVAR16->VAR30 = VAR14->VAR30;\nbreak;\ncase VAR31:\nVAR15 =\n(struct VAR32 *) (VAR3 +\n2);\nVAR15->VAR33.VAR21 = (VAR4) VAR9;\nVAR15->VAR33.VAR34 =\nVAR31;\nFUN3(VAR1->VAR17, VAR25,\n\"STR\"\n\"STR\",\nVAR15->VAR35 & VAR10);\nFUN4((VAR4 *) &VAR1->VAR36.VAR37.\nVAR38, VAR15,\nVAR15->VAR33.VAR21 + 2);\nbreak;\ndefault:\nVAR12 = false;\nbreak;\n}\nVAR3 += (VAR9 + sizeof(VAR13->VAR19));\nVAR7 -= (VAR9 + sizeof(VAR13->VAR19));\n}\nFUN5(VAR1, VAR15);\nFUN6(VAR1);\nreturn 0;\n}\n",
      "code_after_change_raw": "int mwifiex_ret_wmm_get_status(struct mwifiex_private *priv,\nconst struct host_cmd_ds_command *resp)\n{\nu8 *curr = (u8 *) &resp->params.get_wmm_status;\nuint16_t resp_len = le16_to_cpu(resp->size), tlv_len;\nint mask = IEEE80211_WMM_IE_AP_QOSINFO_PARAM_SET_CNT_MASK;\nbool valid = true;\nstruct mwifiex_ie_types_data *tlv_hdr;\nstruct mwifiex_ie_types_wmm_queue_status *tlv_wmm_qstatus;\nstruct ieee_types_wmm_parameter *wmm_param_ie = NULL;\nstruct mwifiex_wmm_ac_status *ac_status;\nmwifiex_dbg(priv->adapter, INFO,\n\"info: WMM: WMM_GET_STATUS cmdresp received: %d\\n\",\nresp_len);\nwhile ((resp_len >= sizeof(tlv_hdr->header)) && valid) {\ntlv_hdr = (struct mwifiex_ie_types_data *) curr;\ntlv_len = le16_to_cpu(tlv_hdr->header.len);\nif (resp_len < tlv_len + sizeof(tlv_hdr->header))\nbreak;\nswitch (le16_to_cpu(tlv_hdr->header.type)) {\ncase TLV_TYPE_WMMQSTATUS:\ntlv_wmm_qstatus =\n(struct mwifiex_ie_types_wmm_queue_status *)\ntlv_hdr;\nmwifiex_dbg(priv->adapter, CMD,\n\"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\"QSTATUS TLV: %d, %d, %d\\n\",\ntlv_wmm_qstatus->queue_index,\ntlv_wmm_qstatus->flow_required,\ntlv_wmm_qstatus->disabled);\nac_status = &priv->wmm.ac_status[tlv_wmm_qstatus->\nqueue_index];\nac_status->disabled = tlv_wmm_qstatus->disabled;\nac_status->flow_required =\ntlv_wmm_qstatus->flow_required;\nac_status->flow_created = tlv_wmm_qstatus->flow_created;\nbreak;\ncase WLAN_EID_VENDOR_SPECIFIC:\nwmm_param_ie =\n(struct ieee_types_wmm_parameter *) (curr +\n2);\nwmm_param_ie->vend_hdr.len = (u8) tlv_len;\nwmm_param_ie->vend_hdr.element_id =\nWLAN_EID_VENDOR_SPECIFIC;\nmwifiex_dbg(priv->adapter, CMD,\n\"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\"WMM Parameter Set Count: %d\\n\",\nwmm_param_ie->qos_info_bitmap & mask);\nif (wmm_param_ie->vend_hdr.len + 2 >\nsizeof(struct ieee_types_wmm_parameter))\nbreak;\nmemcpy((u8 *) &priv->curr_bss_params.bss_descriptor.\nwmm_ie, wmm_param_ie,\nwmm_param_ie->vend_hdr.len + 2);\nbreak;\ndefault:\nvalid = false;\nbreak;\n}\ncurr += (tlv_len + sizeof(tlv_hdr->header));\nresp_len -= (tlv_len + sizeof(tlv_hdr->header));\n}\nmwifiex_wmm_setup_queue_priorities(priv, wmm_param_ie);\nmwifiex_wmm_setup_ac_downgrade(priv);\nreturn 0;\n}\n",
      "code_before_change_raw": "int mwifiex_ret_wmm_get_status(struct mwifiex_private *priv,\nconst struct host_cmd_ds_command *resp)\n{\nu8 *curr = (u8 *) &resp->params.get_wmm_status;\nuint16_t resp_len = le16_to_cpu(resp->size), tlv_len;\nint mask = IEEE80211_WMM_IE_AP_QOSINFO_PARAM_SET_CNT_MASK;\nbool valid = true;\nstruct mwifiex_ie_types_data *tlv_hdr;\nstruct mwifiex_ie_types_wmm_queue_status *tlv_wmm_qstatus;\nstruct ieee_types_wmm_parameter *wmm_param_ie = NULL;\nstruct mwifiex_wmm_ac_status *ac_status;\nmwifiex_dbg(priv->adapter, INFO,\n\"info: WMM: WMM_GET_STATUS cmdresp received: %d\\n\",\nresp_len);\nwhile ((resp_len >= sizeof(tlv_hdr->header)) && valid) {\ntlv_hdr = (struct mwifiex_ie_types_data *) curr;\ntlv_len = le16_to_cpu(tlv_hdr->header.len);\nif (resp_len < tlv_len + sizeof(tlv_hdr->header))\nbreak;\nswitch (le16_to_cpu(tlv_hdr->header.type)) {\ncase TLV_TYPE_WMMQSTATUS:\ntlv_wmm_qstatus =\n(struct mwifiex_ie_types_wmm_queue_status *)\ntlv_hdr;\nmwifiex_dbg(priv->adapter, CMD,\n\"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\"QSTATUS TLV: %d, %d, %d\\n\",\ntlv_wmm_qstatus->queue_index,\ntlv_wmm_qstatus->flow_required,\ntlv_wmm_qstatus->disabled);\nac_status = &priv->wmm.ac_status[tlv_wmm_qstatus->\nqueue_index];\nac_status->disabled = tlv_wmm_qstatus->disabled;\nac_status->flow_required =\ntlv_wmm_qstatus->flow_required;\nac_status->flow_created = tlv_wmm_qstatus->flow_created;\nbreak;\ncase WLAN_EID_VENDOR_SPECIFIC:\nwmm_param_ie =\n(struct ieee_types_wmm_parameter *) (curr +\n2);\nwmm_param_ie->vend_hdr.len = (u8) tlv_len;\nwmm_param_ie->vend_hdr.element_id =\nWLAN_EID_VENDOR_SPECIFIC;\nmwifiex_dbg(priv->adapter, CMD,\n\"info: CMD_RESP: WMM_GET_STATUS:\\t\"\n\"WMM Parameter Set Count: %d\\n\",\nwmm_param_ie->qos_info_bitmap & mask);\nmemcpy((u8 *) &priv->curr_bss_params.bss_descriptor.\nwmm_ie, wmm_param_ie,\nwmm_param_ie->vend_hdr.len + 2);\nbreak;\ndefault:\nvalid = false;\nbreak;\n}\ncurr += (tlv_len + sizeof(tlv_hdr->header));\nresp_len -= (tlv_len + sizeof(tlv_hdr->header));\n}\nmwifiex_wmm_setup_queue_priorities(priv, wmm_param_ie);\nmwifiex_wmm_setup_ac_downgrade(priv);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2020-12659": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper headroom validation in the xdp_umem_reg function.",
        "trigger_condition": "A user with the CAP_NET_ADMIN capability provides a headroom value that exceeds a certain limit during the registration process, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the headroom value to ensure it does not exceed a specific limit (chunk_size - XDP_PACKET_HEADROOM), allowing a user to trigger an out-of-bounds write by providing a malicious headroom value.",
        "solution": "To mitigate the vulnerability, it is necessary to add a validation check for the headroom value to ensure it does not exceed the limit of (chunk_size - XDP_PACKET_HEADROOM). By performing this validation, the code prevents the possibility of an out-of-bounds write and enhances the security of the xdp_umem_reg function."
      },
      "GPT_analysis": "The modification made to the code snippet is necessary to fix a vulnerability identified as CVE-2020-12659. The vulnerability involves an out-of-bounds write in the `xdp_umem_reg` function due to a lack of headroom validation. By not properly validating the headroom value, a user with the CAP_NET_ADMIN capability could trigger an out-of-bounds write, potentially leading to a security issue.\n\nThe fix introduced in the modified code snippet adds a validation check for the headroom value to ensure that it does not exceed a certain limit (`chunk_size - XDP_PACKET_HEADROOM`). If the headroom value is greater than or equal to this limit, the function returns an error (`-EINVAL`), preventing the out-of-bounds write vulnerability from being exploited.\n\nTherefore, by adding the headroom validation check in the code, the vulnerability is addressed, and the code is made more secure against potential attacks exploiting this specific issue.",
      "GPT_purpose": "Registering an XDP umem (user memory) region with specified parameters.",
      "GPT_function": "\n1. Registering an XDP umem region.\n2. Validating umem registration parameters.\n3. Setting umem attributes based on registration parameters.\n4. Handling umem registration errors.",
      "CVE_id": "CVE-2020-12659",
      "code_before_change": "static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tunsigned int chunks, chunks_per_page;\n\tu64 addr = mr->addr, size = mr->len;\n\tint size_chk, err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t/* Strictly speaking we could support this, if:\n\t\t * - huge pages, or*\n\t\t * - using an IOMMU, or\n\t\t * - making sure the memory area is consecutive\n\t\t * but for now, we simply say \"computer says no\".\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |\n\t\t\tXDP_UMEM_USES_NEED_WAKEUP))\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks && !is_power_of_2(chunk_size))\n\t\treturn -EINVAL;\n\n\tif (!PAGE_ALIGNED(addr)) {\n\t\t/* Memory area has to be page size aligned. For\n\t\t * simplicity, this might change.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif ((addr + size) < addr)\n\t\treturn -EINVAL;\n\n\tchunks = (unsigned int)div_u64(size, chunk_size);\n\tif (chunks == 0)\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks) {\n\t\tchunks_per_page = PAGE_SIZE / chunk_size;\n\t\tif (chunks < chunks_per_page || chunks % chunks_per_page)\n\t\t\treturn -EINVAL;\n\t}\n\n\tsize_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;\n\tif (size_chk < 0)\n\t\treturn -EINVAL;\n\n\tumem->address = (unsigned long)addr;\n\tumem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n\t\t\t\t\t    : ~((u64)chunk_size - 1);\n\tumem->size = size;\n\tumem->headroom = headroom;\n\tumem->chunk_size_nohr = chunk_size - headroom;\n\tumem->npgs = size / PAGE_SIZE;\n\tumem->pgs = NULL;\n\tumem->user = NULL;\n\tumem->flags = mr->flags;\n\tINIT_LIST_HEAD(&umem->xsk_list);\n\tspin_lock_init(&umem->xsk_list_lock);\n\n\trefcount_set(&umem->users, 1);\n\n\terr = xdp_umem_account_pages(umem);\n\tif (err)\n\t\treturn err;\n\n\terr = xdp_umem_pin_pages(umem);\n\tif (err)\n\t\tgoto out_account;\n\n\tumem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),\n\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!umem->pages) {\n\t\terr = -ENOMEM;\n\t\tgoto out_pin;\n\t}\n\n\terr = xdp_umem_map_pages(umem);\n\tif (!err)\n\t\treturn 0;\n\n\tkvfree(umem->pages);\n\nout_pin:\n\txdp_umem_unpin_pages(umem);\nout_account:\n\txdp_umem_unaccount_pages(umem);\n\treturn err;\n}",
      "code_after_change": "static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\n\tbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\n\tu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\n\tunsigned int chunks, chunks_per_page;\n\tu64 addr = mr->addr, size = mr->len;\n\tint err;\n\n\tif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\n\t\t/* Strictly speaking we could support this, if:\n\t\t * - huge pages, or*\n\t\t * - using an IOMMU, or\n\t\t * - making sure the memory area is consecutive\n\t\t * but for now, we simply say \"computer says no\".\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |\n\t\t\tXDP_UMEM_USES_NEED_WAKEUP))\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks && !is_power_of_2(chunk_size))\n\t\treturn -EINVAL;\n\n\tif (!PAGE_ALIGNED(addr)) {\n\t\t/* Memory area has to be page size aligned. For\n\t\t * simplicity, this might change.\n\t\t */\n\t\treturn -EINVAL;\n\t}\n\n\tif ((addr + size) < addr)\n\t\treturn -EINVAL;\n\n\tchunks = (unsigned int)div_u64(size, chunk_size);\n\tif (chunks == 0)\n\t\treturn -EINVAL;\n\n\tif (!unaligned_chunks) {\n\t\tchunks_per_page = PAGE_SIZE / chunk_size;\n\t\tif (chunks < chunks_per_page || chunks % chunks_per_page)\n\t\t\treturn -EINVAL;\n\t}\n\n\tif (headroom >= chunk_size - XDP_PACKET_HEADROOM)\n\t\treturn -EINVAL;\n\n\tumem->address = (unsigned long)addr;\n\tumem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n\t\t\t\t\t    : ~((u64)chunk_size - 1);\n\tumem->size = size;\n\tumem->headroom = headroom;\n\tumem->chunk_size_nohr = chunk_size - headroom;\n\tumem->npgs = size / PAGE_SIZE;\n\tumem->pgs = NULL;\n\tumem->user = NULL;\n\tumem->flags = mr->flags;\n\tINIT_LIST_HEAD(&umem->xsk_list);\n\tspin_lock_init(&umem->xsk_list_lock);\n\n\trefcount_set(&umem->users, 1);\n\n\terr = xdp_umem_account_pages(umem);\n\tif (err)\n\t\treturn err;\n\n\terr = xdp_umem_pin_pages(umem);\n\tif (err)\n\t\tgoto out_account;\n\n\tumem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),\n\t\t\t       GFP_KERNEL_ACCOUNT);\n\tif (!umem->pages) {\n\t\terr = -ENOMEM;\n\t\tgoto out_pin;\n\t}\n\n\terr = xdp_umem_map_pages(umem);\n\tif (!err)\n\t\treturn 0;\n\n\tkvfree(umem->pages);\n\nout_pin:\n\txdp_umem_unpin_pages(umem);\nout_account:\n\txdp_umem_unaccount_pages(umem);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tint err;",
          "\tif (headroom >= chunk_size - XDP_PACKET_HEADROOM)"
        ],
        "deleted": [
          "\tint size_chk, err;",
          "\tsize_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;",
          "\tif (size_chk < 0)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper headroom validation in the xdp_umem_reg function.",
      "trigger_condition": "A user with the CAP_NET_ADMIN capability provides a headroom value that exceeds a certain limit during the registration process, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the headroom value to ensure it does not exceed a specific limit (chunk_size - XDP_PACKET_HEADROOM), allowing a user to trigger an out-of-bounds write by providing a malicious headroom value.",
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check for the headroom value to ensure it does not exceed the limit of (chunk_size - XDP_PACKET_HEADROOM). By performing this validation, the code prevents the possibility of an out-of-bounds write and enhances the security of the xdp_umem_reg function.",
      "id": 74,
      "code_after_change_normalized": "static int FUN1(struct xdp_umem *VAR1, struct xdp_umem_reg *VAR2)\n{\nbool VAR3 = VAR2->VAR4 & VAR5;\nu32 VAR6 = VAR2->VAR6, VAR7 = VAR2->VAR7;\nunsigned int VAR8, VAR9;\nu64 VAR10 = VAR2->VAR10, VAR11 = VAR2->VAR12;\nint VAR13;\nif (VAR6 < VAR14 || VAR6 > VAR15) {\nreturn -VAR16;\n}\nif (VAR2->VAR4 & ~(VAR5 |\nVAR17))\nreturn -VAR16;\nif (!VAR3 && !FUN2(VAR6))\nreturn -VAR16;\nif (!FUN3(VAR10)) {\nreturn -VAR16;\n}\nif ((VAR10 + VAR11) < VAR10)\nreturn -VAR16;\nVAR8 = (unsigned int)FUN4(VAR11, VAR6);\nif (VAR8 == 0)\nreturn -VAR16;\nif (!VAR3) {\nVAR9 = VAR15 / VAR6;\nif (VAR8 < VAR9 || VAR8 % VAR9)\nreturn -VAR16;\n}\nif (VAR7 >= VAR6 - VAR18)\nreturn -VAR16;\nVAR1->VAR19 = (unsigned long)VAR10;\nVAR1->VAR20 = VAR3 ? VAR21\n: ~((VAR22)VAR6 - 1);\nVAR1->VAR11 = VAR11;\nVAR1->VAR7 = VAR7;\nVAR1->VAR23 = VAR6 - VAR7;\nVAR1->VAR24 = VAR11 / VAR15;\nVAR1->VAR25 = NULL;\nVAR1->VAR26 = NULL;\nVAR1->VAR4 = VAR2->VAR4;\nFUN5(&VAR1->VAR27);\nFUN6(&VAR1->VAR28);\nFUN7(&VAR1->VAR29, 1);\nVAR13 = FUN8(VAR1);\nif (VAR13)\nreturn VAR13;\nVAR13 = FUN9(VAR1);\nif (VAR13)\ngoto VAR30;\nVAR1->VAR31 = FUN10(VAR1->VAR24, sizeof(*VAR1->VAR31),\nVAR32);\nif (!VAR1->VAR31) {\nVAR13 = -VAR33;\ngoto VAR34;\n}\nVAR13 = FUN11(VAR1);\nif (!VAR13)\nreturn 0;\nFUN12(VAR1->VAR31);\nVAR34:\nFUN13(VAR1);\nVAR30:\nFUN14(VAR1);\nreturn VAR13;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct xdp_umem *VAR1, struct xdp_umem_reg *VAR2)\n{\nbool VAR3 = VAR2->VAR4 & VAR5;\nu32 VAR6 = VAR2->VAR6, VAR7 = VAR2->VAR7;\nunsigned int VAR8, VAR9;\nu64 VAR10 = VAR2->VAR10, VAR11 = VAR2->VAR12;\nint VAR13, VAR14;\nif (VAR6 < VAR15 || VAR6 > VAR16) {\nreturn -VAR17;\n}\nif (VAR2->VAR4 & ~(VAR5 |\nVAR18))\nreturn -VAR17;\nif (!VAR3 && !FUN2(VAR6))\nreturn -VAR17;\nif (!FUN3(VAR10)) {\nreturn -VAR17;\n}\nif ((VAR10 + VAR11) < VAR10)\nreturn -VAR17;\nVAR8 = (unsigned int)FUN4(VAR11, VAR6);\nif (VAR8 == 0)\nreturn -VAR17;\nif (!VAR3) {\nVAR9 = VAR16 / VAR6;\nif (VAR8 < VAR9 || VAR8 % VAR9)\nreturn -VAR17;\n}\nVAR13 = VAR6 - VAR7 - VAR19;\nif (VAR13 < 0)\nreturn -VAR17;\nVAR1->VAR20 = (unsigned long)VAR10;\nVAR1->VAR21 = VAR3 ? VAR22\n: ~((VAR23)VAR6 - 1);\nVAR1->VAR11 = VAR11;\nVAR1->VAR7 = VAR7;\nVAR1->VAR24 = VAR6 - VAR7;\nVAR1->VAR25 = VAR11 / VAR16;\nVAR1->VAR26 = NULL;\nVAR1->VAR27 = NULL;\nVAR1->VAR4 = VAR2->VAR4;\nFUN5(&VAR1->VAR28);\nFUN6(&VAR1->VAR29);\nFUN7(&VAR1->VAR30, 1);\nVAR14 = FUN8(VAR1);\nif (VAR14)\nreturn VAR14;\nVAR14 = FUN9(VAR1);\nif (VAR14)\ngoto VAR31;\nVAR1->VAR32 = FUN10(VAR1->VAR25, sizeof(*VAR1->VAR32),\nVAR33);\nif (!VAR1->VAR32) {\nVAR14 = -VAR34;\ngoto VAR35;\n}\nVAR14 = FUN11(VAR1);\nif (!VAR14)\nreturn 0;\nFUN12(VAR1->VAR32);\nVAR35:\nFUN13(VAR1);\nVAR31:\nFUN14(VAR1);\nreturn VAR14;\n}\n",
      "code_after_change_raw": "static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\nbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\nu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\nunsigned int chunks, chunks_per_page;\nu64 addr = mr->addr, size = mr->len;\nint err;\nif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\nreturn -EINVAL;\n}\nif (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |\nXDP_UMEM_USES_NEED_WAKEUP))\nreturn -EINVAL;\nif (!unaligned_chunks && !is_power_of_2(chunk_size))\nreturn -EINVAL;\nif (!PAGE_ALIGNED(addr)) {\nreturn -EINVAL;\n}\nif ((addr + size) < addr)\nreturn -EINVAL;\nchunks = (unsigned int)div_u64(size, chunk_size);\nif (chunks == 0)\nreturn -EINVAL;\nif (!unaligned_chunks) {\nchunks_per_page = PAGE_SIZE / chunk_size;\nif (chunks < chunks_per_page || chunks % chunks_per_page)\nreturn -EINVAL;\n}\nif (headroom >= chunk_size - XDP_PACKET_HEADROOM)\nreturn -EINVAL;\numem->address = (unsigned long)addr;\numem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n: ~((u64)chunk_size - 1);\numem->size = size;\numem->headroom = headroom;\numem->chunk_size_nohr = chunk_size - headroom;\numem->npgs = size / PAGE_SIZE;\numem->pgs = NULL;\numem->user = NULL;\numem->flags = mr->flags;\nINIT_LIST_HEAD(&umem->xsk_list);\nspin_lock_init(&umem->xsk_list_lock);\nrefcount_set(&umem->users, 1);\nerr = xdp_umem_account_pages(umem);\nif (err)\nreturn err;\nerr = xdp_umem_pin_pages(umem);\nif (err)\ngoto out_account;\numem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),\nGFP_KERNEL_ACCOUNT);\nif (!umem->pages) {\nerr = -ENOMEM;\ngoto out_pin;\n}\nerr = xdp_umem_map_pages(umem);\nif (!err)\nreturn 0;\nkvfree(umem->pages);\nout_pin:\nxdp_umem_unpin_pages(umem);\nout_account:\nxdp_umem_unaccount_pages(umem);\nreturn err;\n}\n",
      "code_before_change_raw": "static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)\n{\nbool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;\nu32 chunk_size = mr->chunk_size, headroom = mr->headroom;\nunsigned int chunks, chunks_per_page;\nu64 addr = mr->addr, size = mr->len;\nint size_chk, err;\nif (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {\nreturn -EINVAL;\n}\nif (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |\nXDP_UMEM_USES_NEED_WAKEUP))\nreturn -EINVAL;\nif (!unaligned_chunks && !is_power_of_2(chunk_size))\nreturn -EINVAL;\nif (!PAGE_ALIGNED(addr)) {\nreturn -EINVAL;\n}\nif ((addr + size) < addr)\nreturn -EINVAL;\nchunks = (unsigned int)div_u64(size, chunk_size);\nif (chunks == 0)\nreturn -EINVAL;\nif (!unaligned_chunks) {\nchunks_per_page = PAGE_SIZE / chunk_size;\nif (chunks < chunks_per_page || chunks % chunks_per_page)\nreturn -EINVAL;\n}\nsize_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;\nif (size_chk < 0)\nreturn -EINVAL;\numem->address = (unsigned long)addr;\numem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK\n: ~((u64)chunk_size - 1);\numem->size = size;\numem->headroom = headroom;\numem->chunk_size_nohr = chunk_size - headroom;\numem->npgs = size / PAGE_SIZE;\numem->pgs = NULL;\numem->user = NULL;\numem->flags = mr->flags;\nINIT_LIST_HEAD(&umem->xsk_list);\nspin_lock_init(&umem->xsk_list_lock);\nrefcount_set(&umem->users, 1);\nerr = xdp_umem_account_pages(umem);\nif (err)\nreturn err;\nerr = xdp_umem_pin_pages(umem);\nif (err)\ngoto out_account;\numem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),\nGFP_KERNEL_ACCOUNT);\nif (!umem->pages) {\nerr = -ENOMEM;\ngoto out_pin;\n}\nerr = xdp_umem_map_pages(umem);\nif (!err)\nreturn 0;\nkvfree(umem->pages);\nout_pin:\nxdp_umem_unpin_pages(umem);\nout_account:\nxdp_umem_unaccount_pages(umem);\nreturn err;\n}\n"
    }
  ],
  "CVE-2020-14305": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Incorrect initialization of nf_ct_helper structures with an incorrect size argument.",
        "trigger_condition": "An unauthenticated remote user sends malicious input to the system, triggering the out-of-bounds memory write flaw.",
        "specific_code_behavior_causing_vulnerability": "The code initializes nf_ct_helper structures for IPv4 and IPv6 FTP connections with an incorrect size argument, leading to potential memory corruption and security risks."
      },
      "solution": "To mitigate the vulnerability, it is necessary to modify the nf_ct_helper_init calls by removing the incorrect size argument and adding the 'help' parameter in its place. This ensures that the nf_ct_helper structures are initialized correctly and prevents potential memory corruption issues. By making this modification, the code is updated to address the vulnerability and improve the security of the system.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-14305, which involves an out-of-bounds memory write flaw in the Linux kernel's Voice Over IP H.323 connection tracking functionality. The vulnerability specifically affects connections on IPv6 port 1720 and allows an unauthenticated remote user to crash the system, leading to a denial of service.\n\nIn the provided code snippet, the vulnerability is related to the initialization of the nf_ct_helper structures for IPv4 and IPv6 FTP connections. The original code initializes these structures with a size argument that is not correct, leading to potential memory corruption and security risks.\n\nThe correct way to fix this vulnerability is to modify the nf_ct_helper_init calls by removing the incorrect size argument and adding the 'help' parameter in its place. This change ensures that the nf_ct_helper structures are initialized correctly and mitigates the risk of out-of-bounds memory writes.\n\nBy making this modification, the code is updated to address the vulnerability and improve the security of the system by preventing potential memory corruption issues.",
      "GPT_purpose": "Initialize and register FTP connection tracking helpers for both IPv4 and IPv6 connections.",
      "GPT_function": "\n1. Initialize nf_conntrack_ftp module.\n2. Allocate memory for ftp_buffer.\n3. Initialize nf_conntrack helpers for FTP connections on IPv4 and IPv6.\n4. Register nf_conntrack helpers for FTP connections.",
      "CVE_id": "CVE-2020-14305",
      "code_before_change": "static int __init nf_conntrack_ftp_init(void)\n{\n\tint i, ret = 0;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_ftp_master));\n\n\tftp_buffer = kmalloc(65536, GFP_KERNEL);\n\tif (!ftp_buffer)\n\t\treturn -ENOMEM;\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = FTP_PORT;\n\n\t/* FIXME should be configurable whether IPv4 and IPv6 FTP connections\n\t\t are tracked or not - YK */\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&ftp[2 * i], AF_INET, IPPROTO_TCP, \"ftp\",\n\t\t\t\t  FTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n\t\t\t\t  0, sizeof(struct nf_ct_ftp_master), help,\n\t\t\t\t  nf_ct_ftp_from_nlattr, THIS_MODULE);\n\t\tnf_ct_helper_init(&ftp[2 * i + 1], AF_INET6, IPPROTO_TCP, \"ftp\",\n\t\t\t\t  FTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n\t\t\t\t  0, sizeof(struct nf_ct_ftp_master), help,\n\t\t\t\t  nf_ct_ftp_from_nlattr, THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(ftp, ports_c * 2);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\tkfree(ftp_buffer);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}",
      "code_after_change": "static int __init nf_conntrack_ftp_init(void)\n{\n\tint i, ret = 0;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_ftp_master));\n\n\tftp_buffer = kmalloc(65536, GFP_KERNEL);\n\tif (!ftp_buffer)\n\t\treturn -ENOMEM;\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = FTP_PORT;\n\n\t/* FIXME should be configurable whether IPv4 and IPv6 FTP connections\n\t\t are tracked or not - YK */\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&ftp[2 * i], AF_INET, IPPROTO_TCP, \"ftp\",\n\t\t\t\t  FTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n\t\t\t\t  0, help, nf_ct_ftp_from_nlattr, THIS_MODULE);\n\t\tnf_ct_helper_init(&ftp[2 * i + 1], AF_INET6, IPPROTO_TCP, \"ftp\",\n\t\t\t\t  FTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n\t\t\t\t  0, help, nf_ct_ftp_from_nlattr, THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(ftp, ports_c * 2);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\tkfree(ftp_buffer);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t  0, help, nf_ct_ftp_from_nlattr, THIS_MODULE);",
          "\t\t\t\t  0, help, nf_ct_ftp_from_nlattr, THIS_MODULE);"
        ],
        "deleted": [
          "\t\t\t\t  0, sizeof(struct nf_ct_ftp_master), help,",
          "\t\t\t\t  nf_ct_ftp_from_nlattr, THIS_MODULE);",
          "\t\t\t\t  0, sizeof(struct nf_ct_ftp_master), help,",
          "\t\t\t\t  nf_ct_ftp_from_nlattr, THIS_MODULE);"
        ]
      },
      "preconditions_for_vulnerability": "Incorrect initialization of nf_ct_helper structures with an incorrect size argument.",
      "trigger_condition": "An unauthenticated remote user sends malicious input to the system, triggering the out-of-bounds memory write flaw.",
      "specific_code_behavior_causing_vulnerability": "The code initializes nf_ct_helper structures for IPv4 and IPv6 FTP connections with an incorrect size argument, leading to potential memory corruption and security risks.",
      "id": 75,
      "code_after_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2 = 0;\nFUN2(sizeof(struct VAR3));\nVAR4 = FUN3(65536, VAR5);\nif (!VAR4)\nreturn -VAR6;\nif (VAR7 == 0)\nVAR8[VAR7++] = VAR9;\nfor (VAR1 = 0; VAR1 < VAR7; VAR1++) {\nFUN4(&VAR10[2 * VAR1], VAR11, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1], &VAR13,\n0, VAR14, VAR15, VAR16);\nFUN4(&VAR10[2 * VAR1 + 1], VAR17, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1], &VAR13,\n0, VAR14, VAR15, VAR16);\n}\nVAR2 = FUN5(VAR10, VAR7 * 2);\nif (VAR2 < 0) {\nFUN6(\"STR\");\nFUN7(VAR4);\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2 = 0;\nFUN2(sizeof(struct VAR3));\nVAR4 = FUN3(65536, VAR5);\nif (!VAR4)\nreturn -VAR6;\nif (VAR7 == 0)\nVAR8[VAR7++] = VAR9;\nfor (VAR1 = 0; VAR1 < VAR7; VAR1++) {\nFUN4(&VAR10[2 * VAR1], VAR11, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1], &VAR13,\n0, sizeof(struct VAR3), VAR14,\nVAR15, VAR16);\nFUN4(&VAR10[2 * VAR1 + 1], VAR17, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1], &VAR13,\n0, sizeof(struct VAR3), VAR14,\nVAR15, VAR16);\n}\nVAR2 = FUN5(VAR10, VAR7 * 2);\nif (VAR2 < 0) {\nFUN6(\"STR\");\nFUN7(VAR4);\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int __init nf_conntrack_ftp_init(void)\n{\nint i, ret = 0;\nNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_ftp_master));\nftp_buffer = kmalloc(65536, GFP_KERNEL);\nif (!ftp_buffer)\nreturn -ENOMEM;\nif (ports_c == 0)\nports[ports_c++] = FTP_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&ftp[2 * i], AF_INET, IPPROTO_TCP, \"ftp\",\nFTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n0, help, nf_ct_ftp_from_nlattr, THIS_MODULE);\nnf_ct_helper_init(&ftp[2 * i + 1], AF_INET6, IPPROTO_TCP, \"ftp\",\nFTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n0, help, nf_ct_ftp_from_nlattr, THIS_MODULE);\n}\nret = nf_conntrack_helpers_register(ftp, ports_c * 2);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nkfree(ftp_buffer);\nreturn ret;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int __init nf_conntrack_ftp_init(void)\n{\nint i, ret = 0;\nNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_ftp_master));\nftp_buffer = kmalloc(65536, GFP_KERNEL);\nif (!ftp_buffer)\nreturn -ENOMEM;\nif (ports_c == 0)\nports[ports_c++] = FTP_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&ftp[2 * i], AF_INET, IPPROTO_TCP, \"ftp\",\nFTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n0, sizeof(struct nf_ct_ftp_master), help,\nnf_ct_ftp_from_nlattr, THIS_MODULE);\nnf_ct_helper_init(&ftp[2 * i + 1], AF_INET6, IPPROTO_TCP, \"ftp\",\nFTP_PORT, ports[i], ports[i], &ftp_exp_policy,\n0, sizeof(struct nf_ct_ftp_master), help,\nnf_ct_ftp_from_nlattr, THIS_MODULE);\n}\nret = nf_conntrack_helpers_register(ftp, ports_c * 2);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nkfree(ftp_buffer);\nreturn ret;\n}\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper size validation in the nf_ct_helper_init function calls for IPv4 and IPv6 connections.",
        "trigger_condition": "An unauthenticated remote user exploits the out-of-bounds memory write flaw in the handling of connections on IPv6 port 1720, causing a denial of service.",
        "specific_code_behavior_causing_vulnerability": "The code performs an out-of-bounds memory write operation by passing sizeof(struct nf_ct_sane_master) as the size argument in the nf_ct_helper_init calls for both IPv4 and IPv6 connections, leading to a vulnerability that can be exploited remotely."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the size argument from the nf_ct_helper_init calls for both IPv4 and IPv6 connections. This prevents the out-of-bounds memory write operation and addresses the vulnerability by ensuring proper memory access and avoiding potential denial of service attacks.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-14305, which involves an out-of-bounds memory write flaw in the Linux kernel's Voice Over IP H.323 connection tracking functionality on IPv6 port 1720. \n\nIn the original code snippet, the nf_ct_helper_init function is called with a size argument of sizeof(struct nf_ct_sane_master) for both IPv4 and IPv6 connections. However, the vulnerability allows an unauthenticated remote user to crash the system by exploiting the handling of connections on IPv6 port 1720.\n\nTo fix this vulnerability, the modification removes the size argument from the nf_ct_helper_init calls for both IPv4 and IPv6 connections. By doing so, the code no longer performs an out-of-bounds memory write operation, thus preventing the potential denial of service and maintaining system availability, integrity, and confidentiality.",
      "GPT_purpose": "Initialize a netfilter connection tracking helper for the SANE protocol on specified ports for both IPv4 and IPv6, potentially leading to a denial of service vulnerability.",
      "GPT_function": "\n1. Initialize nf_conntrack_sane module.\n2. Allocate memory for sane_buffer.\n3. Initialize nf_conntrack helpers for IPv4 and IPv6 connections on specified ports.\n4. Register nf_conntrack helpers.\n5. Handle error cases.",
      "CVE_id": "CVE-2020-14305",
      "code_before_change": "static int __init nf_conntrack_sane_init(void)\n{\n\tint i, ret = 0;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sane_master));\n\n\tsane_buffer = kmalloc(65536, GFP_KERNEL);\n\tif (!sane_buffer)\n\t\treturn -ENOMEM;\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = SANE_PORT;\n\n\t/* FIXME should be configurable whether IPv4 and IPv6 connections\n\t\t are tracked or not - YK */\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&sane[2 * i], AF_INET, IPPROTO_TCP, \"sane\",\n\t\t\t\t  SANE_PORT, ports[i], ports[i],\n\t\t\t\t  &sane_exp_policy, 0,\n\t\t\t\t  sizeof(struct nf_ct_sane_master), help, NULL,\n\t\t\t\t  THIS_MODULE);\n\t\tnf_ct_helper_init(&sane[2 * i + 1], AF_INET6, IPPROTO_TCP, \"sane\",\n\t\t\t\t  SANE_PORT, ports[i], ports[i],\n\t\t\t\t  &sane_exp_policy, 0,\n\t\t\t\t  sizeof(struct nf_ct_sane_master), help, NULL,\n\t\t\t\t  THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(sane, ports_c * 2);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\tkfree(sane_buffer);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}",
      "code_after_change": "static int __init nf_conntrack_sane_init(void)\n{\n\tint i, ret = 0;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sane_master));\n\n\tsane_buffer = kmalloc(65536, GFP_KERNEL);\n\tif (!sane_buffer)\n\t\treturn -ENOMEM;\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = SANE_PORT;\n\n\t/* FIXME should be configurable whether IPv4 and IPv6 connections\n\t\t are tracked or not - YK */\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&sane[2 * i], AF_INET, IPPROTO_TCP, \"sane\",\n\t\t\t\t  SANE_PORT, ports[i], ports[i],\n\t\t\t\t  &sane_exp_policy, 0, help, NULL,\n\t\t\t\t  THIS_MODULE);\n\t\tnf_ct_helper_init(&sane[2 * i + 1], AF_INET6, IPPROTO_TCP, \"sane\",\n\t\t\t\t  SANE_PORT, ports[i], ports[i],\n\t\t\t\t  &sane_exp_policy, 0, help, NULL,\n\t\t\t\t  THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(sane, ports_c * 2);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\tkfree(sane_buffer);\n\t\treturn ret;\n\t}\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t  &sane_exp_policy, 0, help, NULL,",
          "\t\t\t\t  &sane_exp_policy, 0, help, NULL,"
        ],
        "deleted": [
          "\t\t\t\t  &sane_exp_policy, 0,",
          "\t\t\t\t  sizeof(struct nf_ct_sane_master), help, NULL,",
          "\t\t\t\t  &sane_exp_policy, 0,",
          "\t\t\t\t  sizeof(struct nf_ct_sane_master), help, NULL,"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper size validation in the nf_ct_helper_init function calls for IPv4 and IPv6 connections.",
      "trigger_condition": "An unauthenticated remote user exploits the out-of-bounds memory write flaw in the handling of connections on IPv6 port 1720, causing a denial of service.",
      "specific_code_behavior_causing_vulnerability": "The code performs an out-of-bounds memory write operation by passing sizeof(struct nf_ct_sane_master) as the size argument in the nf_ct_helper_init calls for both IPv4 and IPv6 connections, leading to a vulnerability that can be exploited remotely.",
      "id": 76,
      "code_after_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2 = 0;\nFUN2(sizeof(struct VAR3));\nVAR4 = FUN3(65536, VAR5);\nif (!VAR4)\nreturn -VAR6;\nif (VAR7 == 0)\nVAR8[VAR7++] = VAR9;\nfor (VAR1 = 0; VAR1 < VAR7; VAR1++) {\nFUN4(&VAR10[2 * VAR1], VAR11, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1],\n&VAR13, 0, VAR14, NULL,\nVAR15);\nFUN4(&VAR10[2 * VAR1 + 1], VAR16, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1],\n&VAR13, 0, VAR14, NULL,\nVAR15);\n}\nVAR2 = FUN5(VAR10, VAR7 * 2);\nif (VAR2 < 0) {\nFUN6(\"STR\");\nFUN7(VAR4);\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2 = 0;\nFUN2(sizeof(struct VAR3));\nVAR4 = FUN3(65536, VAR5);\nif (!VAR4)\nreturn -VAR6;\nif (VAR7 == 0)\nVAR8[VAR7++] = VAR9;\nfor (VAR1 = 0; VAR1 < VAR7; VAR1++) {\nFUN4(&VAR10[2 * VAR1], VAR11, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1],\n&VAR13, 0,\nsizeof(struct VAR3), VAR14, NULL,\nVAR15);\nFUN4(&VAR10[2 * VAR1 + 1], VAR16, VAR12, \"STR\",\nVAR9, VAR8[VAR1], VAR8[VAR1],\n&VAR13, 0,\nsizeof(struct VAR3), VAR14, NULL,\nVAR15);\n}\nVAR2 = FUN5(VAR10, VAR7 * 2);\nif (VAR2 < 0) {\nFUN6(\"STR\");\nFUN7(VAR4);\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int __init nf_conntrack_sane_init(void)\n{\nint i, ret = 0;\nNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sane_master));\nsane_buffer = kmalloc(65536, GFP_KERNEL);\nif (!sane_buffer)\nreturn -ENOMEM;\nif (ports_c == 0)\nports[ports_c++] = SANE_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&sane[2 * i], AF_INET, IPPROTO_TCP, \"sane\",\nSANE_PORT, ports[i], ports[i],\n&sane_exp_policy, 0, help, NULL,\nTHIS_MODULE);\nnf_ct_helper_init(&sane[2 * i + 1], AF_INET6, IPPROTO_TCP, \"sane\",\nSANE_PORT, ports[i], ports[i],\n&sane_exp_policy, 0, help, NULL,\nTHIS_MODULE);\n}\nret = nf_conntrack_helpers_register(sane, ports_c * 2);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nkfree(sane_buffer);\nreturn ret;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int __init nf_conntrack_sane_init(void)\n{\nint i, ret = 0;\nNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sane_master));\nsane_buffer = kmalloc(65536, GFP_KERNEL);\nif (!sane_buffer)\nreturn -ENOMEM;\nif (ports_c == 0)\nports[ports_c++] = SANE_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&sane[2 * i], AF_INET, IPPROTO_TCP, \"sane\",\nSANE_PORT, ports[i], ports[i],\n&sane_exp_policy, 0,\nsizeof(struct nf_ct_sane_master), help, NULL,\nTHIS_MODULE);\nnf_ct_helper_init(&sane[2 * i + 1], AF_INET6, IPPROTO_TCP, \"sane\",\nSANE_PORT, ports[i], ports[i],\n&sane_exp_policy, 0,\nsizeof(struct nf_ct_sane_master), help, NULL,\nTHIS_MODULE);\n}\nret = nf_conntrack_helpers_register(sane, ports_c * 2);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nkfree(sane_buffer);\nreturn ret;\n}\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Incorrect handling of memory allocation size in function calls.",
        "trigger_condition": "Passing the incorrect size of a structure in memory allocation functions leads to out-of-bounds memory write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code passes the size of a structure incorrectly in memory allocation functions, causing out-of-bounds memory writes and potential system crashes."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the incorrect size argument from the memory allocation function calls. This ensures that the memory allocation is handled correctly and prevents out-of-bounds memory writes. In this case, the solution involves modifying the function calls to exclude the size argument, allowing the functions to allocate memory properly without causing vulnerabilities.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2020-14305, which involves an out-of-bounds memory write flaw in the Linux kernel's Voice Over IP H.323 connection tracking functionality. \n\nThe vulnerability occurs due to the incorrect handling of connections on ipv6 port 1720, which allows an unauthenticated remote user to crash the system, leading to a denial of service. The highest threat from this vulnerability is to confidentiality, integrity, and system availability.\n\nThe modification involves removing the `sizeof(struct nf_ct_sip_master)` argument from the `nf_ct_helper_init` calls. This change is necessary because passing the incorrect size of the structure can lead to out-of-bounds memory writes, which is the root cause of the vulnerability. By removing this argument, the function calls will correctly handle the memory allocation and prevent the out-of-bounds write issue, thus fixing the vulnerability.",
      "GPT_purpose": "Initialize SIP connection tracking helpers for both IPv4 and IPv6 protocols on specified ports.",
      "GPT_function": "\n1. Initialize nf_conntrack SIP helper for UDP and TCP protocols on IPv4 and IPv6.\n2. Register the SIP helpers for connection tracking.\n3. Handle SIP connections on specified ports.",
      "CVE_id": "CVE-2020-14305",
      "code_before_change": "static int __init nf_conntrack_sip_init(void)\n{\n\tint i, ret;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sip_master));\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = SIP_PORT;\n\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&sip[4 * i], AF_INET, IPPROTO_UDP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX,\n\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_udp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&sip[4 * i + 1], AF_INET, IPPROTO_TCP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX,\n\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_tcp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&sip[4 * i + 2], AF_INET6, IPPROTO_UDP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX,\n\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_udp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&sip[4 * i + 3], AF_INET6, IPPROTO_TCP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX,\n\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_tcp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(sip, ports_c * 4);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\treturn ret;\n\t}\n\treturn 0;\n}",
      "code_after_change": "static int __init nf_conntrack_sip_init(void)\n{\n\tint i, ret;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sip_master));\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = SIP_PORT;\n\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&sip[4 * i], AF_INET, IPPROTO_UDP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX, sip_help_udp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&sip[4 * i + 1], AF_INET, IPPROTO_TCP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX, sip_help_tcp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&sip[4 * i + 2], AF_INET6, IPPROTO_UDP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX, sip_help_udp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&sip[4 * i + 3], AF_INET6, IPPROTO_TCP, \"sip\",\n\t\t\t\t  SIP_PORT, ports[i], i, sip_exp_policy,\n\t\t\t\t  SIP_EXPECT_MAX, sip_help_tcp,\n\t\t\t\t  NULL, THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(sip, ports_c * 4);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\treturn ret;\n\t}\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t  SIP_EXPECT_MAX, sip_help_udp,",
          "\t\t\t\t  SIP_EXPECT_MAX, sip_help_tcp,",
          "\t\t\t\t  SIP_EXPECT_MAX, sip_help_udp,",
          "\t\t\t\t  SIP_EXPECT_MAX, sip_help_tcp,"
        ],
        "deleted": [
          "\t\t\t\t  SIP_EXPECT_MAX,",
          "\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_udp,",
          "\t\t\t\t  SIP_EXPECT_MAX,",
          "\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_tcp,",
          "\t\t\t\t  SIP_EXPECT_MAX,",
          "\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_udp,",
          "\t\t\t\t  SIP_EXPECT_MAX,",
          "\t\t\t\t  sizeof(struct nf_ct_sip_master), sip_help_tcp,"
        ]
      },
      "preconditions_for_vulnerability": "Incorrect handling of memory allocation size in function calls.",
      "trigger_condition": "Passing the incorrect size of a structure in memory allocation functions leads to out-of-bounds memory write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code passes the size of a structure incorrectly in memory allocation functions, causing out-of-bounds memory writes and potential system crashes.",
      "id": 77,
      "code_after_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2;\nFUN2(sizeof(struct VAR3));\nif (VAR4 == 0)\nVAR5[VAR4++] = VAR6;\nfor (VAR1 = 0; VAR1 < VAR4; VAR1++) {\nFUN3(&VAR7[4 * VAR1], VAR8, VAR9, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11, VAR12,\nNULL, VAR13);\nFUN3(&VAR7[4 * VAR1 + 1], VAR8, VAR14, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11, VAR15,\nNULL, VAR13);\nFUN3(&VAR7[4 * VAR1 + 2], VAR16, VAR9, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11, VAR12,\nNULL, VAR13);\nFUN3(&VAR7[4 * VAR1 + 3], VAR16, VAR14, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11, VAR15,\nNULL, VAR13);\n}\nVAR2 = FUN4(VAR7, VAR4 * 4);\nif (VAR2 < 0) {\nFUN5(\"STR\");\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2;\nFUN2(sizeof(struct VAR3));\nif (VAR4 == 0)\nVAR5[VAR4++] = VAR6;\nfor (VAR1 = 0; VAR1 < VAR4; VAR1++) {\nFUN3(&VAR7[4 * VAR1], VAR8, VAR9, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11,\nsizeof(struct VAR3), VAR12,\nNULL, VAR13);\nFUN3(&VAR7[4 * VAR1 + 1], VAR8, VAR14, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11,\nsizeof(struct VAR3), VAR15,\nNULL, VAR13);\nFUN3(&VAR7[4 * VAR1 + 2], VAR16, VAR9, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11,\nsizeof(struct VAR3), VAR12,\nNULL, VAR13);\nFUN3(&VAR7[4 * VAR1 + 3], VAR16, VAR14, \"STR\",\nVAR6, VAR5[VAR1], VAR1, VAR10,\nVAR11,\nsizeof(struct VAR3), VAR15,\nNULL, VAR13);\n}\nVAR2 = FUN4(VAR7, VAR4 * 4);\nif (VAR2 < 0) {\nFUN5(\"STR\");\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int __init nf_conntrack_sip_init(void)\n{\nint i, ret;\nNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sip_master));\nif (ports_c == 0)\nports[ports_c++] = SIP_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&sip[4 * i], AF_INET, IPPROTO_UDP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX, sip_help_udp,\nNULL, THIS_MODULE);\nnf_ct_helper_init(&sip[4 * i + 1], AF_INET, IPPROTO_TCP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX, sip_help_tcp,\nNULL, THIS_MODULE);\nnf_ct_helper_init(&sip[4 * i + 2], AF_INET6, IPPROTO_UDP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX, sip_help_udp,\nNULL, THIS_MODULE);\nnf_ct_helper_init(&sip[4 * i + 3], AF_INET6, IPPROTO_TCP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX, sip_help_tcp,\nNULL, THIS_MODULE);\n}\nret = nf_conntrack_helpers_register(sip, ports_c * 4);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nreturn ret;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int __init nf_conntrack_sip_init(void)\n{\nint i, ret;\nNF_CT_HELPER_BUILD_BUG_ON(sizeof(struct nf_ct_sip_master));\nif (ports_c == 0)\nports[ports_c++] = SIP_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&sip[4 * i], AF_INET, IPPROTO_UDP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX,\nsizeof(struct nf_ct_sip_master), sip_help_udp,\nNULL, THIS_MODULE);\nnf_ct_helper_init(&sip[4 * i + 1], AF_INET, IPPROTO_TCP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX,\nsizeof(struct nf_ct_sip_master), sip_help_tcp,\nNULL, THIS_MODULE);\nnf_ct_helper_init(&sip[4 * i + 2], AF_INET6, IPPROTO_UDP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX,\nsizeof(struct nf_ct_sip_master), sip_help_udp,\nNULL, THIS_MODULE);\nnf_ct_helper_init(&sip[4 * i + 3], AF_INET6, IPPROTO_TCP, \"sip\",\nSIP_PORT, ports[i], i, sip_exp_policy,\nSIP_EXPECT_MAX,\nsizeof(struct nf_ct_sip_master), sip_help_tcp,\nNULL, THIS_MODULE);\n}\nret = nf_conntrack_helpers_register(sip, ports_c * 4);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nreturn ret;\n}\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper parameter handling in the nf_ct_helper_init function calls.",
        "trigger_condition": "The unnecessary parameters '0, 0' in the nf_ct_helper_init function calls lead to out-of-bounds memory writes.",
        "specific_code_behavior_causing_vulnerability": "The code includes unnecessary parameters '0, 0' in the nf_ct_helper_init function calls, which can result in out-of-bounds memory writes and potential system crashes."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the unnecessary parameters '0, 0' from the nf_ct_helper_init function calls. By eliminating these parameters, the code is made safer and the risk of out-of-bounds memory writes is reduced, preventing potential system crashes.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2020-14305, which involves an out-of-bounds memory write flaw in the Linux kernel's Voice Over IP H.323 connection tracking functionality. The vulnerability allows an unauthenticated remote user to crash the system, leading to a denial of service.\n\nThe correction involves removing the unnecessary parameters \"0, 0\" from the `nf_ct_helper_init` function calls. These parameters are not needed and can potentially lead to out-of-bounds memory writes, which is the root cause of the vulnerability. By removing these parameters, the code is made safer and the vulnerability is mitigated.",
      "GPT_purpose": "Initialize netfilter connection tracking helpers for TFTP protocol on both IPv4 and IPv6.",
      "GPT_function": "\n1. Initialize nf_conntrack_tftp module.\n2. Register TFTP helpers for IPv4 and IPv6.\n3. Handle TFTP connections on specified ports.",
      "CVE_id": "CVE-2020-14305",
      "code_before_change": "static int __init nf_conntrack_tftp_init(void)\n{\n\tint i, ret;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(0);\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = TFTP_PORT;\n\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&tftp[2 * i], AF_INET, IPPROTO_UDP, \"tftp\",\n\t\t\t\t  TFTP_PORT, ports[i], i, &tftp_exp_policy,\n\t\t\t\t  0, 0, tftp_help, NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&tftp[2 * i + 1], AF_INET6, IPPROTO_UDP, \"tftp\",\n\t\t\t\t  TFTP_PORT, ports[i], i, &tftp_exp_policy,\n\t\t\t\t  0, 0, tftp_help, NULL, THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(tftp, ports_c * 2);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\treturn ret;\n\t}\n\treturn 0;\n}",
      "code_after_change": "static int __init nf_conntrack_tftp_init(void)\n{\n\tint i, ret;\n\n\tNF_CT_HELPER_BUILD_BUG_ON(0);\n\n\tif (ports_c == 0)\n\t\tports[ports_c++] = TFTP_PORT;\n\n\tfor (i = 0; i < ports_c; i++) {\n\t\tnf_ct_helper_init(&tftp[2 * i], AF_INET, IPPROTO_UDP, \"tftp\",\n\t\t\t\t  TFTP_PORT, ports[i], i, &tftp_exp_policy,\n\t\t\t\t  0, tftp_help, NULL, THIS_MODULE);\n\t\tnf_ct_helper_init(&tftp[2 * i + 1], AF_INET6, IPPROTO_UDP, \"tftp\",\n\t\t\t\t  TFTP_PORT, ports[i], i, &tftp_exp_policy,\n\t\t\t\t  0, tftp_help, NULL, THIS_MODULE);\n\t}\n\n\tret = nf_conntrack_helpers_register(tftp, ports_c * 2);\n\tif (ret < 0) {\n\t\tpr_err(\"failed to register helpers\\n\");\n\t\treturn ret;\n\t}\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t  0, tftp_help, NULL, THIS_MODULE);",
          "\t\t\t\t  0, tftp_help, NULL, THIS_MODULE);"
        ],
        "deleted": [
          "\t\t\t\t  0, 0, tftp_help, NULL, THIS_MODULE);",
          "\t\t\t\t  0, 0, tftp_help, NULL, THIS_MODULE);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper parameter handling in the nf_ct_helper_init function calls.",
      "trigger_condition": "The unnecessary parameters '0, 0' in the nf_ct_helper_init function calls lead to out-of-bounds memory writes.",
      "specific_code_behavior_causing_vulnerability": "The code includes unnecessary parameters '0, 0' in the nf_ct_helper_init function calls, which can result in out-of-bounds memory writes and potential system crashes.",
      "id": 78,
      "code_after_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2;\nFUN2(0);\nif (VAR3 == 0)\nVAR4[VAR3++] = VAR5;\nfor (VAR1 = 0; VAR1 < VAR3; VAR1++) {\nFUN3(&VAR6[2 * VAR1], VAR7, VAR8, \"STR\",\nVAR5, VAR4[VAR1], VAR1, &VAR9,\n0, VAR10, NULL, VAR11);\nFUN3(&VAR6[2 * VAR1 + 1], VAR12, VAR8, \"STR\",\nVAR5, VAR4[VAR1], VAR1, &VAR9,\n0, VAR10, NULL, VAR11);\n}\nVAR2 = FUN4(VAR6, VAR3 * 2);\nif (VAR2 < 0) {\nFUN5(\"STR\");\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int __init FUN1(void)\n{\nint VAR1, VAR2;\nFUN2(0);\nif (VAR3 == 0)\nVAR4[VAR3++] = VAR5;\nfor (VAR1 = 0; VAR1 < VAR3; VAR1++) {\nFUN3(&VAR6[2 * VAR1], VAR7, VAR8, \"STR\",\nVAR5, VAR4[VAR1], VAR1, &VAR9,\n0, 0, VAR10, NULL, VAR11);\nFUN3(&VAR6[2 * VAR1 + 1], VAR12, VAR8, \"STR\",\nVAR5, VAR4[VAR1], VAR1, &VAR9,\n0, 0, VAR10, NULL, VAR11);\n}\nVAR2 = FUN4(VAR6, VAR3 * 2);\nif (VAR2 < 0) {\nFUN5(\"STR\");\nreturn VAR2;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int __init nf_conntrack_tftp_init(void)\n{\nint i, ret;\nNF_CT_HELPER_BUILD_BUG_ON(0);\nif (ports_c == 0)\nports[ports_c++] = TFTP_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&tftp[2 * i], AF_INET, IPPROTO_UDP, \"tftp\",\nTFTP_PORT, ports[i], i, &tftp_exp_policy,\n0, tftp_help, NULL, THIS_MODULE);\nnf_ct_helper_init(&tftp[2 * i + 1], AF_INET6, IPPROTO_UDP, \"tftp\",\nTFTP_PORT, ports[i], i, &tftp_exp_policy,\n0, tftp_help, NULL, THIS_MODULE);\n}\nret = nf_conntrack_helpers_register(tftp, ports_c * 2);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nreturn ret;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int __init nf_conntrack_tftp_init(void)\n{\nint i, ret;\nNF_CT_HELPER_BUILD_BUG_ON(0);\nif (ports_c == 0)\nports[ports_c++] = TFTP_PORT;\nfor (i = 0; i < ports_c; i++) {\nnf_ct_helper_init(&tftp[2 * i], AF_INET, IPPROTO_UDP, \"tftp\",\nTFTP_PORT, ports[i], i, &tftp_exp_policy,\n0, 0, tftp_help, NULL, THIS_MODULE);\nnf_ct_helper_init(&tftp[2 * i + 1], AF_INET6, IPPROTO_UDP, \"tftp\",\nTFTP_PORT, ports[i], i, &tftp_exp_policy,\n0, 0, tftp_help, NULL, THIS_MODULE);\n}\nret = nf_conntrack_helpers_register(tftp, ports_c * 2);\nif (ret < 0) {\npr_err(\"failed to register helpers\\n\");\nreturn ret;\n}\nreturn 0;\n}\n"
    }
  ],
  "CVE-2020-14331": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking on a pointer increment operation within a loop.",
        "trigger_condition": "A local attacker attempts to resize the console, causing an out-of-bounds write vulnerability to occur due to unchecked pointer increment.",
        "specific_code_behavior_causing_vulnerability": "The code increments a pointer without verifying if the resulting value exceeds the allocated buffer size, leading to potential out-of-bounds write operations."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add bounds checking on the pointer increment operation within the loop. This ensures that the pointer does not go beyond the allocated buffer size, preventing out-of-bounds writes. In this case, the solution involves checking if the incremented pointer value exceeds the buffer size and resetting the pointer to the beginning of the buffer if necessary.",
      "GPT_analysis": "The modification made to the code snippet is necessary to prevent an out-of-bounds write vulnerability that exists in the original code. The vulnerability occurs when the tail pointer is incremented without checking if it exceeds the size of the buffer. This can lead to writing data outside the bounds of the allocated memory, potentially causing a system crash or allowing an attacker to escalate their privileges.\n\nBy adding the check `(vgacon_scrollback_cur->tail + c->vc_size_row) > vgacon_scrollback_cur->size`, the modified code ensures that the tail pointer does not go beyond the allocated buffer size. If the tail pointer would exceed the buffer size after incrementing it, the modification resets the tail pointer to the beginning of the buffer (`vgacon_scrollback_cur->tail = 0`) to prevent the out-of-bounds write.\n\nThis modification helps to mitigate the vulnerability by enforcing proper bounds checking on the tail pointer, thereby improving the security and stability of the code.",
      "GPT_purpose": "Update the VGA console scrollback buffer with new data.",
      "GPT_function": "\n1. Update the VGA console scrollback buffer with data from the console.\n2. Increment the count of characters in the scrollback buffer.\n3. Update the current position in the scrollback buffer.",
      "CVE_id": "CVE-2020-14331",
      "code_before_change": "static void vgacon_scrollback_update(struct vc_data *c, int t, int count)\n{\n\tvoid *p;\n\n\tif (!vgacon_scrollback_cur->data || !vgacon_scrollback_cur->size ||\n\t    c->vc_num != fg_console)\n\t\treturn;\n\n\tp = (void *) (c->vc_origin + t * c->vc_size_row);\n\n\twhile (count--) {\n\t\tscr_memcpyw(vgacon_scrollback_cur->data +\n\t\t\t    vgacon_scrollback_cur->tail,\n\t\t\t    p, c->vc_size_row);\n\n\t\tvgacon_scrollback_cur->cnt++;\n\t\tp += c->vc_size_row;\n\t\tvgacon_scrollback_cur->tail += c->vc_size_row;\n\n\t\tif (vgacon_scrollback_cur->tail >= vgacon_scrollback_cur->size)\n\t\t\tvgacon_scrollback_cur->tail = 0;\n\n\t\tif (vgacon_scrollback_cur->cnt > vgacon_scrollback_cur->rows)\n\t\t\tvgacon_scrollback_cur->cnt = vgacon_scrollback_cur->rows;\n\n\t\tvgacon_scrollback_cur->cur = vgacon_scrollback_cur->cnt;\n\t}\n}",
      "code_after_change": "static void vgacon_scrollback_update(struct vc_data *c, int t, int count)\n{\n\tvoid *p;\n\n\tif (!vgacon_scrollback_cur->data || !vgacon_scrollback_cur->size ||\n\t    c->vc_num != fg_console)\n\t\treturn;\n\n\tp = (void *) (c->vc_origin + t * c->vc_size_row);\n\n\twhile (count--) {\n\t\tif ((vgacon_scrollback_cur->tail + c->vc_size_row) >\n\t\t    vgacon_scrollback_cur->size)\n\t\t\tvgacon_scrollback_cur->tail = 0;\n\n\t\tscr_memcpyw(vgacon_scrollback_cur->data +\n\t\t\t    vgacon_scrollback_cur->tail,\n\t\t\t    p, c->vc_size_row);\n\n\t\tvgacon_scrollback_cur->cnt++;\n\t\tp += c->vc_size_row;\n\t\tvgacon_scrollback_cur->tail += c->vc_size_row;\n\n\t\tif (vgacon_scrollback_cur->tail >= vgacon_scrollback_cur->size)\n\t\t\tvgacon_scrollback_cur->tail = 0;\n\n\t\tif (vgacon_scrollback_cur->cnt > vgacon_scrollback_cur->rows)\n\t\t\tvgacon_scrollback_cur->cnt = vgacon_scrollback_cur->rows;\n\n\t\tvgacon_scrollback_cur->cur = vgacon_scrollback_cur->cnt;\n\t}\n}",
      "modified_lines": {
        "added": [
          "\t\tif ((vgacon_scrollback_cur->tail + c->vc_size_row) >",
          "\t\t    vgacon_scrollback_cur->size)",
          "\t\t\tvgacon_scrollback_cur->tail = 0;",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking on a pointer increment operation within a loop.",
      "trigger_condition": "A local attacker attempts to resize the console, causing an out-of-bounds write vulnerability to occur due to unchecked pointer increment.",
      "specific_code_behavior_causing_vulnerability": "The code increments a pointer without verifying if the resulting value exceeds the allocated buffer size, leading to potential out-of-bounds write operations.",
      "id": 79,
      "code_after_change_normalized": "static void FUN1(struct vc_data *VAR1, int VAR2, int VAR3)\n{\nvoid *VAR4;\nif (!VAR5->VAR6 || !VAR5->VAR7 ||\nVAR1->VAR8 != VAR9)\nreturn;\nVAR4 = (void *) (VAR1->VAR10 + VAR2 * VAR1->VAR11);\nwhile (VAR3--) {\nif ((VAR5->VAR12 + VAR1->VAR11) >\nVAR5->VAR7)\nVAR5->VAR12 = 0;\nFUN2(VAR5->VAR6 +\nVAR5->VAR12,\nVAR4, VAR1->VAR11);\nVAR5->VAR13++;\nVAR4 += VAR1->VAR11;\nVAR5->VAR12 += VAR1->VAR11;\nif (VAR5->VAR12 >= VAR5->VAR7)\nVAR5->VAR12 = 0;\nif (VAR5->VAR13 > VAR5->VAR14)\nVAR5->VAR13 = VAR5->VAR14;\nVAR5->VAR15 = VAR5->VAR13;\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct vc_data *VAR1, int VAR2, int VAR3)\n{\nvoid *VAR4;\nif (!VAR5->VAR6 || !VAR5->VAR7 ||\nVAR1->VAR8 != VAR9)\nreturn;\nVAR4 = (void *) (VAR1->VAR10 + VAR2 * VAR1->VAR11);\nwhile (VAR3--) {\nFUN2(VAR5->VAR6 +\nVAR5->VAR12,\nVAR4, VAR1->VAR11);\nVAR5->VAR13++;\nVAR4 += VAR1->VAR11;\nVAR5->VAR12 += VAR1->VAR11;\nif (VAR5->VAR12 >= VAR5->VAR7)\nVAR5->VAR12 = 0;\nif (VAR5->VAR13 > VAR5->VAR14)\nVAR5->VAR13 = VAR5->VAR14;\nVAR5->VAR15 = VAR5->VAR13;\n}\n}\n",
      "code_after_change_raw": "static void vgacon_scrollback_update(struct vc_data *c, int t, int count)\n{\nvoid *p;\nif (!vgacon_scrollback_cur->data || !vgacon_scrollback_cur->size ||\nc->vc_num != fg_console)\nreturn;\np = (void *) (c->vc_origin + t * c->vc_size_row);\nwhile (count--) {\nif ((vgacon_scrollback_cur->tail + c->vc_size_row) >\nvgacon_scrollback_cur->size)\nvgacon_scrollback_cur->tail = 0;\nscr_memcpyw(vgacon_scrollback_cur->data +\nvgacon_scrollback_cur->tail,\np, c->vc_size_row);\nvgacon_scrollback_cur->cnt++;\np += c->vc_size_row;\nvgacon_scrollback_cur->tail += c->vc_size_row;\nif (vgacon_scrollback_cur->tail >= vgacon_scrollback_cur->size)\nvgacon_scrollback_cur->tail = 0;\nif (vgacon_scrollback_cur->cnt > vgacon_scrollback_cur->rows)\nvgacon_scrollback_cur->cnt = vgacon_scrollback_cur->rows;\nvgacon_scrollback_cur->cur = vgacon_scrollback_cur->cnt;\n}\n}\n",
      "code_before_change_raw": "static void vgacon_scrollback_update(struct vc_data *c, int t, int count)\n{\nvoid *p;\nif (!vgacon_scrollback_cur->data || !vgacon_scrollback_cur->size ||\nc->vc_num != fg_console)\nreturn;\np = (void *) (c->vc_origin + t * c->vc_size_row);\nwhile (count--) {\nscr_memcpyw(vgacon_scrollback_cur->data +\nvgacon_scrollback_cur->tail,\np, c->vc_size_row);\nvgacon_scrollback_cur->cnt++;\np += c->vc_size_row;\nvgacon_scrollback_cur->tail += c->vc_size_row;\nif (vgacon_scrollback_cur->tail >= vgacon_scrollback_cur->size)\nvgacon_scrollback_cur->tail = 0;\nif (vgacon_scrollback_cur->cnt > vgacon_scrollback_cur->rows)\nvgacon_scrollback_cur->cnt = vgacon_scrollback_cur->rows;\nvgacon_scrollback_cur->cur = vgacon_scrollback_cur->cnt;\n}\n}\n"
    }
  ],
  "CVE-2020-14386": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking for the 'netoff' variable, which can lead to an unsigned short overflow.",
        "trigger_condition": "When the 'netoff' variable exceeds the maximum value allowed for an unsigned short (USHRT_MAX), it can result in an overflow, potentially leading to memory corruption.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the 'netoff' variable exceeds the maximum value for an unsigned short, which can result in an overflow condition and memory corruption vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the 'netoff' variable does not exceed the maximum value allowed for an unsigned short (USHRT_MAX). If 'netoff' exceeds this limit, appropriate actions should be taken to handle the situation, such as incrementing the 'tp_drops' counter and proceeding with the necessary cleanup steps. By adding this check, the code can prevent the overflow condition and potential memory corruption vulnerability.",
      "GPT_analysis": "The above modification is necessary to address the memory corruption vulnerability identified as CVE-2020-14386 in the provided code snippet. The vulnerability allows unprivileged processes to gain root privileges by exploiting the memory corruption issue.\n\nThe specific changes made in the code snippet aim to prevent the memory corruption vulnerability by ensuring proper handling of memory operations and avoiding potential buffer overflows. In this case, the modification includes checking if the `netoff` variable exceeds the maximum value allowed for an unsigned short (`USHRT_MAX`). If `netoff` exceeds this limit, the code now increments the `tp_drops` counter and goes to the `drop_n_restore` label to handle the situation appropriately.\n\nBy making these changes, the code is more robust and less susceptible to memory corruption issues that could potentially lead to privilege escalation attacks. It is important to promptly address such vulnerabilities to ensure the security and integrity of the system.",
      "GPT_purpose": "Receive and process incoming packets in the Linux kernel networking subsystem.",
      "GPT_function": "\n1. Receiving and processing packets in the tpacket_rcv function.\n2. Checking packet type and network device equality.\n3. Handling packet filtering and room availability.\n4. Managing packet data alignment and sizes.\n5. Setting packet status and timestamps.\n6. Populating packet headers and metadata.\n7. Handling memory cache flushing and status updates.\n8. Dropping or consuming packets based on conditions.",
      "CVE_id": "CVE-2020-14386",
      "code_before_change": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, netoff, hdrlen;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
      "code_after_change": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\n\t\t       struct packet_type *pt, struct net_device *orig_dev)\n{\n\tstruct sock *sk;\n\tstruct packet_sock *po;\n\tstruct sockaddr_ll *sll;\n\tunion tpacket_uhdr h;\n\tu8 *skb_head = skb->data;\n\tint skb_len = skb->len;\n\tunsigned int snaplen, res;\n\tunsigned long status = TP_STATUS_USER;\n\tunsigned short macoff, hdrlen;\n\tunsigned int netoff;\n\tstruct sk_buff *copy_skb = NULL;\n\tstruct timespec64 ts;\n\t__u32 ts_status;\n\tbool is_drop_n_account = false;\n\tunsigned int slot_id = 0;\n\tbool do_vnet = false;\n\n\t/* struct tpacket{2,3}_hdr is aligned to a multiple of TPACKET_ALIGNMENT.\n\t * We may add members to them until current aligned size without forcing\n\t * userspace to call getsockopt(..., PACKET_HDRLEN, ...).\n\t */\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\n\tBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\n\n\tif (skb->pkt_type == PACKET_LOOPBACK)\n\t\tgoto drop;\n\n\tsk = pt->af_packet_priv;\n\tpo = pkt_sk(sk);\n\n\tif (!net_eq(dev_net(dev), sock_net(sk)))\n\t\tgoto drop;\n\n\tif (dev->header_ops) {\n\t\tif (sk->sk_type != SOCK_DGRAM)\n\t\t\tskb_push(skb, skb->data - skb_mac_header(skb));\n\t\telse if (skb->pkt_type == PACKET_OUTGOING) {\n\t\t\t/* Special case: outgoing packets have ll header at head */\n\t\t\tskb_pull(skb, skb_network_offset(skb));\n\t\t}\n\t}\n\n\tsnaplen = skb->len;\n\n\tres = run_filter(skb, sk, snaplen);\n\tif (!res)\n\t\tgoto drop_n_restore;\n\n\t/* If we are flooded, just give up */\n\tif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\n\tif (skb->ip_summed == CHECKSUM_PARTIAL)\n\t\tstatus |= TP_STATUS_CSUMNOTREADY;\n\telse if (skb->pkt_type != PACKET_OUTGOING &&\n\t\t (skb->ip_summed == CHECKSUM_COMPLETE ||\n\t\t  skb_csum_unnecessary(skb)))\n\t\tstatus |= TP_STATUS_CSUM_VALID;\n\n\tif (snaplen > res)\n\t\tsnaplen = res;\n\n\tif (sk->sk_type == SOCK_DGRAM) {\n\t\tmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\n\t\t\t\t  po->tp_reserve;\n\t} else {\n\t\tunsigned int maclen = skb_network_offset(skb);\n\t\tnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n\t\t\t\t       (maclen < 16 ? 16 : maclen)) +\n\t\t\t\t       po->tp_reserve;\n\t\tif (po->has_vnet_hdr) {\n\t\t\tnetoff += sizeof(struct virtio_net_hdr);\n\t\t\tdo_vnet = true;\n\t\t}\n\t\tmacoff = netoff - maclen;\n\t}\n\tif (netoff > USHRT_MAX) {\n\t\tatomic_inc(&po->tp_drops);\n\t\tgoto drop_n_restore;\n\t}\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tif (macoff + snaplen > po->rx_ring.frame_size) {\n\t\t\tif (po->copy_thresh &&\n\t\t\t    atomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\n\t\t\t\tif (skb_shared(skb)) {\n\t\t\t\t\tcopy_skb = skb_clone(skb, GFP_ATOMIC);\n\t\t\t\t} else {\n\t\t\t\t\tcopy_skb = skb_get(skb);\n\t\t\t\t\tskb_head = skb->data;\n\t\t\t\t}\n\t\t\t\tif (copy_skb)\n\t\t\t\t\tskb_set_owner_r(copy_skb, sk);\n\t\t\t}\n\t\t\tsnaplen = po->rx_ring.frame_size - macoff;\n\t\t\tif ((int)snaplen < 0) {\n\t\t\t\tsnaplen = 0;\n\t\t\t\tdo_vnet = false;\n\t\t\t}\n\t\t}\n\t} else if (unlikely(macoff + snaplen >\n\t\t\t    GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\n\t\tu32 nval;\n\n\t\tnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\n\t\tpr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\n\t\t\t    snaplen, nval, macoff);\n\t\tsnaplen = nval;\n\t\tif (unlikely((int)snaplen < 0)) {\n\t\t\tsnaplen = 0;\n\t\t\tmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\n\t\t\tdo_vnet = false;\n\t\t}\n\t}\n\tspin_lock(&sk->sk_receive_queue.lock);\n\th.raw = packet_current_rx_frame(po, skb,\n\t\t\t\t\tTP_STATUS_KERNEL, (macoff+snaplen));\n\tif (!h.raw)\n\t\tgoto drop_n_account;\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tslot_id = po->rx_ring.head;\n\t\tif (test_bit(slot_id, po->rx_ring.rx_owner_map))\n\t\t\tgoto drop_n_account;\n\t\t__set_bit(slot_id, po->rx_ring.rx_owner_map);\n\t}\n\n\tif (do_vnet &&\n\t    virtio_net_hdr_from_skb(skb, h.raw + macoff -\n\t\t\t\t    sizeof(struct virtio_net_hdr),\n\t\t\t\t    vio_le(), true, 0)) {\n\t\tif (po->tp_version == TPACKET_V3)\n\t\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t\tgoto drop_n_account;\n\t}\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tpacket_increment_rx_head(po, &po->rx_ring);\n\t/*\n\t * LOSING will be reported till you read the stats,\n\t * because it's COR - Clear On Read.\n\t * Anyways, moving it for V1/V2 only as V3 doesn't need this\n\t * at packet level.\n\t */\n\t\tif (atomic_read(&po->tp_drops))\n\t\t\tstatus |= TP_STATUS_LOSING;\n\t}\n\n\tpo->stats.stats1.tp_packets++;\n\tif (copy_skb) {\n\t\tstatus |= TP_STATUS_COPY;\n\t\t__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n\t}\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\n\tskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\n\n\tif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\n\t\tktime_get_real_ts64(&ts);\n\n\tstatus |= ts_status;\n\n\tswitch (po->tp_version) {\n\tcase TPACKET_V1:\n\t\th.h1->tp_len = skb->len;\n\t\th.h1->tp_snaplen = snaplen;\n\t\th.h1->tp_mac = macoff;\n\t\th.h1->tp_net = netoff;\n\t\th.h1->tp_sec = ts.tv_sec;\n\t\th.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\n\t\thdrlen = sizeof(*h.h1);\n\t\tbreak;\n\tcase TPACKET_V2:\n\t\th.h2->tp_len = skb->len;\n\t\th.h2->tp_snaplen = snaplen;\n\t\th.h2->tp_mac = macoff;\n\t\th.h2->tp_net = netoff;\n\t\th.h2->tp_sec = ts.tv_sec;\n\t\th.h2->tp_nsec = ts.tv_nsec;\n\t\tif (skb_vlan_tag_present(skb)) {\n\t\t\th.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\n\t\t\th.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\n\t\t\tstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n\t\t} else {\n\t\t\th.h2->tp_vlan_tci = 0;\n\t\t\th.h2->tp_vlan_tpid = 0;\n\t\t}\n\t\tmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\n\t\thdrlen = sizeof(*h.h2);\n\t\tbreak;\n\tcase TPACKET_V3:\n\t\t/* tp_nxt_offset,vlan are already populated above.\n\t\t * So DONT clear those fields here\n\t\t */\n\t\th.h3->tp_status |= status;\n\t\th.h3->tp_len = skb->len;\n\t\th.h3->tp_snaplen = snaplen;\n\t\th.h3->tp_mac = macoff;\n\t\th.h3->tp_net = netoff;\n\t\th.h3->tp_sec  = ts.tv_sec;\n\t\th.h3->tp_nsec = ts.tv_nsec;\n\t\tmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\n\t\thdrlen = sizeof(*h.h3);\n\t\tbreak;\n\tdefault:\n\t\tBUG();\n\t}\n\n\tsll = h.raw + TPACKET_ALIGN(hdrlen);\n\tsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\n\tsll->sll_family = AF_PACKET;\n\tsll->sll_hatype = dev->type;\n\tsll->sll_protocol = skb->protocol;\n\tsll->sll_pkttype = skb->pkt_type;\n\tif (unlikely(po->origdev))\n\t\tsll->sll_ifindex = orig_dev->ifindex;\n\telse\n\t\tsll->sll_ifindex = dev->ifindex;\n\n\tsmp_mb();\n\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tu8 *start, *end;\n\n\t\tend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\n\t\t\t\t\tmacoff + snaplen);\n\n\t\tfor (start = h.raw; start < end; start += PAGE_SIZE)\n\t\t\tflush_dcache_page(pgv_to_page(start));\n\t}\n\tsmp_wmb();\n#endif\n\n\tif (po->tp_version <= TPACKET_V2) {\n\t\tspin_lock(&sk->sk_receive_queue.lock);\n\t\t__packet_set_status(po, h.raw, status);\n\t\t__clear_bit(slot_id, po->rx_ring.rx_owner_map);\n\t\tspin_unlock(&sk->sk_receive_queue.lock);\n\t\tsk->sk_data_ready(sk);\n\t} else if (po->tp_version == TPACKET_V3) {\n\t\tprb_clear_blk_fill_status(&po->rx_ring);\n\t}\n\ndrop_n_restore:\n\tif (skb_head != skb->data && skb_shared(skb)) {\n\t\tskb->data = skb_head;\n\t\tskb->len = skb_len;\n\t}\ndrop:\n\tif (!is_drop_n_account)\n\t\tconsume_skb(skb);\n\telse\n\t\tkfree_skb(skb);\n\treturn 0;\n\ndrop_n_account:\n\tspin_unlock(&sk->sk_receive_queue.lock);\n\tatomic_inc(&po->tp_drops);\n\tis_drop_n_account = true;\n\n\tsk->sk_data_ready(sk);\n\tkfree_skb(copy_skb);\n\tgoto drop_n_restore;\n}",
      "modified_lines": {
        "added": [
          "\tunsigned short macoff, hdrlen;",
          "\tunsigned int netoff;",
          "\t}",
          "\tif (netoff > USHRT_MAX) {",
          "\t\tatomic_inc(&po->tp_drops);",
          "\t\tgoto drop_n_restore;"
        ],
        "deleted": [
          "\tunsigned short macoff, netoff, hdrlen;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking for the 'netoff' variable, which can lead to an unsigned short overflow.",
      "trigger_condition": "When the 'netoff' variable exceeds the maximum value allowed for an unsigned short (USHRT_MAX), it can result in an overflow, potentially leading to memory corruption.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the 'netoff' variable exceeds the maximum value for an unsigned short, which can result in an overflow condition and memory corruption vulnerability.",
      "id": 80,
      "code_after_change_normalized": "static int FUN1(struct sk_buff *VAR1, struct net_device *VAR2,\nstruct packet_type *VAR3, struct net_device *VAR4)\n{\nstruct sock *VAR5;\nstruct packet_sock *VAR6;\nstruct sockaddr_ll *VAR7;\nunion tpacket_uhdr VAR8;\nu8 *VAR9 = VAR1->VAR10;\nint VAR11 = VAR1->VAR12;\nunsigned int VAR13, VAR14;\nunsigned long VAR15 = VAR16;\nunsigned short VAR17, VAR18;\nunsigned int VAR19;\nstruct sk_buff *VAR20 = NULL;\nstruct timespec64 VAR21;\n__u32 VAR22;\nbool VAR23 = false;\nunsigned int VAR24 = 0;\nbool VAR25 = false;\nFUN2(FUN3(sizeof(*VAR8.VAR26)) != 32);\nFUN2(FUN3(sizeof(*VAR8.VAR27)) != 48);\nif (VAR1->VAR28 == VAR29)\ngoto VAR30;\nVAR5 = VAR3->VAR31;\nVAR6 = FUN4(VAR5);\nif (!FUN5(FUN6(VAR2), FUN7(VAR5)))\ngoto VAR30;\nif (VAR2->VAR32) {\nif (VAR5->VAR33 != VAR34)\nFUN8(VAR1, VAR1->VAR10 - FUN9(VAR1));\nelse if (VAR1->VAR28 == VAR35) {\nFUN10(VAR1, FUN11(VAR1));\n}\n}\nVAR13 = VAR1->VAR12;\nVAR14 = FUN12(VAR1, VAR5, VAR13);\nif (!VAR14)\ngoto VAR36;\nif (FUN13(VAR6, VAR1) == VAR37) {\nFUN14(&VAR6->VAR38);\ngoto VAR36;\n}\nif (VAR1->VAR39 == VAR40)\nVAR15 |= VAR41;\nelse if (VAR1->VAR28 != VAR35 &&\n(VAR1->VAR39 == VAR42 ||\nFUN15(VAR1)))\nVAR15 |= VAR43;\nif (VAR13 > VAR14)\nVAR13 = VAR14;\nif (VAR5->VAR33 == VAR34) {\nVAR17 = VAR19 = FUN3(VAR6->VAR44) + 16 +\nVAR6->VAR45;\n} else {\nunsigned int VAR46 = FUN11(VAR1);\nVAR19 = FUN3(VAR6->VAR44 +\n(VAR46 < 16 ? 16 : VAR46)) +\nVAR6->VAR45;\nif (VAR6->VAR47) {\nVAR19 += sizeof(struct VAR48);\nVAR25 = true;\n}\nVAR17 = VAR19 - VAR46;\n}\nif (VAR19 > VAR49) {\nFUN14(&VAR6->VAR38);\ngoto VAR36;\n}\nif (VAR6->VAR50 <= VAR51) {\nif (VAR17 + VAR13 > VAR6->VAR52.VAR53) {\nif (VAR6->VAR54 &&\nFUN16(&VAR5->VAR55) < VAR5->VAR56) {\nif (FUN17(VAR1)) {\nVAR20 = FUN18(VAR1, VAR57);\n} else {\nVAR20 = FUN19(VAR1);\nVAR9 = VAR1->VAR10;\n}\nif (VAR20)\nFUN20(VAR20, VAR5);\n}\nVAR13 = VAR6->VAR52.VAR53 - VAR17;\nif ((int)VAR13 < 0) {\nVAR13 = 0;\nVAR25 = false;\n}\n}\n} else if (FUN21(VAR17 + VAR13 >\nFUN22(&VAR6->VAR52)->VAR58)) {\nu32 VAR59;\nVAR59 = FUN22(&VAR6->VAR52)->VAR58 - VAR17;\nFUN23(\"STR\",\nVAR13, VAR59, VAR17);\nVAR13 = VAR59;\nif (FUN21((int)VAR13 < 0)) {\nVAR13 = 0;\nVAR17 = FUN22(&VAR6->VAR52)->VAR58;\nVAR25 = false;\n}\n}\nFUN24(&VAR5->VAR60.VAR61);\nVAR8.VAR62 = FUN25(VAR6, VAR1,\nVAR63, (VAR17+VAR13));\nif (!VAR8.VAR62)\ngoto VAR64;\nif (VAR6->VAR50 <= VAR51) {\nVAR24 = VAR6->VAR52.VAR65;\nif (FUN26(VAR24, VAR6->VAR52.VAR66))\ngoto VAR64;\nFUN27(VAR24, VAR6->VAR52.VAR66);\n}\nif (VAR25 &&\nFUN28(VAR1, VAR8.VAR62 + VAR17 -\nsizeof(struct VAR48),\nFUN29(), true, 0)) {\nif (VAR6->VAR50 == VAR67)\nFUN30(&VAR6->VAR52);\ngoto VAR64;\n}\nif (VAR6->VAR50 <= VAR51) {\nFUN31(VAR6, &VAR6->VAR52);\nif (FUN16(&VAR6->VAR38))\nVAR15 |= VAR68;\n}\nVAR6->VAR69.VAR70.VAR71++;\nif (VAR20) {\nVAR15 |= VAR72;\nFUN32(&VAR5->VAR60, VAR20);\n}\nFUN33(&VAR5->VAR60.VAR61);\nFUN34(VAR1, 0, VAR8.VAR62 + VAR17, VAR13);\nif (!(VAR22 = FUN35(VAR1, &VAR21, VAR6->VAR73)))\nFUN36(&VAR21);\nVAR15 |= VAR22;\nswitch (VAR6->VAR50) {\ncase VAR74:\nVAR8.VAR75->VAR76 = VAR1->VAR12;\nVAR8.VAR75->VAR77 = VAR13;\nVAR8.VAR75->VAR78 = VAR17;\nVAR8.VAR75->VAR79 = VAR19;\nVAR8.VAR75->VAR80 = VAR21.VAR81;\nVAR8.VAR75->VAR82 = VAR21.VAR83 / VAR84;\nVAR18 = sizeof(*VAR8.VAR75);\nbreak;\ncase VAR51:\nVAR8.VAR26->VAR76 = VAR1->VAR12;\nVAR8.VAR26->VAR77 = VAR13;\nVAR8.VAR26->VAR78 = VAR17;\nVAR8.VAR26->VAR79 = VAR19;\nVAR8.VAR26->VAR80 = VAR21.VAR81;\nVAR8.VAR26->VAR85 = VAR21.VAR83;\nif (FUN37(VAR1)) {\nVAR8.VAR26->VAR86 = FUN38(VAR1);\nVAR8.VAR26->VAR87 = FUN39(VAR1->VAR88);\nVAR15 |= VAR89 | VAR90;\n} else {\nVAR8.VAR26->VAR86 = 0;\nVAR8.VAR26->VAR87 = 0;\n}\nFUN40(VAR8.VAR26->VAR91, 0, sizeof(VAR8.VAR26->VAR91));\nVAR18 = sizeof(*VAR8.VAR26);\nbreak;\ncase VAR67:\nVAR8.VAR27->VAR92 |= VAR15;\nVAR8.VAR27->VAR76 = VAR1->VAR12;\nVAR8.VAR27->VAR77 = VAR13;\nVAR8.VAR27->VAR78 = VAR17;\nVAR8.VAR27->VAR79 = VAR19;\nVAR8.VAR27->VAR80  = VAR21.VAR81;\nVAR8.VAR27->VAR85 = VAR21.VAR83;\nFUN40(VAR8.VAR27->VAR91, 0, sizeof(VAR8.VAR27->VAR91));\nVAR18 = sizeof(*VAR8.VAR27);\nbreak;\ndefault:\nFUN41();\n}\nVAR7 = VAR8.VAR62 + FUN3(VAR18);\nVAR7->VAR93 = FUN42(VAR1, VAR7->VAR94);\nVAR7->VAR95 = VAR96;\nVAR7->VAR97 = VAR2->VAR98;\nVAR7->VAR99 = VAR1->VAR100;\nVAR7->VAR101 = VAR1->VAR28;\nif (FUN21(VAR6->VAR102))\nVAR7->VAR103 = VAR4->VAR104;\nelse\nVAR7->VAR103 = VAR2->VAR104;\nFUN43();\n#if VAR105 == 1\nif (VAR6->VAR50 <= VAR51) {\nu8 *VAR106, *VAR107;\nVAR107 = (VAR108 *) FUN44((unsigned long) VAR8.VAR62 +\nVAR17 + VAR13);\nfor (VAR106 = VAR8.VAR62; VAR106 < VAR107; VAR106 += VAR109)\nFUN45(FUN46(VAR106));\n}\nFUN47();\n#VAR110\nif (VAR6->VAR50 <= VAR51) {\nFUN24(&VAR5->VAR60.VAR61);\nFUN48(VAR6, VAR8.VAR62, VAR15);\nFUN49(VAR24, VAR6->VAR52.VAR66);\nFUN33(&VAR5->VAR60.VAR61);\nVAR5->FUN50(VAR5);\n} else if (VAR6->VAR50 == VAR67) {\nFUN30(&VAR6->VAR52);\n}\nVAR36:\nif (VAR9 != VAR1->VAR10 && FUN17(VAR1)) {\nVAR1->VAR10 = VAR9;\nVAR1->VAR12 = VAR11;\n}\nVAR30:\nif (!VAR23)\nFUN51(VAR1);\nelse\nFUN52(VAR1);\nreturn 0;\nVAR64:\nFUN33(&VAR5->VAR60.VAR61);\nFUN14(&VAR6->VAR38);\nVAR23 = true;\nVAR5->FUN50(VAR5);\nFUN52(VAR20);\ngoto VAR36;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sk_buff *VAR1, struct net_device *VAR2,\nstruct packet_type *VAR3, struct net_device *VAR4)\n{\nstruct sock *VAR5;\nstruct packet_sock *VAR6;\nstruct sockaddr_ll *VAR7;\nunion tpacket_uhdr VAR8;\nu8 *VAR9 = VAR1->VAR10;\nint VAR11 = VAR1->VAR12;\nunsigned int VAR13, VAR14;\nunsigned long VAR15 = VAR16;\nunsigned short VAR17, VAR18, VAR19;\nstruct sk_buff *VAR20 = NULL;\nstruct timespec64 VAR21;\n__u32 VAR22;\nbool VAR23 = false;\nunsigned int VAR24 = 0;\nbool VAR25 = false;\nFUN2(FUN3(sizeof(*VAR8.VAR26)) != 32);\nFUN2(FUN3(sizeof(*VAR8.VAR27)) != 48);\nif (VAR1->VAR28 == VAR29)\ngoto VAR30;\nVAR5 = VAR3->VAR31;\nVAR6 = FUN4(VAR5);\nif (!FUN5(FUN6(VAR2), FUN7(VAR5)))\ngoto VAR30;\nif (VAR2->VAR32) {\nif (VAR5->VAR33 != VAR34)\nFUN8(VAR1, VAR1->VAR10 - FUN9(VAR1));\nelse if (VAR1->VAR28 == VAR35) {\nFUN10(VAR1, FUN11(VAR1));\n}\n}\nVAR13 = VAR1->VAR12;\nVAR14 = FUN12(VAR1, VAR5, VAR13);\nif (!VAR14)\ngoto VAR36;\nif (FUN13(VAR6, VAR1) == VAR37) {\nFUN14(&VAR6->VAR38);\ngoto VAR36;\n}\nif (VAR1->VAR39 == VAR40)\nVAR15 |= VAR41;\nelse if (VAR1->VAR28 != VAR35 &&\n(VAR1->VAR39 == VAR42 ||\nFUN15(VAR1)))\nVAR15 |= VAR43;\nif (VAR13 > VAR14)\nVAR13 = VAR14;\nif (VAR5->VAR33 == VAR34) {\nVAR17 = VAR18 = FUN3(VAR6->VAR44) + 16 +\nVAR6->VAR45;\n} else {\nunsigned int VAR46 = FUN11(VAR1);\nVAR18 = FUN3(VAR6->VAR44 +\n(VAR46 < 16 ? 16 : VAR46)) +\nVAR6->VAR45;\nif (VAR6->VAR47) {\nVAR18 += sizeof(struct VAR48);\nVAR25 = true;\n}\nVAR17 = VAR18 - VAR46;\n}\nif (VAR6->VAR49 <= VAR50) {\nif (VAR17 + VAR13 > VAR6->VAR51.VAR52) {\nif (VAR6->VAR53 &&\nFUN16(&VAR5->VAR54) < VAR5->VAR55) {\nif (FUN17(VAR1)) {\nVAR20 = FUN18(VAR1, VAR56);\n} else {\nVAR20 = FUN19(VAR1);\nVAR9 = VAR1->VAR10;\n}\nif (VAR20)\nFUN20(VAR20, VAR5);\n}\nVAR13 = VAR6->VAR51.VAR52 - VAR17;\nif ((int)VAR13 < 0) {\nVAR13 = 0;\nVAR25 = false;\n}\n}\n} else if (FUN21(VAR17 + VAR13 >\nFUN22(&VAR6->VAR51)->VAR57)) {\nu32 VAR58;\nVAR58 = FUN22(&VAR6->VAR51)->VAR57 - VAR17;\nFUN23(\"STR\",\nVAR13, VAR58, VAR17);\nVAR13 = VAR58;\nif (FUN21((int)VAR13 < 0)) {\nVAR13 = 0;\nVAR17 = FUN22(&VAR6->VAR51)->VAR57;\nVAR25 = false;\n}\n}\nFUN24(&VAR5->VAR59.VAR60);\nVAR8.VAR61 = FUN25(VAR6, VAR1,\nVAR62, (VAR17+VAR13));\nif (!VAR8.VAR61)\ngoto VAR63;\nif (VAR6->VAR49 <= VAR50) {\nVAR24 = VAR6->VAR51.VAR64;\nif (FUN26(VAR24, VAR6->VAR51.VAR65))\ngoto VAR63;\nFUN27(VAR24, VAR6->VAR51.VAR65);\n}\nif (VAR25 &&\nFUN28(VAR1, VAR8.VAR61 + VAR17 -\nsizeof(struct VAR48),\nFUN29(), true, 0)) {\nif (VAR6->VAR49 == VAR66)\nFUN30(&VAR6->VAR51);\ngoto VAR63;\n}\nif (VAR6->VAR49 <= VAR50) {\nFUN31(VAR6, &VAR6->VAR51);\nif (FUN16(&VAR6->VAR38))\nVAR15 |= VAR67;\n}\nVAR6->VAR68.VAR69.VAR70++;\nif (VAR20) {\nVAR15 |= VAR71;\nFUN32(&VAR5->VAR59, VAR20);\n}\nFUN33(&VAR5->VAR59.VAR60);\nFUN34(VAR1, 0, VAR8.VAR61 + VAR17, VAR13);\nif (!(VAR22 = FUN35(VAR1, &VAR21, VAR6->VAR72)))\nFUN36(&VAR21);\nVAR15 |= VAR22;\nswitch (VAR6->VAR49) {\ncase VAR73:\nVAR8.VAR74->VAR75 = VAR1->VAR12;\nVAR8.VAR74->VAR76 = VAR13;\nVAR8.VAR74->VAR77 = VAR17;\nVAR8.VAR74->VAR78 = VAR18;\nVAR8.VAR74->VAR79 = VAR21.VAR80;\nVAR8.VAR74->VAR81 = VAR21.VAR82 / VAR83;\nVAR19 = sizeof(*VAR8.VAR74);\nbreak;\ncase VAR50:\nVAR8.VAR26->VAR75 = VAR1->VAR12;\nVAR8.VAR26->VAR76 = VAR13;\nVAR8.VAR26->VAR77 = VAR17;\nVAR8.VAR26->VAR78 = VAR18;\nVAR8.VAR26->VAR79 = VAR21.VAR80;\nVAR8.VAR26->VAR84 = VAR21.VAR82;\nif (FUN37(VAR1)) {\nVAR8.VAR26->VAR85 = FUN38(VAR1);\nVAR8.VAR26->VAR86 = FUN39(VAR1->VAR87);\nVAR15 |= VAR88 | VAR89;\n} else {\nVAR8.VAR26->VAR85 = 0;\nVAR8.VAR26->VAR86 = 0;\n}\nFUN40(VAR8.VAR26->VAR90, 0, sizeof(VAR8.VAR26->VAR90));\nVAR19 = sizeof(*VAR8.VAR26);\nbreak;\ncase VAR66:\nVAR8.VAR27->VAR91 |= VAR15;\nVAR8.VAR27->VAR75 = VAR1->VAR12;\nVAR8.VAR27->VAR76 = VAR13;\nVAR8.VAR27->VAR77 = VAR17;\nVAR8.VAR27->VAR78 = VAR18;\nVAR8.VAR27->VAR79  = VAR21.VAR80;\nVAR8.VAR27->VAR84 = VAR21.VAR82;\nFUN40(VAR8.VAR27->VAR90, 0, sizeof(VAR8.VAR27->VAR90));\nVAR19 = sizeof(*VAR8.VAR27);\nbreak;\ndefault:\nFUN41();\n}\nVAR7 = VAR8.VAR61 + FUN3(VAR19);\nVAR7->VAR92 = FUN42(VAR1, VAR7->VAR93);\nVAR7->VAR94 = VAR95;\nVAR7->VAR96 = VAR2->VAR97;\nVAR7->VAR98 = VAR1->VAR99;\nVAR7->VAR100 = VAR1->VAR28;\nif (FUN21(VAR6->VAR101))\nVAR7->VAR102 = VAR4->VAR103;\nelse\nVAR7->VAR102 = VAR2->VAR103;\nFUN43();\n#if VAR104 == 1\nif (VAR6->VAR49 <= VAR50) {\nu8 *VAR105, *VAR106;\nVAR106 = (VAR107 *) FUN44((unsigned long) VAR8.VAR61 +\nVAR17 + VAR13);\nfor (VAR105 = VAR8.VAR61; VAR105 < VAR106; VAR105 += VAR108)\nFUN45(FUN46(VAR105));\n}\nFUN47();\n#VAR109\nif (VAR6->VAR49 <= VAR50) {\nFUN24(&VAR5->VAR59.VAR60);\nFUN48(VAR6, VAR8.VAR61, VAR15);\nFUN49(VAR24, VAR6->VAR51.VAR65);\nFUN33(&VAR5->VAR59.VAR60);\nVAR5->FUN50(VAR5);\n} else if (VAR6->VAR49 == VAR66) {\nFUN30(&VAR6->VAR51);\n}\nVAR36:\nif (VAR9 != VAR1->VAR10 && FUN17(VAR1)) {\nVAR1->VAR10 = VAR9;\nVAR1->VAR12 = VAR11;\n}\nVAR30:\nif (!VAR23)\nFUN51(VAR1);\nelse\nFUN52(VAR1);\nreturn 0;\nVAR63:\nFUN33(&VAR5->VAR59.VAR60);\nFUN14(&VAR6->VAR38);\nVAR23 = true;\nVAR5->FUN50(VAR5);\nFUN52(VAR20);\ngoto VAR36;\n}\n",
      "code_after_change_raw": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\nstruct packet_type *pt, struct net_device *orig_dev)\n{\nstruct sock *sk;\nstruct packet_sock *po;\nstruct sockaddr_ll *sll;\nunion tpacket_uhdr h;\nu8 *skb_head = skb->data;\nint skb_len = skb->len;\nunsigned int snaplen, res;\nunsigned long status = TP_STATUS_USER;\nunsigned short macoff, hdrlen;\nunsigned int netoff;\nstruct sk_buff *copy_skb = NULL;\nstruct timespec64 ts;\n__u32 ts_status;\nbool is_drop_n_account = false;\nunsigned int slot_id = 0;\nbool do_vnet = false;\nBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\nBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\nif (skb->pkt_type == PACKET_LOOPBACK)\ngoto drop;\nsk = pt->af_packet_priv;\npo = pkt_sk(sk);\nif (!net_eq(dev_net(dev), sock_net(sk)))\ngoto drop;\nif (dev->header_ops) {\nif (sk->sk_type != SOCK_DGRAM)\nskb_push(skb, skb->data - skb_mac_header(skb));\nelse if (skb->pkt_type == PACKET_OUTGOING) {\nskb_pull(skb, skb_network_offset(skb));\n}\n}\nsnaplen = skb->len;\nres = run_filter(skb, sk, snaplen);\nif (!res)\ngoto drop_n_restore;\nif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\natomic_inc(&po->tp_drops);\ngoto drop_n_restore;\n}\nif (skb->ip_summed == CHECKSUM_PARTIAL)\nstatus |= TP_STATUS_CSUMNOTREADY;\nelse if (skb->pkt_type != PACKET_OUTGOING &&\n(skb->ip_summed == CHECKSUM_COMPLETE ||\nskb_csum_unnecessary(skb)))\nstatus |= TP_STATUS_CSUM_VALID;\nif (snaplen > res)\nsnaplen = res;\nif (sk->sk_type == SOCK_DGRAM) {\nmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\npo->tp_reserve;\n} else {\nunsigned int maclen = skb_network_offset(skb);\nnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n(maclen < 16 ? 16 : maclen)) +\npo->tp_reserve;\nif (po->has_vnet_hdr) {\nnetoff += sizeof(struct virtio_net_hdr);\ndo_vnet = true;\n}\nmacoff = netoff - maclen;\n}\nif (netoff > USHRT_MAX) {\natomic_inc(&po->tp_drops);\ngoto drop_n_restore;\n}\nif (po->tp_version <= TPACKET_V2) {\nif (macoff + snaplen > po->rx_ring.frame_size) {\nif (po->copy_thresh &&\natomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\nif (skb_shared(skb)) {\ncopy_skb = skb_clone(skb, GFP_ATOMIC);\n} else {\ncopy_skb = skb_get(skb);\nskb_head = skb->data;\n}\nif (copy_skb)\nskb_set_owner_r(copy_skb, sk);\n}\nsnaplen = po->rx_ring.frame_size - macoff;\nif ((int)snaplen < 0) {\nsnaplen = 0;\ndo_vnet = false;\n}\n}\n} else if (unlikely(macoff + snaplen >\nGET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\nu32 nval;\nnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\npr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\nsnaplen, nval, macoff);\nsnaplen = nval;\nif (unlikely((int)snaplen < 0)) {\nsnaplen = 0;\nmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\ndo_vnet = false;\n}\n}\nspin_lock(&sk->sk_receive_queue.lock);\nh.raw = packet_current_rx_frame(po, skb,\nTP_STATUS_KERNEL, (macoff+snaplen));\nif (!h.raw)\ngoto drop_n_account;\nif (po->tp_version <= TPACKET_V2) {\nslot_id = po->rx_ring.head;\nif (test_bit(slot_id, po->rx_ring.rx_owner_map))\ngoto drop_n_account;\n__set_bit(slot_id, po->rx_ring.rx_owner_map);\n}\nif (do_vnet &&\nvirtio_net_hdr_from_skb(skb, h.raw + macoff -\nsizeof(struct virtio_net_hdr),\nvio_le(), true, 0)) {\nif (po->tp_version == TPACKET_V3)\nprb_clear_blk_fill_status(&po->rx_ring);\ngoto drop_n_account;\n}\nif (po->tp_version <= TPACKET_V2) {\npacket_increment_rx_head(po, &po->rx_ring);\nif (atomic_read(&po->tp_drops))\nstatus |= TP_STATUS_LOSING;\n}\npo->stats.stats1.tp_packets++;\nif (copy_skb) {\nstatus |= TP_STATUS_COPY;\n__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n}\nspin_unlock(&sk->sk_receive_queue.lock);\nskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\nif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\nktime_get_real_ts64(&ts);\nstatus |= ts_status;\nswitch (po->tp_version) {\ncase TPACKET_V1:\nh.h1->tp_len = skb->len;\nh.h1->tp_snaplen = snaplen;\nh.h1->tp_mac = macoff;\nh.h1->tp_net = netoff;\nh.h1->tp_sec = ts.tv_sec;\nh.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\nhdrlen = sizeof(*h.h1);\nbreak;\ncase TPACKET_V2:\nh.h2->tp_len = skb->len;\nh.h2->tp_snaplen = snaplen;\nh.h2->tp_mac = macoff;\nh.h2->tp_net = netoff;\nh.h2->tp_sec = ts.tv_sec;\nh.h2->tp_nsec = ts.tv_nsec;\nif (skb_vlan_tag_present(skb)) {\nh.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\nh.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\nstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n} else {\nh.h2->tp_vlan_tci = 0;\nh.h2->tp_vlan_tpid = 0;\n}\nmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\nhdrlen = sizeof(*h.h2);\nbreak;\ncase TPACKET_V3:\nh.h3->tp_status |= status;\nh.h3->tp_len = skb->len;\nh.h3->tp_snaplen = snaplen;\nh.h3->tp_mac = macoff;\nh.h3->tp_net = netoff;\nh.h3->tp_sec  = ts.tv_sec;\nh.h3->tp_nsec = ts.tv_nsec;\nmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\nhdrlen = sizeof(*h.h3);\nbreak;\ndefault:\nBUG();\n}\nsll = h.raw + TPACKET_ALIGN(hdrlen);\nsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\nsll->sll_family = AF_PACKET;\nsll->sll_hatype = dev->type;\nsll->sll_protocol = skb->protocol;\nsll->sll_pkttype = skb->pkt_type;\nif (unlikely(po->origdev))\nsll->sll_ifindex = orig_dev->ifindex;\nelse\nsll->sll_ifindex = dev->ifindex;\nsmp_mb();\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\nif (po->tp_version <= TPACKET_V2) {\nu8 *start, *end;\nend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\nmacoff + snaplen);\nfor (start = h.raw; start < end; start += PAGE_SIZE)\nflush_dcache_page(pgv_to_page(start));\n}\nsmp_wmb();\n#endif\nif (po->tp_version <= TPACKET_V2) {\nspin_lock(&sk->sk_receive_queue.lock);\n__packet_set_status(po, h.raw, status);\n__clear_bit(slot_id, po->rx_ring.rx_owner_map);\nspin_unlock(&sk->sk_receive_queue.lock);\nsk->sk_data_ready(sk);\n} else if (po->tp_version == TPACKET_V3) {\nprb_clear_blk_fill_status(&po->rx_ring);\n}\ndrop_n_restore:\nif (skb_head != skb->data && skb_shared(skb)) {\nskb->data = skb_head;\nskb->len = skb_len;\n}\ndrop:\nif (!is_drop_n_account)\nconsume_skb(skb);\nelse\nkfree_skb(skb);\nreturn 0;\ndrop_n_account:\nspin_unlock(&sk->sk_receive_queue.lock);\natomic_inc(&po->tp_drops);\nis_drop_n_account = true;\nsk->sk_data_ready(sk);\nkfree_skb(copy_skb);\ngoto drop_n_restore;\n}\n",
      "code_before_change_raw": "static int tpacket_rcv(struct sk_buff *skb, struct net_device *dev,\nstruct packet_type *pt, struct net_device *orig_dev)\n{\nstruct sock *sk;\nstruct packet_sock *po;\nstruct sockaddr_ll *sll;\nunion tpacket_uhdr h;\nu8 *skb_head = skb->data;\nint skb_len = skb->len;\nunsigned int snaplen, res;\nunsigned long status = TP_STATUS_USER;\nunsigned short macoff, netoff, hdrlen;\nstruct sk_buff *copy_skb = NULL;\nstruct timespec64 ts;\n__u32 ts_status;\nbool is_drop_n_account = false;\nunsigned int slot_id = 0;\nbool do_vnet = false;\nBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h2)) != 32);\nBUILD_BUG_ON(TPACKET_ALIGN(sizeof(*h.h3)) != 48);\nif (skb->pkt_type == PACKET_LOOPBACK)\ngoto drop;\nsk = pt->af_packet_priv;\npo = pkt_sk(sk);\nif (!net_eq(dev_net(dev), sock_net(sk)))\ngoto drop;\nif (dev->header_ops) {\nif (sk->sk_type != SOCK_DGRAM)\nskb_push(skb, skb->data - skb_mac_header(skb));\nelse if (skb->pkt_type == PACKET_OUTGOING) {\nskb_pull(skb, skb_network_offset(skb));\n}\n}\nsnaplen = skb->len;\nres = run_filter(skb, sk, snaplen);\nif (!res)\ngoto drop_n_restore;\nif (__packet_rcv_has_room(po, skb) == ROOM_NONE) {\natomic_inc(&po->tp_drops);\ngoto drop_n_restore;\n}\nif (skb->ip_summed == CHECKSUM_PARTIAL)\nstatus |= TP_STATUS_CSUMNOTREADY;\nelse if (skb->pkt_type != PACKET_OUTGOING &&\n(skb->ip_summed == CHECKSUM_COMPLETE ||\nskb_csum_unnecessary(skb)))\nstatus |= TP_STATUS_CSUM_VALID;\nif (snaplen > res)\nsnaplen = res;\nif (sk->sk_type == SOCK_DGRAM) {\nmacoff = netoff = TPACKET_ALIGN(po->tp_hdrlen) + 16 +\npo->tp_reserve;\n} else {\nunsigned int maclen = skb_network_offset(skb);\nnetoff = TPACKET_ALIGN(po->tp_hdrlen +\n(maclen < 16 ? 16 : maclen)) +\npo->tp_reserve;\nif (po->has_vnet_hdr) {\nnetoff += sizeof(struct virtio_net_hdr);\ndo_vnet = true;\n}\nmacoff = netoff - maclen;\n}\nif (po->tp_version <= TPACKET_V2) {\nif (macoff + snaplen > po->rx_ring.frame_size) {\nif (po->copy_thresh &&\natomic_read(&sk->sk_rmem_alloc) < sk->sk_rcvbuf) {\nif (skb_shared(skb)) {\ncopy_skb = skb_clone(skb, GFP_ATOMIC);\n} else {\ncopy_skb = skb_get(skb);\nskb_head = skb->data;\n}\nif (copy_skb)\nskb_set_owner_r(copy_skb, sk);\n}\nsnaplen = po->rx_ring.frame_size - macoff;\nif ((int)snaplen < 0) {\nsnaplen = 0;\ndo_vnet = false;\n}\n}\n} else if (unlikely(macoff + snaplen >\nGET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len)) {\nu32 nval;\nnval = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len - macoff;\npr_err_once(\"tpacket_rcv: packet too big, clamped from %u to %u. macoff=%u\\n\",\nsnaplen, nval, macoff);\nsnaplen = nval;\nif (unlikely((int)snaplen < 0)) {\nsnaplen = 0;\nmacoff = GET_PBDQC_FROM_RB(&po->rx_ring)->max_frame_len;\ndo_vnet = false;\n}\n}\nspin_lock(&sk->sk_receive_queue.lock);\nh.raw = packet_current_rx_frame(po, skb,\nTP_STATUS_KERNEL, (macoff+snaplen));\nif (!h.raw)\ngoto drop_n_account;\nif (po->tp_version <= TPACKET_V2) {\nslot_id = po->rx_ring.head;\nif (test_bit(slot_id, po->rx_ring.rx_owner_map))\ngoto drop_n_account;\n__set_bit(slot_id, po->rx_ring.rx_owner_map);\n}\nif (do_vnet &&\nvirtio_net_hdr_from_skb(skb, h.raw + macoff -\nsizeof(struct virtio_net_hdr),\nvio_le(), true, 0)) {\nif (po->tp_version == TPACKET_V3)\nprb_clear_blk_fill_status(&po->rx_ring);\ngoto drop_n_account;\n}\nif (po->tp_version <= TPACKET_V2) {\npacket_increment_rx_head(po, &po->rx_ring);\nif (atomic_read(&po->tp_drops))\nstatus |= TP_STATUS_LOSING;\n}\npo->stats.stats1.tp_packets++;\nif (copy_skb) {\nstatus |= TP_STATUS_COPY;\n__skb_queue_tail(&sk->sk_receive_queue, copy_skb);\n}\nspin_unlock(&sk->sk_receive_queue.lock);\nskb_copy_bits(skb, 0, h.raw + macoff, snaplen);\nif (!(ts_status = tpacket_get_timestamp(skb, &ts, po->tp_tstamp)))\nktime_get_real_ts64(&ts);\nstatus |= ts_status;\nswitch (po->tp_version) {\ncase TPACKET_V1:\nh.h1->tp_len = skb->len;\nh.h1->tp_snaplen = snaplen;\nh.h1->tp_mac = macoff;\nh.h1->tp_net = netoff;\nh.h1->tp_sec = ts.tv_sec;\nh.h1->tp_usec = ts.tv_nsec / NSEC_PER_USEC;\nhdrlen = sizeof(*h.h1);\nbreak;\ncase TPACKET_V2:\nh.h2->tp_len = skb->len;\nh.h2->tp_snaplen = snaplen;\nh.h2->tp_mac = macoff;\nh.h2->tp_net = netoff;\nh.h2->tp_sec = ts.tv_sec;\nh.h2->tp_nsec = ts.tv_nsec;\nif (skb_vlan_tag_present(skb)) {\nh.h2->tp_vlan_tci = skb_vlan_tag_get(skb);\nh.h2->tp_vlan_tpid = ntohs(skb->vlan_proto);\nstatus |= TP_STATUS_VLAN_VALID | TP_STATUS_VLAN_TPID_VALID;\n} else {\nh.h2->tp_vlan_tci = 0;\nh.h2->tp_vlan_tpid = 0;\n}\nmemset(h.h2->tp_padding, 0, sizeof(h.h2->tp_padding));\nhdrlen = sizeof(*h.h2);\nbreak;\ncase TPACKET_V3:\nh.h3->tp_status |= status;\nh.h3->tp_len = skb->len;\nh.h3->tp_snaplen = snaplen;\nh.h3->tp_mac = macoff;\nh.h3->tp_net = netoff;\nh.h3->tp_sec  = ts.tv_sec;\nh.h3->tp_nsec = ts.tv_nsec;\nmemset(h.h3->tp_padding, 0, sizeof(h.h3->tp_padding));\nhdrlen = sizeof(*h.h3);\nbreak;\ndefault:\nBUG();\n}\nsll = h.raw + TPACKET_ALIGN(hdrlen);\nsll->sll_halen = dev_parse_header(skb, sll->sll_addr);\nsll->sll_family = AF_PACKET;\nsll->sll_hatype = dev->type;\nsll->sll_protocol = skb->protocol;\nsll->sll_pkttype = skb->pkt_type;\nif (unlikely(po->origdev))\nsll->sll_ifindex = orig_dev->ifindex;\nelse\nsll->sll_ifindex = dev->ifindex;\nsmp_mb();\n#if ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE == 1\nif (po->tp_version <= TPACKET_V2) {\nu8 *start, *end;\nend = (u8 *) PAGE_ALIGN((unsigned long) h.raw +\nmacoff + snaplen);\nfor (start = h.raw; start < end; start += PAGE_SIZE)\nflush_dcache_page(pgv_to_page(start));\n}\nsmp_wmb();\n#endif\nif (po->tp_version <= TPACKET_V2) {\nspin_lock(&sk->sk_receive_queue.lock);\n__packet_set_status(po, h.raw, status);\n__clear_bit(slot_id, po->rx_ring.rx_owner_map);\nspin_unlock(&sk->sk_receive_queue.lock);\nsk->sk_data_ready(sk);\n} else if (po->tp_version == TPACKET_V3) {\nprb_clear_blk_fill_status(&po->rx_ring);\n}\ndrop_n_restore:\nif (skb_head != skb->data && skb_shared(skb)) {\nskb->data = skb_head;\nskb->len = skb_len;\n}\ndrop:\nif (!is_drop_n_account)\nconsume_skb(skb);\nelse\nkfree_skb(skb);\nreturn 0;\ndrop_n_account:\nspin_unlock(&sk->sk_receive_queue.lock);\natomic_inc(&po->tp_drops);\nis_drop_n_account = true;\nsk->sk_data_ready(sk);\nkfree_skb(copy_skb);\ngoto drop_n_restore;\n}\n"
    }
  ],
  "CVE-2020-14390": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate bounds checking and calculations when changing screen size in the code.",
        "trigger_condition": "Changing the screen size triggers the vulnerable code path, leading to an out-of-bounds memory write.",
        "specific_code_behavior_causing_vulnerability": "The original code snippet contains conditional checks and calculations that can result in accessing memory outside the allocated buffer when changing the screen size. This behavior can lead to memory corruption or denial of service due to out-of-bounds memory writes."
      },
      "solution": "To mitigate the vulnerability, the code should be simplified to always return a valid memory location within the bounds of the allocated buffer. By removing the conditional checks and calculations that can lead to out-of-bounds memory writes, the code becomes safer and more secure. In this case, the solution involves modifying the code to always return (u16 *) (vc->vc_origin + offset) without additional checks or calculations.",
      "GPT_analysis": "The modification made to the code snippet is necessary to address the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write issue that can lead to memory corruption or denial of service. By simplifying the function to always return `(u16 *) (vc->vc_origin + offset)`, the potential for out-of-bounds memory writes is eliminated.\n\nThe original code snippet had conditional checks and calculations that could result in accessing memory outside the allocated buffer when changing the screen size. This vulnerability could be exploited to corrupt memory or disrupt the system's operation. By removing the conditional checks and calculations, the code is made safer and more secure.\n\nTherefore, the modification ensures that the function always returns a valid memory location within the bounds of the allocated buffer, effectively mitigating the vulnerability and reducing the risk of memory corruption or denial of service attacks.",
      "GPT_purpose": "Calculate the screen position for a given virtual console and offset, considering softback lines for scrolling.",
      "GPT_function": "\n1. Determines the screen position based on the virtual console data and offset.\n2. Checks if the virtual console number matches the foreground console and if softback_lines is enabled.\n3. Calculates the line number based on the offset and virtual console row size.\n4. Handles scenarios where the line exceeds the softback_lines limit.\n5. Updates the memory position based on softback_curr, offset, and softback_end.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static u16 *fbcon_screen_pos(struct vc_data *vc, int offset)\n{\n\tunsigned long p;\n\tint line;\n\n\tif (vc->vc_num != fg_console || !softback_lines)\n\t\treturn (u16 *) (vc->vc_origin + offset);\n\tline = offset / vc->vc_size_row;\n\tif (line >= softback_lines)\n\t\treturn (u16 *) (vc->vc_origin + offset -\n\t\t\t\tsoftback_lines * vc->vc_size_row);\n\tp = softback_curr + offset;\n\tif (p >= softback_end)\n\t\tp += softback_buf - softback_end;\n\treturn (u16 *) p;\n}",
      "code_after_change": "static u16 *fbcon_screen_pos(struct vc_data *vc, int offset)\n{\n\treturn (u16 *) (vc->vc_origin + offset);\n}",
      "modified_lines": {
        "added": [
          "\treturn (u16 *) (vc->vc_origin + offset);"
        ],
        "deleted": [
          "\tunsigned long p;",
          "\tint line;",
          "",
          "\tif (vc->vc_num != fg_console || !softback_lines)",
          "\t\treturn (u16 *) (vc->vc_origin + offset);",
          "\tline = offset / vc->vc_size_row;",
          "\tif (line >= softback_lines)",
          "\t\treturn (u16 *) (vc->vc_origin + offset -",
          "\t\t\t\tsoftback_lines * vc->vc_size_row);",
          "\tp = softback_curr + offset;",
          "\tif (p >= softback_end)",
          "\t\tp += softback_buf - softback_end;",
          "\treturn (u16 *) p;"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate bounds checking and calculations when changing screen size in the code.",
      "trigger_condition": "Changing the screen size triggers the vulnerable code path, leading to an out-of-bounds memory write.",
      "specific_code_behavior_causing_vulnerability": "The original code snippet contains conditional checks and calculations that can result in accessing memory outside the allocated buffer when changing the screen size. This behavior can lead to memory corruption or denial of service due to out-of-bounds memory writes.",
      "id": 81,
      "code_after_change_normalized": "static u16 *FUN1(struct vc_data *VAR1, int VAR2)\n{\nreturn (VAR3 *) (VAR1->VAR4 + VAR2);\n}\n",
      "code_before_change_normalized": "static u16 *FUN1(struct vc_data *VAR1, int VAR2)\n{\nunsigned long VAR3;\nint VAR4;\nif (VAR1->VAR5 != VAR6 || !VAR7)\nreturn (VAR8 *) (VAR1->VAR9 + VAR2);\nVAR4 = VAR2 / VAR1->VAR10;\nif (VAR4 >= VAR7)\nreturn (VAR8 *) (VAR1->VAR9 + VAR2 -\nVAR7 * VAR1->VAR10);\nVAR3 = VAR11 + VAR2;\nif (VAR3 >= VAR12)\nVAR3 += VAR13 - VAR12;\nreturn (VAR8 *) VAR3;\n}\n",
      "code_after_change_raw": "static u16 *fbcon_screen_pos(struct vc_data *vc, int offset)\n{\nreturn (u16 *) (vc->vc_origin + offset);\n}\n",
      "code_before_change_raw": "static u16 *fbcon_screen_pos(struct vc_data *vc, int offset)\n{\nunsigned long p;\nint line;\nif (vc->vc_num != fg_console || !softback_lines)\nreturn (u16 *) (vc->vc_origin + offset);\nline = offset / vc->vc_size_row;\nif (line >= softback_lines)\nreturn (u16 *) (vc->vc_origin + offset -\nsoftback_lines * vc->vc_size_row);\np = softback_curr + offset;\nif (p >= softback_end)\np += softback_buf - softback_end;\nreturn (u16 *) p;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate bounds checking when changing screen size, leading to an out-of-bounds memory write.",
        "trigger_condition": "Changing the screen size triggers the vulnerable code path, potentially causing memory corruption or denial of service.",
        "specific_code_behavior_causing_vulnerability": "The code snippet does not perform proper bounds checking when resizing the screen, which can result in an out-of-bounds memory write, leading to memory corruption or denial of service. Specifically, the vulnerability arises from not ensuring that the memory write operation stays within the allocated bounds during screen size changes."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that all memory write operations are performed within the bounds of the allocated memory when changing the screen size. This involves adding bounds checking logic to prevent out-of-bounds writes and potential memory corruption. In this case, the solution requires modifying the code to include proper bounds checking mechanisms to constrain memory write operations within the allocated memory space during screen size changes.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write issue that can lead to memory corruption or denial of service when changing the screen size. By making the necessary changes in the code snippet, you can prevent the out-of-bounds memory write and ensure that the operation is performed within the bounds of the allocated memory, thus addressing the vulnerability and enhancing the security of the code.",
      "GPT_purpose": "Set the font for the framebuffer console with potential vulnerability CVE-2020-14390.",
      "GPT_function": "\n1. Set font for the framebuffer console.\n2. Check if console is visible and adjust softback lines.\n3. Resize font and update display accordingly.\n4. Clear margins and update screen if console is visible in text mode.\n5. Free memory of old font data if necessary.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static int fbcon_do_set_font(struct vc_data *vc, int w, int h,\n\t\t\t     const u8 * data, int userfont)\n{\n\tstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\n\tstruct fbcon_ops *ops = info->fbcon_par;\n\tstruct fbcon_display *p = &fb_display[vc->vc_num];\n\tint resize;\n\tint cnt;\n\tchar *old_data = NULL;\n\n\tif (con_is_visible(vc) && softback_lines)\n\t\tfbcon_set_origin(vc);\n\n\tresize = (w != vc->vc_font.width) || (h != vc->vc_font.height);\n\tif (p->userfont)\n\t\told_data = vc->vc_font.data;\n\tif (userfont)\n\t\tcnt = FNTCHARCNT(data);\n\telse\n\t\tcnt = 256;\n\tvc->vc_font.data = (void *)(p->fontdata = data);\n\tif ((p->userfont = userfont))\n\t\tREFCOUNT(data)++;\n\tvc->vc_font.width = w;\n\tvc->vc_font.height = h;\n\tif (vc->vc_hi_font_mask && cnt == 256)\n\t\tset_vc_hi_font(vc, false);\n\telse if (!vc->vc_hi_font_mask && cnt == 512)\n\t\tset_vc_hi_font(vc, true);\n\n\tif (resize) {\n\t\tint cols, rows;\n\n\t\tcols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\n\t\trows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\n\t\tcols /= w;\n\t\trows /= h;\n\t\tvc_resize(vc, cols, rows);\n\t\tif (con_is_visible(vc) && softback_buf)\n\t\t\tfbcon_update_softback(vc);\n\t} else if (con_is_visible(vc)\n\t\t   && vc->vc_mode == KD_TEXT) {\n\t\tfbcon_clear_margins(vc, 0);\n\t\tupdate_screen(vc);\n\t}\n\n\tif (old_data && (--REFCOUNT(old_data) == 0))\n\t\tkfree(old_data - FONT_EXTRA_WORDS * sizeof(int));\n\treturn 0;\n}",
      "code_after_change": "static int fbcon_do_set_font(struct vc_data *vc, int w, int h,\n\t\t\t     const u8 * data, int userfont)\n{\n\tstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\n\tstruct fbcon_ops *ops = info->fbcon_par;\n\tstruct fbcon_display *p = &fb_display[vc->vc_num];\n\tint resize;\n\tint cnt;\n\tchar *old_data = NULL;\n\n\tresize = (w != vc->vc_font.width) || (h != vc->vc_font.height);\n\tif (p->userfont)\n\t\told_data = vc->vc_font.data;\n\tif (userfont)\n\t\tcnt = FNTCHARCNT(data);\n\telse\n\t\tcnt = 256;\n\tvc->vc_font.data = (void *)(p->fontdata = data);\n\tif ((p->userfont = userfont))\n\t\tREFCOUNT(data)++;\n\tvc->vc_font.width = w;\n\tvc->vc_font.height = h;\n\tif (vc->vc_hi_font_mask && cnt == 256)\n\t\tset_vc_hi_font(vc, false);\n\telse if (!vc->vc_hi_font_mask && cnt == 512)\n\t\tset_vc_hi_font(vc, true);\n\n\tif (resize) {\n\t\tint cols, rows;\n\n\t\tcols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\n\t\trows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\n\t\tcols /= w;\n\t\trows /= h;\n\t\tvc_resize(vc, cols, rows);\n\t} else if (con_is_visible(vc)\n\t\t   && vc->vc_mode == KD_TEXT) {\n\t\tfbcon_clear_margins(vc, 0);\n\t\tupdate_screen(vc);\n\t}\n\n\tif (old_data && (--REFCOUNT(old_data) == 0))\n\t\tkfree(old_data - FONT_EXTRA_WORDS * sizeof(int));\n\treturn 0;\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "",
          "\tif (con_is_visible(vc) && softback_lines)",
          "\t\tfbcon_set_origin(vc);",
          "\t\tif (con_is_visible(vc) && softback_buf)",
          "\t\t\tfbcon_update_softback(vc);"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate bounds checking when changing screen size, leading to an out-of-bounds memory write.",
      "trigger_condition": "Changing the screen size triggers the vulnerable code path, potentially causing memory corruption or denial of service.",
      "specific_code_behavior_causing_vulnerability": "The code snippet does not perform proper bounds checking when resizing the screen, which can result in an out-of-bounds memory write, leading to memory corruption or denial of service. Specifically, the vulnerability arises from not ensuring that the memory write operation stays within the allocated bounds during screen size changes.",
      "id": 82,
      "code_after_change_normalized": "static int FUN1(struct vc_data *VAR1, int VAR2, int VAR3,\nconst VAR4 * VAR5, int VAR6)\n{\nstruct fb_info *VAR7 = VAR8[VAR9[VAR1->VAR10]];\nstruct fbcon_ops *VAR11 = VAR7->VAR12;\nstruct fbcon_display *VAR13 = &VAR14[VAR1->VAR10];\nint VAR15;\nint VAR16;\nchar *VAR17 = NULL;\nVAR15 = (VAR2 != VAR1->VAR18.VAR19) || (VAR3 != VAR1->VAR18.VAR20);\nif (VAR13->VAR6)\nVAR17 = VAR1->VAR18.VAR5;\nif (VAR6)\nVAR16 = FUN2(VAR5);\nelse\nVAR16 = 256;\nVAR1->VAR18.VAR5 = (void *)(VAR13->VAR21 = VAR5);\nif ((VAR13->VAR6 = VAR6))\nFUN3(VAR5)++;\nVAR1->VAR18.VAR19 = VAR2;\nVAR1->VAR18.VAR20 = VAR3;\nif (VAR1->VAR22 && VAR16 == 256)\nFUN4(VAR1, false);\nelse if (!VAR1->VAR22 && VAR16 == 512)\nFUN4(VAR1, true);\nif (VAR15) {\nint VAR23, VAR24;\nVAR23 = FUN5(VAR11->VAR25, VAR7->VAR26.VAR27, VAR7->VAR26.VAR28);\nVAR24 = FUN5(VAR11->VAR25, VAR7->VAR26.VAR28, VAR7->VAR26.VAR27);\nVAR23 /= VAR2;\nVAR24 /= VAR3;\nFUN6(VAR1, VAR23, VAR24);\n} else if (FUN7(VAR1)\n&& VAR1->VAR29 == VAR30) {\nFUN8(VAR1, 0);\nFUN9(VAR1);\n}\nif (VAR17 && (--FUN3(VAR17) == 0))\nFUN10(VAR17 - VAR31 * sizeof(int));\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct vc_data *VAR1, int VAR2, int VAR3,\nconst VAR4 * VAR5, int VAR6)\n{\nstruct fb_info *VAR7 = VAR8[VAR9[VAR1->VAR10]];\nstruct fbcon_ops *VAR11 = VAR7->VAR12;\nstruct fbcon_display *VAR13 = &VAR14[VAR1->VAR10];\nint VAR15;\nint VAR16;\nchar *VAR17 = NULL;\nif (FUN2(VAR1) && VAR18)\nFUN3(VAR1);\nVAR15 = (VAR2 != VAR1->VAR19.VAR20) || (VAR3 != VAR1->VAR19.VAR21);\nif (VAR13->VAR6)\nVAR17 = VAR1->VAR19.VAR5;\nif (VAR6)\nVAR16 = FUN4(VAR5);\nelse\nVAR16 = 256;\nVAR1->VAR19.VAR5 = (void *)(VAR13->VAR22 = VAR5);\nif ((VAR13->VAR6 = VAR6))\nFUN5(VAR5)++;\nVAR1->VAR19.VAR20 = VAR2;\nVAR1->VAR19.VAR21 = VAR3;\nif (VAR1->VAR23 && VAR16 == 256)\nFUN6(VAR1, false);\nelse if (!VAR1->VAR23 && VAR16 == 512)\nFUN6(VAR1, true);\nif (VAR15) {\nint VAR24, VAR25;\nVAR24 = FUN7(VAR11->VAR26, VAR7->VAR27.VAR28, VAR7->VAR27.VAR29);\nVAR25 = FUN7(VAR11->VAR26, VAR7->VAR27.VAR29, VAR7->VAR27.VAR28);\nVAR24 /= VAR2;\nVAR25 /= VAR3;\nFUN8(VAR1, VAR24, VAR25);\nif (FUN2(VAR1) && VAR30)\nFUN9(VAR1);\n} else if (FUN2(VAR1)\n&& VAR1->VAR31 == VAR32) {\nFUN10(VAR1, 0);\nFUN11(VAR1);\n}\nif (VAR17 && (--FUN5(VAR17) == 0))\nFUN12(VAR17 - VAR33 * sizeof(int));\nreturn 0;\n}\n",
      "code_after_change_raw": "static int fbcon_do_set_font(struct vc_data *vc, int w, int h,\nconst u8 * data, int userfont)\n{\nstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\nstruct fbcon_ops *ops = info->fbcon_par;\nstruct fbcon_display *p = &fb_display[vc->vc_num];\nint resize;\nint cnt;\nchar *old_data = NULL;\nresize = (w != vc->vc_font.width) || (h != vc->vc_font.height);\nif (p->userfont)\nold_data = vc->vc_font.data;\nif (userfont)\ncnt = FNTCHARCNT(data);\nelse\ncnt = 256;\nvc->vc_font.data = (void *)(p->fontdata = data);\nif ((p->userfont = userfont))\nREFCOUNT(data)++;\nvc->vc_font.width = w;\nvc->vc_font.height = h;\nif (vc->vc_hi_font_mask && cnt == 256)\nset_vc_hi_font(vc, false);\nelse if (!vc->vc_hi_font_mask && cnt == 512)\nset_vc_hi_font(vc, true);\nif (resize) {\nint cols, rows;\ncols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\nrows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\ncols /= w;\nrows /= h;\nvc_resize(vc, cols, rows);\n} else if (con_is_visible(vc)\n&& vc->vc_mode == KD_TEXT) {\nfbcon_clear_margins(vc, 0);\nupdate_screen(vc);\n}\nif (old_data && (--REFCOUNT(old_data) == 0))\nkfree(old_data - FONT_EXTRA_WORDS * sizeof(int));\nreturn 0;\n}\n",
      "code_before_change_raw": "static int fbcon_do_set_font(struct vc_data *vc, int w, int h,\nconst u8 * data, int userfont)\n{\nstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\nstruct fbcon_ops *ops = info->fbcon_par;\nstruct fbcon_display *p = &fb_display[vc->vc_num];\nint resize;\nint cnt;\nchar *old_data = NULL;\nif (con_is_visible(vc) && softback_lines)\nfbcon_set_origin(vc);\nresize = (w != vc->vc_font.width) || (h != vc->vc_font.height);\nif (p->userfont)\nold_data = vc->vc_font.data;\nif (userfont)\ncnt = FNTCHARCNT(data);\nelse\ncnt = 256;\nvc->vc_font.data = (void *)(p->fontdata = data);\nif ((p->userfont = userfont))\nREFCOUNT(data)++;\nvc->vc_font.width = w;\nvc->vc_font.height = h;\nif (vc->vc_hi_font_mask && cnt == 256)\nset_vc_hi_font(vc, false);\nelse if (!vc->vc_hi_font_mask && cnt == 512)\nset_vc_hi_font(vc, true);\nif (resize) {\nint cols, rows;\ncols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\nrows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\ncols /= w;\nrows /= h;\nvc_resize(vc, cols, rows);\nif (con_is_visible(vc) && softback_buf)\nfbcon_update_softback(vc);\n} else if (con_is_visible(vc)\n&& vc->vc_mode == KD_TEXT) {\nfbcon_clear_margins(vc, 0);\nupdate_screen(vc);\n}\nif (old_data && (--REFCOUNT(old_data) == 0))\nkfree(old_data - FONT_EXTRA_WORDS * sizeof(int));\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking for a variable used in a memory write operation.",
        "trigger_condition": "Changing screen size triggers the vulnerable code path, leading to an out-of-bounds memory write.",
        "specific_code_behavior_causing_vulnerability": "The code uses a variable without proper bounds checking in a memory write operation, potentially allowing writing outside the allocated memory region when changing screen size."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking for the variable used in the memory write operation. In this case, the solution involves replacing the variable with a constant value to prevent out-of-bounds memory writes and potential memory corruption.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write leading to memory corruption or a denial of service when changing screen size. The vulnerability occurs due to the use of the 'softback_lines' variable without proper bounds checking, which can result in writing outside the allocated memory region.\n\nIn the modified code snippet, the 'softback_lines' variable has been replaced with a constant value of 0 in the 'ops->cursor' function call. By doing this, the vulnerability related to the out-of-bounds memory write is mitigated, as the 'softback_lines' variable is no longer used in a way that could lead to memory corruption.\n\nIt is important to always ensure proper bounds checking and handling of memory operations to prevent vulnerabilities like out-of-bounds writes, which can be exploited for malicious purposes.",
      "GPT_purpose": "Flash the cursor on the framebuffer console.",
      "GPT_function": "\n1. Flash the cursor on the framebuffer.\n2. Check for console lock availability.\n3. Retrieve the current virtual console data.\n4. Check conditions for flashing the cursor.\n5. Determine the cursor mode (erase or draw).\n6. Update the cursor appearance on the screen.\n7. Release the console lock.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static void fb_flashcursor(struct work_struct *work)\n{\n\tstruct fb_info *info = container_of(work, struct fb_info, queue);\n\tstruct fbcon_ops *ops = info->fbcon_par;\n\tstruct vc_data *vc = NULL;\n\tint c;\n\tint mode;\n\tint ret;\n\n\t/* FIXME: we should sort out the unbind locking instead */\n\t/* instead we just fail to flash the cursor if we can't get\n\t * the lock instead of blocking fbcon deinit */\n\tret = console_trylock();\n\tif (ret == 0)\n\t\treturn;\n\n\tif (ops && ops->currcon != -1)\n\t\tvc = vc_cons[ops->currcon].d;\n\n\tif (!vc || !con_is_visible(vc) ||\n \t    registered_fb[con2fb_map[vc->vc_num]] != info ||\n\t    vc->vc_deccm != 1) {\n\t\tconsole_unlock();\n\t\treturn;\n\t}\n\n\tc = scr_readw((u16 *) vc->vc_pos);\n\tmode = (!ops->cursor_flash || ops->cursor_state.enable) ?\n\t\tCM_ERASE : CM_DRAW;\n\tops->cursor(vc, info, mode, softback_lines, get_color(vc, info, c, 1),\n\t\t    get_color(vc, info, c, 0));\n\tconsole_unlock();\n}",
      "code_after_change": "static void fb_flashcursor(struct work_struct *work)\n{\n\tstruct fb_info *info = container_of(work, struct fb_info, queue);\n\tstruct fbcon_ops *ops = info->fbcon_par;\n\tstruct vc_data *vc = NULL;\n\tint c;\n\tint mode;\n\tint ret;\n\n\t/* FIXME: we should sort out the unbind locking instead */\n\t/* instead we just fail to flash the cursor if we can't get\n\t * the lock instead of blocking fbcon deinit */\n\tret = console_trylock();\n\tif (ret == 0)\n\t\treturn;\n\n\tif (ops && ops->currcon != -1)\n\t\tvc = vc_cons[ops->currcon].d;\n\n\tif (!vc || !con_is_visible(vc) ||\n \t    registered_fb[con2fb_map[vc->vc_num]] != info ||\n\t    vc->vc_deccm != 1) {\n\t\tconsole_unlock();\n\t\treturn;\n\t}\n\n\tc = scr_readw((u16 *) vc->vc_pos);\n\tmode = (!ops->cursor_flash || ops->cursor_state.enable) ?\n\t\tCM_ERASE : CM_DRAW;\n\tops->cursor(vc, info, mode, 0, get_color(vc, info, c, 1),\n\t\t    get_color(vc, info, c, 0));\n\tconsole_unlock();\n}",
      "modified_lines": {
        "added": [
          "\tops->cursor(vc, info, mode, 0, get_color(vc, info, c, 1),"
        ],
        "deleted": [
          "\tops->cursor(vc, info, mode, softback_lines, get_color(vc, info, c, 1),"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking for a variable used in a memory write operation.",
      "trigger_condition": "Changing screen size triggers the vulnerable code path, leading to an out-of-bounds memory write.",
      "specific_code_behavior_causing_vulnerability": "The code uses a variable without proper bounds checking in a memory write operation, potentially allowing writing outside the allocated memory region when changing screen size.",
      "id": 83,
      "code_after_change_normalized": "static void FUN1(struct work_struct *VAR1)\n{\nstruct VAR3 *VAR2 = FUN2(VAR1, struct VAR3, VAR4);\nstruct fbcon_ops *VAR5 = VAR2->VAR6;\nstruct vc_data *VAR7 = NULL;\nint VAR8;\nint VAR9;\nint VAR10;\nVAR10 = FUN3();\nif (VAR10 == 0)\nreturn;\nif (VAR5 && VAR5->VAR11 != -1)\nVAR7 = VAR12[VAR5->VAR11].VAR13;\nif (!VAR7 || !FUN4(VAR7) ||\nVAR14[VAR15[VAR7->VAR16]] != VAR2 ||\nVAR7->VAR17 != 1) {\nFUN5();\nreturn;\n}\nVAR8 = FUN6((VAR18 *) VAR7->VAR19);\nVAR9 = (!VAR5->VAR20 || VAR5->VAR21.VAR22) ?\nVAR23 : VAR24;\nVAR5->FUN7(VAR7, VAR2, VAR9, 0, FUN8(VAR7, VAR2, VAR8, 1),\nFUN8(VAR7, VAR2, VAR8, 0));\nFUN5();\n}\n",
      "code_before_change_normalized": "static void FUN1(struct work_struct *VAR1)\n{\nstruct VAR3 *VAR2 = FUN2(VAR1, struct VAR3, VAR4);\nstruct fbcon_ops *VAR5 = VAR2->VAR6;\nstruct vc_data *VAR7 = NULL;\nint VAR8;\nint VAR9;\nint VAR10;\nVAR10 = FUN3();\nif (VAR10 == 0)\nreturn;\nif (VAR5 && VAR5->VAR11 != -1)\nVAR7 = VAR12[VAR5->VAR11].VAR13;\nif (!VAR7 || !FUN4(VAR7) ||\nVAR14[VAR15[VAR7->VAR16]] != VAR2 ||\nVAR7->VAR17 != 1) {\nFUN5();\nreturn;\n}\nVAR8 = FUN6((VAR18 *) VAR7->VAR19);\nVAR9 = (!VAR5->VAR20 || VAR5->VAR21.VAR22) ?\nVAR23 : VAR24;\nVAR5->FUN7(VAR7, VAR2, VAR9, VAR25, FUN8(VAR7, VAR2, VAR8, 1),\nFUN8(VAR7, VAR2, VAR8, 0));\nFUN5();\n}\n",
      "code_after_change_raw": "static void fb_flashcursor(struct work_struct *work)\n{\nstruct fb_info *info = container_of(work, struct fb_info, queue);\nstruct fbcon_ops *ops = info->fbcon_par;\nstruct vc_data *vc = NULL;\nint c;\nint mode;\nint ret;\nret = console_trylock();\nif (ret == 0)\nreturn;\nif (ops && ops->currcon != -1)\nvc = vc_cons[ops->currcon].d;\nif (!vc || !con_is_visible(vc) ||\nregistered_fb[con2fb_map[vc->vc_num]] != info ||\nvc->vc_deccm != 1) {\nconsole_unlock();\nreturn;\n}\nc = scr_readw((u16 *) vc->vc_pos);\nmode = (!ops->cursor_flash || ops->cursor_state.enable) ?\nCM_ERASE : CM_DRAW;\nops->cursor(vc, info, mode, 0, get_color(vc, info, c, 1),\nget_color(vc, info, c, 0));\nconsole_unlock();\n}\n",
      "code_before_change_raw": "static void fb_flashcursor(struct work_struct *work)\n{\nstruct fb_info *info = container_of(work, struct fb_info, queue);\nstruct fbcon_ops *ops = info->fbcon_par;\nstruct vc_data *vc = NULL;\nint c;\nint mode;\nint ret;\nret = console_trylock();\nif (ret == 0)\nreturn;\nif (ops && ops->currcon != -1)\nvc = vc_cons[ops->currcon].d;\nif (!vc || !con_is_visible(vc) ||\nregistered_fb[con2fb_map[vc->vc_num]] != info ||\nvc->vc_deccm != 1) {\nconsole_unlock();\nreturn;\n}\nc = scr_readw((u16 *) vc->vc_pos);\nmode = (!ops->cursor_flash || ops->cursor_state.enable) ?\nCM_ERASE : CM_DRAW;\nops->cursor(vc, info, mode, softback_lines, get_color(vc, info, c, 1),\nget_color(vc, info, c, 0));\nconsole_unlock();\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking during screen size change operation.",
        "trigger_condition": "When changing the screen size, an out-of-bounds memory write can occur due to improper handling of the new dimensions, leading to memory corruption or denial of service.",
        "specific_code_behavior_causing_vulnerability": "The code does not adequately validate the new screen size dimensions before performing calculations and memory operations, potentially allowing the write operation to access memory outside the allocated bounds."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking and validation of the new screen size dimensions before proceeding with memory operations. This involves verifying that the calculated columns and rows based on the new dimensions do not exceed the allocated memory space. By adding appropriate checks and bounds validation, the code can prevent out-of-bounds memory writes and enhance the security of the system.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write issue that can lead to memory corruption or a denial of service when changing the screen size. By making the specified changes, you can prevent this vulnerability from being exploited and ensure the security and stability of the system.",
      "GPT_purpose": "Update the display mode for the framebuffer console when the screen size changes.",
      "GPT_function": "\n1. Check for valid fbcon operations and current console.\n2. Update display settings based on the current console's mode.\n3. Resize the virtual console based on the display dimensions.\n4. Update scroll mode and reset scrollback settings.\n5. Set palette colors and update the screen.\n6. Update softback buffer if present.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static void fbcon_modechanged(struct fb_info *info)\n{\n\tstruct fbcon_ops *ops = info->fbcon_par;\n\tstruct vc_data *vc;\n\tstruct fbcon_display *p;\n\tint rows, cols;\n\n\tif (!ops || ops->currcon < 0)\n\t\treturn;\n\tvc = vc_cons[ops->currcon].d;\n\tif (vc->vc_mode != KD_TEXT ||\n\t    registered_fb[con2fb_map[ops->currcon]] != info)\n\t\treturn;\n\n\tp = &fb_display[vc->vc_num];\n\tset_blitting_type(vc, info);\n\n\tif (con_is_visible(vc)) {\n\t\tvar_to_display(p, &info->var, info);\n\t\tcols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\n\t\trows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\n\t\tcols /= vc->vc_font.width;\n\t\trows /= vc->vc_font.height;\n\t\tvc_resize(vc, cols, rows);\n\t\tupdatescrollmode(p, info, vc);\n\t\tscrollback_max = 0;\n\t\tscrollback_current = 0;\n\n\t\tif (!fbcon_is_inactive(vc, info)) {\n\t\t    ops->var.xoffset = ops->var.yoffset = p->yscroll = 0;\n\t\t    ops->update_start(info);\n\t\t}\n\n\t\tfbcon_set_palette(vc, color_table);\n\t\tupdate_screen(vc);\n\t\tif (softback_buf)\n\t\t\tfbcon_update_softback(vc);\n\t}\n}",
      "code_after_change": "static void fbcon_modechanged(struct fb_info *info)\n{\n\tstruct fbcon_ops *ops = info->fbcon_par;\n\tstruct vc_data *vc;\n\tstruct fbcon_display *p;\n\tint rows, cols;\n\n\tif (!ops || ops->currcon < 0)\n\t\treturn;\n\tvc = vc_cons[ops->currcon].d;\n\tif (vc->vc_mode != KD_TEXT ||\n\t    registered_fb[con2fb_map[ops->currcon]] != info)\n\t\treturn;\n\n\tp = &fb_display[vc->vc_num];\n\tset_blitting_type(vc, info);\n\n\tif (con_is_visible(vc)) {\n\t\tvar_to_display(p, &info->var, info);\n\t\tcols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\n\t\trows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\n\t\tcols /= vc->vc_font.width;\n\t\trows /= vc->vc_font.height;\n\t\tvc_resize(vc, cols, rows);\n\t\tupdatescrollmode(p, info, vc);\n\t\tscrollback_max = 0;\n\t\tscrollback_current = 0;\n\n\t\tif (!fbcon_is_inactive(vc, info)) {\n\t\t    ops->var.xoffset = ops->var.yoffset = p->yscroll = 0;\n\t\t    ops->update_start(info);\n\t\t}\n\n\t\tfbcon_set_palette(vc, color_table);\n\t\tupdate_screen(vc);\n\t}\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\t\tif (softback_buf)",
          "\t\t\tfbcon_update_softback(vc);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking during screen size change operation.",
      "trigger_condition": "When changing the screen size, an out-of-bounds memory write can occur due to improper handling of the new dimensions, leading to memory corruption or denial of service.",
      "specific_code_behavior_causing_vulnerability": "The code does not adequately validate the new screen size dimensions before performing calculations and memory operations, potentially allowing the write operation to access memory outside the allocated bounds.",
      "id": 84,
      "code_after_change_normalized": "static void FUN1(struct fb_info *VAR1)\n{\nstruct fbcon_ops *VAR2 = VAR1->VAR3;\nstruct vc_data *VAR4;\nstruct fbcon_display *VAR5;\nint VAR6, VAR7;\nif (!VAR2 || VAR2->VAR8 < 0)\nreturn;\nVAR4 = VAR9[VAR2->VAR8].VAR10;\nif (VAR4->VAR11 != VAR12 ||\nVAR13[VAR14[VAR2->VAR8]] != VAR1)\nreturn;\nVAR5 = &VAR15[VAR4->VAR16];\nFUN2(VAR4, VAR1);\nif (FUN3(VAR4)) {\nFUN4(VAR5, &VAR1->VAR17, VAR1);\nVAR7 = FUN5(VAR2->VAR18, VAR1->VAR17.VAR19, VAR1->VAR17.VAR20);\nVAR6 = FUN5(VAR2->VAR18, VAR1->VAR17.VAR20, VAR1->VAR17.VAR19);\nVAR7 /= VAR4->VAR21.VAR22;\nVAR6 /= VAR4->VAR21.VAR23;\nFUN6(VAR4, VAR7, VAR6);\nFUN7(VAR5, VAR1, VAR4);\nVAR24 = 0;\nVAR25 = 0;\nif (!FUN8(VAR4, VAR1)) {\nVAR2->VAR17.VAR26 = VAR2->VAR17.VAR27 = VAR5->VAR28 = 0;\nVAR2->FUN9(VAR1);\n}\nFUN10(VAR4, VAR29);\nFUN11(VAR4);\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct fb_info *VAR1)\n{\nstruct fbcon_ops *VAR2 = VAR1->VAR3;\nstruct vc_data *VAR4;\nstruct fbcon_display *VAR5;\nint VAR6, VAR7;\nif (!VAR2 || VAR2->VAR8 < 0)\nreturn;\nVAR4 = VAR9[VAR2->VAR8].VAR10;\nif (VAR4->VAR11 != VAR12 ||\nVAR13[VAR14[VAR2->VAR8]] != VAR1)\nreturn;\nVAR5 = &VAR15[VAR4->VAR16];\nFUN2(VAR4, VAR1);\nif (FUN3(VAR4)) {\nFUN4(VAR5, &VAR1->VAR17, VAR1);\nVAR7 = FUN5(VAR2->VAR18, VAR1->VAR17.VAR19, VAR1->VAR17.VAR20);\nVAR6 = FUN5(VAR2->VAR18, VAR1->VAR17.VAR20, VAR1->VAR17.VAR19);\nVAR7 /= VAR4->VAR21.VAR22;\nVAR6 /= VAR4->VAR21.VAR23;\nFUN6(VAR4, VAR7, VAR6);\nFUN7(VAR5, VAR1, VAR4);\nVAR24 = 0;\nVAR25 = 0;\nif (!FUN8(VAR4, VAR1)) {\nVAR2->VAR17.VAR26 = VAR2->VAR17.VAR27 = VAR5->VAR28 = 0;\nVAR2->FUN9(VAR1);\n}\nFUN10(VAR4, VAR29);\nFUN11(VAR4);\nif (VAR30)\nFUN12(VAR4);\n}\n}\n",
      "code_after_change_raw": "static void fbcon_modechanged(struct fb_info *info)\n{\nstruct fbcon_ops *ops = info->fbcon_par;\nstruct vc_data *vc;\nstruct fbcon_display *p;\nint rows, cols;\nif (!ops || ops->currcon < 0)\nreturn;\nvc = vc_cons[ops->currcon].d;\nif (vc->vc_mode != KD_TEXT ||\nregistered_fb[con2fb_map[ops->currcon]] != info)\nreturn;\np = &fb_display[vc->vc_num];\nset_blitting_type(vc, info);\nif (con_is_visible(vc)) {\nvar_to_display(p, &info->var, info);\ncols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\nrows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\ncols /= vc->vc_font.width;\nrows /= vc->vc_font.height;\nvc_resize(vc, cols, rows);\nupdatescrollmode(p, info, vc);\nscrollback_max = 0;\nscrollback_current = 0;\nif (!fbcon_is_inactive(vc, info)) {\nops->var.xoffset = ops->var.yoffset = p->yscroll = 0;\nops->update_start(info);\n}\nfbcon_set_palette(vc, color_table);\nupdate_screen(vc);\n}\n}\n",
      "code_before_change_raw": "static void fbcon_modechanged(struct fb_info *info)\n{\nstruct fbcon_ops *ops = info->fbcon_par;\nstruct vc_data *vc;\nstruct fbcon_display *p;\nint rows, cols;\nif (!ops || ops->currcon < 0)\nreturn;\nvc = vc_cons[ops->currcon].d;\nif (vc->vc_mode != KD_TEXT ||\nregistered_fb[con2fb_map[ops->currcon]] != info)\nreturn;\np = &fb_display[vc->vc_num];\nset_blitting_type(vc, info);\nif (con_is_visible(vc)) {\nvar_to_display(p, &info->var, info);\ncols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\nrows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\ncols /= vc->vc_font.width;\nrows /= vc->vc_font.height;\nvc_resize(vc, cols, rows);\nupdatescrollmode(p, info, vc);\nscrollback_max = 0;\nscrollback_current = 0;\nif (!fbcon_is_inactive(vc, info)) {\nops->var.xoffset = ops->var.yoffset = p->yscroll = 0;\nops->update_start(info);\n}\nfbcon_set_palette(vc, color_table);\nupdate_screen(vc);\nif (softback_buf)\nfbcon_update_softback(vc);\n}\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking and validation for user-supplied input parameters.",
        "trigger_condition": "When processing user-provided options, the code does not properly validate the input length, leading to an out-of-bounds memory write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code iterates over user-supplied options without verifying the length of each option, allowing a potential buffer overflow if the input exceeds the expected size."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add proper bounds checking and validation for user-supplied input parameters. Specifically, when processing options, ensure that the length of each option is within the expected boundaries to prevent an out-of-bounds memory write. In this case, the solution involves validating the length of each option before performing any operations on it to prevent buffer overflows.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write issue that can lead to memory corruption or denial of service when changing the screen size. By adding the line `pr_warn(\"Ignoring scrollback size option\\n\");` and removing the vulnerable code block related to the \"scrollback\" option, the vulnerable code path is effectively disabled. This modification prevents the out-of-bounds memory write from occurring and helps mitigate the risk of memory corruption or denial of service attacks.",
      "GPT_purpose": "Parse and set various framebuffer console options during initialization.",
      "GPT_function": "\n1. Parse options provided in the input string.\n2. Handle different options such as setting font, scrollback size, console mapping, virtual console settings, rotation, margin color, logo position, and logo count.\n3. Check for a specific option related to deferred takeover in the framebuffer console configuration.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static int __init fb_console_setup(char *this_opt)\n{\n\tchar *options;\n\tint i, j;\n\n\tif (!this_opt || !*this_opt)\n\t\treturn 1;\n\n\twhile ((options = strsep(&this_opt, \",\")) != NULL) {\n\t\tif (!strncmp(options, \"font:\", 5)) {\n\t\t\tstrlcpy(fontname, options + 5, sizeof(fontname));\n\t\t\tcontinue;\n\t\t}\n\t\t\n\t\tif (!strncmp(options, \"scrollback:\", 11)) {\n\t\t\toptions += 11;\n\t\t\tif (*options) {\n\t\t\t\tfbcon_softback_size = simple_strtoul(options, &options, 0);\n\t\t\t\tif (*options == 'k' || *options == 'K') {\n\t\t\t\t\tfbcon_softback_size *= 1024;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\t\t\n\t\tif (!strncmp(options, \"map:\", 4)) {\n\t\t\toptions += 4;\n\t\t\tif (*options) {\n\t\t\t\tfor (i = 0, j = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\t\tif (!options[j])\n\t\t\t\t\t\tj = 0;\n\t\t\t\t\tcon2fb_map_boot[i] =\n\t\t\t\t\t\t(options[j++]-'0') % FB_MAX;\n\t\t\t\t}\n\n\t\t\t\tfbcon_map_override();\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"vc:\", 3)) {\n\t\t\toptions += 3;\n\t\t\tif (*options)\n\t\t\t\tfirst_fb_vc = simple_strtoul(options, &options, 10) - 1;\n\t\t\tif (first_fb_vc < 0)\n\t\t\t\tfirst_fb_vc = 0;\n\t\t\tif (*options++ == '-')\n\t\t\t\tlast_fb_vc = simple_strtoul(options, &options, 10) - 1;\n\t\t\tfbcon_is_default = 0; \n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"rotate:\", 7)) {\n\t\t\toptions += 7;\n\t\t\tif (*options)\n\t\t\t\tinitial_rotation = simple_strtoul(options, &options, 0);\n\t\t\tif (initial_rotation > 3)\n\t\t\t\tinitial_rotation = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"margin:\", 7)) {\n\t\t\toptions += 7;\n\t\t\tif (*options)\n\t\t\t\tmargin_color = simple_strtoul(options, &options, 0);\n\t\t\tcontinue;\n\t\t}\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\n\t\tif (!strcmp(options, \"nodefer\")) {\n\t\t\tdeferred_takeover = false;\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\tif (!strncmp(options, \"logo-pos:\", 9)) {\n\t\t\toptions += 9;\n\t\t\tif (!strcmp(options, \"center\"))\n\t\t\t\tfb_center_logo = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"logo-count:\", 11)) {\n\t\t\toptions += 11;\n\t\t\tif (*options)\n\t\t\t\tfb_logo_count = simple_strtol(options, &options, 0);\n\t\t\tcontinue;\n\t\t}\n\t}\n\treturn 1;\n}",
      "code_after_change": "static int __init fb_console_setup(char *this_opt)\n{\n\tchar *options;\n\tint i, j;\n\n\tif (!this_opt || !*this_opt)\n\t\treturn 1;\n\n\twhile ((options = strsep(&this_opt, \",\")) != NULL) {\n\t\tif (!strncmp(options, \"font:\", 5)) {\n\t\t\tstrlcpy(fontname, options + 5, sizeof(fontname));\n\t\t\tcontinue;\n\t\t}\n\t\t\n\t\tif (!strncmp(options, \"scrollback:\", 11)) {\n\t\t\tpr_warn(\"Ignoring scrollback size option\\n\");\n\t\t\tcontinue;\n\t\t}\n\t\t\n\t\tif (!strncmp(options, \"map:\", 4)) {\n\t\t\toptions += 4;\n\t\t\tif (*options) {\n\t\t\t\tfor (i = 0, j = 0; i < MAX_NR_CONSOLES; i++) {\n\t\t\t\t\tif (!options[j])\n\t\t\t\t\t\tj = 0;\n\t\t\t\t\tcon2fb_map_boot[i] =\n\t\t\t\t\t\t(options[j++]-'0') % FB_MAX;\n\t\t\t\t}\n\n\t\t\t\tfbcon_map_override();\n\t\t\t}\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"vc:\", 3)) {\n\t\t\toptions += 3;\n\t\t\tif (*options)\n\t\t\t\tfirst_fb_vc = simple_strtoul(options, &options, 10) - 1;\n\t\t\tif (first_fb_vc < 0)\n\t\t\t\tfirst_fb_vc = 0;\n\t\t\tif (*options++ == '-')\n\t\t\t\tlast_fb_vc = simple_strtoul(options, &options, 10) - 1;\n\t\t\tfbcon_is_default = 0; \n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"rotate:\", 7)) {\n\t\t\toptions += 7;\n\t\t\tif (*options)\n\t\t\t\tinitial_rotation = simple_strtoul(options, &options, 0);\n\t\t\tif (initial_rotation > 3)\n\t\t\t\tinitial_rotation = 0;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"margin:\", 7)) {\n\t\t\toptions += 7;\n\t\t\tif (*options)\n\t\t\t\tmargin_color = simple_strtoul(options, &options, 0);\n\t\t\tcontinue;\n\t\t}\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\n\t\tif (!strcmp(options, \"nodefer\")) {\n\t\t\tdeferred_takeover = false;\n\t\t\tcontinue;\n\t\t}\n#endif\n\n\t\tif (!strncmp(options, \"logo-pos:\", 9)) {\n\t\t\toptions += 9;\n\t\t\tif (!strcmp(options, \"center\"))\n\t\t\t\tfb_center_logo = true;\n\t\t\tcontinue;\n\t\t}\n\n\t\tif (!strncmp(options, \"logo-count:\", 11)) {\n\t\t\toptions += 11;\n\t\t\tif (*options)\n\t\t\t\tfb_logo_count = simple_strtol(options, &options, 0);\n\t\t\tcontinue;\n\t\t}\n\t}\n\treturn 1;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tpr_warn(\"Ignoring scrollback size option\\n\");"
        ],
        "deleted": [
          "\t\t\toptions += 11;",
          "\t\t\tif (*options) {",
          "\t\t\t\tfbcon_softback_size = simple_strtoul(options, &options, 0);",
          "\t\t\t\tif (*options == 'k' || *options == 'K') {",
          "\t\t\t\t\tfbcon_softback_size *= 1024;",
          "\t\t\t\t}",
          "\t\t\t}"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking and validation for user-supplied input parameters.",
      "trigger_condition": "When processing user-provided options, the code does not properly validate the input length, leading to an out-of-bounds memory write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code iterates over user-supplied options without verifying the length of each option, allowing a potential buffer overflow if the input exceeds the expected size.",
      "id": 85,
      "code_after_change_normalized": "static int __init FUN1(char *VAR1)\n{\nchar *VAR2;\nint VAR3, VAR4;\nif (!VAR1 || !*VAR1)\nreturn 1;\nwhile ((VAR2 = FUN2(&VAR1, \"STR\")) != NULL) {\nif (!FUN3(VAR2, \"STR\", 5)) {\nFUN4(VAR5, VAR2 + 5, sizeof(VAR5));\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 11)) {\nFUN5(\"STR\");\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 4)) {\nVAR2 += 4;\nif (*VAR2) {\nfor (VAR3 = 0, VAR4 = 0; VAR3 < VAR6; VAR3++) {\nif (!VAR2[VAR4])\nVAR4 = 0;\nVAR7[VAR3] =\n(VAR2[VAR4++]-) % VAR8;\n}\nFUN6();\n}\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 3)) {\nVAR2 += 3;\nif (*VAR2)\nVAR9 = FUN7(VAR2, &VAR2, 10) - 1;\nif (VAR9 < 0)\nVAR9 = 0;\nif (*VAR2++ == )\nVAR10 = FUN7(VAR2, &VAR2, 10) - 1;\nVAR11 = 0;\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 7)) {\nVAR2 += 7;\nif (*VAR2)\nVAR12 = FUN7(VAR2, &VAR2, 0);\nif (VAR12 > 3)\nVAR12 = 0;\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 7)) {\nVAR2 += 7;\nif (*VAR2)\nVAR13 = FUN7(VAR2, &VAR2, 0);\ncontinue;\n}\n#ifdef VAR14\nif (!FUN8(VAR2, \"STR\")) {\nVAR15 = false;\ncontinue;\n}\n#VAR16\nif (!FUN3(VAR2, \"STR\", 9)) {\nVAR2 += 9;\nif (!FUN8(VAR2, \"STR\"))\nVAR17 = true;\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 11)) {\nVAR2 += 11;\nif (*VAR2)\nVAR18 = FUN9(VAR2, &VAR2, 0);\ncontinue;\n}\n}\nreturn 1;\n}\n",
      "code_before_change_normalized": "static int __init FUN1(char *VAR1)\n{\nchar *VAR2;\nint VAR3, VAR4;\nif (!VAR1 || !*VAR1)\nreturn 1;\nwhile ((VAR2 = FUN2(&VAR1, \"STR\")) != NULL) {\nif (!FUN3(VAR2, \"STR\", 5)) {\nFUN4(VAR5, VAR2 + 5, sizeof(VAR5));\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 11)) {\nVAR2 += 11;\nif (*VAR2) {\nVAR6 = FUN5(VAR2, &VAR2, 0);\nif (*VAR2 ==  || *VAR2 == ) {\nVAR6 *= 1024;\n}\n}\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 4)) {\nVAR2 += 4;\nif (*VAR2) {\nfor (VAR3 = 0, VAR4 = 0; VAR3 < VAR7; VAR3++) {\nif (!VAR2[VAR4])\nVAR4 = 0;\nVAR8[VAR3] =\n(VAR2[VAR4++]-) % VAR9;\n}\nFUN6();\n}\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 3)) {\nVAR2 += 3;\nif (*VAR2)\nVAR10 = FUN5(VAR2, &VAR2, 10) - 1;\nif (VAR10 < 0)\nVAR10 = 0;\nif (*VAR2++ == )\nVAR11 = FUN5(VAR2, &VAR2, 10) - 1;\nVAR12 = 0;\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 7)) {\nVAR2 += 7;\nif (*VAR2)\nVAR13 = FUN5(VAR2, &VAR2, 0);\nif (VAR13 > 3)\nVAR13 = 0;\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 7)) {\nVAR2 += 7;\nif (*VAR2)\nVAR14 = FUN5(VAR2, &VAR2, 0);\ncontinue;\n}\n#ifdef VAR15\nif (!FUN7(VAR2, \"STR\")) {\nVAR16 = false;\ncontinue;\n}\n#VAR17\nif (!FUN3(VAR2, \"STR\", 9)) {\nVAR2 += 9;\nif (!FUN7(VAR2, \"STR\"))\nVAR18 = true;\ncontinue;\n}\nif (!FUN3(VAR2, \"STR\", 11)) {\nVAR2 += 11;\nif (*VAR2)\nVAR19 = FUN8(VAR2, &VAR2, 0);\ncontinue;\n}\n}\nreturn 1;\n}\n",
      "code_after_change_raw": "static int __init fb_console_setup(char *this_opt)\n{\nchar *options;\nint i, j;\nif (!this_opt || !*this_opt)\nreturn 1;\nwhile ((options = strsep(&this_opt, \",\")) != NULL) {\nif (!strncmp(options, \"font:\", 5)) {\nstrlcpy(fontname, options + 5, sizeof(fontname));\ncontinue;\n}\nif (!strncmp(options, \"scrollback:\", 11)) {\npr_warn(\"Ignoring scrollback size option\\n\");\ncontinue;\n}\nif (!strncmp(options, \"map:\", 4)) {\noptions += 4;\nif (*options) {\nfor (i = 0, j = 0; i < MAX_NR_CONSOLES; i++) {\nif (!options[j])\nj = 0;\ncon2fb_map_boot[i] =\n(options[j++]-'0') % FB_MAX;\n}\nfbcon_map_override();\n}\ncontinue;\n}\nif (!strncmp(options, \"vc:\", 3)) {\noptions += 3;\nif (*options)\nfirst_fb_vc = simple_strtoul(options, &options, 10) - 1;\nif (first_fb_vc < 0)\nfirst_fb_vc = 0;\nif (*options++ == '-')\nlast_fb_vc = simple_strtoul(options, &options, 10) - 1;\nfbcon_is_default = 0;\ncontinue;\n}\nif (!strncmp(options, \"rotate:\", 7)) {\noptions += 7;\nif (*options)\ninitial_rotation = simple_strtoul(options, &options, 0);\nif (initial_rotation > 3)\ninitial_rotation = 0;\ncontinue;\n}\nif (!strncmp(options, \"margin:\", 7)) {\noptions += 7;\nif (*options)\nmargin_color = simple_strtoul(options, &options, 0);\ncontinue;\n}\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\nif (!strcmp(options, \"nodefer\")) {\ndeferred_takeover = false;\ncontinue;\n}\n#endif\nif (!strncmp(options, \"logo-pos:\", 9)) {\noptions += 9;\nif (!strcmp(options, \"center\"))\nfb_center_logo = true;\ncontinue;\n}\nif (!strncmp(options, \"logo-count:\", 11)) {\noptions += 11;\nif (*options)\nfb_logo_count = simple_strtol(options, &options, 0);\ncontinue;\n}\n}\nreturn 1;\n}\n",
      "code_before_change_raw": "static int __init fb_console_setup(char *this_opt)\n{\nchar *options;\nint i, j;\nif (!this_opt || !*this_opt)\nreturn 1;\nwhile ((options = strsep(&this_opt, \",\")) != NULL) {\nif (!strncmp(options, \"font:\", 5)) {\nstrlcpy(fontname, options + 5, sizeof(fontname));\ncontinue;\n}\nif (!strncmp(options, \"scrollback:\", 11)) {\noptions += 11;\nif (*options) {\nfbcon_softback_size = simple_strtoul(options, &options, 0);\nif (*options == 'k' || *options == 'K') {\nfbcon_softback_size *= 1024;\n}\n}\ncontinue;\n}\nif (!strncmp(options, \"map:\", 4)) {\noptions += 4;\nif (*options) {\nfor (i = 0, j = 0; i < MAX_NR_CONSOLES; i++) {\nif (!options[j])\nj = 0;\ncon2fb_map_boot[i] =\n(options[j++]-'0') % FB_MAX;\n}\nfbcon_map_override();\n}\ncontinue;\n}\nif (!strncmp(options, \"vc:\", 3)) {\noptions += 3;\nif (*options)\nfirst_fb_vc = simple_strtoul(options, &options, 10) - 1;\nif (first_fb_vc < 0)\nfirst_fb_vc = 0;\nif (*options++ == '-')\nlast_fb_vc = simple_strtoul(options, &options, 10) - 1;\nfbcon_is_default = 0;\ncontinue;\n}\nif (!strncmp(options, \"rotate:\", 7)) {\noptions += 7;\nif (*options)\ninitial_rotation = simple_strtoul(options, &options, 0);\nif (initial_rotation > 3)\ninitial_rotation = 0;\ncontinue;\n}\nif (!strncmp(options, \"margin:\", 7)) {\noptions += 7;\nif (*options)\nmargin_color = simple_strtoul(options, &options, 0);\ncontinue;\n}\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\nif (!strcmp(options, \"nodefer\")) {\ndeferred_takeover = false;\ncontinue;\n}\n#endif\nif (!strncmp(options, \"logo-pos:\", 9)) {\noptions += 9;\nif (!strcmp(options, \"center\"))\nfb_center_logo = true;\ncontinue;\n}\nif (!strncmp(options, \"logo-count:\", 11)) {\noptions += 11;\nif (*options)\nfb_logo_count = simple_strtol(options, &options, 0);\ncontinue;\n}\n}\nreturn 1;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate bounds checking when changing screen size can lead to an out-of-bounds memory write.",
        "trigger_condition": "When the screen size is changed, the code performs calculations without proper validation, potentially causing the out-of-bounds memory write.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly validate the calculations for new screen size dimensions, leading to potential out-of-bounds memory writes and memory corruption."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking and validation of calculations when changing the screen size. This involves validating the new screen size dimensions to prevent out-of-bounds memory writes. In this case, the solution requires adding bounds checking and validation checks to the code that calculates the new screen size dimensions.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write that can lead to memory corruption or a denial of service when changing screen size. By making the necessary changes to the code, you can prevent this vulnerability from being exploited and ensure the security and stability of the system.",
      "GPT_purpose": "Initialize the framebuffer console for a virtual console and handle various display settings.",
      "GPT_function": "\n1. Initialize the framebuffer console (fbcon) for a virtual console.\n2. Set up font data for the console.\n3. Handle console rotation and blitting type.\n4. Update console size and mode based on the framebuffer information.\n5. Prepare logo display and update softback buffer if needed.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static void fbcon_init(struct vc_data *vc, int init)\n{\n\tstruct fb_info *info;\n\tstruct fbcon_ops *ops;\n\tstruct vc_data **default_mode = vc->vc_display_fg;\n\tstruct vc_data *svc = *default_mode;\n\tstruct fbcon_display *t, *p = &fb_display[vc->vc_num];\n\tint logo = 1, new_rows, new_cols, rows, cols, charcnt = 256;\n\tint cap, ret;\n\n\tif (WARN_ON(info_idx == -1))\n\t    return;\n\n\tif (con2fb_map[vc->vc_num] == -1)\n\t\tcon2fb_map[vc->vc_num] = info_idx;\n\n\tinfo = registered_fb[con2fb_map[vc->vc_num]];\n\tcap = info->flags;\n\n\tif (logo_shown < 0 && console_loglevel <= CONSOLE_LOGLEVEL_QUIET)\n\t\tlogo_shown = FBCON_LOGO_DONTSHOW;\n\n\tif (vc != svc || logo_shown == FBCON_LOGO_DONTSHOW ||\n\t    (info->fix.type == FB_TYPE_TEXT))\n\t\tlogo = 0;\n\n\tif (var_to_display(p, &info->var, info))\n\t\treturn;\n\n\tif (!info->fbcon_par)\n\t\tcon2fb_acquire_newinfo(vc, info, vc->vc_num, -1);\n\n\t/* If we are not the first console on this\n\t   fb, copy the font from that console */\n\tt = &fb_display[fg_console];\n\tif (!p->fontdata) {\n\t\tif (t->fontdata) {\n\t\t\tstruct vc_data *fvc = vc_cons[fg_console].d;\n\n\t\t\tvc->vc_font.data = (void *)(p->fontdata =\n\t\t\t\t\t\t    fvc->vc_font.data);\n\t\t\tvc->vc_font.width = fvc->vc_font.width;\n\t\t\tvc->vc_font.height = fvc->vc_font.height;\n\t\t\tp->userfont = t->userfont;\n\n\t\t\tif (p->userfont)\n\t\t\t\tREFCOUNT(p->fontdata)++;\n\t\t} else {\n\t\t\tconst struct font_desc *font = NULL;\n\n\t\t\tif (!fontname[0] || !(font = find_font(fontname)))\n\t\t\t\tfont = get_default_font(info->var.xres,\n\t\t\t\t\t\t\tinfo->var.yres,\n\t\t\t\t\t\t\tinfo->pixmap.blit_x,\n\t\t\t\t\t\t\tinfo->pixmap.blit_y);\n\t\t\tvc->vc_font.width = font->width;\n\t\t\tvc->vc_font.height = font->height;\n\t\t\tvc->vc_font.data = (void *)(p->fontdata = font->data);\n\t\t\tvc->vc_font.charcount = 256; /* FIXME  Need to\n\t\t\t\t\t\t\tsupport more fonts */\n\t\t}\n\t}\n\n\tif (p->userfont)\n\t\tcharcnt = FNTCHARCNT(p->fontdata);\n\n\tvc->vc_can_do_color = (fb_get_color_depth(&info->var, &info->fix)!=1);\n\tvc->vc_complement_mask = vc->vc_can_do_color ? 0x7700 : 0x0800;\n\tif (charcnt == 256) {\n\t\tvc->vc_hi_font_mask = 0;\n\t} else {\n\t\tvc->vc_hi_font_mask = 0x100;\n\t\tif (vc->vc_can_do_color)\n\t\t\tvc->vc_complement_mask <<= 1;\n\t}\n\n\tif (!*svc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(svc);\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_copy_unimap(vc, svc);\n\n\tops = info->fbcon_par;\n\tops->cur_blink_jiffies = msecs_to_jiffies(vc->vc_cur_blink_ms);\n\n\tp->con_rotate = initial_rotation;\n\tif (p->con_rotate == -1)\n\t\tp->con_rotate = info->fbcon_rotate_hint;\n\tif (p->con_rotate == -1)\n\t\tp->con_rotate = FB_ROTATE_UR;\n\n\tset_blitting_type(vc, info);\n\n\tcols = vc->vc_cols;\n\trows = vc->vc_rows;\n\tnew_cols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\n\tnew_rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\n\tnew_cols /= vc->vc_font.width;\n\tnew_rows /= vc->vc_font.height;\n\n\t/*\n\t * We must always set the mode. The mode of the previous console\n\t * driver could be in the same resolution but we are using different\n\t * hardware so we have to initialize the hardware.\n\t *\n\t * We need to do it in fbcon_init() to prevent screen corruption.\n\t */\n\tif (con_is_visible(vc) && vc->vc_mode == KD_TEXT) {\n\t\tif (info->fbops->fb_set_par &&\n\t\t    !(ops->flags & FBCON_FLAGS_INIT)) {\n\t\t\tret = info->fbops->fb_set_par(info);\n\n\t\t\tif (ret)\n\t\t\t\tprintk(KERN_ERR \"fbcon_init: detected \"\n\t\t\t\t\t\"unhandled fb_set_par error, \"\n\t\t\t\t\t\"error code %d\\n\", ret);\n\t\t}\n\n\t\tops->flags |= FBCON_FLAGS_INIT;\n\t}\n\n\tops->graphics = 0;\n\n\tif ((cap & FBINFO_HWACCEL_COPYAREA) &&\n\t    !(cap & FBINFO_HWACCEL_DISABLED))\n\t\tp->scrollmode = SCROLL_MOVE;\n\telse /* default to something safe */\n\t\tp->scrollmode = SCROLL_REDRAW;\n\n\t/*\n\t *  ++guenther: console.c:vc_allocate() relies on initializing\n\t *  vc_{cols,rows}, but we must not set those if we are only\n\t *  resizing the console.\n\t */\n\tif (init) {\n\t\tvc->vc_cols = new_cols;\n\t\tvc->vc_rows = new_rows;\n\t} else\n\t\tvc_resize(vc, new_cols, new_rows);\n\n\tif (logo)\n\t\tfbcon_prepare_logo(vc, info, cols, rows, new_cols, new_rows);\n\n\tif (vc == svc && softback_buf)\n\t\tfbcon_update_softback(vc);\n\n\tif (ops->rotate_font && ops->rotate_font(info, vc)) {\n\t\tops->rotate = FB_ROTATE_UR;\n\t\tset_blitting_type(vc, info);\n\t}\n\n\tops->p = &fb_display[fg_console];\n}",
      "code_after_change": "static void fbcon_init(struct vc_data *vc, int init)\n{\n\tstruct fb_info *info;\n\tstruct fbcon_ops *ops;\n\tstruct vc_data **default_mode = vc->vc_display_fg;\n\tstruct vc_data *svc = *default_mode;\n\tstruct fbcon_display *t, *p = &fb_display[vc->vc_num];\n\tint logo = 1, new_rows, new_cols, rows, cols, charcnt = 256;\n\tint cap, ret;\n\n\tif (WARN_ON(info_idx == -1))\n\t    return;\n\n\tif (con2fb_map[vc->vc_num] == -1)\n\t\tcon2fb_map[vc->vc_num] = info_idx;\n\n\tinfo = registered_fb[con2fb_map[vc->vc_num]];\n\tcap = info->flags;\n\n\tif (logo_shown < 0 && console_loglevel <= CONSOLE_LOGLEVEL_QUIET)\n\t\tlogo_shown = FBCON_LOGO_DONTSHOW;\n\n\tif (vc != svc || logo_shown == FBCON_LOGO_DONTSHOW ||\n\t    (info->fix.type == FB_TYPE_TEXT))\n\t\tlogo = 0;\n\n\tif (var_to_display(p, &info->var, info))\n\t\treturn;\n\n\tif (!info->fbcon_par)\n\t\tcon2fb_acquire_newinfo(vc, info, vc->vc_num, -1);\n\n\t/* If we are not the first console on this\n\t   fb, copy the font from that console */\n\tt = &fb_display[fg_console];\n\tif (!p->fontdata) {\n\t\tif (t->fontdata) {\n\t\t\tstruct vc_data *fvc = vc_cons[fg_console].d;\n\n\t\t\tvc->vc_font.data = (void *)(p->fontdata =\n\t\t\t\t\t\t    fvc->vc_font.data);\n\t\t\tvc->vc_font.width = fvc->vc_font.width;\n\t\t\tvc->vc_font.height = fvc->vc_font.height;\n\t\t\tp->userfont = t->userfont;\n\n\t\t\tif (p->userfont)\n\t\t\t\tREFCOUNT(p->fontdata)++;\n\t\t} else {\n\t\t\tconst struct font_desc *font = NULL;\n\n\t\t\tif (!fontname[0] || !(font = find_font(fontname)))\n\t\t\t\tfont = get_default_font(info->var.xres,\n\t\t\t\t\t\t\tinfo->var.yres,\n\t\t\t\t\t\t\tinfo->pixmap.blit_x,\n\t\t\t\t\t\t\tinfo->pixmap.blit_y);\n\t\t\tvc->vc_font.width = font->width;\n\t\t\tvc->vc_font.height = font->height;\n\t\t\tvc->vc_font.data = (void *)(p->fontdata = font->data);\n\t\t\tvc->vc_font.charcount = 256; /* FIXME  Need to\n\t\t\t\t\t\t\tsupport more fonts */\n\t\t}\n\t}\n\n\tif (p->userfont)\n\t\tcharcnt = FNTCHARCNT(p->fontdata);\n\n\tvc->vc_can_do_color = (fb_get_color_depth(&info->var, &info->fix)!=1);\n\tvc->vc_complement_mask = vc->vc_can_do_color ? 0x7700 : 0x0800;\n\tif (charcnt == 256) {\n\t\tvc->vc_hi_font_mask = 0;\n\t} else {\n\t\tvc->vc_hi_font_mask = 0x100;\n\t\tif (vc->vc_can_do_color)\n\t\t\tvc->vc_complement_mask <<= 1;\n\t}\n\n\tif (!*svc->vc_uni_pagedir_loc)\n\t\tcon_set_default_unimap(svc);\n\tif (!*vc->vc_uni_pagedir_loc)\n\t\tcon_copy_unimap(vc, svc);\n\n\tops = info->fbcon_par;\n\tops->cur_blink_jiffies = msecs_to_jiffies(vc->vc_cur_blink_ms);\n\n\tp->con_rotate = initial_rotation;\n\tif (p->con_rotate == -1)\n\t\tp->con_rotate = info->fbcon_rotate_hint;\n\tif (p->con_rotate == -1)\n\t\tp->con_rotate = FB_ROTATE_UR;\n\n\tset_blitting_type(vc, info);\n\n\tcols = vc->vc_cols;\n\trows = vc->vc_rows;\n\tnew_cols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\n\tnew_rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\n\tnew_cols /= vc->vc_font.width;\n\tnew_rows /= vc->vc_font.height;\n\n\t/*\n\t * We must always set the mode. The mode of the previous console\n\t * driver could be in the same resolution but we are using different\n\t * hardware so we have to initialize the hardware.\n\t *\n\t * We need to do it in fbcon_init() to prevent screen corruption.\n\t */\n\tif (con_is_visible(vc) && vc->vc_mode == KD_TEXT) {\n\t\tif (info->fbops->fb_set_par &&\n\t\t    !(ops->flags & FBCON_FLAGS_INIT)) {\n\t\t\tret = info->fbops->fb_set_par(info);\n\n\t\t\tif (ret)\n\t\t\t\tprintk(KERN_ERR \"fbcon_init: detected \"\n\t\t\t\t\t\"unhandled fb_set_par error, \"\n\t\t\t\t\t\"error code %d\\n\", ret);\n\t\t}\n\n\t\tops->flags |= FBCON_FLAGS_INIT;\n\t}\n\n\tops->graphics = 0;\n\n\tif ((cap & FBINFO_HWACCEL_COPYAREA) &&\n\t    !(cap & FBINFO_HWACCEL_DISABLED))\n\t\tp->scrollmode = SCROLL_MOVE;\n\telse /* default to something safe */\n\t\tp->scrollmode = SCROLL_REDRAW;\n\n\t/*\n\t *  ++guenther: console.c:vc_allocate() relies on initializing\n\t *  vc_{cols,rows}, but we must not set those if we are only\n\t *  resizing the console.\n\t */\n\tif (init) {\n\t\tvc->vc_cols = new_cols;\n\t\tvc->vc_rows = new_rows;\n\t} else\n\t\tvc_resize(vc, new_cols, new_rows);\n\n\tif (logo)\n\t\tfbcon_prepare_logo(vc, info, cols, rows, new_cols, new_rows);\n\n\tif (ops->rotate_font && ops->rotate_font(info, vc)) {\n\t\tops->rotate = FB_ROTATE_UR;\n\t\tset_blitting_type(vc, info);\n\t}\n\n\tops->p = &fb_display[fg_console];\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tif (vc == svc && softback_buf)",
          "\t\tfbcon_update_softback(vc);",
          ""
        ]
      },
      "preconditions_for_vulnerability": "Inadequate bounds checking when changing screen size can lead to an out-of-bounds memory write.",
      "trigger_condition": "When the screen size is changed, the code performs calculations without proper validation, potentially causing the out-of-bounds memory write.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly validate the calculations for new screen size dimensions, leading to potential out-of-bounds memory writes and memory corruption.",
      "id": 86,
      "code_after_change_normalized": "static void FUN1(struct vc_data *VAR1, int VAR2)\n{\nstruct fb_info *VAR3;\nstruct fbcon_ops *VAR4;\nstruct vc_data **VAR5 = VAR1->VAR6;\nstruct vc_data *VAR7 = *VAR5;\nstruct fbcon_display *VAR8, *VAR9 = &VAR10[VAR1->VAR11];\nint VAR12 = 1, VAR13, VAR14, VAR15, VAR16, VAR17 = 256;\nint VAR18, VAR19;\nif (FUN2(VAR20 == -1))\nreturn;\nif (VAR21[VAR1->VAR11] == -1)\nVAR21[VAR1->VAR11] = VAR20;\nVAR3 = VAR22[VAR21[VAR1->VAR11]];\nVAR18 = VAR3->VAR23;\nif (VAR24 < 0 && VAR25 <= VAR26)\nVAR24 = VAR27;\nif (VAR1 != VAR7 || VAR24 == VAR27 ||\n(VAR3->VAR28.VAR29 == VAR30))\nVAR12 = 0;\nif (FUN3(VAR9, &VAR3->VAR31, VAR3))\nreturn;\nif (!VAR3->VAR32)\nFUN4(VAR1, VAR3, VAR1->VAR11, -1);\nVAR8 = &VAR10[VAR33];\nif (!VAR9->VAR34) {\nif (VAR8->VAR34) {\nstruct vc_data *VAR35 = VAR36[VAR33].VAR37;\nVAR1->VAR38.VAR39 = (void *)(VAR9->VAR34 =\nVAR35->VAR38.VAR39);\nVAR1->VAR38.VAR40 = VAR35->VAR38.VAR40;\nVAR1->VAR38.VAR41 = VAR35->VAR38.VAR41;\nVAR9->VAR42 = VAR8->VAR42;\nif (VAR9->VAR42)\nFUN5(VAR9->VAR34)++;\n} else {\nconst struct font_desc *VAR43 = NULL;\nif (!VAR44[0] || !(VAR43 = FUN6(VAR44)))\nVAR43 = FUN7(VAR3->VAR31.VAR45,\nVAR3->VAR31.VAR46,\nVAR3->VAR47.VAR48,\nVAR3->VAR47.VAR49);\nVAR1->VAR38.VAR40 = VAR43->VAR40;\nVAR1->VAR38.VAR41 = VAR43->VAR41;\nVAR1->VAR38.VAR39 = (void *)(VAR9->VAR34 = VAR43->VAR39);\nVAR1->VAR38.VAR50 = 256; \n}\n}\nif (VAR9->VAR42)\nVAR17 = FUN8(VAR9->VAR34);\nVAR1->VAR51 = (FUN9(&VAR3->VAR31, &VAR3->VAR28)!=1);\nVAR1->VAR52 = VAR1->VAR51 ? VAR53 : VAR53;\nif (VAR17 == 256) {\nVAR1->VAR54 = 0;\n} else {\nVAR1->VAR54 = VAR53;\nif (VAR1->VAR51)\nVAR1->VAR52 <<= 1;\n}\nif (!*VAR7->VAR55)\nFUN10(VAR7);\nif (!*VAR1->VAR55)\nFUN11(VAR1, VAR7);\nVAR4 = VAR3->VAR32;\nVAR4->VAR56 = FUN12(VAR1->VAR57);\nVAR9->VAR58 = VAR59;\nif (VAR9->VAR58 == -1)\nVAR9->VAR58 = VAR3->VAR60;\nif (VAR9->VAR58 == -1)\nVAR9->VAR58 = VAR61;\nFUN13(VAR1, VAR3);\nVAR16 = VAR1->VAR62;\nVAR15 = VAR1->VAR63;\nVAR14 = FUN14(VAR4->VAR64, VAR3->VAR31.VAR45, VAR3->VAR31.VAR46);\nVAR13 = FUN14(VAR4->VAR64, VAR3->VAR31.VAR46, VAR3->VAR31.VAR45);\nVAR14 /= VAR1->VAR38.VAR40;\nVAR13 /= VAR1->VAR38.VAR41;\nif (FUN15(VAR1) && VAR1->VAR65 == VAR66) {\nif (VAR3->VAR67->VAR68 &&\n!(VAR4->VAR23 & VAR69)) {\nVAR19 = VAR3->VAR67->FUN16(VAR3);\nif (VAR19)\nFUN17(VAR70 \"STR\"\n\"STR\"\n\"STR\", VAR19);\n}\nVAR4->VAR23 |= VAR69;\n}\nVAR4->VAR71 = 0;\nif ((VAR18 & VAR72) &&\n!(VAR18 & VAR73))\nVAR9->VAR74 = VAR75;\nelse \nVAR9->VAR74 = VAR76;\nif (VAR2) {\nVAR1->VAR62 = VAR14;\nVAR1->VAR63 = VAR13;\n} else\nFUN18(VAR1, VAR14, VAR13);\nif (VAR12)\nFUN19(VAR1, VAR3, VAR16, VAR15, VAR14, VAR13);\nif (VAR4->VAR77 && VAR4->FUN20(VAR3, VAR1)) {\nVAR4->VAR64 = VAR61;\nFUN13(VAR1, VAR3);\n}\nVAR4->VAR9 = &VAR10[VAR33];\n}\n",
      "code_before_change_normalized": "static void FUN1(struct vc_data *VAR1, int VAR2)\n{\nstruct fb_info *VAR3;\nstruct fbcon_ops *VAR4;\nstruct vc_data **VAR5 = VAR1->VAR6;\nstruct vc_data *VAR7 = *VAR5;\nstruct fbcon_display *VAR8, *VAR9 = &VAR10[VAR1->VAR11];\nint VAR12 = 1, VAR13, VAR14, VAR15, VAR16, VAR17 = 256;\nint VAR18, VAR19;\nif (FUN2(VAR20 == -1))\nreturn;\nif (VAR21[VAR1->VAR11] == -1)\nVAR21[VAR1->VAR11] = VAR20;\nVAR3 = VAR22[VAR21[VAR1->VAR11]];\nVAR18 = VAR3->VAR23;\nif (VAR24 < 0 && VAR25 <= VAR26)\nVAR24 = VAR27;\nif (VAR1 != VAR7 || VAR24 == VAR27 ||\n(VAR3->VAR28.VAR29 == VAR30))\nVAR12 = 0;\nif (FUN3(VAR9, &VAR3->VAR31, VAR3))\nreturn;\nif (!VAR3->VAR32)\nFUN4(VAR1, VAR3, VAR1->VAR11, -1);\nVAR8 = &VAR10[VAR33];\nif (!VAR9->VAR34) {\nif (VAR8->VAR34) {\nstruct vc_data *VAR35 = VAR36[VAR33].VAR37;\nVAR1->VAR38.VAR39 = (void *)(VAR9->VAR34 =\nVAR35->VAR38.VAR39);\nVAR1->VAR38.VAR40 = VAR35->VAR38.VAR40;\nVAR1->VAR38.VAR41 = VAR35->VAR38.VAR41;\nVAR9->VAR42 = VAR8->VAR42;\nif (VAR9->VAR42)\nFUN5(VAR9->VAR34)++;\n} else {\nconst struct font_desc *VAR43 = NULL;\nif (!VAR44[0] || !(VAR43 = FUN6(VAR44)))\nVAR43 = FUN7(VAR3->VAR31.VAR45,\nVAR3->VAR31.VAR46,\nVAR3->VAR47.VAR48,\nVAR3->VAR47.VAR49);\nVAR1->VAR38.VAR40 = VAR43->VAR40;\nVAR1->VAR38.VAR41 = VAR43->VAR41;\nVAR1->VAR38.VAR39 = (void *)(VAR9->VAR34 = VAR43->VAR39);\nVAR1->VAR38.VAR50 = 256; \n}\n}\nif (VAR9->VAR42)\nVAR17 = FUN8(VAR9->VAR34);\nVAR1->VAR51 = (FUN9(&VAR3->VAR31, &VAR3->VAR28)!=1);\nVAR1->VAR52 = VAR1->VAR51 ? VAR53 : VAR53;\nif (VAR17 == 256) {\nVAR1->VAR54 = 0;\n} else {\nVAR1->VAR54 = VAR53;\nif (VAR1->VAR51)\nVAR1->VAR52 <<= 1;\n}\nif (!*VAR7->VAR55)\nFUN10(VAR7);\nif (!*VAR1->VAR55)\nFUN11(VAR1, VAR7);\nVAR4 = VAR3->VAR32;\nVAR4->VAR56 = FUN12(VAR1->VAR57);\nVAR9->VAR58 = VAR59;\nif (VAR9->VAR58 == -1)\nVAR9->VAR58 = VAR3->VAR60;\nif (VAR9->VAR58 == -1)\nVAR9->VAR58 = VAR61;\nFUN13(VAR1, VAR3);\nVAR16 = VAR1->VAR62;\nVAR15 = VAR1->VAR63;\nVAR14 = FUN14(VAR4->VAR64, VAR3->VAR31.VAR45, VAR3->VAR31.VAR46);\nVAR13 = FUN14(VAR4->VAR64, VAR3->VAR31.VAR46, VAR3->VAR31.VAR45);\nVAR14 /= VAR1->VAR38.VAR40;\nVAR13 /= VAR1->VAR38.VAR41;\nif (FUN15(VAR1) && VAR1->VAR65 == VAR66) {\nif (VAR3->VAR67->VAR68 &&\n!(VAR4->VAR23 & VAR69)) {\nVAR19 = VAR3->VAR67->FUN16(VAR3);\nif (VAR19)\nFUN17(VAR70 \"STR\"\n\"STR\"\n\"STR\", VAR19);\n}\nVAR4->VAR23 |= VAR69;\n}\nVAR4->VAR71 = 0;\nif ((VAR18 & VAR72) &&\n!(VAR18 & VAR73))\nVAR9->VAR74 = VAR75;\nelse \nVAR9->VAR74 = VAR76;\nif (VAR2) {\nVAR1->VAR62 = VAR14;\nVAR1->VAR63 = VAR13;\n} else\nFUN18(VAR1, VAR14, VAR13);\nif (VAR12)\nFUN19(VAR1, VAR3, VAR16, VAR15, VAR14, VAR13);\nif (VAR1 == VAR7 && VAR77)\nFUN20(VAR1);\nif (VAR4->VAR78 && VAR4->FUN21(VAR3, VAR1)) {\nVAR4->VAR64 = VAR61;\nFUN13(VAR1, VAR3);\n}\nVAR4->VAR9 = &VAR10[VAR33];\n}\n",
      "code_after_change_raw": "static void fbcon_init(struct vc_data *vc, int init)\n{\nstruct fb_info *info;\nstruct fbcon_ops *ops;\nstruct vc_data **default_mode = vc->vc_display_fg;\nstruct vc_data *svc = *default_mode;\nstruct fbcon_display *t, *p = &fb_display[vc->vc_num];\nint logo = 1, new_rows, new_cols, rows, cols, charcnt = 256;\nint cap, ret;\nif (WARN_ON(info_idx == -1))\nreturn;\nif (con2fb_map[vc->vc_num] == -1)\ncon2fb_map[vc->vc_num] = info_idx;\ninfo = registered_fb[con2fb_map[vc->vc_num]];\ncap = info->flags;\nif (logo_shown < 0 && console_loglevel <= CONSOLE_LOGLEVEL_QUIET)\nlogo_shown = FBCON_LOGO_DONTSHOW;\nif (vc != svc || logo_shown == FBCON_LOGO_DONTSHOW ||\n(info->fix.type == FB_TYPE_TEXT))\nlogo = 0;\nif (var_to_display(p, &info->var, info))\nreturn;\nif (!info->fbcon_par)\ncon2fb_acquire_newinfo(vc, info, vc->vc_num, -1);\nt = &fb_display[fg_console];\nif (!p->fontdata) {\nif (t->fontdata) {\nstruct vc_data *fvc = vc_cons[fg_console].d;\nvc->vc_font.data = (void *)(p->fontdata =\nfvc->vc_font.data);\nvc->vc_font.width = fvc->vc_font.width;\nvc->vc_font.height = fvc->vc_font.height;\np->userfont = t->userfont;\nif (p->userfont)\nREFCOUNT(p->fontdata)++;\n} else {\nconst struct font_desc *font = NULL;\nif (!fontname[0] || !(font = find_font(fontname)))\nfont = get_default_font(info->var.xres,\ninfo->var.yres,\ninfo->pixmap.blit_x,\ninfo->pixmap.blit_y);\nvc->vc_font.width = font->width;\nvc->vc_font.height = font->height;\nvc->vc_font.data = (void *)(p->fontdata = font->data);\nvc->vc_font.charcount = 256; \n}\n}\nif (p->userfont)\ncharcnt = FNTCHARCNT(p->fontdata);\nvc->vc_can_do_color = (fb_get_color_depth(&info->var, &info->fix)!=1);\nvc->vc_complement_mask = vc->vc_can_do_color ? 0x7700 : 0x0800;\nif (charcnt == 256) {\nvc->vc_hi_font_mask = 0;\n} else {\nvc->vc_hi_font_mask = 0x100;\nif (vc->vc_can_do_color)\nvc->vc_complement_mask <<= 1;\n}\nif (!*svc->vc_uni_pagedir_loc)\ncon_set_default_unimap(svc);\nif (!*vc->vc_uni_pagedir_loc)\ncon_copy_unimap(vc, svc);\nops = info->fbcon_par;\nops->cur_blink_jiffies = msecs_to_jiffies(vc->vc_cur_blink_ms);\np->con_rotate = initial_rotation;\nif (p->con_rotate == -1)\np->con_rotate = info->fbcon_rotate_hint;\nif (p->con_rotate == -1)\np->con_rotate = FB_ROTATE_UR;\nset_blitting_type(vc, info);\ncols = vc->vc_cols;\nrows = vc->vc_rows;\nnew_cols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\nnew_rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\nnew_cols /= vc->vc_font.width;\nnew_rows /= vc->vc_font.height;\nif (con_is_visible(vc) && vc->vc_mode == KD_TEXT) {\nif (info->fbops->fb_set_par &&\n!(ops->flags & FBCON_FLAGS_INIT)) {\nret = info->fbops->fb_set_par(info);\nif (ret)\nprintk(KERN_ERR \"fbcon_init: detected \"\n\"unhandled fb_set_par error, \"\n\"error code %d\\n\", ret);\n}\nops->flags |= FBCON_FLAGS_INIT;\n}\nops->graphics = 0;\nif ((cap & FBINFO_HWACCEL_COPYAREA) &&\n!(cap & FBINFO_HWACCEL_DISABLED))\np->scrollmode = SCROLL_MOVE;\nelse \np->scrollmode = SCROLL_REDRAW;\nif (init) {\nvc->vc_cols = new_cols;\nvc->vc_rows = new_rows;\n} else\nvc_resize(vc, new_cols, new_rows);\nif (logo)\nfbcon_prepare_logo(vc, info, cols, rows, new_cols, new_rows);\nif (ops->rotate_font && ops->rotate_font(info, vc)) {\nops->rotate = FB_ROTATE_UR;\nset_blitting_type(vc, info);\n}\nops->p = &fb_display[fg_console];\n}\n",
      "code_before_change_raw": "static void fbcon_init(struct vc_data *vc, int init)\n{\nstruct fb_info *info;\nstruct fbcon_ops *ops;\nstruct vc_data **default_mode = vc->vc_display_fg;\nstruct vc_data *svc = *default_mode;\nstruct fbcon_display *t, *p = &fb_display[vc->vc_num];\nint logo = 1, new_rows, new_cols, rows, cols, charcnt = 256;\nint cap, ret;\nif (WARN_ON(info_idx == -1))\nreturn;\nif (con2fb_map[vc->vc_num] == -1)\ncon2fb_map[vc->vc_num] = info_idx;\ninfo = registered_fb[con2fb_map[vc->vc_num]];\ncap = info->flags;\nif (logo_shown < 0 && console_loglevel <= CONSOLE_LOGLEVEL_QUIET)\nlogo_shown = FBCON_LOGO_DONTSHOW;\nif (vc != svc || logo_shown == FBCON_LOGO_DONTSHOW ||\n(info->fix.type == FB_TYPE_TEXT))\nlogo = 0;\nif (var_to_display(p, &info->var, info))\nreturn;\nif (!info->fbcon_par)\ncon2fb_acquire_newinfo(vc, info, vc->vc_num, -1);\nt = &fb_display[fg_console];\nif (!p->fontdata) {\nif (t->fontdata) {\nstruct vc_data *fvc = vc_cons[fg_console].d;\nvc->vc_font.data = (void *)(p->fontdata =\nfvc->vc_font.data);\nvc->vc_font.width = fvc->vc_font.width;\nvc->vc_font.height = fvc->vc_font.height;\np->userfont = t->userfont;\nif (p->userfont)\nREFCOUNT(p->fontdata)++;\n} else {\nconst struct font_desc *font = NULL;\nif (!fontname[0] || !(font = find_font(fontname)))\nfont = get_default_font(info->var.xres,\ninfo->var.yres,\ninfo->pixmap.blit_x,\ninfo->pixmap.blit_y);\nvc->vc_font.width = font->width;\nvc->vc_font.height = font->height;\nvc->vc_font.data = (void *)(p->fontdata = font->data);\nvc->vc_font.charcount = 256; \n}\n}\nif (p->userfont)\ncharcnt = FNTCHARCNT(p->fontdata);\nvc->vc_can_do_color = (fb_get_color_depth(&info->var, &info->fix)!=1);\nvc->vc_complement_mask = vc->vc_can_do_color ? 0x7700 : 0x0800;\nif (charcnt == 256) {\nvc->vc_hi_font_mask = 0;\n} else {\nvc->vc_hi_font_mask = 0x100;\nif (vc->vc_can_do_color)\nvc->vc_complement_mask <<= 1;\n}\nif (!*svc->vc_uni_pagedir_loc)\ncon_set_default_unimap(svc);\nif (!*vc->vc_uni_pagedir_loc)\ncon_copy_unimap(vc, svc);\nops = info->fbcon_par;\nops->cur_blink_jiffies = msecs_to_jiffies(vc->vc_cur_blink_ms);\np->con_rotate = initial_rotation;\nif (p->con_rotate == -1)\np->con_rotate = info->fbcon_rotate_hint;\nif (p->con_rotate == -1)\np->con_rotate = FB_ROTATE_UR;\nset_blitting_type(vc, info);\ncols = vc->vc_cols;\nrows = vc->vc_rows;\nnew_cols = FBCON_SWAP(ops->rotate, info->var.xres, info->var.yres);\nnew_rows = FBCON_SWAP(ops->rotate, info->var.yres, info->var.xres);\nnew_cols /= vc->vc_font.width;\nnew_rows /= vc->vc_font.height;\nif (con_is_visible(vc) && vc->vc_mode == KD_TEXT) {\nif (info->fbops->fb_set_par &&\n!(ops->flags & FBCON_FLAGS_INIT)) {\nret = info->fbops->fb_set_par(info);\nif (ret)\nprintk(KERN_ERR \"fbcon_init: detected \"\n\"unhandled fb_set_par error, \"\n\"error code %d\\n\", ret);\n}\nops->flags |= FBCON_FLAGS_INIT;\n}\nops->graphics = 0;\nif ((cap & FBINFO_HWACCEL_COPYAREA) &&\n!(cap & FBINFO_HWACCEL_DISABLED))\np->scrollmode = SCROLL_MOVE;\nelse \np->scrollmode = SCROLL_REDRAW;\nif (init) {\nvc->vc_cols = new_cols;\nvc->vc_rows = new_rows;\n} else\nvc_resize(vc, new_cols, new_rows);\nif (logo)\nfbcon_prepare_logo(vc, info, cols, rows, new_cols, new_rows);\nif (vc == svc && softback_buf)\nfbcon_update_softback(vc);\nif (ops->rotate_font && ops->rotate_font(info, vc)) {\nops->rotate = FB_ROTATE_UR;\nset_blitting_type(vc, info);\n}\nops->p = &fb_display[fg_console];\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper cleanup for deferred I/O resources associated with the framebuffer device during the exit process.",
        "trigger_condition": "Changing the screen size triggers the exit process without cleaning up deferred I/O resources, leading to an out-of-bounds memory write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code snippet does not handle the cleanup of deferred I/O resources associated with the framebuffer device when exiting. This omission can result in an out-of-bounds memory write issue when changing the screen size, potentially causing memory corruption or denial of service."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper cleanup of deferred I/O resources associated with the framebuffer device during the exit process. This involves adding a line to call the 'fb_deferred_io_cleanup' function to clean up deferred I/O resources before releasing the framebuffer. By doing so, the code addresses the out-of-bounds memory write vulnerability and enhances the security of the system.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write issue that can lead to memory corruption or denial of service when changing the screen size. By adding the following lines to the code snippet, you can mitigate this vulnerability:\n\n```\nif (info->node)\n    fb_deferred_io_cleanup(info);\n```\n\nThis line ensures that the `fb_deferred_io_cleanup` function is called to clean up deferred I/O resources associated with the framebuffer device before releasing the framebuffer. This helps prevent the out-of-bounds memory write issue and improves the overall security of the code.",
      "GPT_purpose": "Deallocate resources and perform cleanup tasks when exiting the framebuffer console.",
      "GPT_function": "\n1. Freeing softback_buf memory.\n2. Cancelling pending work for registered framebuffers.\n3. Releasing framebuffer resources and associated memory.\n4. Deleting cursor timer and freeing cursor-related memory.\n5. Handling the fb_flashcursor function.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static void fbcon_exit(void)\n{\n\tstruct fb_info *info;\n\tint i, j, mapped;\n\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\n\tif (deferred_takeover) {\n\t\tdummycon_unregister_output_notifier(&fbcon_output_nb);\n\t\tdeferred_takeover = false;\n\t}\n#endif\n\n\tkvfree((void *)softback_buf);\n\tsoftback_buf = 0UL;\n\n\tfor_each_registered_fb(i) {\n\t\tint pending = 0;\n\n\t\tmapped = 0;\n\t\tinfo = registered_fb[i];\n\n\t\tif (info->queue.func)\n\t\t\tpending = cancel_work_sync(&info->queue);\n\t\tDPRINTK(\"fbcon: %s pending work\\n\", (pending ? \"canceled\" :\n\t\t\t\"no\"));\n\n\t\tfor (j = first_fb_vc; j <= last_fb_vc; j++) {\n\t\t\tif (con2fb_map[j] == i) {\n\t\t\t\tmapped = 1;\n\t\t\t\tcon2fb_map[j] = -1;\n\t\t\t}\n\t\t}\n\n\t\tif (mapped) {\n\t\t\tif (info->fbops->fb_release)\n\t\t\t\tinfo->fbops->fb_release(info, 0);\n\t\t\tmodule_put(info->fbops->owner);\n\n\t\t\tif (info->fbcon_par) {\n\t\t\t\tstruct fbcon_ops *ops = info->fbcon_par;\n\n\t\t\t\tfbcon_del_cursor_timer(info);\n\t\t\t\tkfree(ops->cursor_src);\n\t\t\t\tkfree(ops->cursor_state.mask);\n\t\t\t\tkfree(info->fbcon_par);\n\t\t\t\tinfo->fbcon_par = NULL;\n\t\t\t}\n\n\t\t\tif (info->queue.func == fb_flashcursor)\n\t\t\t\tinfo->queue.func = NULL;\n\t\t}\n\t}\n}",
      "code_after_change": "static void fbcon_exit(void)\n{\n\tstruct fb_info *info;\n\tint i, j, mapped;\n\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\n\tif (deferred_takeover) {\n\t\tdummycon_unregister_output_notifier(&fbcon_output_nb);\n\t\tdeferred_takeover = false;\n\t}\n#endif\n\n\tfor_each_registered_fb(i) {\n\t\tint pending = 0;\n\n\t\tmapped = 0;\n\t\tinfo = registered_fb[i];\n\n\t\tif (info->queue.func)\n\t\t\tpending = cancel_work_sync(&info->queue);\n\t\tDPRINTK(\"fbcon: %s pending work\\n\", (pending ? \"canceled\" :\n\t\t\t\"no\"));\n\n\t\tfor (j = first_fb_vc; j <= last_fb_vc; j++) {\n\t\t\tif (con2fb_map[j] == i) {\n\t\t\t\tmapped = 1;\n\t\t\t\tcon2fb_map[j] = -1;\n\t\t\t}\n\t\t}\n\n\t\tif (mapped) {\n\t\t\tif (info->fbops->fb_release)\n\t\t\t\tinfo->fbops->fb_release(info, 0);\n\t\t\tmodule_put(info->fbops->owner);\n\n\t\t\tif (info->fbcon_par) {\n\t\t\t\tstruct fbcon_ops *ops = info->fbcon_par;\n\n\t\t\t\tfbcon_del_cursor_timer(info);\n\t\t\t\tkfree(ops->cursor_src);\n\t\t\t\tkfree(ops->cursor_state.mask);\n\t\t\t\tkfree(info->fbcon_par);\n\t\t\t\tinfo->fbcon_par = NULL;\n\t\t\t}\n\n\t\t\tif (info->queue.func == fb_flashcursor)\n\t\t\t\tinfo->queue.func = NULL;\n\t\t}\n\t}\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "",
          "\tkvfree((void *)softback_buf);",
          "\tsoftback_buf = 0UL;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper cleanup for deferred I/O resources associated with the framebuffer device during the exit process.",
      "trigger_condition": "Changing the screen size triggers the exit process without cleaning up deferred I/O resources, leading to an out-of-bounds memory write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code snippet does not handle the cleanup of deferred I/O resources associated with the framebuffer device when exiting. This omission can result in an out-of-bounds memory write issue when changing the screen size, potentially causing memory corruption or denial of service.",
      "id": 87,
      "code_after_change_normalized": "static void FUN1(void)\n{\nstruct fb_info *VAR1;\nint VAR2, VAR3, VAR4;\n#ifdef VAR5\nif (VAR6) {\nFUN2(&VAR7);\nVAR6 = false;\n}\n#VAR8\nFUN3(VAR2) {\nint VAR9 = 0;\nVAR4 = 0;\nVAR1 = VAR10[VAR2];\nif (VAR1->VAR11.VAR12)\nVAR9 = FUN4(&VAR1->VAR11);\nFUN5(\"STR\", (VAR9 ? \"STR\" :\n\"STR\"));\nfor (VAR3 = VAR13; VAR3 <= VAR14; VAR3++) {\nif (VAR15[VAR3] == VAR2) {\nVAR4 = 1;\nVAR15[VAR3] = -1;\n}\n}\nif (VAR4) {\nif (VAR1->VAR16->VAR17)\nVAR1->VAR16->FUN6(VAR1, 0);\nFUN7(VAR1->VAR16->VAR18);\nif (VAR1->VAR19) {\nstruct fbcon_ops *VAR20 = VAR1->VAR19;\nFUN8(VAR1);\nFUN9(VAR20->VAR21);\nFUN9(VAR20->VAR22.VAR23);\nFUN9(VAR1->VAR19);\nVAR1->VAR19 = NULL;\n}\nif (VAR1->VAR11.VAR12 == VAR24)\nVAR1->VAR11.VAR12 = NULL;\n}\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(void)\n{\nstruct fb_info *VAR1;\nint VAR2, VAR3, VAR4;\n#ifdef VAR5\nif (VAR6) {\nFUN2(&VAR7);\nVAR6 = false;\n}\n#VAR8\nFUN3((void *)VAR9);\nVAR9 = 0UL;\nFUN4(VAR2) {\nint VAR10 = 0;\nVAR4 = 0;\nVAR1 = VAR11[VAR2];\nif (VAR1->VAR12.VAR13)\nVAR10 = FUN5(&VAR1->VAR12);\nFUN6(\"STR\", (VAR10 ? \"STR\" :\n\"STR\"));\nfor (VAR3 = VAR14; VAR3 <= VAR15; VAR3++) {\nif (VAR16[VAR3] == VAR2) {\nVAR4 = 1;\nVAR16[VAR3] = -1;\n}\n}\nif (VAR4) {\nif (VAR1->VAR17->VAR18)\nVAR1->VAR17->FUN7(VAR1, 0);\nFUN8(VAR1->VAR17->VAR19);\nif (VAR1->VAR20) {\nstruct fbcon_ops *VAR21 = VAR1->VAR20;\nFUN9(VAR1);\nFUN10(VAR21->VAR22);\nFUN10(VAR21->VAR23.VAR24);\nFUN10(VAR1->VAR20);\nVAR1->VAR20 = NULL;\n}\nif (VAR1->VAR12.VAR13 == VAR25)\nVAR1->VAR12.VAR13 = NULL;\n}\n}\n}\n",
      "code_after_change_raw": "static void fbcon_exit(void)\n{\nstruct fb_info *info;\nint i, j, mapped;\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\nif (deferred_takeover) {\ndummycon_unregister_output_notifier(&fbcon_output_nb);\ndeferred_takeover = false;\n}\n#endif\nfor_each_registered_fb(i) {\nint pending = 0;\nmapped = 0;\ninfo = registered_fb[i];\nif (info->queue.func)\npending = cancel_work_sync(&info->queue);\nDPRINTK(\"fbcon: %s pending work\\n\", (pending ? \"canceled\" :\n\"no\"));\nfor (j = first_fb_vc; j <= last_fb_vc; j++) {\nif (con2fb_map[j] == i) {\nmapped = 1;\ncon2fb_map[j] = -1;\n}\n}\nif (mapped) {\nif (info->fbops->fb_release)\ninfo->fbops->fb_release(info, 0);\nmodule_put(info->fbops->owner);\nif (info->fbcon_par) {\nstruct fbcon_ops *ops = info->fbcon_par;\nfbcon_del_cursor_timer(info);\nkfree(ops->cursor_src);\nkfree(ops->cursor_state.mask);\nkfree(info->fbcon_par);\ninfo->fbcon_par = NULL;\n}\nif (info->queue.func == fb_flashcursor)\ninfo->queue.func = NULL;\n}\n}\n}\n",
      "code_before_change_raw": "static void fbcon_exit(void)\n{\nstruct fb_info *info;\nint i, j, mapped;\n#ifdef CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER\nif (deferred_takeover) {\ndummycon_unregister_output_notifier(&fbcon_output_nb);\ndeferred_takeover = false;\n}\n#endif\nkvfree((void *)softback_buf);\nsoftback_buf = 0UL;\nfor_each_registered_fb(i) {\nint pending = 0;\nmapped = 0;\ninfo = registered_fb[i];\nif (info->queue.func)\npending = cancel_work_sync(&info->queue);\nDPRINTK(\"fbcon: %s pending work\\n\", (pending ? \"canceled\" :\n\"no\"));\nfor (j = first_fb_vc; j <= last_fb_vc; j++) {\nif (con2fb_map[j] == i) {\nmapped = 1;\ncon2fb_map[j] = -1;\n}\n}\nif (mapped) {\nif (info->fbops->fb_release)\ninfo->fbops->fb_release(info, 0);\nmodule_put(info->fbops->owner);\nif (info->fbcon_par) {\nstruct fbcon_ops *ops = info->fbcon_par;\nfbcon_del_cursor_timer(info);\nkfree(ops->cursor_src);\nkfree(ops->cursor_state.mask);\nkfree(info->fbcon_par);\ninfo->fbcon_par = NULL;\n}\nif (info->queue.func == fb_flashcursor)\ninfo->queue.func = NULL;\n}\n}\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate bounds checking when changing screen size, leading to an out-of-bounds memory write.",
        "trigger_condition": "Changing the screen size triggers the vulnerable code path, potentially causing memory corruption or a denial of service.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly handle the screen size change, allowing an out-of-bounds memory write to occur, leading to memory corruption or a denial of service. This vulnerability can be exploited due to the lack of bounds checking during the screen size change operation."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper bounds checking is performed when changing the screen size to prevent out-of-bounds memory writes. This involves adding bounds checking logic to the code snippet to validate the screen size change operation and prevent memory corruption or denial of service. By implementing bounds checking, the code can prevent the out-of-bounds memory write vulnerability and enhance the security of the system.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-14390, which involves an out-of-bounds memory write issue that can lead to memory corruption or a denial of service when changing screen size. By making the specified changes in the code snippet, you can mitigate the vulnerability and prevent potential exploitation that could result in memory corruption or a denial of service.",
      "GPT_purpose": "Scroll the framebuffer console display up or down based on the specified direction and count.",
      "GPT_function": "\n1. Scroll the screen content up or down based on the specified direction and count.\n2. Handle different scrolling modes such as SCROLL_MOVE, SCROLL_WRAP_MOVE, SCROLL_PAN_REDRAW, SCROLL_PAN_MOVE, and SCROLL_REDRAW.\n3. Perform screen redraw operations and clear the affected areas during scrolling.",
      "CVE_id": "CVE-2020-14390",
      "code_before_change": "static bool fbcon_scroll(struct vc_data *vc, unsigned int t, unsigned int b,\n\t\tenum con_scroll dir, unsigned int count)\n{\n\tstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\n\tstruct fbcon_display *p = &fb_display[vc->vc_num];\n\tint scroll_partial = info->flags & FBINFO_PARTIAL_PAN_OK;\n\n\tif (fbcon_is_inactive(vc, info))\n\t\treturn true;\n\n\tfbcon_cursor(vc, CM_ERASE);\n\n\t/*\n\t * ++Geert: Only use ywrap/ypan if the console is in text mode\n\t * ++Andrew: Only use ypan on hardware text mode when scrolling the\n\t *           whole screen (prevents flicker).\n\t */\n\n\tswitch (dir) {\n\tcase SM_UP:\n\t\tif (count > vc->vc_rows)\t/* Maximum realistic size */\n\t\t\tcount = vc->vc_rows;\n\t\tif (softback_top)\n\t\t\tfbcon_softback_note(vc, t, count);\n\t\tif (logo_shown >= 0)\n\t\t\tgoto redraw_up;\n\t\tswitch (p->scrollmode) {\n\t\tcase SCROLL_MOVE:\n\t\t\tfbcon_redraw_blit(vc, info, p, t, b - t - count,\n\t\t\t\t     count);\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\t(b - count)),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t\tbreak;\n\n\t\tcase SCROLL_WRAP_MOVE:\n\t\t\tif (b - t - count > 3 * vc->vc_rows >> 2) {\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, 0, 0, count, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\tywrap_up(vc, count);\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b - count, 0, b, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t + count, 0, t, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_up;\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_REDRAW:\n\t\t\tif ((p->yscroll + count <=\n\t\t\t     2 * (p->vrows - vc->vc_rows))\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, 0, t, count);\n\t\t\t\typan_up_redraw(vc, t, count);\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, b,\n\t\t\t\t\t\t\t  vc->vc_rows - b, b);\n\t\t\t} else\n\t\t\t\tfbcon_redraw_move(vc, p, t + count, b - t - count, t);\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_MOVE:\n\t\t\tif ((p->yscroll + count <=\n\t\t\t     2 * (p->vrows - vc->vc_rows))\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, 0, 0, count, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\typan_up(vc, count);\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b - count, 0, b, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t + count, 0, t, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_up;\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_REDRAW:\n\t\t      redraw_up:\n\t\t\tfbcon_redraw(vc, p, t, b - t - count,\n\t\t\t\t     count * vc->vc_cols);\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\t(b - count)),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\n\tcase SM_DOWN:\n\t\tif (count > vc->vc_rows)\t/* Maximum realistic size */\n\t\t\tcount = vc->vc_rows;\n\t\tif (logo_shown >= 0)\n\t\t\tgoto redraw_down;\n\t\tswitch (p->scrollmode) {\n\t\tcase SCROLL_MOVE:\n\t\t\tfbcon_redraw_blit(vc, info, p, b - 1, b - t - count,\n\t\t\t\t     -count);\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\tt),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t\tbreak;\n\n\t\tcase SCROLL_WRAP_MOVE:\n\t\t\tif (b - t - count > 3 * vc->vc_rows >> 2) {\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b, 0, b - count, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\tywrap_down(vc, count);\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, count, 0, 0, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t, 0, t + count, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_down;\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_MOVE:\n\t\t\tif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b, 0, b - count, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\typan_down(vc, count);\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, count, 0, 0, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t, 0, t + count, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_down;\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_REDRAW:\n\t\t\tif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, b, vc->vc_rows - b,\n\t\t\t\t\t\t\t  b - count);\n\t\t\t\typan_down_redraw(vc, t, count);\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, count, t, 0);\n\t\t\t} else\n\t\t\t\tfbcon_redraw_move(vc, p, t, b - t - count, t + count);\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_REDRAW:\n\t\t      redraw_down:\n\t\t\tfbcon_redraw(vc, p, b - 1, b - t - count,\n\t\t\t\t     -count * vc->vc_cols);\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\tt),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}",
      "code_after_change": "static bool fbcon_scroll(struct vc_data *vc, unsigned int t, unsigned int b,\n\t\tenum con_scroll dir, unsigned int count)\n{\n\tstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\n\tstruct fbcon_display *p = &fb_display[vc->vc_num];\n\tint scroll_partial = info->flags & FBINFO_PARTIAL_PAN_OK;\n\n\tif (fbcon_is_inactive(vc, info))\n\t\treturn true;\n\n\tfbcon_cursor(vc, CM_ERASE);\n\n\t/*\n\t * ++Geert: Only use ywrap/ypan if the console is in text mode\n\t * ++Andrew: Only use ypan on hardware text mode when scrolling the\n\t *           whole screen (prevents flicker).\n\t */\n\n\tswitch (dir) {\n\tcase SM_UP:\n\t\tif (count > vc->vc_rows)\t/* Maximum realistic size */\n\t\t\tcount = vc->vc_rows;\n\t\tif (logo_shown >= 0)\n\t\t\tgoto redraw_up;\n\t\tswitch (p->scrollmode) {\n\t\tcase SCROLL_MOVE:\n\t\t\tfbcon_redraw_blit(vc, info, p, t, b - t - count,\n\t\t\t\t     count);\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\t(b - count)),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t\tbreak;\n\n\t\tcase SCROLL_WRAP_MOVE:\n\t\t\tif (b - t - count > 3 * vc->vc_rows >> 2) {\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, 0, 0, count, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\tywrap_up(vc, count);\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b - count, 0, b, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t + count, 0, t, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_up;\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_REDRAW:\n\t\t\tif ((p->yscroll + count <=\n\t\t\t     2 * (p->vrows - vc->vc_rows))\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, 0, t, count);\n\t\t\t\typan_up_redraw(vc, t, count);\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, b,\n\t\t\t\t\t\t\t  vc->vc_rows - b, b);\n\t\t\t} else\n\t\t\t\tfbcon_redraw_move(vc, p, t + count, b - t - count, t);\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_MOVE:\n\t\t\tif ((p->yscroll + count <=\n\t\t\t     2 * (p->vrows - vc->vc_rows))\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, 0, 0, count, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\typan_up(vc, count);\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b - count, 0, b, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t + count, 0, t, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_up;\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_REDRAW:\n\t\t      redraw_up:\n\t\t\tfbcon_redraw(vc, p, t, b - t - count,\n\t\t\t\t     count * vc->vc_cols);\n\t\t\tfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\t(b - count)),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t}\n\t\tbreak;\n\n\tcase SM_DOWN:\n\t\tif (count > vc->vc_rows)\t/* Maximum realistic size */\n\t\t\tcount = vc->vc_rows;\n\t\tif (logo_shown >= 0)\n\t\t\tgoto redraw_down;\n\t\tswitch (p->scrollmode) {\n\t\tcase SCROLL_MOVE:\n\t\t\tfbcon_redraw_blit(vc, info, p, b - 1, b - t - count,\n\t\t\t\t     -count);\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\tt),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t\tbreak;\n\n\t\tcase SCROLL_WRAP_MOVE:\n\t\t\tif (b - t - count > 3 * vc->vc_rows >> 2) {\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b, 0, b - count, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\tywrap_down(vc, count);\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, count, 0, 0, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t, 0, t + count, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_down;\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_MOVE:\n\t\t\tif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_bmove(vc, b, 0, b - count, 0,\n\t\t\t\t\t\t    vc->vc_rows - b,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t\typan_down(vc, count);\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_bmove(vc, count, 0, 0, 0, t,\n\t\t\t\t\t\t    vc->vc_cols);\n\t\t\t} else if (info->flags & FBINFO_READS_FAST)\n\t\t\t\tfbcon_bmove(vc, t, 0, t + count, 0,\n\t\t\t\t\t    b - t - count, vc->vc_cols);\n\t\t\telse\n\t\t\t\tgoto redraw_down;\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_PAN_REDRAW:\n\t\t\tif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n\t\t\t    && ((!scroll_partial && (b - t == vc->vc_rows))\n\t\t\t\t|| (scroll_partial\n\t\t\t\t    && (b - t - count >\n\t\t\t\t\t3 * vc->vc_rows >> 2)))) {\n\t\t\t\tif (vc->vc_rows - b > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, b, vc->vc_rows - b,\n\t\t\t\t\t\t\t  b - count);\n\t\t\t\typan_down_redraw(vc, t, count);\n\t\t\t\tif (t > 0)\n\t\t\t\t\tfbcon_redraw_move(vc, p, count, t, 0);\n\t\t\t} else\n\t\t\t\tfbcon_redraw_move(vc, p, t, b - t - count, t + count);\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tbreak;\n\n\t\tcase SCROLL_REDRAW:\n\t\t      redraw_down:\n\t\t\tfbcon_redraw(vc, p, b - 1, b - t - count,\n\t\t\t\t     -count * vc->vc_cols);\n\t\t\tfbcon_clear(vc, t, 0, count, vc->vc_cols);\n\t\t\tscr_memsetw((unsigned short *) (vc->vc_origin +\n\t\t\t\t\t\t\tvc->vc_size_row *\n\t\t\t\t\t\t\tt),\n\t\t\t\t    vc->vc_video_erase_char,\n\t\t\t\t    vc->vc_size_row * count);\n\t\t\treturn true;\n\t\t}\n\t}\n\treturn false;\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\t\tif (softback_top)",
          "\t\t\tfbcon_softback_note(vc, t, count);"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate bounds checking when changing screen size, leading to an out-of-bounds memory write.",
      "trigger_condition": "Changing the screen size triggers the vulnerable code path, potentially causing memory corruption or a denial of service.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly handle the screen size change, allowing an out-of-bounds memory write to occur, leading to memory corruption or a denial of service. This vulnerability can be exploited due to the lack of bounds checking during the screen size change operation.",
      "id": 88,
      "code_after_change_normalized": "static bool FUN1(struct vc_data *VAR1, unsigned int VAR2, unsigned int VAR3,\nenum con_scroll VAR4, unsigned int VAR5)\n{\nstruct fb_info *VAR6 = VAR7[VAR8[VAR1->VAR9]];\nstruct fbcon_display *VAR10 = &VAR11[VAR1->VAR9];\nint VAR12 = VAR6->VAR13 & VAR14;\nif (FUN2(VAR1, VAR6))\nreturn true;\nFUN3(VAR1, VAR15);\nswitch (VAR4) {\ncase VAR16:\nif (VAR5 > VAR1->VAR17)\t\nVAR5 = VAR1->VAR17;\nif (VAR18 >= 0)\ngoto VAR19;\nswitch (VAR10->VAR20) {\ncase VAR21:\nFUN4(VAR1, VAR6, VAR10, VAR2, VAR3 - VAR2 - VAR5,\nVAR5);\nFUN5(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR22);\nFUN6((unsigned short *) (VAR1->VAR23 +\nVAR1->VAR24 *\n(VAR3 - VAR5)),\nVAR1->VAR25,\nVAR1->VAR24 * VAR5);\nreturn true;\nbreak;\ncase VAR26:\nif (VAR3 - VAR2 - VAR5 > 3 * VAR1->VAR17 >> 2) {\nif (VAR2 > 0)\nFUN7(VAR1, 0, 0, VAR5, 0, VAR2,\nVAR1->VAR22);\nFUN8(VAR1, VAR5);\nif (VAR1->VAR17 - VAR3 > 0)\nFUN7(VAR1, VAR3 - VAR5, 0, VAR3, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR22);\n} else if (VAR6->VAR13 & VAR27)\nFUN7(VAR1, VAR2 + VAR5, 0, VAR2, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR22);\nelse\ngoto VAR19;\nFUN5(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR22);\nbreak;\ncase VAR28:\nif ((VAR10->VAR29 + VAR5 <=\n2 * (VAR10->VAR30 - VAR1->VAR17))\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR2 > 0)\nFUN9(VAR1, VAR10, 0, VAR2, VAR5);\nFUN10(VAR1, VAR2, VAR5);\nif (VAR1->VAR17 - VAR3 > 0)\nFUN9(VAR1, VAR10, VAR3,\nVAR1->VAR17 - VAR3, VAR3);\n} else\nFUN9(VAR1, VAR10, VAR2 + VAR5, VAR3 - VAR2 - VAR5, VAR2);\nFUN5(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR22);\nbreak;\ncase VAR31:\nif ((VAR10->VAR29 + VAR5 <=\n2 * (VAR10->VAR30 - VAR1->VAR17))\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR2 > 0)\nFUN7(VAR1, 0, 0, VAR5, 0, VAR2,\nVAR1->VAR22);\nFUN11(VAR1, VAR5);\nif (VAR1->VAR17 - VAR3 > 0)\nFUN7(VAR1, VAR3 - VAR5, 0, VAR3, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR22);\n} else if (VAR6->VAR13 & VAR27)\nFUN7(VAR1, VAR2 + VAR5, 0, VAR2, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR22);\nelse\ngoto VAR19;\nFUN5(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR22);\nbreak;\ncase VAR32:\nVAR19:\nFUN12(VAR1, VAR10, VAR2, VAR3 - VAR2 - VAR5,\nVAR5 * VAR1->VAR22);\nFUN5(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR22);\nFUN6((unsigned short *) (VAR1->VAR23 +\nVAR1->VAR24 *\n(VAR3 - VAR5)),\nVAR1->VAR25,\nVAR1->VAR24 * VAR5);\nreturn true;\n}\nbreak;\ncase VAR33:\nif (VAR5 > VAR1->VAR17)\t\nVAR5 = VAR1->VAR17;\nif (VAR18 >= 0)\ngoto VAR34;\nswitch (VAR10->VAR20) {\ncase VAR21:\nFUN4(VAR1, VAR6, VAR10, VAR3 - 1, VAR3 - VAR2 - VAR5,\n-VAR5);\nFUN5(VAR1, VAR2, 0, VAR5, VAR1->VAR22);\nFUN6((unsigned short *) (VAR1->VAR23 +\nVAR1->VAR24 *\nVAR2),\nVAR1->VAR25,\nVAR1->VAR24 * VAR5);\nreturn true;\nbreak;\ncase VAR26:\nif (VAR3 - VAR2 - VAR5 > 3 * VAR1->VAR17 >> 2) {\nif (VAR1->VAR17 - VAR3 > 0)\nFUN7(VAR1, VAR3, 0, VAR3 - VAR5, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR22);\nFUN13(VAR1, VAR5);\nif (VAR2 > 0)\nFUN7(VAR1, VAR5, 0, 0, 0, VAR2,\nVAR1->VAR22);\n} else if (VAR6->VAR13 & VAR27)\nFUN7(VAR1, VAR2, 0, VAR2 + VAR5, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR22);\nelse\ngoto VAR34;\nFUN5(VAR1, VAR2, 0, VAR5, VAR1->VAR22);\nbreak;\ncase VAR31:\nif ((VAR5 - VAR10->VAR29 <= VAR10->VAR30 - VAR1->VAR17)\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR1->VAR17 - VAR3 > 0)\nFUN7(VAR1, VAR3, 0, VAR3 - VAR5, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR22);\nFUN14(VAR1, VAR5);\nif (VAR2 > 0)\nFUN7(VAR1, VAR5, 0, 0, 0, VAR2,\nVAR1->VAR22);\n} else if (VAR6->VAR13 & VAR27)\nFUN7(VAR1, VAR2, 0, VAR2 + VAR5, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR22);\nelse\ngoto VAR34;\nFUN5(VAR1, VAR2, 0, VAR5, VAR1->VAR22);\nbreak;\ncase VAR28:\nif ((VAR5 - VAR10->VAR29 <= VAR10->VAR30 - VAR1->VAR17)\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR1->VAR17 - VAR3 > 0)\nFUN9(VAR1, VAR10, VAR3, VAR1->VAR17 - VAR3,\nVAR3 - VAR5);\nFUN15(VAR1, VAR2, VAR5);\nif (VAR2 > 0)\nFUN9(VAR1, VAR10, VAR5, VAR2, 0);\n} else\nFUN9(VAR1, VAR10, VAR2, VAR3 - VAR2 - VAR5, VAR2 + VAR5);\nFUN5(VAR1, VAR2, 0, VAR5, VAR1->VAR22);\nbreak;\ncase VAR32:\nVAR34:\nFUN12(VAR1, VAR10, VAR3 - 1, VAR3 - VAR2 - VAR5,\n-VAR5 * VAR1->VAR22);\nFUN5(VAR1, VAR2, 0, VAR5, VAR1->VAR22);\nFUN6((unsigned short *) (VAR1->VAR23 +\nVAR1->VAR24 *\nVAR2),\nVAR1->VAR25,\nVAR1->VAR24 * VAR5);\nreturn true;\n}\n}\nreturn false;\n}\n",
      "code_before_change_normalized": "static bool FUN1(struct vc_data *VAR1, unsigned int VAR2, unsigned int VAR3,\nenum con_scroll VAR4, unsigned int VAR5)\n{\nstruct fb_info *VAR6 = VAR7[VAR8[VAR1->VAR9]];\nstruct fbcon_display *VAR10 = &VAR11[VAR1->VAR9];\nint VAR12 = VAR6->VAR13 & VAR14;\nif (FUN2(VAR1, VAR6))\nreturn true;\nFUN3(VAR1, VAR15);\nswitch (VAR4) {\ncase VAR16:\nif (VAR5 > VAR1->VAR17)\t\nVAR5 = VAR1->VAR17;\nif (VAR18)\nFUN4(VAR1, VAR2, VAR5);\nif (VAR19 >= 0)\ngoto VAR20;\nswitch (VAR10->VAR21) {\ncase VAR22:\nFUN5(VAR1, VAR6, VAR10, VAR2, VAR3 - VAR2 - VAR5,\nVAR5);\nFUN6(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR23);\nFUN7((unsigned short *) (VAR1->VAR24 +\nVAR1->VAR25 *\n(VAR3 - VAR5)),\nVAR1->VAR26,\nVAR1->VAR25 * VAR5);\nreturn true;\nbreak;\ncase VAR27:\nif (VAR3 - VAR2 - VAR5 > 3 * VAR1->VAR17 >> 2) {\nif (VAR2 > 0)\nFUN8(VAR1, 0, 0, VAR5, 0, VAR2,\nVAR1->VAR23);\nFUN9(VAR1, VAR5);\nif (VAR1->VAR17 - VAR3 > 0)\nFUN8(VAR1, VAR3 - VAR5, 0, VAR3, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR23);\n} else if (VAR6->VAR13 & VAR28)\nFUN8(VAR1, VAR2 + VAR5, 0, VAR2, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR23);\nelse\ngoto VAR20;\nFUN6(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR23);\nbreak;\ncase VAR29:\nif ((VAR10->VAR30 + VAR5 <=\n2 * (VAR10->VAR31 - VAR1->VAR17))\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR2 > 0)\nFUN10(VAR1, VAR10, 0, VAR2, VAR5);\nFUN11(VAR1, VAR2, VAR5);\nif (VAR1->VAR17 - VAR3 > 0)\nFUN10(VAR1, VAR10, VAR3,\nVAR1->VAR17 - VAR3, VAR3);\n} else\nFUN10(VAR1, VAR10, VAR2 + VAR5, VAR3 - VAR2 - VAR5, VAR2);\nFUN6(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR23);\nbreak;\ncase VAR32:\nif ((VAR10->VAR30 + VAR5 <=\n2 * (VAR10->VAR31 - VAR1->VAR17))\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR2 > 0)\nFUN8(VAR1, 0, 0, VAR5, 0, VAR2,\nVAR1->VAR23);\nFUN12(VAR1, VAR5);\nif (VAR1->VAR17 - VAR3 > 0)\nFUN8(VAR1, VAR3 - VAR5, 0, VAR3, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR23);\n} else if (VAR6->VAR13 & VAR28)\nFUN8(VAR1, VAR2 + VAR5, 0, VAR2, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR23);\nelse\ngoto VAR20;\nFUN6(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR23);\nbreak;\ncase VAR33:\nVAR20:\nFUN13(VAR1, VAR10, VAR2, VAR3 - VAR2 - VAR5,\nVAR5 * VAR1->VAR23);\nFUN6(VAR1, VAR3 - VAR5, 0, VAR5, VAR1->VAR23);\nFUN7((unsigned short *) (VAR1->VAR24 +\nVAR1->VAR25 *\n(VAR3 - VAR5)),\nVAR1->VAR26,\nVAR1->VAR25 * VAR5);\nreturn true;\n}\nbreak;\ncase VAR34:\nif (VAR5 > VAR1->VAR17)\t\nVAR5 = VAR1->VAR17;\nif (VAR19 >= 0)\ngoto VAR35;\nswitch (VAR10->VAR21) {\ncase VAR22:\nFUN5(VAR1, VAR6, VAR10, VAR3 - 1, VAR3 - VAR2 - VAR5,\n-VAR5);\nFUN6(VAR1, VAR2, 0, VAR5, VAR1->VAR23);\nFUN7((unsigned short *) (VAR1->VAR24 +\nVAR1->VAR25 *\nVAR2),\nVAR1->VAR26,\nVAR1->VAR25 * VAR5);\nreturn true;\nbreak;\ncase VAR27:\nif (VAR3 - VAR2 - VAR5 > 3 * VAR1->VAR17 >> 2) {\nif (VAR1->VAR17 - VAR3 > 0)\nFUN8(VAR1, VAR3, 0, VAR3 - VAR5, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR23);\nFUN14(VAR1, VAR5);\nif (VAR2 > 0)\nFUN8(VAR1, VAR5, 0, 0, 0, VAR2,\nVAR1->VAR23);\n} else if (VAR6->VAR13 & VAR28)\nFUN8(VAR1, VAR2, 0, VAR2 + VAR5, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR23);\nelse\ngoto VAR35;\nFUN6(VAR1, VAR2, 0, VAR5, VAR1->VAR23);\nbreak;\ncase VAR32:\nif ((VAR5 - VAR10->VAR30 <= VAR10->VAR31 - VAR1->VAR17)\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR1->VAR17 - VAR3 > 0)\nFUN8(VAR1, VAR3, 0, VAR3 - VAR5, 0,\nVAR1->VAR17 - VAR3,\nVAR1->VAR23);\nFUN15(VAR1, VAR5);\nif (VAR2 > 0)\nFUN8(VAR1, VAR5, 0, 0, 0, VAR2,\nVAR1->VAR23);\n} else if (VAR6->VAR13 & VAR28)\nFUN8(VAR1, VAR2, 0, VAR2 + VAR5, 0,\nVAR3 - VAR2 - VAR5, VAR1->VAR23);\nelse\ngoto VAR35;\nFUN6(VAR1, VAR2, 0, VAR5, VAR1->VAR23);\nbreak;\ncase VAR29:\nif ((VAR5 - VAR10->VAR30 <= VAR10->VAR31 - VAR1->VAR17)\n&& ((!VAR12 && (VAR3 - VAR2 == VAR1->VAR17))\n|| (VAR12\n&& (VAR3 - VAR2 - VAR5 >\n3 * VAR1->VAR17 >> 2)))) {\nif (VAR1->VAR17 - VAR3 > 0)\nFUN10(VAR1, VAR10, VAR3, VAR1->VAR17 - VAR3,\nVAR3 - VAR5);\nFUN16(VAR1, VAR2, VAR5);\nif (VAR2 > 0)\nFUN10(VAR1, VAR10, VAR5, VAR2, 0);\n} else\nFUN10(VAR1, VAR10, VAR2, VAR3 - VAR2 - VAR5, VAR2 + VAR5);\nFUN6(VAR1, VAR2, 0, VAR5, VAR1->VAR23);\nbreak;\ncase VAR33:\nVAR35:\nFUN13(VAR1, VAR10, VAR3 - 1, VAR3 - VAR2 - VAR5,\n-VAR5 * VAR1->VAR23);\nFUN6(VAR1, VAR2, 0, VAR5, VAR1->VAR23);\nFUN7((unsigned short *) (VAR1->VAR24 +\nVAR1->VAR25 *\nVAR2),\nVAR1->VAR26,\nVAR1->VAR25 * VAR5);\nreturn true;\n}\n}\nreturn false;\n}\n",
      "code_after_change_raw": "static bool fbcon_scroll(struct vc_data *vc, unsigned int t, unsigned int b,\nenum con_scroll dir, unsigned int count)\n{\nstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\nstruct fbcon_display *p = &fb_display[vc->vc_num];\nint scroll_partial = info->flags & FBINFO_PARTIAL_PAN_OK;\nif (fbcon_is_inactive(vc, info))\nreturn true;\nfbcon_cursor(vc, CM_ERASE);\nswitch (dir) {\ncase SM_UP:\nif (count > vc->vc_rows)\t\ncount = vc->vc_rows;\nif (logo_shown >= 0)\ngoto redraw_up;\nswitch (p->scrollmode) {\ncase SCROLL_MOVE:\nfbcon_redraw_blit(vc, info, p, t, b - t - count,\ncount);\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\n(b - count)),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\nbreak;\ncase SCROLL_WRAP_MOVE:\nif (b - t - count > 3 * vc->vc_rows >> 2) {\nif (t > 0)\nfbcon_bmove(vc, 0, 0, count, 0, t,\nvc->vc_cols);\nywrap_up(vc, count);\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b - count, 0, b, 0,\nvc->vc_rows - b,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t + count, 0, t, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_up;\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_REDRAW:\nif ((p->yscroll + count <=\n2 * (p->vrows - vc->vc_rows))\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (t > 0)\nfbcon_redraw_move(vc, p, 0, t, count);\nypan_up_redraw(vc, t, count);\nif (vc->vc_rows - b > 0)\nfbcon_redraw_move(vc, p, b,\nvc->vc_rows - b, b);\n} else\nfbcon_redraw_move(vc, p, t + count, b - t - count, t);\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_MOVE:\nif ((p->yscroll + count <=\n2 * (p->vrows - vc->vc_rows))\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (t > 0)\nfbcon_bmove(vc, 0, 0, count, 0, t,\nvc->vc_cols);\nypan_up(vc, count);\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b - count, 0, b, 0,\nvc->vc_rows - b,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t + count, 0, t, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_up;\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_REDRAW:\nredraw_up:\nfbcon_redraw(vc, p, t, b - t - count,\ncount * vc->vc_cols);\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\n(b - count)),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\n}\nbreak;\ncase SM_DOWN:\nif (count > vc->vc_rows)\t\ncount = vc->vc_rows;\nif (logo_shown >= 0)\ngoto redraw_down;\nswitch (p->scrollmode) {\ncase SCROLL_MOVE:\nfbcon_redraw_blit(vc, info, p, b - 1, b - t - count,\n-count);\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\nt),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\nbreak;\ncase SCROLL_WRAP_MOVE:\nif (b - t - count > 3 * vc->vc_rows >> 2) {\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b, 0, b - count, 0,\nvc->vc_rows - b,\nvc->vc_cols);\nywrap_down(vc, count);\nif (t > 0)\nfbcon_bmove(vc, count, 0, 0, 0, t,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t, 0, t + count, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_down;\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_MOVE:\nif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b, 0, b - count, 0,\nvc->vc_rows - b,\nvc->vc_cols);\nypan_down(vc, count);\nif (t > 0)\nfbcon_bmove(vc, count, 0, 0, 0, t,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t, 0, t + count, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_down;\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_REDRAW:\nif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (vc->vc_rows - b > 0)\nfbcon_redraw_move(vc, p, b, vc->vc_rows - b,\nb - count);\nypan_down_redraw(vc, t, count);\nif (t > 0)\nfbcon_redraw_move(vc, p, count, t, 0);\n} else\nfbcon_redraw_move(vc, p, t, b - t - count, t + count);\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_REDRAW:\nredraw_down:\nfbcon_redraw(vc, p, b - 1, b - t - count,\n-count * vc->vc_cols);\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\nt),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\n}\n}\nreturn false;\n}\n",
      "code_before_change_raw": "static bool fbcon_scroll(struct vc_data *vc, unsigned int t, unsigned int b,\nenum con_scroll dir, unsigned int count)\n{\nstruct fb_info *info = registered_fb[con2fb_map[vc->vc_num]];\nstruct fbcon_display *p = &fb_display[vc->vc_num];\nint scroll_partial = info->flags & FBINFO_PARTIAL_PAN_OK;\nif (fbcon_is_inactive(vc, info))\nreturn true;\nfbcon_cursor(vc, CM_ERASE);\nswitch (dir) {\ncase SM_UP:\nif (count > vc->vc_rows)\t\ncount = vc->vc_rows;\nif (softback_top)\nfbcon_softback_note(vc, t, count);\nif (logo_shown >= 0)\ngoto redraw_up;\nswitch (p->scrollmode) {\ncase SCROLL_MOVE:\nfbcon_redraw_blit(vc, info, p, t, b - t - count,\ncount);\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\n(b - count)),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\nbreak;\ncase SCROLL_WRAP_MOVE:\nif (b - t - count > 3 * vc->vc_rows >> 2) {\nif (t > 0)\nfbcon_bmove(vc, 0, 0, count, 0, t,\nvc->vc_cols);\nywrap_up(vc, count);\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b - count, 0, b, 0,\nvc->vc_rows - b,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t + count, 0, t, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_up;\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_REDRAW:\nif ((p->yscroll + count <=\n2 * (p->vrows - vc->vc_rows))\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (t > 0)\nfbcon_redraw_move(vc, p, 0, t, count);\nypan_up_redraw(vc, t, count);\nif (vc->vc_rows - b > 0)\nfbcon_redraw_move(vc, p, b,\nvc->vc_rows - b, b);\n} else\nfbcon_redraw_move(vc, p, t + count, b - t - count, t);\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_MOVE:\nif ((p->yscroll + count <=\n2 * (p->vrows - vc->vc_rows))\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (t > 0)\nfbcon_bmove(vc, 0, 0, count, 0, t,\nvc->vc_cols);\nypan_up(vc, count);\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b - count, 0, b, 0,\nvc->vc_rows - b,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t + count, 0, t, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_up;\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_REDRAW:\nredraw_up:\nfbcon_redraw(vc, p, t, b - t - count,\ncount * vc->vc_cols);\nfbcon_clear(vc, b - count, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\n(b - count)),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\n}\nbreak;\ncase SM_DOWN:\nif (count > vc->vc_rows)\t\ncount = vc->vc_rows;\nif (logo_shown >= 0)\ngoto redraw_down;\nswitch (p->scrollmode) {\ncase SCROLL_MOVE:\nfbcon_redraw_blit(vc, info, p, b - 1, b - t - count,\n-count);\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\nt),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\nbreak;\ncase SCROLL_WRAP_MOVE:\nif (b - t - count > 3 * vc->vc_rows >> 2) {\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b, 0, b - count, 0,\nvc->vc_rows - b,\nvc->vc_cols);\nywrap_down(vc, count);\nif (t > 0)\nfbcon_bmove(vc, count, 0, 0, 0, t,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t, 0, t + count, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_down;\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_MOVE:\nif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (vc->vc_rows - b > 0)\nfbcon_bmove(vc, b, 0, b - count, 0,\nvc->vc_rows - b,\nvc->vc_cols);\nypan_down(vc, count);\nif (t > 0)\nfbcon_bmove(vc, count, 0, 0, 0, t,\nvc->vc_cols);\n} else if (info->flags & FBINFO_READS_FAST)\nfbcon_bmove(vc, t, 0, t + count, 0,\nb - t - count, vc->vc_cols);\nelse\ngoto redraw_down;\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_PAN_REDRAW:\nif ((count - p->yscroll <= p->vrows - vc->vc_rows)\n&& ((!scroll_partial && (b - t == vc->vc_rows))\n|| (scroll_partial\n&& (b - t - count >\n3 * vc->vc_rows >> 2)))) {\nif (vc->vc_rows - b > 0)\nfbcon_redraw_move(vc, p, b, vc->vc_rows - b,\nb - count);\nypan_down_redraw(vc, t, count);\nif (t > 0)\nfbcon_redraw_move(vc, p, count, t, 0);\n} else\nfbcon_redraw_move(vc, p, t, b - t - count, t + count);\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nbreak;\ncase SCROLL_REDRAW:\nredraw_down:\nfbcon_redraw(vc, p, b - 1, b - t - count,\n-count * vc->vc_cols);\nfbcon_clear(vc, t, 0, count, vc->vc_cols);\nscr_memsetw((unsigned short *) (vc->vc_origin +\nvc->vc_size_row *\nt),\nvc->vc_video_erase_char,\nvc->vc_size_row * count);\nreturn true;\n}\n}\nreturn false;\n}\n"
    }
  ],
  "CVE-2020-25212": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate handling of Time-of-Check Time-of-Use (TOCTOU) issues in the NFS client code.",
        "trigger_condition": "A local attacker exploits the TOCTOU mismatch by manipulating the size check to corrupt memory or cause other unspecified impacts.",
        "specific_code_behavior_causing_vulnerability": "The size check is performed in the wrong location (fs/nfs/nfs4proc.c instead of fs/nfs/nfs4xdr.c), leading to a race condition where the attacker can potentially corrupt memory due to the TOCTOU mismatch."
      },
      "solution": "To mitigate the vulnerability, the size check should be moved to the correct location in the fs/nfs/nfs4xdr.c file where the data is being processed. This ensures that the size check is performed at the right time to prevent exploitation of the TOCTOU issue. By making this adjustment, the code will handle the size check properly and avoid potential memory corruption or other impacts caused by the race condition.",
      "GPT_analysis": "The modification is necessary to address the Time-of-Check Time-of-Use (TOCTOU) vulnerability identified in the code snippet. In this case, the vulnerability arises from a size check being performed in the `fs/nfs/nfs4proc.c` file instead of the `fs/nfs/nfs4xdr.c` file. This discrepancy can lead to a race condition where an attacker could potentially manipulate the size check to corrupt memory or cause other unspecified impacts.\n\nTo fix this vulnerability, the size check should be moved to the appropriate location in the `fs/nfs/nfs4xdr.c` file where the data is being processed, ensuring that the check is performed at the correct time to prevent any potential exploitation of the TOCTOU issue.",
      "GPT_purpose": "Retrieve the security label of a file from an NFS server.",
      "GPT_function": "\n1. `_nfs4_get_security_label`: Retrieves the security label for a given inode.\n2. `nfs_fattr_init`: Initializes the NFS file attributes structure.\n3. `nfs4_call_sync`: Synchronously calls an NFSv4 procedure on the server.",
      "CVE_id": "CVE-2020-25212",
      "code_before_change": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\n\t\t\t\t\tsize_t buflen)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tstruct nfs_fattr fattr;\n\tstruct nfs4_label label = {0, 0, buflen, buf};\n\n\tu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\n\tstruct nfs4_getattr_arg arg = {\n\t\t.fh\t\t= NFS_FH(inode),\n\t\t.bitmask\t= bitmask,\n\t};\n\tstruct nfs4_getattr_res res = {\n\t\t.fattr\t\t= &fattr,\n\t\t.label\t\t= &label,\n\t\t.server\t\t= server,\n\t};\n\tstruct rpc_message msg = {\n\t\t.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n\t\t.rpc_argp\t= &arg,\n\t\t.rpc_resp\t= &res,\n\t};\n\tint ret;\n\n\tnfs_fattr_init(&fattr);\n\n\tret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\treturn ret;\n\tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n\t\treturn -ENOENT;\n\tif (buflen < label.len)\n\t\treturn -ERANGE;\n\treturn 0;\n}",
      "code_after_change": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\n\t\t\t\t\tsize_t buflen)\n{\n\tstruct nfs_server *server = NFS_SERVER(inode);\n\tstruct nfs_fattr fattr;\n\tstruct nfs4_label label = {0, 0, buflen, buf};\n\n\tu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\n\tstruct nfs4_getattr_arg arg = {\n\t\t.fh\t\t= NFS_FH(inode),\n\t\t.bitmask\t= bitmask,\n\t};\n\tstruct nfs4_getattr_res res = {\n\t\t.fattr\t\t= &fattr,\n\t\t.label\t\t= &label,\n\t\t.server\t\t= server,\n\t};\n\tstruct rpc_message msg = {\n\t\t.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n\t\t.rpc_argp\t= &arg,\n\t\t.rpc_resp\t= &res,\n\t};\n\tint ret;\n\n\tnfs_fattr_init(&fattr);\n\n\tret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\n\tif (ret)\n\t\treturn ret;\n\tif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\n\t\treturn -ENOENT;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tif (buflen < label.len)",
          "\t\treturn -ERANGE;"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate handling of Time-of-Check Time-of-Use (TOCTOU) issues in the NFS client code.",
      "trigger_condition": "A local attacker exploits the TOCTOU mismatch by manipulating the size check to corrupt memory or cause other unspecified impacts.",
      "specific_code_behavior_causing_vulnerability": "The size check is performed in the wrong location (fs/nfs/nfs4proc.c instead of fs/nfs/nfs4xdr.c), leading to a race condition where the attacker can potentially corrupt memory due to the TOCTOU mismatch.",
      "id": 89,
      "code_after_change_normalized": "static int FUN1(struct VAR1 *VAR1, void *VAR2,\nsize_t VAR3)\n{\nstruct nfs_server *VAR4 = FUN2(VAR1);\nstruct nfs_fattr VAR5;\nstruct nfs4_label VAR6 = {0, 0, VAR3, VAR2};\nu32 VAR7[3] = { 0, 0, VAR8 };\nstruct nfs4_getattr_arg VAR9 = {\n.VAR10\t\t= FUN3(VAR1),\n.VAR7\t= VAR7,\n};\nstruct nfs4_getattr_res VAR11 = {\n.VAR5\t\t= &VAR5,\n.VAR6\t\t= &VAR6,\n.VAR4\t\t= VAR4,\n};\nstruct rpc_message VAR12 = {\n.VAR13\t= &VAR14[VAR15],\n.VAR16\t= &VAR9,\n.VAR17\t= &VAR11,\n};\nint VAR18;\nFUN4(&VAR5);\nVAR18 = FUN5(VAR4->VAR19, VAR4, &VAR12, &VAR9.VAR20, &VAR11.VAR21, 0);\nif (VAR18)\nreturn VAR18;\nif (!(VAR5.VAR22 & VAR23))\nreturn -VAR24;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct VAR1 *VAR1, void *VAR2,\nsize_t VAR3)\n{\nstruct nfs_server *VAR4 = FUN2(VAR1);\nstruct nfs_fattr VAR5;\nstruct nfs4_label VAR6 = {0, 0, VAR3, VAR2};\nu32 VAR7[3] = { 0, 0, VAR8 };\nstruct nfs4_getattr_arg VAR9 = {\n.VAR10\t\t= FUN3(VAR1),\n.VAR7\t= VAR7,\n};\nstruct nfs4_getattr_res VAR11 = {\n.VAR5\t\t= &VAR5,\n.VAR6\t\t= &VAR6,\n.VAR4\t\t= VAR4,\n};\nstruct rpc_message VAR12 = {\n.VAR13\t= &VAR14[VAR15],\n.VAR16\t= &VAR9,\n.VAR17\t= &VAR11,\n};\nint VAR18;\nFUN4(&VAR5);\nVAR18 = FUN5(VAR4->VAR19, VAR4, &VAR12, &VAR9.VAR20, &VAR11.VAR21, 0);\nif (VAR18)\nreturn VAR18;\nif (!(VAR5.VAR22 & VAR23))\nreturn -VAR24;\nif (VAR3 < VAR6.VAR25)\nreturn -VAR26;\nreturn 0;\n}\n",
      "code_after_change_raw": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\nsize_t buflen)\n{\nstruct nfs_server *server = NFS_SERVER(inode);\nstruct nfs_fattr fattr;\nstruct nfs4_label label = {0, 0, buflen, buf};\nu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\nstruct nfs4_getattr_arg arg = {\n.fh\t\t= NFS_FH(inode),\n.bitmask\t= bitmask,\n};\nstruct nfs4_getattr_res res = {\n.fattr\t\t= &fattr,\n.label\t\t= &label,\n.server\t\t= server,\n};\nstruct rpc_message msg = {\n.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n.rpc_argp\t= &arg,\n.rpc_resp\t= &res,\n};\nint ret;\nnfs_fattr_init(&fattr);\nret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\nif (ret)\nreturn ret;\nif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\nreturn -ENOENT;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int _nfs4_get_security_label(struct inode *inode, void *buf,\nsize_t buflen)\n{\nstruct nfs_server *server = NFS_SERVER(inode);\nstruct nfs_fattr fattr;\nstruct nfs4_label label = {0, 0, buflen, buf};\nu32 bitmask[3] = { 0, 0, FATTR4_WORD2_SECURITY_LABEL };\nstruct nfs4_getattr_arg arg = {\n.fh\t\t= NFS_FH(inode),\n.bitmask\t= bitmask,\n};\nstruct nfs4_getattr_res res = {\n.fattr\t\t= &fattr,\n.label\t\t= &label,\n.server\t\t= server,\n};\nstruct rpc_message msg = {\n.rpc_proc\t= &nfs4_procedures[NFSPROC4_CLNT_GETATTR],\n.rpc_argp\t= &arg,\n.rpc_resp\t= &res,\n};\nint ret;\nnfs_fattr_init(&fattr);\nret = nfs4_call_sync(server->client, server, &msg, &arg.seq_args, &res.seq_res, 0);\nif (ret)\nreturn ret;\nif (!(fattr.valid & NFS_ATTR_FATTR_V4_SECURITY_LABEL))\nreturn -ENOENT;\nif (buflen < label.len)\nreturn -ERANGE;\nreturn 0;\n}\n"
    }
  ],
  "CVE-2020-25285": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper synchronization in handling hugetlb sysctl operations.",
        "trigger_condition": "Race condition between hugetlb sysctl handlers in the Linux kernel before version 5.8.8.",
        "specific_code_behavior_causing_vulnerability": "The code does not use the correct function to handle hugetlb sysctl operations, leading to a race condition vulnerability that could be exploited by local attackers to corrupt memory, cause a NULL pointer dereference, or have other unspecified impacts."
      },
      "solution": "To mitigate the vulnerability, it is necessary to replace the call to the incorrect function with the appropriate function that handles hugetlb sysctl operations securely. By using the correct function, the code ensures proper synchronization and prevents local attackers from exploiting the race condition to cause memory corruption or other adverse effects. In this case, the solution involves replacing the call to proc_doulongvec_minmax with proc_hugetlb_doulongvec_minmax to address the vulnerability.",
      "GPT_analysis": "The modification is necessary to address the race condition vulnerability described in CVE-2020-25285. By replacing the call to `proc_doulongvec_minmax` with `proc_hugetlb_doulongvec_minmax`, the code is updated to use the correct function that handles hugetlb sysctl operations in a safer manner. This modification helps prevent local attackers from exploiting the race condition to corrupt memory, cause a NULL pointer dereference, or potentially have other unspecified impacts on the system.",
      "GPT_purpose": "Handle overcommit settings for huge pages in the Linux kernel.",
      "GPT_function": "\n1. Handle overcommit of huge pages.\n2. Check if hugepages are supported.\n3. Update the number of overcommitted huge pages.",
      "CVE_id": "CVE-2020-25285",
      "code_before_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\ttable->data = &tmp;\n\ttable->maxlen = sizeof(unsigned long);\n\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
      "code_after_change": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\n\t\tvoid *buffer, size_t *length, loff_t *ppos)\n{\n\tstruct hstate *h = &default_hstate;\n\tunsigned long tmp;\n\tint ret;\n\n\tif (!hugepages_supported())\n\t\treturn -EOPNOTSUPP;\n\n\ttmp = h->nr_overcommit_huge_pages;\n\n\tif (write && hstate_is_gigantic(h))\n\t\treturn -EINVAL;\n\n\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n\t\t\t\t\t     &tmp);\n\tif (ret)\n\t\tgoto out;\n\n\tif (write) {\n\t\tspin_lock(&hugetlb_lock);\n\t\th->nr_overcommit_huge_pages = tmp;\n\t\tspin_unlock(&hugetlb_lock);\n\t}\nout:\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,",
          "\t\t\t\t\t     &tmp);"
        ],
        "deleted": [
          "\ttable->data = &tmp;",
          "\ttable->maxlen = sizeof(unsigned long);",
          "\tret = proc_doulongvec_minmax(table, write, buffer, length, ppos);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper synchronization in handling hugetlb sysctl operations.",
      "trigger_condition": "Race condition between hugetlb sysctl handlers in the Linux kernel before version 5.8.8.",
      "specific_code_behavior_causing_vulnerability": "The code does not use the correct function to handle hugetlb sysctl operations, leading to a race condition vulnerability that could be exploited by local attackers to corrupt memory, cause a NULL pointer dereference, or have other unspecified impacts.",
      "id": 90,
      "code_after_change_normalized": "int FUN1(struct ctl_table *VAR1, int VAR2,\nvoid *VAR3, size_t *VAR4, loff_t *VAR5)\n{\nstruct hstate *VAR6 = &VAR7;\nunsigned long VAR8;\nint VAR9;\nif (!FUN2())\nreturn -VAR10;\nVAR8 = VAR6->VAR11;\nif (VAR2 && FUN3(VAR6))\nreturn -VAR12;\nVAR9 = FUN4(VAR1, VAR2, VAR3, VAR4, VAR5,\n&VAR8);\nif (VAR9)\ngoto VAR13;\nif (VAR2) {\nFUN5(&VAR14);\nVAR6->VAR11 = VAR8;\nFUN6(&VAR14);\n}\nVAR13:\nreturn VAR9;\n}\n",
      "code_before_change_normalized": "int FUN1(struct ctl_table *VAR1, int VAR2,\nvoid *VAR3, size_t *VAR4, loff_t *VAR5)\n{\nstruct hstate *VAR6 = &VAR7;\nunsigned long VAR8;\nint VAR9;\nif (!FUN2())\nreturn -VAR10;\nVAR8 = VAR6->VAR11;\nif (VAR2 && FUN3(VAR6))\nreturn -VAR12;\nVAR1->VAR13 = &VAR8;\nVAR1->VAR14 = sizeof(unsigned long);\nVAR9 = FUN4(VAR1, VAR2, VAR3, VAR4, VAR5);\nif (VAR9)\ngoto VAR15;\nif (VAR2) {\nFUN5(&VAR16);\nVAR6->VAR11 = VAR8;\nFUN6(&VAR16);\n}\nVAR15:\nreturn VAR9;\n}\n",
      "code_after_change_raw": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\nvoid *buffer, size_t *length, loff_t *ppos)\n{\nstruct hstate *h = &default_hstate;\nunsigned long tmp;\nint ret;\nif (!hugepages_supported())\nreturn -EOPNOTSUPP;\ntmp = h->nr_overcommit_huge_pages;\nif (write && hstate_is_gigantic(h))\nreturn -EINVAL;\nret = proc_hugetlb_doulongvec_minmax(table, write, buffer, length, ppos,\n&tmp);\nif (ret)\ngoto out;\nif (write) {\nspin_lock(&hugetlb_lock);\nh->nr_overcommit_huge_pages = tmp;\nspin_unlock(&hugetlb_lock);\n}\nout:\nreturn ret;\n}\n",
      "code_before_change_raw": "int hugetlb_overcommit_handler(struct ctl_table *table, int write,\nvoid *buffer, size_t *length, loff_t *ppos)\n{\nstruct hstate *h = &default_hstate;\nunsigned long tmp;\nint ret;\nif (!hugepages_supported())\nreturn -EOPNOTSUPP;\ntmp = h->nr_overcommit_huge_pages;\nif (write && hstate_is_gigantic(h))\nreturn -EINVAL;\ntable->data = &tmp;\ntable->maxlen = sizeof(unsigned long);\nret = proc_doulongvec_minmax(table, write, buffer, length, ppos);\nif (ret)\ngoto out;\nif (write) {\nspin_lock(&hugetlb_lock);\nh->nr_overcommit_huge_pages = tmp;\nspin_unlock(&hugetlb_lock);\n}\nout:\nreturn ret;\n}\n"
    }
  ],
  "CVE-2020-8835": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper restriction of register bounds for 32-bit operations in the bpf verifier.",
        "trigger_condition": "Execution of code that manipulates register bounds without proper validation and restriction for 32-bit operations.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly restrict register bounds for 32-bit operations in the bpf verifier, leading to out-of-bounds reads and writes in kernel memory. This vulnerability arises due to inadequate validation and handling of register bounds during 32-bit operations, allowing for potential memory corruption and security risks."
      },
      "solution": "To mitigate the vulnerability, it is essential to ensure proper restriction and validation of register bounds for 32-bit operations in the bpf verifier. This involves updating the code to accurately handle register bounds during 32-bit operations, preventing out-of-bounds reads and writes in kernel memory. Specifically, the fix includes modifying the code to enforce appropriate restrictions on register bounds to prevent memory corruption and enhance system security.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-8835, which allows for out-of-bounds reads and writes in kernel memory due to improper restriction of register bounds for 32-bit operations in the bpf verifier. By making the specified changes, the code can be updated to prevent this security issue and ensure the proper handling of register bounds, thereby enhancing the security and stability of the system.",
      "GPT_purpose": "Set minimum and maximum bounds for registers based on the given value and opcode for BPF programs.",
      "GPT_function": "\n1. Set minimum and maximum values for registers based on the opcode and value provided.\n2. Update register bounds and offset based on the calculated values.\n3. Handle different jump conditions (JEQ, JNE, JSET, JGE, JGT, JSGE, JSGT, JLE, JLT, JSLE, JSLT) and adjust register values accordingly.",
      "CVE_id": "CVE-2020-8835",
      "code_before_change": "static void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode, bool is_jmp32)\n{\n\ts64 sval;\n\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tval = is_jmp32 ? (u32)val : val;\n\tsval = is_jmp32 ? (s64)(s32)val : (s64)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\tcase BPF_JNE:\n\t{\n\t\tstruct bpf_reg_state *reg =\n\t\t\topcode == BPF_JEQ ? true_reg : false_reg;\n\n\t\tif (is_jmp32) {\n\t\t\tu64 old_v = reg->var_off.value;\n\t\t\tu64 hi_mask = ~0xffffffffULL;\n\n\t\t\treg->var_off.value = (old_v & hi_mask) | val;\n\t\t\treg->var_off.mask &= hi_mask;\n\t\t} else {\n\t\t\t__mark_reg_known(reg, val);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tu64 false_umin = opcode == BPF_JGT ? val    : val + 1;\n\t\tu64 true_umax = opcode == BPF_JGT ? val - 1 : val;\n\n\t\tif (is_jmp32) {\n\t\t\tfalse_umin += gen_hi_min(false_reg->var_off);\n\t\t\ttrue_umax += gen_hi_max(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\ts64 false_smin = opcode == BPF_JSGT ? sval    : sval + 1;\n\t\ts64 true_smax = opcode == BPF_JSGT ? sval - 1 : sval;\n\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tu64 false_umax = opcode == BPF_JLT ? val    : val - 1;\n\t\tu64 true_umin = opcode == BPF_JLT ? val + 1 : val;\n\n\t\tif (is_jmp32) {\n\t\t\tfalse_umax += gen_hi_max(false_reg->var_off);\n\t\t\ttrue_umin += gen_hi_min(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\ts64 false_smax = opcode == BPF_JSLT ? sval    : sval - 1;\n\t\ts64 true_smin = opcode == BPF_JSLT ? sval + 1 : sval;\n\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\tif (is_jmp32) {\n\t\t__reg_bound_offset32(false_reg);\n\t\t__reg_bound_offset32(true_reg);\n\t}\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}",
      "code_after_change": "static void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\n\t\t\t\tstruct bpf_reg_state *false_reg, u64 val,\n\t\t\t\tu8 opcode, bool is_jmp32)\n{\n\ts64 sval;\n\n\tif (__is_pointer_value(false, false_reg))\n\t\treturn;\n\n\tval = is_jmp32 ? (u32)val : val;\n\tsval = is_jmp32 ? (s64)(s32)val : (s64)val;\n\n\tswitch (opcode) {\n\tcase BPF_JEQ:\n\tcase BPF_JNE:\n\t{\n\t\tstruct bpf_reg_state *reg =\n\t\t\topcode == BPF_JEQ ? true_reg : false_reg;\n\n\t\tif (is_jmp32) {\n\t\t\tu64 old_v = reg->var_off.value;\n\t\t\tu64 hi_mask = ~0xffffffffULL;\n\n\t\t\treg->var_off.value = (old_v & hi_mask) | val;\n\t\t\treg->var_off.mask &= hi_mask;\n\t\t} else {\n\t\t\t__mark_reg_known(reg, val);\n\t\t}\n\t\tbreak;\n\t}\n\tcase BPF_JSET:\n\t\tfalse_reg->var_off = tnum_and(false_reg->var_off,\n\t\t\t\t\t      tnum_const(~val));\n\t\tif (is_power_of_2(val))\n\t\t\ttrue_reg->var_off = tnum_or(true_reg->var_off,\n\t\t\t\t\t\t    tnum_const(val));\n\t\tbreak;\n\tcase BPF_JGE:\n\tcase BPF_JGT:\n\t{\n\t\tu64 false_umin = opcode == BPF_JGT ? val    : val + 1;\n\t\tu64 true_umax = opcode == BPF_JGT ? val - 1 : val;\n\n\t\tif (is_jmp32) {\n\t\t\tfalse_umin += gen_hi_min(false_reg->var_off);\n\t\t\ttrue_umax += gen_hi_max(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\n\t\ttrue_reg->umax_value = min(true_reg->umax_value, true_umax);\n\t\tbreak;\n\t}\n\tcase BPF_JSGE:\n\tcase BPF_JSGT:\n\t{\n\t\ts64 false_smin = opcode == BPF_JSGT ? sval    : sval + 1;\n\t\ts64 true_smax = opcode == BPF_JSGT ? sval - 1 : sval;\n\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\n\t\ttrue_reg->smax_value = min(true_reg->smax_value, true_smax);\n\t\tbreak;\n\t}\n\tcase BPF_JLE:\n\tcase BPF_JLT:\n\t{\n\t\tu64 false_umax = opcode == BPF_JLT ? val    : val - 1;\n\t\tu64 true_umin = opcode == BPF_JLT ? val + 1 : val;\n\n\t\tif (is_jmp32) {\n\t\t\tfalse_umax += gen_hi_max(false_reg->var_off);\n\t\t\ttrue_umin += gen_hi_min(true_reg->var_off);\n\t\t}\n\t\tfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\n\t\ttrue_reg->umin_value = max(true_reg->umin_value, true_umin);\n\t\tbreak;\n\t}\n\tcase BPF_JSLE:\n\tcase BPF_JSLT:\n\t{\n\t\ts64 false_smax = opcode == BPF_JSLT ? sval    : sval - 1;\n\t\ts64 true_smin = opcode == BPF_JSLT ? sval + 1 : sval;\n\n\t\tif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\n\t\t\tbreak;\n\t\tfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\n\t\ttrue_reg->smin_value = max(true_reg->smin_value, true_smin);\n\t\tbreak;\n\t}\n\tdefault:\n\t\tbreak;\n\t}\n\n\t__reg_deduce_bounds(false_reg);\n\t__reg_deduce_bounds(true_reg);\n\t/* We might have learned some bits from the bounds. */\n\t__reg_bound_offset(false_reg);\n\t__reg_bound_offset(true_reg);\n\t/* Intersecting with the old var_off might have improved our bounds\n\t * slightly.  e.g. if umax was 0x7f...f and var_off was (0; 0xf...fc),\n\t * then new var_off is (0; 0x7f...fc) which improves our umax.\n\t */\n\t__update_reg_bounds(false_reg);\n\t__update_reg_bounds(true_reg);\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tif (is_jmp32) {",
          "\t\t__reg_bound_offset32(false_reg);",
          "\t\t__reg_bound_offset32(true_reg);",
          "\t}"
        ]
      },
      "preconditions_for_vulnerability": "Improper restriction of register bounds for 32-bit operations in the bpf verifier.",
      "trigger_condition": "Execution of code that manipulates register bounds without proper validation and restriction for 32-bit operations.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly restrict register bounds for 32-bit operations in the bpf verifier, leading to out-of-bounds reads and writes in kernel memory. This vulnerability arises due to inadequate validation and handling of register bounds during 32-bit operations, allowing for potential memory corruption and security risks.",
      "id": 91,
      "code_after_change_normalized": "static void FUN1(struct bpf_reg_state *VAR1,\nstruct bpf_reg_state *VAR2, u64 VAR3,\nu8 VAR4, bool VAR5)\n{\ns64 VAR6;\nif (FUN2(false, VAR2))\nreturn;\nVAR3 = VAR5 ? (VAR7)VAR3 : VAR3;\nVAR6 = VAR5 ? (VAR8)(VAR9)VAR3 : (VAR8)VAR3;\nswitch (VAR4) {\ncase VAR10:\ncase VAR11:\n{\nstruct bpf_reg_state *VAR12 =\nVAR4 == VAR10 ? VAR1 : VAR2;\nif (VAR5) {\nu64 VAR13 = VAR12->VAR14.VAR15;\nu64 VAR16 = ~VAR17;\nVAR12->VAR14.VAR15 = (VAR13 & VAR16) | VAR3;\nVAR12->VAR14.VAR18 &= VAR16;\n} else {\nFUN3(VAR12, VAR3);\n}\nbreak;\n}\ncase VAR19:\nVAR2->VAR14 = FUN4(VAR2->VAR14,\nFUN5(~VAR3));\nif (FUN6(VAR3))\nVAR1->VAR14 = FUN7(VAR1->VAR14,\nFUN5(VAR3));\nbreak;\ncase VAR20:\ncase VAR21:\n{\nu64 VAR22 = VAR4 == VAR21 ? VAR3    : VAR3 + 1;\nu64 VAR23 = VAR4 == VAR21 ? VAR3 - 1 : VAR3;\nif (VAR5) {\nVAR22 += FUN8(VAR2->VAR14);\nVAR23 += FUN9(VAR1->VAR14);\n}\nVAR2->VAR24 = FUN10(VAR2->VAR24, VAR22);\nVAR1->VAR25 = FUN11(VAR1->VAR25, VAR23);\nbreak;\n}\ncase VAR26:\ncase VAR27:\n{\ns64 VAR28 = VAR4 == VAR27 ? VAR6    : VAR6 + 1;\ns64 VAR29 = VAR4 == VAR27 ? VAR6 - 1 : VAR6;\nif (VAR5 && !FUN12(VAR6, VAR2))\nbreak;\nVAR2->VAR30 = FUN10(VAR2->VAR30, VAR28);\nVAR1->VAR31 = FUN11(VAR1->VAR31, VAR29);\nbreak;\n}\ncase VAR32:\ncase VAR33:\n{\nu64 VAR34 = VAR4 == VAR33 ? VAR3    : VAR3 - 1;\nu64 VAR35 = VAR4 == VAR33 ? VAR3 + 1 : VAR3;\nif (VAR5) {\nVAR34 += FUN9(VAR2->VAR14);\nVAR35 += FUN8(VAR1->VAR14);\n}\nVAR2->VAR25 = FUN11(VAR2->VAR25, VAR34);\nVAR1->VAR24 = FUN10(VAR1->VAR24, VAR35);\nbreak;\n}\ncase VAR36:\ncase VAR37:\n{\ns64 VAR38 = VAR4 == VAR37 ? VAR6    : VAR6 - 1;\ns64 VAR39 = VAR4 == VAR37 ? VAR6 + 1 : VAR6;\nif (VAR5 && !FUN12(VAR6, VAR2))\nbreak;\nVAR2->VAR31 = FUN11(VAR2->VAR31, VAR38);\nVAR1->VAR30 = FUN10(VAR1->VAR30, VAR39);\nbreak;\n}\ndefault:\nbreak;\n}\nFUN13(VAR2);\nFUN13(VAR1);\nFUN14(VAR2);\nFUN14(VAR1);\nFUN15(VAR2);\nFUN15(VAR1);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct bpf_reg_state *VAR1,\nstruct bpf_reg_state *VAR2, u64 VAR3,\nu8 VAR4, bool VAR5)\n{\ns64 VAR6;\nif (FUN2(false, VAR2))\nreturn;\nVAR3 = VAR5 ? (VAR7)VAR3 : VAR3;\nVAR6 = VAR5 ? (VAR8)(VAR9)VAR3 : (VAR8)VAR3;\nswitch (VAR4) {\ncase VAR10:\ncase VAR11:\n{\nstruct bpf_reg_state *VAR12 =\nVAR4 == VAR10 ? VAR1 : VAR2;\nif (VAR5) {\nu64 VAR13 = VAR12->VAR14.VAR15;\nu64 VAR16 = ~VAR17;\nVAR12->VAR14.VAR15 = (VAR13 & VAR16) | VAR3;\nVAR12->VAR14.VAR18 &= VAR16;\n} else {\nFUN3(VAR12, VAR3);\n}\nbreak;\n}\ncase VAR19:\nVAR2->VAR14 = FUN4(VAR2->VAR14,\nFUN5(~VAR3));\nif (FUN6(VAR3))\nVAR1->VAR14 = FUN7(VAR1->VAR14,\nFUN5(VAR3));\nbreak;\ncase VAR20:\ncase VAR21:\n{\nu64 VAR22 = VAR4 == VAR21 ? VAR3    : VAR3 + 1;\nu64 VAR23 = VAR4 == VAR21 ? VAR3 - 1 : VAR3;\nif (VAR5) {\nVAR22 += FUN8(VAR2->VAR14);\nVAR23 += FUN9(VAR1->VAR14);\n}\nVAR2->VAR24 = FUN10(VAR2->VAR24, VAR22);\nVAR1->VAR25 = FUN11(VAR1->VAR25, VAR23);\nbreak;\n}\ncase VAR26:\ncase VAR27:\n{\ns64 VAR28 = VAR4 == VAR27 ? VAR6    : VAR6 + 1;\ns64 VAR29 = VAR4 == VAR27 ? VAR6 - 1 : VAR6;\nif (VAR5 && !FUN12(VAR6, VAR2))\nbreak;\nVAR2->VAR30 = FUN10(VAR2->VAR30, VAR28);\nVAR1->VAR31 = FUN11(VAR1->VAR31, VAR29);\nbreak;\n}\ncase VAR32:\ncase VAR33:\n{\nu64 VAR34 = VAR4 == VAR33 ? VAR3    : VAR3 - 1;\nu64 VAR35 = VAR4 == VAR33 ? VAR3 + 1 : VAR3;\nif (VAR5) {\nVAR34 += FUN9(VAR2->VAR14);\nVAR35 += FUN8(VAR1->VAR14);\n}\nVAR2->VAR25 = FUN11(VAR2->VAR25, VAR34);\nVAR1->VAR24 = FUN10(VAR1->VAR24, VAR35);\nbreak;\n}\ncase VAR36:\ncase VAR37:\n{\ns64 VAR38 = VAR4 == VAR37 ? VAR6    : VAR6 - 1;\ns64 VAR39 = VAR4 == VAR37 ? VAR6 + 1 : VAR6;\nif (VAR5 && !FUN12(VAR6, VAR2))\nbreak;\nVAR2->VAR31 = FUN11(VAR2->VAR31, VAR38);\nVAR1->VAR30 = FUN10(VAR1->VAR30, VAR39);\nbreak;\n}\ndefault:\nbreak;\n}\nFUN13(VAR2);\nFUN13(VAR1);\nFUN14(VAR2);\nFUN14(VAR1);\nif (VAR5) {\nFUN15(VAR2);\nFUN15(VAR1);\n}\nFUN16(VAR2);\nFUN16(VAR1);\n}\n",
      "code_after_change_raw": "static void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\nstruct bpf_reg_state *false_reg, u64 val,\nu8 opcode, bool is_jmp32)\n{\ns64 sval;\nif (__is_pointer_value(false, false_reg))\nreturn;\nval = is_jmp32 ? (u32)val : val;\nsval = is_jmp32 ? (s64)(s32)val : (s64)val;\nswitch (opcode) {\ncase BPF_JEQ:\ncase BPF_JNE:\n{\nstruct bpf_reg_state *reg =\nopcode == BPF_JEQ ? true_reg : false_reg;\nif (is_jmp32) {\nu64 old_v = reg->var_off.value;\nu64 hi_mask = ~0xffffffffULL;\nreg->var_off.value = (old_v & hi_mask) | val;\nreg->var_off.mask &= hi_mask;\n} else {\n__mark_reg_known(reg, val);\n}\nbreak;\n}\ncase BPF_JSET:\nfalse_reg->var_off = tnum_and(false_reg->var_off,\ntnum_const(~val));\nif (is_power_of_2(val))\ntrue_reg->var_off = tnum_or(true_reg->var_off,\ntnum_const(val));\nbreak;\ncase BPF_JGE:\ncase BPF_JGT:\n{\nu64 false_umin = opcode == BPF_JGT ? val    : val + 1;\nu64 true_umax = opcode == BPF_JGT ? val - 1 : val;\nif (is_jmp32) {\nfalse_umin += gen_hi_min(false_reg->var_off);\ntrue_umax += gen_hi_max(true_reg->var_off);\n}\nfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\ntrue_reg->umax_value = min(true_reg->umax_value, true_umax);\nbreak;\n}\ncase BPF_JSGE:\ncase BPF_JSGT:\n{\ns64 false_smin = opcode == BPF_JSGT ? sval    : sval + 1;\ns64 true_smax = opcode == BPF_JSGT ? sval - 1 : sval;\nif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\nbreak;\nfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\ntrue_reg->smax_value = min(true_reg->smax_value, true_smax);\nbreak;\n}\ncase BPF_JLE:\ncase BPF_JLT:\n{\nu64 false_umax = opcode == BPF_JLT ? val    : val - 1;\nu64 true_umin = opcode == BPF_JLT ? val + 1 : val;\nif (is_jmp32) {\nfalse_umax += gen_hi_max(false_reg->var_off);\ntrue_umin += gen_hi_min(true_reg->var_off);\n}\nfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\ntrue_reg->umin_value = max(true_reg->umin_value, true_umin);\nbreak;\n}\ncase BPF_JSLE:\ncase BPF_JSLT:\n{\ns64 false_smax = opcode == BPF_JSLT ? sval    : sval - 1;\ns64 true_smin = opcode == BPF_JSLT ? sval + 1 : sval;\nif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\nbreak;\nfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\ntrue_reg->smin_value = max(true_reg->smin_value, true_smin);\nbreak;\n}\ndefault:\nbreak;\n}\n__reg_deduce_bounds(false_reg);\n__reg_deduce_bounds(true_reg);\n__reg_bound_offset(false_reg);\n__reg_bound_offset(true_reg);\n__update_reg_bounds(false_reg);\n__update_reg_bounds(true_reg);\n}\n",
      "code_before_change_raw": "static void reg_set_min_max_inv(struct bpf_reg_state *true_reg,\nstruct bpf_reg_state *false_reg, u64 val,\nu8 opcode, bool is_jmp32)\n{\ns64 sval;\nif (__is_pointer_value(false, false_reg))\nreturn;\nval = is_jmp32 ? (u32)val : val;\nsval = is_jmp32 ? (s64)(s32)val : (s64)val;\nswitch (opcode) {\ncase BPF_JEQ:\ncase BPF_JNE:\n{\nstruct bpf_reg_state *reg =\nopcode == BPF_JEQ ? true_reg : false_reg;\nif (is_jmp32) {\nu64 old_v = reg->var_off.value;\nu64 hi_mask = ~0xffffffffULL;\nreg->var_off.value = (old_v & hi_mask) | val;\nreg->var_off.mask &= hi_mask;\n} else {\n__mark_reg_known(reg, val);\n}\nbreak;\n}\ncase BPF_JSET:\nfalse_reg->var_off = tnum_and(false_reg->var_off,\ntnum_const(~val));\nif (is_power_of_2(val))\ntrue_reg->var_off = tnum_or(true_reg->var_off,\ntnum_const(val));\nbreak;\ncase BPF_JGE:\ncase BPF_JGT:\n{\nu64 false_umin = opcode == BPF_JGT ? val    : val + 1;\nu64 true_umax = opcode == BPF_JGT ? val - 1 : val;\nif (is_jmp32) {\nfalse_umin += gen_hi_min(false_reg->var_off);\ntrue_umax += gen_hi_max(true_reg->var_off);\n}\nfalse_reg->umin_value = max(false_reg->umin_value, false_umin);\ntrue_reg->umax_value = min(true_reg->umax_value, true_umax);\nbreak;\n}\ncase BPF_JSGE:\ncase BPF_JSGT:\n{\ns64 false_smin = opcode == BPF_JSGT ? sval    : sval + 1;\ns64 true_smax = opcode == BPF_JSGT ? sval - 1 : sval;\nif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\nbreak;\nfalse_reg->smin_value = max(false_reg->smin_value, false_smin);\ntrue_reg->smax_value = min(true_reg->smax_value, true_smax);\nbreak;\n}\ncase BPF_JLE:\ncase BPF_JLT:\n{\nu64 false_umax = opcode == BPF_JLT ? val    : val - 1;\nu64 true_umin = opcode == BPF_JLT ? val + 1 : val;\nif (is_jmp32) {\nfalse_umax += gen_hi_max(false_reg->var_off);\ntrue_umin += gen_hi_min(true_reg->var_off);\n}\nfalse_reg->umax_value = min(false_reg->umax_value, false_umax);\ntrue_reg->umin_value = max(true_reg->umin_value, true_umin);\nbreak;\n}\ncase BPF_JSLE:\ncase BPF_JSLT:\n{\ns64 false_smax = opcode == BPF_JSLT ? sval    : sval - 1;\ns64 true_smin = opcode == BPF_JSLT ? sval + 1 : sval;\nif (is_jmp32 && !cmp_val_with_extended_s64(sval, false_reg))\nbreak;\nfalse_reg->smax_value = min(false_reg->smax_value, false_smax);\ntrue_reg->smin_value = max(true_reg->smin_value, true_smin);\nbreak;\n}\ndefault:\nbreak;\n}\n__reg_deduce_bounds(false_reg);\n__reg_deduce_bounds(true_reg);\n__reg_bound_offset(false_reg);\n__reg_bound_offset(true_reg);\nif (is_jmp32) {\n__reg_bound_offset32(false_reg);\n__reg_bound_offset32(true_reg);\n}\n__update_reg_bounds(false_reg);\n__update_reg_bounds(true_reg);\n}\n"
    }
  ],
  "CVE-2020-9391": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Ignoring the top byte in the address passed to the brk system call on the AArch64 architecture.",
        "trigger_condition": "The memory break moves downwards when the application expects it to move upwards, potentially causing heap corruption.",
        "specific_code_behavior_causing_vulnerability": "The code snippet fails to properly validate and adjust the address passed to the mmap system call, leading to the memory break moving in the wrong direction and exposing the system to heap corruption."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation step to check and adjust the address passed to the mmap system call. This ensures that the memory break moves in the expected direction and prevents potential heap corruption. In this case, the solution involves adding the 'addr = arch_mmap_check(addr);' line before the 'addr = untagged_addr(addr);' line in the code snippet.",
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2020-9391, which is caused by the Linux kernel ignoring the top byte in the address passed to the brk system call on the AArch64 architecture. This behavior can lead to the memory break moving downwards when the application expects it to move upwards, potentially causing heap corruption with the GNU C Library malloc implementation.\n\nTo address this vulnerability, the modification involves adding the following line before the `addr = untagged_addr(addr);` line in the code snippet:\n```\naddr = arch_mmap_check(addr);\n```\n\nBy adding this line, the `arch_mmap_check` function ensures that the address passed to the mmap system call is properly validated and adjusted if necessary, preventing the issue of the memory break moving in the wrong direction and mitigating the risk of heap corruption.",
      "GPT_purpose": "This function is responsible for mapping memory regions with specified attributes, including handling huge pages and file mappings.",
      "GPT_function": "\n1. Allocate memory mapping with specified attributes.\n2. Handle file-related operations for memory mapping.\n3. Adjust memory mapping size for huge pages if necessary.\n4. Set up memory mapping for huge pages if requested.\n5. Perform memory mapping operation with specified parameters.\n6. Release file-related resources after memory mapping operation.",
      "CVE_id": "CVE-2020-9391",
      "code_before_change": "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval;\n\n\taddr = untagged_addr(addr);\n\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\taudit_mmap_fd(fd, flags);\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\treturn -EBADF;\n\t\tif (is_file_hugepages(file))\n\t\t\tlen = ALIGN(len, huge_page_size(hstate_file(file)));\n\t\tretval = -EINVAL;\n\t\tif (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))\n\t\t\tgoto out_fput;\n\t} else if (flags & MAP_HUGETLB) {\n\t\tstruct user_struct *user = NULL;\n\t\tstruct hstate *hs;\n\n\t\ths = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (!hs)\n\t\t\treturn -EINVAL;\n\n\t\tlen = ALIGN(len, huge_page_size(hs));\n\t\t/*\n\t\t * VM_NORESERVE is used because the reservations will be\n\t\t * taken when vm_ops->mmap() is called\n\t\t * A dummy user value is used because we are not locking\n\t\t * memory so no accounting is necessary\n\t\t */\n\t\tfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\n\t\t\t\tVM_NORESERVE,\n\t\t\t\t&user, HUGETLB_ANONHUGE_INODE,\n\t\t\t\t(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (IS_ERR(file))\n\t\t\treturn PTR_ERR(file);\n\t}\n\n\tflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\n\tif (file)\n\t\tfput(file);\n\treturn retval;\n}",
      "code_after_change": "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\n\t\t\t      unsigned long prot, unsigned long flags,\n\t\t\t      unsigned long fd, unsigned long pgoff)\n{\n\tstruct file *file = NULL;\n\tunsigned long retval;\n\n\tif (!(flags & MAP_ANONYMOUS)) {\n\t\taudit_mmap_fd(fd, flags);\n\t\tfile = fget(fd);\n\t\tif (!file)\n\t\t\treturn -EBADF;\n\t\tif (is_file_hugepages(file))\n\t\t\tlen = ALIGN(len, huge_page_size(hstate_file(file)));\n\t\tretval = -EINVAL;\n\t\tif (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))\n\t\t\tgoto out_fput;\n\t} else if (flags & MAP_HUGETLB) {\n\t\tstruct user_struct *user = NULL;\n\t\tstruct hstate *hs;\n\n\t\ths = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (!hs)\n\t\t\treturn -EINVAL;\n\n\t\tlen = ALIGN(len, huge_page_size(hs));\n\t\t/*\n\t\t * VM_NORESERVE is used because the reservations will be\n\t\t * taken when vm_ops->mmap() is called\n\t\t * A dummy user value is used because we are not locking\n\t\t * memory so no accounting is necessary\n\t\t */\n\t\tfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\n\t\t\t\tVM_NORESERVE,\n\t\t\t\t&user, HUGETLB_ANONHUGE_INODE,\n\t\t\t\t(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\n\t\tif (IS_ERR(file))\n\t\t\treturn PTR_ERR(file);\n\t}\n\n\tflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\n\n\tretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\n\tif (file)\n\t\tfput(file);\n\treturn retval;\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "",
          "\taddr = untagged_addr(addr);"
        ]
      },
      "preconditions_for_vulnerability": "Ignoring the top byte in the address passed to the brk system call on the AArch64 architecture.",
      "trigger_condition": "The memory break moves downwards when the application expects it to move upwards, potentially causing heap corruption.",
      "specific_code_behavior_causing_vulnerability": "The code snippet fails to properly validate and adjust the address passed to the mmap system call, leading to the memory break moving in the wrong direction and exposing the system to heap corruption.",
      "id": 92,
      "code_after_change_normalized": "unsigned long FUN1(unsigned long VAR1, unsigned long VAR2,\nunsigned long VAR3, unsigned long VAR4,\nunsigned long VAR5, unsigned long VAR6)\n{\nstruct VAR7 *VAR7 = NULL;\nunsigned long VAR8;\nif (!(VAR4 & VAR9)) {\nFUN2(VAR5, VAR4);\nVAR7 = FUN3(VAR5);\nif (!VAR7)\nreturn -VAR10;\nif (FUN4(VAR7))\nVAR2 = FUN5(VAR2, FUN6(FUN7(VAR7)));\nVAR8 = -VAR11;\nif (FUN8(VAR4 & VAR12 && !FUN4(VAR7)))\ngoto VAR13;\n} else if (VAR4 & VAR12) {\nstruct user_struct *VAR14 = NULL;\nstruct hstate *VAR15;\nVAR15 = FUN9((VAR4 >> VAR16) & VAR17);\nif (!VAR15)\nreturn -VAR11;\nVAR2 = FUN5(VAR2, FUN6(VAR15));\nVAR7 = FUN10(VAR18, VAR2,\nVAR19,\n&VAR14, VAR20,\n(VAR4 >> VAR16) & VAR17);\nif (FUN11(VAR7))\nreturn FUN12(VAR7);\n}\nVAR4 &= ~(VAR21 | VAR22);\nVAR8 = FUN13(VAR7, VAR1, VAR2, VAR3, VAR4, VAR6);\nVAR13:\nif (VAR7)\nFUN14(VAR7);\nreturn VAR8;\n}\n",
      "code_before_change_normalized": "unsigned long FUN1(unsigned long VAR1, unsigned long VAR2,\nunsigned long VAR3, unsigned long VAR4,\nunsigned long VAR5, unsigned long VAR6)\n{\nstruct VAR7 *VAR7 = NULL;\nunsigned long VAR8;\nVAR1 = FUN2(VAR1);\nif (!(VAR4 & VAR9)) {\nFUN3(VAR5, VAR4);\nVAR7 = FUN4(VAR5);\nif (!VAR7)\nreturn -VAR10;\nif (FUN5(VAR7))\nVAR2 = FUN6(VAR2, FUN7(FUN8(VAR7)));\nVAR8 = -VAR11;\nif (FUN9(VAR4 & VAR12 && !FUN5(VAR7)))\ngoto VAR13;\n} else if (VAR4 & VAR12) {\nstruct user_struct *VAR14 = NULL;\nstruct hstate *VAR15;\nVAR15 = FUN10((VAR4 >> VAR16) & VAR17);\nif (!VAR15)\nreturn -VAR11;\nVAR2 = FUN6(VAR2, FUN7(VAR15));\nVAR7 = FUN11(VAR18, VAR2,\nVAR19,\n&VAR14, VAR20,\n(VAR4 >> VAR16) & VAR17);\nif (FUN12(VAR7))\nreturn FUN13(VAR7);\n}\nVAR4 &= ~(VAR21 | VAR22);\nVAR8 = FUN14(VAR7, VAR1, VAR2, VAR3, VAR4, VAR6);\nVAR13:\nif (VAR7)\nFUN15(VAR7);\nreturn VAR8;\n}\n",
      "code_after_change_raw": "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\nunsigned long prot, unsigned long flags,\nunsigned long fd, unsigned long pgoff)\n{\nstruct file *file = NULL;\nunsigned long retval;\nif (!(flags & MAP_ANONYMOUS)) {\naudit_mmap_fd(fd, flags);\nfile = fget(fd);\nif (!file)\nreturn -EBADF;\nif (is_file_hugepages(file))\nlen = ALIGN(len, huge_page_size(hstate_file(file)));\nretval = -EINVAL;\nif (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))\ngoto out_fput;\n} else if (flags & MAP_HUGETLB) {\nstruct user_struct *user = NULL;\nstruct hstate *hs;\nhs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\nif (!hs)\nreturn -EINVAL;\nlen = ALIGN(len, huge_page_size(hs));\nfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\nVM_NORESERVE,\n&user, HUGETLB_ANONHUGE_INODE,\n(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\nif (IS_ERR(file))\nreturn PTR_ERR(file);\n}\nflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\nretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\nif (file)\nfput(file);\nreturn retval;\n}\n",
      "code_before_change_raw": "unsigned long ksys_mmap_pgoff(unsigned long addr, unsigned long len,\nunsigned long prot, unsigned long flags,\nunsigned long fd, unsigned long pgoff)\n{\nstruct file *file = NULL;\nunsigned long retval;\naddr = untagged_addr(addr);\nif (!(flags & MAP_ANONYMOUS)) {\naudit_mmap_fd(fd, flags);\nfile = fget(fd);\nif (!file)\nreturn -EBADF;\nif (is_file_hugepages(file))\nlen = ALIGN(len, huge_page_size(hstate_file(file)));\nretval = -EINVAL;\nif (unlikely(flags & MAP_HUGETLB && !is_file_hugepages(file)))\ngoto out_fput;\n} else if (flags & MAP_HUGETLB) {\nstruct user_struct *user = NULL;\nstruct hstate *hs;\nhs = hstate_sizelog((flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\nif (!hs)\nreturn -EINVAL;\nlen = ALIGN(len, huge_page_size(hs));\nfile = hugetlb_file_setup(HUGETLB_ANON_FILE, len,\nVM_NORESERVE,\n&user, HUGETLB_ANONHUGE_INODE,\n(flags >> MAP_HUGE_SHIFT) & MAP_HUGE_MASK);\nif (IS_ERR(file))\nreturn PTR_ERR(file);\n}\nflags &= ~(MAP_EXECUTABLE | MAP_DENYWRITE);\nretval = vm_mmap_pgoff(file, addr, len, prot, flags, pgoff);\nout_fput:\nif (file)\nfput(file);\nreturn retval;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Ignoring the top byte in the address passed to the brk system call on the AArch64 architecture.",
        "trigger_condition": "The code moves the memory break downwards instead of upwards due to the top byte being ignored, potentially causing heap corruption with the GNU C Library malloc implementation.",
        "specific_code_behavior_causing_vulnerability": "The code snippet for the mremap system call ignores the top byte in the address passed to the brk system call, leading to unexpected movement of the memory break downwards instead of upwards. This behavior can result in heap corruption when the application expects the memory break to move in the opposite direction."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the top byte in the address passed to the brk system call is properly handled. This involves modifying the code to consider the full address and prevent the memory break from moving in the wrong direction. By addressing this issue, the code can avoid heap corruption and ensure the correct behavior of memory management operations.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2020-9391, which involves the Linux kernel ignoring the top byte in the address passed to the brk system call on the AArch64 architecture. This behavior can lead to unexpected movement of the memory break downwards instead of upwards, potentially causing heap corruption with the GNU C Library malloc implementation.\n\nBy adding the {modified_lines} to the code snippet, you can ensure that the top byte of the address is properly handled, preventing the memory break from moving in the wrong direction and mitigating the risk of heap corruption. This modification helps to address the vulnerability and ensure the correct behavior of the memory management operations in the kernel.",
      "GPT_purpose": "This function implements the mremap system call in the Linux kernel for remapping memory regions.",
      "GPT_function": "\n1. Handle the mremap system call for memory remapping.\n2. Check and validate the input parameters for the mremap call.\n3. Perform memory remapping operations based on the specified flags and lengths.\n4. Handle scenarios for expanding or shrinking memory regions.\n5. Manage memory mappings and adjustments during the remapping process.\n6. Ensure proper memory allocation and deallocation during remapping.\n7. Address potential issues related to memory break manipulation.",
      "CVE_id": "CVE-2020-9391",
      "code_before_change": " */\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tbool locked = false;\n\tbool downgraded = false;\n\tstruct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;\n\tLIST_HEAD(uf_unmap_early);\n\tLIST_HEAD(uf_unmap);\n\n\taddr = untagged_addr(addr);\n\tnew_addr = untagged_addr(new_addr);\n\n\tif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))\n\t\treturn ret;\n\n\tif (flags & MREMAP_FIXED && !(flags & MREMAP_MAYMOVE))\n\t\treturn ret;\n\n\tif (offset_in_page(addr))\n\t\treturn ret;\n\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\n\t/*\n\t * We allow a zero old-len as a special case\n\t * for DOS-emu \"duplicate shm area\" thing. But\n\t * a zero new-len is nonsensical.\n\t */\n\tif (!new_len)\n\t\treturn ret;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tif (flags & MREMAP_FIXED) {\n\t\tret = mremap_to(addr, old_len, new_addr, new_len,\n\t\t\t\t&locked, &uf, &uf_unmap_early, &uf_unmap);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Always allow a shrinking remap: that just unmaps\n\t * the unnecessary pages..\n\t * __do_munmap does all the needed commit accounting, and\n\t * downgrades mmap_sem to read if so directed.\n\t */\n\tif (old_len >= new_len) {\n\t\tint retval;\n\n\t\tretval = __do_munmap(mm, addr+new_len, old_len - new_len,\n\t\t\t\t  &uf_unmap, true);\n\t\tif (retval < 0 && old_len != new_len) {\n\t\t\tret = retval;\n\t\t\tgoto out;\n\t\t/* Returning 1 indicates mmap_sem is downgraded to read. */\n\t\t} else if (retval == 1)\n\t\t\tdowngraded = true;\n\t\tret = addr;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Ok, we need to grow..\n\t */\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\t/* old_len exactly to the end of the area..\n\t */\n\tif (old_len == vma->vm_end - addr) {\n\t\t/* can we just expand the current mapping? */\n\t\tif (vma_expandable(vma, new_len - old_len)) {\n\t\t\tint pages = (new_len - old_len) >> PAGE_SHIFT;\n\n\t\t\tif (vma_adjust(vma, vma->vm_start, addr + new_len,\n\t\t\t\t       vma->vm_pgoff, NULL)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tvm_stat_account(mm, vma->vm_flags, pages);\n\t\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm += pages;\n\t\t\t\tlocked = true;\n\t\t\t\tnew_addr = addr;\n\t\t\t}\n\t\t\tret = addr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * We weren't able to just expand or shrink the area,\n\t * we need to create a new one and move it..\n\t */\n\tret = -ENOMEM;\n\tif (flags & MREMAP_MAYMOVE) {\n\t\tunsigned long map_flags = 0;\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tmap_flags |= MAP_SHARED;\n\n\t\tnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\n\t\t\t\t\tvma->vm_pgoff +\n\t\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\t\tmap_flags);\n\t\tif (IS_ERR_VALUE(new_addr)) {\n\t\t\tret = new_addr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = move_vma(vma, addr, old_len, new_len, new_addr,\n\t\t\t       &locked, &uf, &uf_unmap);\n\t}\nout:\n\tif (offset_in_page(ret)) {\n\t\tvm_unacct_memory(charged);\n\t\tlocked = 0;\n\t}\n\tif (downgraded)\n\t\tup_read(&current->mm->mmap_sem);\n\telse\n\t\tup_write(&current->mm->mmap_sem);\n\tif (locked && new_len > old_len)\n\t\tmm_populate(new_addr + old_len, new_len - old_len);\n\tuserfaultfd_unmap_complete(mm, &uf_unmap_early);\n\tmremap_userfaultfd_complete(&uf, addr, new_addr, old_len);\n\tuserfaultfd_unmap_complete(mm, &uf_unmap);\n\treturn ret;\n}",
      "code_after_change": " */\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\n\t\tunsigned long, new_len, unsigned long, flags,\n\t\tunsigned long, new_addr)\n{\n\tstruct mm_struct *mm = current->mm;\n\tstruct vm_area_struct *vma;\n\tunsigned long ret = -EINVAL;\n\tunsigned long charged = 0;\n\tbool locked = false;\n\tbool downgraded = false;\n\tstruct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;\n\tLIST_HEAD(uf_unmap_early);\n\tLIST_HEAD(uf_unmap);\n\n\taddr = untagged_addr(addr);\n\n\tif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))\n\t\treturn ret;\n\n\tif (flags & MREMAP_FIXED && !(flags & MREMAP_MAYMOVE))\n\t\treturn ret;\n\n\tif (offset_in_page(addr))\n\t\treturn ret;\n\n\told_len = PAGE_ALIGN(old_len);\n\tnew_len = PAGE_ALIGN(new_len);\n\n\t/*\n\t * We allow a zero old-len as a special case\n\t * for DOS-emu \"duplicate shm area\" thing. But\n\t * a zero new-len is nonsensical.\n\t */\n\tif (!new_len)\n\t\treturn ret;\n\n\tif (down_write_killable(&current->mm->mmap_sem))\n\t\treturn -EINTR;\n\n\tif (flags & MREMAP_FIXED) {\n\t\tret = mremap_to(addr, old_len, new_addr, new_len,\n\t\t\t\t&locked, &uf, &uf_unmap_early, &uf_unmap);\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Always allow a shrinking remap: that just unmaps\n\t * the unnecessary pages..\n\t * __do_munmap does all the needed commit accounting, and\n\t * downgrades mmap_sem to read if so directed.\n\t */\n\tif (old_len >= new_len) {\n\t\tint retval;\n\n\t\tretval = __do_munmap(mm, addr+new_len, old_len - new_len,\n\t\t\t\t  &uf_unmap, true);\n\t\tif (retval < 0 && old_len != new_len) {\n\t\t\tret = retval;\n\t\t\tgoto out;\n\t\t/* Returning 1 indicates mmap_sem is downgraded to read. */\n\t\t} else if (retval == 1)\n\t\t\tdowngraded = true;\n\t\tret = addr;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * Ok, we need to grow..\n\t */\n\tvma = vma_to_resize(addr, old_len, new_len, &charged);\n\tif (IS_ERR(vma)) {\n\t\tret = PTR_ERR(vma);\n\t\tgoto out;\n\t}\n\n\t/* old_len exactly to the end of the area..\n\t */\n\tif (old_len == vma->vm_end - addr) {\n\t\t/* can we just expand the current mapping? */\n\t\tif (vma_expandable(vma, new_len - old_len)) {\n\t\t\tint pages = (new_len - old_len) >> PAGE_SHIFT;\n\n\t\t\tif (vma_adjust(vma, vma->vm_start, addr + new_len,\n\t\t\t\t       vma->vm_pgoff, NULL)) {\n\t\t\t\tret = -ENOMEM;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tvm_stat_account(mm, vma->vm_flags, pages);\n\t\t\tif (vma->vm_flags & VM_LOCKED) {\n\t\t\t\tmm->locked_vm += pages;\n\t\t\t\tlocked = true;\n\t\t\t\tnew_addr = addr;\n\t\t\t}\n\t\t\tret = addr;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/*\n\t * We weren't able to just expand or shrink the area,\n\t * we need to create a new one and move it..\n\t */\n\tret = -ENOMEM;\n\tif (flags & MREMAP_MAYMOVE) {\n\t\tunsigned long map_flags = 0;\n\t\tif (vma->vm_flags & VM_MAYSHARE)\n\t\t\tmap_flags |= MAP_SHARED;\n\n\t\tnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\n\t\t\t\t\tvma->vm_pgoff +\n\t\t\t\t\t((addr - vma->vm_start) >> PAGE_SHIFT),\n\t\t\t\t\tmap_flags);\n\t\tif (IS_ERR_VALUE(new_addr)) {\n\t\t\tret = new_addr;\n\t\t\tgoto out;\n\t\t}\n\n\t\tret = move_vma(vma, addr, old_len, new_len, new_addr,\n\t\t\t       &locked, &uf, &uf_unmap);\n\t}\nout:\n\tif (offset_in_page(ret)) {\n\t\tvm_unacct_memory(charged);\n\t\tlocked = 0;\n\t}\n\tif (downgraded)\n\t\tup_read(&current->mm->mmap_sem);\n\telse\n\t\tup_write(&current->mm->mmap_sem);\n\tif (locked && new_len > old_len)\n\t\tmm_populate(new_addr + old_len, new_len - old_len);\n\tuserfaultfd_unmap_complete(mm, &uf_unmap_early);\n\tmremap_userfaultfd_complete(&uf, addr, new_addr, old_len);\n\tuserfaultfd_unmap_complete(mm, &uf_unmap);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tnew_addr = untagged_addr(new_addr);"
        ]
      },
      "preconditions_for_vulnerability": "Ignoring the top byte in the address passed to the brk system call on the AArch64 architecture.",
      "trigger_condition": "The code moves the memory break downwards instead of upwards due to the top byte being ignored, potentially causing heap corruption with the GNU C Library malloc implementation.",
      "specific_code_behavior_causing_vulnerability": "The code snippet for the mremap system call ignores the top byte in the address passed to the brk system call, leading to unexpected movement of the memory break downwards instead of upwards. This behavior can result in heap corruption when the application expects the memory break to move in the opposite direction.",
      "id": 93,
      "code_after_change_normalized": "*/\nFUN1(VAR1, unsigned long, VAR2, unsigned long, VAR3,\nunsigned long, VAR4, unsigned long, VAR5,\nunsigned long, VAR6)\n{\nstruct mm_struct *VAR7 = VAR8->VAR7;\nstruct vm_area_struct *VAR9;\nunsigned long VAR10 = -VAR11;\nunsigned long VAR12 = 0;\nbool VAR13 = false;\nbool VAR14 = false;\nstruct vm_userfaultfd_ctx VAR15 = VAR16;\nFUN2(VAR17);\nFUN2(VAR18);\nVAR2 = FUN3(VAR2);\nif (VAR5 & ~(VAR19 | VAR20))\nreturn VAR10;\nif (VAR5 & VAR19 && !(VAR5 & VAR20))\nreturn VAR10;\nif (FUN4(VAR2))\nreturn VAR10;\nVAR3 = FUN5(VAR3);\nVAR4 = FUN5(VAR4);\nif (!VAR4)\nreturn VAR10;\nif (FUN6(&VAR8->VAR7->VAR21))\nreturn -VAR22;\nif (VAR5 & VAR19) {\nVAR10 = FUN7(VAR2, VAR3, VAR6, VAR4,\n&VAR13, &VAR15, &VAR17, &VAR18);\ngoto VAR23;\n}\nif (VAR3 >= VAR4) {\nint VAR24;\nVAR24 = FUN8(VAR7, VAR2+VAR4, VAR3 - VAR4,\n&VAR18, true);\nif (VAR24 < 0 && VAR3 != VAR4) {\nVAR10 = VAR24;\ngoto VAR23;\n} else if (VAR24 == 1)\nVAR14 = true;\nVAR10 = VAR2;\ngoto VAR23;\n}\nVAR9 = FUN9(VAR2, VAR3, VAR4, &VAR12);\nif (FUN10(VAR9)) {\nVAR10 = FUN11(VAR9);\ngoto VAR23;\n}\nif (VAR3 == VAR9->VAR25 - VAR2) {\nif (FUN12(VAR9, VAR4 - VAR3)) {\nint VAR26 = (VAR4 - VAR3) >> VAR27;\nif (FUN13(VAR9, VAR9->VAR28, VAR2 + VAR4,\nVAR9->VAR29, NULL)) {\nVAR10 = -VAR30;\ngoto VAR23;\n}\nFUN14(VAR7, VAR9->VAR31, VAR26);\nif (VAR9->VAR31 & VAR32) {\nVAR7->VAR33 += VAR26;\nVAR13 = true;\nVAR6 = VAR2;\n}\nVAR10 = VAR2;\ngoto VAR23;\n}\n}\nVAR10 = -VAR30;\nif (VAR5 & VAR20) {\nunsigned long VAR34 = 0;\nif (VAR9->VAR31 & VAR35)\nVAR34 |= VAR36;\nVAR6 = FUN15(VAR9->VAR37, 0, VAR4,\nVAR9->VAR29 +\n((VAR2 - VAR9->VAR28) >> VAR27),\nVAR34);\nif (FUN16(VAR6)) {\nVAR10 = VAR6;\ngoto VAR23;\n}\nVAR10 = FUN17(VAR9, VAR2, VAR3, VAR4, VAR6,\n&VAR13, &VAR15, &VAR18);\n}\nVAR23:\nif (FUN4(VAR10)) {\nFUN18(VAR12);\nVAR13 = 0;\n}\nif (VAR14)\nFUN19(&VAR8->VAR7->VAR21);\nelse\nFUN20(&VAR8->VAR7->VAR21);\nif (VAR13 && VAR4 > VAR3)\nFUN21(VAR6 + VAR3, VAR4 - VAR3);\nFUN22(VAR7, &VAR17);\nFUN23(&VAR15, VAR2, VAR6, VAR3);\nFUN22(VAR7, &VAR18);\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "*/\nFUN1(VAR1, unsigned long, VAR2, unsigned long, VAR3,\nunsigned long, VAR4, unsigned long, VAR5,\nunsigned long, VAR6)\n{\nstruct mm_struct *VAR7 = VAR8->VAR7;\nstruct vm_area_struct *VAR9;\nunsigned long VAR10 = -VAR11;\nunsigned long VAR12 = 0;\nbool VAR13 = false;\nbool VAR14 = false;\nstruct vm_userfaultfd_ctx VAR15 = VAR16;\nFUN2(VAR17);\nFUN2(VAR18);\nVAR2 = FUN3(VAR2);\nVAR6 = FUN3(VAR6);\nif (VAR5 & ~(VAR19 | VAR20))\nreturn VAR10;\nif (VAR5 & VAR19 && !(VAR5 & VAR20))\nreturn VAR10;\nif (FUN4(VAR2))\nreturn VAR10;\nVAR3 = FUN5(VAR3);\nVAR4 = FUN5(VAR4);\nif (!VAR4)\nreturn VAR10;\nif (FUN6(&VAR8->VAR7->VAR21))\nreturn -VAR22;\nif (VAR5 & VAR19) {\nVAR10 = FUN7(VAR2, VAR3, VAR6, VAR4,\n&VAR13, &VAR15, &VAR17, &VAR18);\ngoto VAR23;\n}\nif (VAR3 >= VAR4) {\nint VAR24;\nVAR24 = FUN8(VAR7, VAR2+VAR4, VAR3 - VAR4,\n&VAR18, true);\nif (VAR24 < 0 && VAR3 != VAR4) {\nVAR10 = VAR24;\ngoto VAR23;\n} else if (VAR24 == 1)\nVAR14 = true;\nVAR10 = VAR2;\ngoto VAR23;\n}\nVAR9 = FUN9(VAR2, VAR3, VAR4, &VAR12);\nif (FUN10(VAR9)) {\nVAR10 = FUN11(VAR9);\ngoto VAR23;\n}\nif (VAR3 == VAR9->VAR25 - VAR2) {\nif (FUN12(VAR9, VAR4 - VAR3)) {\nint VAR26 = (VAR4 - VAR3) >> VAR27;\nif (FUN13(VAR9, VAR9->VAR28, VAR2 + VAR4,\nVAR9->VAR29, NULL)) {\nVAR10 = -VAR30;\ngoto VAR23;\n}\nFUN14(VAR7, VAR9->VAR31, VAR26);\nif (VAR9->VAR31 & VAR32) {\nVAR7->VAR33 += VAR26;\nVAR13 = true;\nVAR6 = VAR2;\n}\nVAR10 = VAR2;\ngoto VAR23;\n}\n}\nVAR10 = -VAR30;\nif (VAR5 & VAR20) {\nunsigned long VAR34 = 0;\nif (VAR9->VAR31 & VAR35)\nVAR34 |= VAR36;\nVAR6 = FUN15(VAR9->VAR37, 0, VAR4,\nVAR9->VAR29 +\n((VAR2 - VAR9->VAR28) >> VAR27),\nVAR34);\nif (FUN16(VAR6)) {\nVAR10 = VAR6;\ngoto VAR23;\n}\nVAR10 = FUN17(VAR9, VAR2, VAR3, VAR4, VAR6,\n&VAR13, &VAR15, &VAR18);\n}\nVAR23:\nif (FUN4(VAR10)) {\nFUN18(VAR12);\nVAR13 = 0;\n}\nif (VAR14)\nFUN19(&VAR8->VAR7->VAR21);\nelse\nFUN20(&VAR8->VAR7->VAR21);\nif (VAR13 && VAR4 > VAR3)\nFUN21(VAR6 + VAR3, VAR4 - VAR3);\nFUN22(VAR7, &VAR17);\nFUN23(&VAR15, VAR2, VAR6, VAR3);\nFUN22(VAR7, &VAR18);\nreturn VAR10;\n}\n",
      "code_after_change_raw": "*/\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\nunsigned long, new_len, unsigned long, flags,\nunsigned long, new_addr)\n{\nstruct mm_struct *mm = current->mm;\nstruct vm_area_struct *vma;\nunsigned long ret = -EINVAL;\nunsigned long charged = 0;\nbool locked = false;\nbool downgraded = false;\nstruct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;\nLIST_HEAD(uf_unmap_early);\nLIST_HEAD(uf_unmap);\naddr = untagged_addr(addr);\nif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))\nreturn ret;\nif (flags & MREMAP_FIXED && !(flags & MREMAP_MAYMOVE))\nreturn ret;\nif (offset_in_page(addr))\nreturn ret;\nold_len = PAGE_ALIGN(old_len);\nnew_len = PAGE_ALIGN(new_len);\nif (!new_len)\nreturn ret;\nif (down_write_killable(&current->mm->mmap_sem))\nreturn -EINTR;\nif (flags & MREMAP_FIXED) {\nret = mremap_to(addr, old_len, new_addr, new_len,\n&locked, &uf, &uf_unmap_early, &uf_unmap);\ngoto out;\n}\nif (old_len >= new_len) {\nint retval;\nretval = __do_munmap(mm, addr+new_len, old_len - new_len,\n&uf_unmap, true);\nif (retval < 0 && old_len != new_len) {\nret = retval;\ngoto out;\n} else if (retval == 1)\ndowngraded = true;\nret = addr;\ngoto out;\n}\nvma = vma_to_resize(addr, old_len, new_len, &charged);\nif (IS_ERR(vma)) {\nret = PTR_ERR(vma);\ngoto out;\n}\nif (old_len == vma->vm_end - addr) {\nif (vma_expandable(vma, new_len - old_len)) {\nint pages = (new_len - old_len) >> PAGE_SHIFT;\nif (vma_adjust(vma, vma->vm_start, addr + new_len,\nvma->vm_pgoff, NULL)) {\nret = -ENOMEM;\ngoto out;\n}\nvm_stat_account(mm, vma->vm_flags, pages);\nif (vma->vm_flags & VM_LOCKED) {\nmm->locked_vm += pages;\nlocked = true;\nnew_addr = addr;\n}\nret = addr;\ngoto out;\n}\n}\nret = -ENOMEM;\nif (flags & MREMAP_MAYMOVE) {\nunsigned long map_flags = 0;\nif (vma->vm_flags & VM_MAYSHARE)\nmap_flags |= MAP_SHARED;\nnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\nvma->vm_pgoff +\n((addr - vma->vm_start) >> PAGE_SHIFT),\nmap_flags);\nif (IS_ERR_VALUE(new_addr)) {\nret = new_addr;\ngoto out;\n}\nret = move_vma(vma, addr, old_len, new_len, new_addr,\n&locked, &uf, &uf_unmap);\n}\nout:\nif (offset_in_page(ret)) {\nvm_unacct_memory(charged);\nlocked = 0;\n}\nif (downgraded)\nup_read(&current->mm->mmap_sem);\nelse\nup_write(&current->mm->mmap_sem);\nif (locked && new_len > old_len)\nmm_populate(new_addr + old_len, new_len - old_len);\nuserfaultfd_unmap_complete(mm, &uf_unmap_early);\nmremap_userfaultfd_complete(&uf, addr, new_addr, old_len);\nuserfaultfd_unmap_complete(mm, &uf_unmap);\nreturn ret;\n}\n",
      "code_before_change_raw": "*/\nSYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,\nunsigned long, new_len, unsigned long, flags,\nunsigned long, new_addr)\n{\nstruct mm_struct *mm = current->mm;\nstruct vm_area_struct *vma;\nunsigned long ret = -EINVAL;\nunsigned long charged = 0;\nbool locked = false;\nbool downgraded = false;\nstruct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;\nLIST_HEAD(uf_unmap_early);\nLIST_HEAD(uf_unmap);\naddr = untagged_addr(addr);\nnew_addr = untagged_addr(new_addr);\nif (flags & ~(MREMAP_FIXED | MREMAP_MAYMOVE))\nreturn ret;\nif (flags & MREMAP_FIXED && !(flags & MREMAP_MAYMOVE))\nreturn ret;\nif (offset_in_page(addr))\nreturn ret;\nold_len = PAGE_ALIGN(old_len);\nnew_len = PAGE_ALIGN(new_len);\nif (!new_len)\nreturn ret;\nif (down_write_killable(&current->mm->mmap_sem))\nreturn -EINTR;\nif (flags & MREMAP_FIXED) {\nret = mremap_to(addr, old_len, new_addr, new_len,\n&locked, &uf, &uf_unmap_early, &uf_unmap);\ngoto out;\n}\nif (old_len >= new_len) {\nint retval;\nretval = __do_munmap(mm, addr+new_len, old_len - new_len,\n&uf_unmap, true);\nif (retval < 0 && old_len != new_len) {\nret = retval;\ngoto out;\n} else if (retval == 1)\ndowngraded = true;\nret = addr;\ngoto out;\n}\nvma = vma_to_resize(addr, old_len, new_len, &charged);\nif (IS_ERR(vma)) {\nret = PTR_ERR(vma);\ngoto out;\n}\nif (old_len == vma->vm_end - addr) {\nif (vma_expandable(vma, new_len - old_len)) {\nint pages = (new_len - old_len) >> PAGE_SHIFT;\nif (vma_adjust(vma, vma->vm_start, addr + new_len,\nvma->vm_pgoff, NULL)) {\nret = -ENOMEM;\ngoto out;\n}\nvm_stat_account(mm, vma->vm_flags, pages);\nif (vma->vm_flags & VM_LOCKED) {\nmm->locked_vm += pages;\nlocked = true;\nnew_addr = addr;\n}\nret = addr;\ngoto out;\n}\n}\nret = -ENOMEM;\nif (flags & MREMAP_MAYMOVE) {\nunsigned long map_flags = 0;\nif (vma->vm_flags & VM_MAYSHARE)\nmap_flags |= MAP_SHARED;\nnew_addr = get_unmapped_area(vma->vm_file, 0, new_len,\nvma->vm_pgoff +\n((addr - vma->vm_start) >> PAGE_SHIFT),\nmap_flags);\nif (IS_ERR_VALUE(new_addr)) {\nret = new_addr;\ngoto out;\n}\nret = move_vma(vma, addr, old_len, new_len, new_addr,\n&locked, &uf, &uf_unmap);\n}\nout:\nif (offset_in_page(ret)) {\nvm_unacct_memory(charged);\nlocked = 0;\n}\nif (downgraded)\nup_read(&current->mm->mmap_sem);\nelse\nup_write(&current->mm->mmap_sem);\nif (locked && new_len > old_len)\nmm_populate(new_addr + old_len, new_len - old_len);\nuserfaultfd_unmap_complete(mm, &uf_unmap_early);\nmremap_userfaultfd_complete(&uf, addr, new_addr, old_len);\nuserfaultfd_unmap_complete(mm, &uf_unmap);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2021-0512": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of buffer size calculations and field registration in a HID parser function.",
        "trigger_condition": "The HID parser function does not correctly validate the buffer size and field registration, leading to a possible heap buffer overflow.",
        "specific_code_behavior_causing_vulnerability": "The vulnerable code calculates the buffer size based on the product of report size and report count, potentially exceeding the maximum buffer size. Additionally, the function incorrectly registers fields with an incorrect number of usages, which can result in a heap buffer overflow."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper validation of buffer size calculations and field registration in the HID parser function. Specifically, the code should accurately calculate the buffer size to prevent exceeding the maximum buffer size and correctly register fields with the appropriate number of usages to avoid heap buffer overflows. In this case, the solution involves modifying the code to register fields with the correct number of usages and validate the buffer size to prevent overflow.",
      "GPT_analysis": "The modification is necessary to address a possible out-of-bounds write vulnerability that could lead to a heap buffer overflow. By changing the line `field = hid_register_field(report, usages, parser->global.report_count);` to `field = hid_register_field(report, usages);`, we ensure that the `hid_register_field` function is called with the correct number of usages, preventing the potential heap buffer overflow.\n\nIn the vulnerable code, the `hid_register_field` function is called with `parser->global.report_count` as an argument, which could lead to writing beyond the allocated memory if `parser->global.report_count` is larger than the actual number of usages. By removing `parser->global.report_count` as an argument in the modified code, we avoid this vulnerability and ensure that the function is called with the correct number of usages.",
      "GPT_purpose": "This function is responsible for adding a field to a HID report structure in a HID parser.",
      "GPT_function": "\n1. Registering a HID report\n2. Checking logical range validity\n3. Registering a HID field",
      "CVE_id": "CVE-2021-0512",
      "code_before_change": "static int hid_add_field(struct hid_parser *parser, unsigned report_type, unsigned flags)\n{\n\tstruct hid_report *report;\n\tstruct hid_field *field;\n\tunsigned int usages;\n\tunsigned int offset;\n\tunsigned int i;\n\tunsigned int application;\n\n\tapplication = hid_lookup_collection(parser, HID_COLLECTION_APPLICATION);\n\n\treport = hid_register_report(parser->device, report_type,\n\t\t\t\t     parser->global.report_id, application);\n\tif (!report) {\n\t\thid_err(parser->device, \"hid_register_report failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* Handle both signed and unsigned cases properly */\n\tif ((parser->global.logical_minimum < 0 &&\n\t\tparser->global.logical_maximum <\n\t\tparser->global.logical_minimum) ||\n\t\t(parser->global.logical_minimum >= 0 &&\n\t\t(__u32)parser->global.logical_maximum <\n\t\t(__u32)parser->global.logical_minimum)) {\n\t\tdbg_hid(\"logical range invalid 0x%x 0x%x\\n\",\n\t\t\tparser->global.logical_minimum,\n\t\t\tparser->global.logical_maximum);\n\t\treturn -1;\n\t}\n\n\toffset = report->size;\n\treport->size += parser->global.report_size * parser->global.report_count;\n\n\t/* Total size check: Allow for possible report index byte */\n\tif (report->size > (HID_MAX_BUFFER_SIZE - 1) << 3) {\n\t\thid_err(parser->device, \"report is too long\\n\");\n\t\treturn -1;\n\t}\n\n\tif (!parser->local.usage_index) /* Ignore padding fields */\n\t\treturn 0;\n\n\tusages = max_t(unsigned, parser->local.usage_index,\n\t\t\t\t parser->global.report_count);\n\n\tfield = hid_register_field(report, usages, parser->global.report_count);\n\tif (!field)\n\t\treturn 0;\n\n\tfield->physical = hid_lookup_collection(parser, HID_COLLECTION_PHYSICAL);\n\tfield->logical = hid_lookup_collection(parser, HID_COLLECTION_LOGICAL);\n\tfield->application = application;\n\n\tfor (i = 0; i < usages; i++) {\n\t\tunsigned j = i;\n\t\t/* Duplicate the last usage we parsed if we have excess values */\n\t\tif (i >= parser->local.usage_index)\n\t\t\tj = parser->local.usage_index - 1;\n\t\tfield->usage[i].hid = parser->local.usage[j];\n\t\tfield->usage[i].collection_index =\n\t\t\tparser->local.collection_index[j];\n\t\tfield->usage[i].usage_index = i;\n\t\tfield->usage[i].resolution_multiplier = 1;\n\t}\n\n\tfield->maxusage = usages;\n\tfield->flags = flags;\n\tfield->report_offset = offset;\n\tfield->report_type = report_type;\n\tfield->report_size = parser->global.report_size;\n\tfield->report_count = parser->global.report_count;\n\tfield->logical_minimum = parser->global.logical_minimum;\n\tfield->logical_maximum = parser->global.logical_maximum;\n\tfield->physical_minimum = parser->global.physical_minimum;\n\tfield->physical_maximum = parser->global.physical_maximum;\n\tfield->unit_exponent = parser->global.unit_exponent;\n\tfield->unit = parser->global.unit;\n\n\treturn 0;\n}",
      "code_after_change": "static int hid_add_field(struct hid_parser *parser, unsigned report_type, unsigned flags)\n{\n\tstruct hid_report *report;\n\tstruct hid_field *field;\n\tunsigned int usages;\n\tunsigned int offset;\n\tunsigned int i;\n\tunsigned int application;\n\n\tapplication = hid_lookup_collection(parser, HID_COLLECTION_APPLICATION);\n\n\treport = hid_register_report(parser->device, report_type,\n\t\t\t\t     parser->global.report_id, application);\n\tif (!report) {\n\t\thid_err(parser->device, \"hid_register_report failed\\n\");\n\t\treturn -1;\n\t}\n\n\t/* Handle both signed and unsigned cases properly */\n\tif ((parser->global.logical_minimum < 0 &&\n\t\tparser->global.logical_maximum <\n\t\tparser->global.logical_minimum) ||\n\t\t(parser->global.logical_minimum >= 0 &&\n\t\t(__u32)parser->global.logical_maximum <\n\t\t(__u32)parser->global.logical_minimum)) {\n\t\tdbg_hid(\"logical range invalid 0x%x 0x%x\\n\",\n\t\t\tparser->global.logical_minimum,\n\t\t\tparser->global.logical_maximum);\n\t\treturn -1;\n\t}\n\n\toffset = report->size;\n\treport->size += parser->global.report_size * parser->global.report_count;\n\n\t/* Total size check: Allow for possible report index byte */\n\tif (report->size > (HID_MAX_BUFFER_SIZE - 1) << 3) {\n\t\thid_err(parser->device, \"report is too long\\n\");\n\t\treturn -1;\n\t}\n\n\tif (!parser->local.usage_index) /* Ignore padding fields */\n\t\treturn 0;\n\n\tusages = max_t(unsigned, parser->local.usage_index,\n\t\t\t\t parser->global.report_count);\n\n\tfield = hid_register_field(report, usages);\n\tif (!field)\n\t\treturn 0;\n\n\tfield->physical = hid_lookup_collection(parser, HID_COLLECTION_PHYSICAL);\n\tfield->logical = hid_lookup_collection(parser, HID_COLLECTION_LOGICAL);\n\tfield->application = application;\n\n\tfor (i = 0; i < usages; i++) {\n\t\tunsigned j = i;\n\t\t/* Duplicate the last usage we parsed if we have excess values */\n\t\tif (i >= parser->local.usage_index)\n\t\t\tj = parser->local.usage_index - 1;\n\t\tfield->usage[i].hid = parser->local.usage[j];\n\t\tfield->usage[i].collection_index =\n\t\t\tparser->local.collection_index[j];\n\t\tfield->usage[i].usage_index = i;\n\t\tfield->usage[i].resolution_multiplier = 1;\n\t}\n\n\tfield->maxusage = usages;\n\tfield->flags = flags;\n\tfield->report_offset = offset;\n\tfield->report_type = report_type;\n\tfield->report_size = parser->global.report_size;\n\tfield->report_count = parser->global.report_count;\n\tfield->logical_minimum = parser->global.logical_minimum;\n\tfield->logical_maximum = parser->global.logical_maximum;\n\tfield->physical_minimum = parser->global.physical_minimum;\n\tfield->physical_maximum = parser->global.physical_maximum;\n\tfield->unit_exponent = parser->global.unit_exponent;\n\tfield->unit = parser->global.unit;\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tfield = hid_register_field(report, usages);"
        ],
        "deleted": [
          "\tfield = hid_register_field(report, usages, parser->global.report_count);"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of buffer size calculations and field registration in a HID parser function.",
      "trigger_condition": "The HID parser function does not correctly validate the buffer size and field registration, leading to a possible heap buffer overflow.",
      "specific_code_behavior_causing_vulnerability": "The vulnerable code calculates the buffer size based on the product of report size and report count, potentially exceeding the maximum buffer size. Additionally, the function incorrectly registers fields with an incorrect number of usages, which can result in a heap buffer overflow.",
      "id": 94,
      "code_after_change_normalized": "static int FUN1(struct hid_parser *VAR1, unsigned VAR2, unsigned VAR3)\n{\nstruct hid_report *VAR4;\nstruct hid_field *VAR5;\nunsigned int VAR6;\nunsigned int VAR7;\nunsigned int VAR8;\nunsigned int VAR9;\nVAR9 = FUN2(VAR1, VAR10);\nVAR4 = FUN3(VAR1->VAR11, VAR2,\nVAR1->VAR12.VAR13, VAR9);\nif (!VAR4) {\nFUN4(VAR1->VAR11, \"STR\");\nreturn -1;\n}\nif ((VAR1->VAR12.VAR14 < 0 &&\nVAR1->VAR12.VAR15 <\nVAR1->VAR12.VAR14) ||\n(VAR1->VAR12.VAR14 >= 0 &&\n(VAR16)VAR1->VAR12.VAR15 <\n(VAR16)VAR1->VAR12.VAR14)) {\nFUN5(\"STR\",\nVAR1->VAR12.VAR14,\nVAR1->VAR12.VAR15);\nreturn -1;\n}\nVAR7 = VAR4->VAR17;\nVAR4->VAR17 += VAR1->VAR12.VAR18 * VAR1->VAR12.VAR19;\nif (VAR4->VAR17 > (VAR20 - 1) << 3) {\nFUN4(VAR1->VAR11, \"STR\");\nreturn -1;\n}\nif (!VAR1->VAR21.VAR22) \nreturn 0;\nVAR6 = FUN6(unsigned, VAR1->VAR21.VAR22,\nVAR1->VAR12.VAR19);\nVAR5 = FUN7(VAR4, VAR6);\nif (!VAR5)\nreturn 0;\nVAR5->VAR23 = FUN2(VAR1, VAR24);\nVAR5->VAR25 = FUN2(VAR1, VAR26);\nVAR5->VAR9 = VAR9;\nfor (VAR8 = 0; VAR8 < VAR6; VAR8++) {\nunsigned VAR27 = VAR8;\nif (VAR8 >= VAR1->VAR21.VAR22)\nVAR27 = VAR1->VAR21.VAR22 - 1;\nVAR5->VAR28[VAR8].VAR29 = VAR1->VAR21.VAR28[VAR27];\nVAR5->VAR28[VAR8].VAR30 =\nVAR1->VAR21.VAR30[VAR27];\nVAR5->VAR28[VAR8].VAR22 = VAR8;\nVAR5->VAR28[VAR8].VAR31 = 1;\n}\nVAR5->VAR32 = VAR6;\nVAR5->VAR3 = VAR3;\nVAR5->VAR33 = VAR7;\nVAR5->VAR2 = VAR2;\nVAR5->VAR18 = VAR1->VAR12.VAR18;\nVAR5->VAR19 = VAR1->VAR12.VAR19;\nVAR5->VAR14 = VAR1->VAR12.VAR14;\nVAR5->VAR15 = VAR1->VAR12.VAR15;\nVAR5->VAR34 = VAR1->VAR12.VAR34;\nVAR5->VAR35 = VAR1->VAR12.VAR35;\nVAR5->VAR36 = VAR1->VAR12.VAR36;\nVAR5->VAR37 = VAR1->VAR12.VAR37;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct hid_parser *VAR1, unsigned VAR2, unsigned VAR3)\n{\nstruct hid_report *VAR4;\nstruct hid_field *VAR5;\nunsigned int VAR6;\nunsigned int VAR7;\nunsigned int VAR8;\nunsigned int VAR9;\nVAR9 = FUN2(VAR1, VAR10);\nVAR4 = FUN3(VAR1->VAR11, VAR2,\nVAR1->VAR12.VAR13, VAR9);\nif (!VAR4) {\nFUN4(VAR1->VAR11, \"STR\");\nreturn -1;\n}\nif ((VAR1->VAR12.VAR14 < 0 &&\nVAR1->VAR12.VAR15 <\nVAR1->VAR12.VAR14) ||\n(VAR1->VAR12.VAR14 >= 0 &&\n(VAR16)VAR1->VAR12.VAR15 <\n(VAR16)VAR1->VAR12.VAR14)) {\nFUN5(\"STR\",\nVAR1->VAR12.VAR14,\nVAR1->VAR12.VAR15);\nreturn -1;\n}\nVAR7 = VAR4->VAR17;\nVAR4->VAR17 += VAR1->VAR12.VAR18 * VAR1->VAR12.VAR19;\nif (VAR4->VAR17 > (VAR20 - 1) << 3) {\nFUN4(VAR1->VAR11, \"STR\");\nreturn -1;\n}\nif (!VAR1->VAR21.VAR22) \nreturn 0;\nVAR6 = FUN6(unsigned, VAR1->VAR21.VAR22,\nVAR1->VAR12.VAR19);\nVAR5 = FUN7(VAR4, VAR6, VAR1->VAR12.VAR19);\nif (!VAR5)\nreturn 0;\nVAR5->VAR23 = FUN2(VAR1, VAR24);\nVAR5->VAR25 = FUN2(VAR1, VAR26);\nVAR5->VAR9 = VAR9;\nfor (VAR8 = 0; VAR8 < VAR6; VAR8++) {\nunsigned VAR27 = VAR8;\nif (VAR8 >= VAR1->VAR21.VAR22)\nVAR27 = VAR1->VAR21.VAR22 - 1;\nVAR5->VAR28[VAR8].VAR29 = VAR1->VAR21.VAR28[VAR27];\nVAR5->VAR28[VAR8].VAR30 =\nVAR1->VAR21.VAR30[VAR27];\nVAR5->VAR28[VAR8].VAR22 = VAR8;\nVAR5->VAR28[VAR8].VAR31 = 1;\n}\nVAR5->VAR32 = VAR6;\nVAR5->VAR3 = VAR3;\nVAR5->VAR33 = VAR7;\nVAR5->VAR2 = VAR2;\nVAR5->VAR18 = VAR1->VAR12.VAR18;\nVAR5->VAR19 = VAR1->VAR12.VAR19;\nVAR5->VAR14 = VAR1->VAR12.VAR14;\nVAR5->VAR15 = VAR1->VAR12.VAR15;\nVAR5->VAR34 = VAR1->VAR12.VAR34;\nVAR5->VAR35 = VAR1->VAR12.VAR35;\nVAR5->VAR36 = VAR1->VAR12.VAR36;\nVAR5->VAR37 = VAR1->VAR12.VAR37;\nreturn 0;\n}\n",
      "code_after_change_raw": "static int hid_add_field(struct hid_parser *parser, unsigned report_type, unsigned flags)\n{\nstruct hid_report *report;\nstruct hid_field *field;\nunsigned int usages;\nunsigned int offset;\nunsigned int i;\nunsigned int application;\napplication = hid_lookup_collection(parser, HID_COLLECTION_APPLICATION);\nreport = hid_register_report(parser->device, report_type,\nparser->global.report_id, application);\nif (!report) {\nhid_err(parser->device, \"hid_register_report failed\\n\");\nreturn -1;\n}\nif ((parser->global.logical_minimum < 0 &&\nparser->global.logical_maximum <\nparser->global.logical_minimum) ||\n(parser->global.logical_minimum >= 0 &&\n(__u32)parser->global.logical_maximum <\n(__u32)parser->global.logical_minimum)) {\ndbg_hid(\"logical range invalid 0x%x 0x%x\\n\",\nparser->global.logical_minimum,\nparser->global.logical_maximum);\nreturn -1;\n}\noffset = report->size;\nreport->size += parser->global.report_size * parser->global.report_count;\nif (report->size > (HID_MAX_BUFFER_SIZE - 1) << 3) {\nhid_err(parser->device, \"report is too long\\n\");\nreturn -1;\n}\nif (!parser->local.usage_index) \nreturn 0;\nusages = max_t(unsigned, parser->local.usage_index,\nparser->global.report_count);\nfield = hid_register_field(report, usages);\nif (!field)\nreturn 0;\nfield->physical = hid_lookup_collection(parser, HID_COLLECTION_PHYSICAL);\nfield->logical = hid_lookup_collection(parser, HID_COLLECTION_LOGICAL);\nfield->application = application;\nfor (i = 0; i < usages; i++) {\nunsigned j = i;\nif (i >= parser->local.usage_index)\nj = parser->local.usage_index - 1;\nfield->usage[i].hid = parser->local.usage[j];\nfield->usage[i].collection_index =\nparser->local.collection_index[j];\nfield->usage[i].usage_index = i;\nfield->usage[i].resolution_multiplier = 1;\n}\nfield->maxusage = usages;\nfield->flags = flags;\nfield->report_offset = offset;\nfield->report_type = report_type;\nfield->report_size = parser->global.report_size;\nfield->report_count = parser->global.report_count;\nfield->logical_minimum = parser->global.logical_minimum;\nfield->logical_maximum = parser->global.logical_maximum;\nfield->physical_minimum = parser->global.physical_minimum;\nfield->physical_maximum = parser->global.physical_maximum;\nfield->unit_exponent = parser->global.unit_exponent;\nfield->unit = parser->global.unit;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int hid_add_field(struct hid_parser *parser, unsigned report_type, unsigned flags)\n{\nstruct hid_report *report;\nstruct hid_field *field;\nunsigned int usages;\nunsigned int offset;\nunsigned int i;\nunsigned int application;\napplication = hid_lookup_collection(parser, HID_COLLECTION_APPLICATION);\nreport = hid_register_report(parser->device, report_type,\nparser->global.report_id, application);\nif (!report) {\nhid_err(parser->device, \"hid_register_report failed\\n\");\nreturn -1;\n}\nif ((parser->global.logical_minimum < 0 &&\nparser->global.logical_maximum <\nparser->global.logical_minimum) ||\n(parser->global.logical_minimum >= 0 &&\n(__u32)parser->global.logical_maximum <\n(__u32)parser->global.logical_minimum)) {\ndbg_hid(\"logical range invalid 0x%x 0x%x\\n\",\nparser->global.logical_minimum,\nparser->global.logical_maximum);\nreturn -1;\n}\noffset = report->size;\nreport->size += parser->global.report_size * parser->global.report_count;\nif (report->size > (HID_MAX_BUFFER_SIZE - 1) << 3) {\nhid_err(parser->device, \"report is too long\\n\");\nreturn -1;\n}\nif (!parser->local.usage_index) \nreturn 0;\nusages = max_t(unsigned, parser->local.usage_index,\nparser->global.report_count);\nfield = hid_register_field(report, usages, parser->global.report_count);\nif (!field)\nreturn 0;\nfield->physical = hid_lookup_collection(parser, HID_COLLECTION_PHYSICAL);\nfield->logical = hid_lookup_collection(parser, HID_COLLECTION_LOGICAL);\nfield->application = application;\nfor (i = 0; i < usages; i++) {\nunsigned j = i;\nif (i >= parser->local.usage_index)\nj = parser->local.usage_index - 1;\nfield->usage[i].hid = parser->local.usage[j];\nfield->usage[i].collection_index =\nparser->local.collection_index[j];\nfield->usage[i].usage_index = i;\nfield->usage[i].resolution_multiplier = 1;\n}\nfield->maxusage = usages;\nfield->flags = flags;\nfield->report_offset = offset;\nfield->report_type = report_type;\nfield->report_size = parser->global.report_size;\nfield->report_count = parser->global.report_count;\nfield->logical_minimum = parser->global.logical_minimum;\nfield->logical_maximum = parser->global.logical_maximum;\nfield->physical_minimum = parser->global.physical_minimum;\nfield->physical_maximum = parser->global.physical_maximum;\nfield->unit_exponent = parser->global.unit_exponent;\nfield->unit = parser->global.unit;\nreturn 0;\n}\n"
    }
  ],
  "CVE-2021-0935": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for updating peer information during a socket connection process.",
        "trigger_condition": "Updating peer information without saving the current values, leading to a potential out-of-bounds write due to a use-after-free issue.",
        "specific_code_behavior_causing_vulnerability": "The code updates peer information without saving the current values, which can result in memory corruption if an error occurs during the update process, leading to a use-after-free vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to save the current peer information before updating it and then restore it in case of an error during the update process. This ensures that the socket peer information remains consistent and prevents potential memory corruption issues that could lead to security vulnerabilities, such as local escalation of privilege. The solution involves adding code to save and restore the current peer information to maintain the integrity of the socket state.",
      "GPT_analysis": "The above modification is necessary to prevent a possible out-of-bounds write vulnerability due to a use-after-free issue in the code. By saving the current peer information before updating it and then restoring it in case of an error during the update process, we ensure that the socket peer information remains consistent and avoids potential memory corruption issues that could lead to security vulnerabilities, such as local escalation of privilege. This modification helps maintain the integrity of the socket state and prevents exploitation of the vulnerability described in CVE-2021-0935.",
      "GPT_purpose": "Connects a socket to a specified IPv6 address and port, handling various address types and setting up the necessary parameters for communication.",
      "GPT_function": "\n1. Establishing an IPv6 datagram connection.\n2. Handling IPv4 connections if the address family is AF_INET.\n3. Updating the destination cache for the datagram.",
      "CVE_id": "CVE-2021-0935",
      "code_before_change": "int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t\t*daddr;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\t__be32\t\t\tfl6_flowlabel = 0;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tif (np->sndflow)\n\t\tfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6_flowlabel;\n\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\terr = ip6_datagram_dst_update(sk, true);\n\tif (err) {\n\t\t/* Reset daddr and dport so that udp_v6_early_demux()\n\t\t * fails to find this socket\n\t\t */\n\t\tmemset(&sk->sk_v6_daddr, 0, sizeof(sk->sk_v6_daddr));\n\t\tinet->inet_dport = 0;\n\t\tgoto out;\n\t}\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\treturn err;\n}",
      "code_after_change": "int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\n\t\t\t   int addr_len)\n{\n\tstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\n\tstruct inet_sock\t*inet = inet_sk(sk);\n\tstruct ipv6_pinfo\t*np = inet6_sk(sk);\n\tstruct in6_addr\t\t*daddr, old_daddr;\n\t__be32\t\t\tfl6_flowlabel = 0;\n\t__be32\t\t\told_fl6_flowlabel;\n\t__be32\t\t\told_dport;\n\tint\t\t\taddr_type;\n\tint\t\t\terr;\n\n\tif (usin->sin6_family == AF_INET) {\n\t\tif (__ipv6_only_sock(sk))\n\t\t\treturn -EAFNOSUPPORT;\n\t\terr = __ip4_datagram_connect(sk, uaddr, addr_len);\n\t\tgoto ipv4_connected;\n\t}\n\n\tif (addr_len < SIN6_LEN_RFC2133)\n\t\treturn -EINVAL;\n\n\tif (usin->sin6_family != AF_INET6)\n\t\treturn -EAFNOSUPPORT;\n\n\tif (np->sndflow)\n\t\tfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\n\n\tif (ipv6_addr_any(&usin->sin6_addr)) {\n\t\t/*\n\t\t *\tconnect to self\n\t\t */\n\t\tif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\n\t\t\tipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n\t\t\t\t\t       &usin->sin6_addr);\n\t\telse\n\t\t\tusin->sin6_addr = in6addr_loopback;\n\t}\n\n\taddr_type = ipv6_addr_type(&usin->sin6_addr);\n\n\tdaddr = &usin->sin6_addr;\n\n\tif (addr_type & IPV6_ADDR_MAPPED) {\n\t\tstruct sockaddr_in sin;\n\n\t\tif (__ipv6_only_sock(sk)) {\n\t\t\terr = -ENETUNREACH;\n\t\t\tgoto out;\n\t\t}\n\t\tsin.sin_family = AF_INET;\n\t\tsin.sin_addr.s_addr = daddr->s6_addr32[3];\n\t\tsin.sin_port = usin->sin6_port;\n\n\t\terr = __ip4_datagram_connect(sk,\n\t\t\t\t\t     (struct sockaddr *) &sin,\n\t\t\t\t\t     sizeof(sin));\n\nipv4_connected:\n\t\tif (err)\n\t\t\tgoto out;\n\n\t\tipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\n\n\t\tif (ipv6_addr_any(&np->saddr) ||\n\t\t    ipv6_mapped_addr_any(&np->saddr))\n\t\t\tipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\n\n\t\tif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\n\t\t    ipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\n\t\t\tipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n\t\t\t\t\t       &sk->sk_v6_rcv_saddr);\n\t\t\tif (sk->sk_prot->rehash)\n\t\t\t\tsk->sk_prot->rehash(sk);\n\t\t}\n\n\t\tgoto out;\n\t}\n\n\tif (__ipv6_addr_needs_scope_id(addr_type)) {\n\t\tif (addr_len >= sizeof(struct sockaddr_in6) &&\n\t\t    usin->sin6_scope_id) {\n\t\t\tif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tsk->sk_bound_dev_if = usin->sin6_scope_id;\n\t\t}\n\n\t\tif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\n\t\t\tsk->sk_bound_dev_if = np->mcast_oif;\n\n\t\t/* Connect to link-local address requires an interface */\n\t\tif (!sk->sk_bound_dev_if) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\t/* save the current peer information before updating it */\n\told_daddr = sk->sk_v6_daddr;\n\told_fl6_flowlabel = np->flow_label;\n\told_dport = inet->inet_dport;\n\n\tsk->sk_v6_daddr = *daddr;\n\tnp->flow_label = fl6_flowlabel;\n\tinet->inet_dport = usin->sin6_port;\n\n\t/*\n\t *\tCheck for a route to destination an obtain the\n\t *\tdestination cache for it.\n\t */\n\n\terr = ip6_datagram_dst_update(sk, true);\n\tif (err) {\n\t\t/* Restore the socket peer info, to keep it consistent with\n\t\t * the old socket state\n\t\t */\n\t\tsk->sk_v6_daddr = old_daddr;\n\t\tnp->flow_label = old_fl6_flowlabel;\n\t\tinet->inet_dport = old_dport;\n\t\tgoto out;\n\t}\n\n\tsk->sk_state = TCP_ESTABLISHED;\n\tsk_set_txhash(sk);\nout:\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tstruct in6_addr\t\t*daddr, old_daddr;",
          "\t__be32\t\t\tfl6_flowlabel = 0;",
          "\t__be32\t\t\told_fl6_flowlabel;",
          "\t__be32\t\t\told_dport;",
          "\t/* save the current peer information before updating it */",
          "\told_daddr = sk->sk_v6_daddr;",
          "\told_fl6_flowlabel = np->flow_label;",
          "\told_dport = inet->inet_dport;",
          "",
          "\t\t/* Restore the socket peer info, to keep it consistent with",
          "\t\t * the old socket state",
          "\t\tsk->sk_v6_daddr = old_daddr;",
          "\t\tnp->flow_label = old_fl6_flowlabel;",
          "\t\tinet->inet_dport = old_dport;"
        ],
        "deleted": [
          "\tstruct in6_addr\t\t*daddr;",
          "\t__be32\t\t\tfl6_flowlabel = 0;",
          "",
          "\t\t/* Reset daddr and dport so that udp_v6_early_demux()",
          "\t\t * fails to find this socket",
          "\t\tmemset(&sk->sk_v6_daddr, 0, sizeof(sk->sk_v6_daddr));",
          "\t\tinet->inet_dport = 0;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper handling for updating peer information during a socket connection process.",
      "trigger_condition": "Updating peer information without saving the current values, leading to a potential out-of-bounds write due to a use-after-free issue.",
      "specific_code_behavior_causing_vulnerability": "The code updates peer information without saving the current values, which can result in memory corruption if an error occurs during the update process, leading to a use-after-free vulnerability.",
      "id": 95,
      "code_after_change_normalized": "int FUN1(struct sock *VAR1, struct sockaddr *VAR2,\nint VAR3)\n{\nstruct VAR5\t*VAR4 = (struct VAR5 *) VAR2;\nstruct inet_sock\t*VAR6 = FUN2(VAR1);\nstruct ipv6_pinfo\t*VAR7 = FUN3(VAR1);\nstruct in6_addr\t\t*VAR8, VAR9;\n__be32\t\t\tVAR10 = 0;\n__be32\t\t\tVAR11;\n__be32\t\t\tVAR12;\nint\t\t\tVAR13;\nint\t\t\tVAR14;\nif (VAR4->VAR15 == VAR16) {\nif (FUN4(VAR1))\nreturn -VAR17;\nVAR14 = FUN5(VAR1, VAR2, VAR3);\ngoto VAR18;\n}\nif (VAR3 < VAR19)\nreturn -VAR20;\nif (VAR4->VAR15 != VAR21)\nreturn -VAR17;\nif (VAR7->VAR22)\nVAR10 = VAR4->VAR23 & VAR24;\nif (FUN6(&VAR4->VAR25)) {\nif (FUN7(&VAR1->VAR26))\nFUN8(FUN9(VAR27),\n&VAR4->VAR25);\nelse\nVAR4->VAR25 = VAR28;\n}\nVAR13 = FUN10(&VAR4->VAR25);\nVAR8 = &VAR4->VAR25;\nif (VAR13 & VAR29) {\nstruct sockaddr_in VAR30;\nif (FUN4(VAR1)) {\nVAR14 = -VAR31;\ngoto VAR32;\n}\nVAR30.VAR33 = VAR16;\nVAR30.VAR34.VAR35 = VAR8->VAR36[3];\nVAR30.VAR37 = VAR4->VAR38;\nVAR14 = FUN5(VAR1,\n(struct VAR39 *) &VAR30,\nsizeof(VAR30));\nVAR18:\nif (VAR14)\ngoto VAR32;\nFUN8(VAR6->VAR40, &VAR1->VAR41);\nif (FUN6(&VAR7->VAR42) ||\nFUN11(&VAR7->VAR42))\nFUN8(VAR6->VAR43, &VAR7->VAR42);\nif (FUN6(&VAR1->VAR26) ||\nFUN11(&VAR1->VAR26)) {\nFUN8(VAR6->VAR44,\n&VAR1->VAR26);\nif (VAR1->VAR45->VAR46)\nVAR1->VAR45->FUN12(VAR1);\n}\ngoto VAR32;\n}\nif (FUN13(VAR13)) {\nif (VAR3 >= sizeof(struct VAR5) &&\nVAR4->VAR47) {\nif (!FUN14(VAR1, VAR4->VAR47)) {\nVAR14 = -VAR20;\ngoto VAR32;\n}\nVAR1->VAR48 = VAR4->VAR47;\n}\nif (!VAR1->VAR48 && (VAR13 & VAR49))\nVAR1->VAR48 = VAR7->VAR50;\nif (!VAR1->VAR48) {\nVAR14 = -VAR20;\ngoto VAR32;\n}\n}\nVAR9 = VAR1->VAR41;\nVAR11 = VAR7->VAR51;\nVAR12 = VAR6->VAR52;\nVAR1->VAR41 = *VAR8;\nVAR7->VAR51 = VAR10;\nVAR6->VAR52 = VAR4->VAR38;\nVAR14 = FUN15(VAR1, true);\nif (VAR14) {\nVAR1->VAR41 = VAR9;\nVAR7->VAR51 = VAR11;\nVAR6->VAR52 = VAR12;\ngoto VAR32;\n}\nVAR1->VAR53 = VAR54;\nFUN16(VAR1);\nVAR32:\nreturn VAR14;\n}\n",
      "code_before_change_normalized": "int FUN1(struct sock *VAR1, struct sockaddr *VAR2,\nint VAR3)\n{\nstruct VAR5\t*VAR4 = (struct VAR5 *) VAR2;\nstruct inet_sock\t*VAR6 = FUN2(VAR1);\nstruct ipv6_pinfo\t*VAR7 = FUN3(VAR1);\nstruct in6_addr\t\t*VAR8;\nint\t\t\tVAR9;\nint\t\t\tVAR10;\n__be32\t\t\tVAR11 = 0;\nif (VAR4->VAR12 == VAR13) {\nif (FUN4(VAR1))\nreturn -VAR14;\nVAR10 = FUN5(VAR1, VAR2, VAR3);\ngoto VAR15;\n}\nif (VAR3 < VAR16)\nreturn -VAR17;\nif (VAR4->VAR12 != VAR18)\nreturn -VAR14;\nif (VAR7->VAR19)\nVAR11 = VAR4->VAR20 & VAR21;\nif (FUN6(&VAR4->VAR22)) {\nif (FUN7(&VAR1->VAR23))\nFUN8(FUN9(VAR24),\n&VAR4->VAR22);\nelse\nVAR4->VAR22 = VAR25;\n}\nVAR9 = FUN10(&VAR4->VAR22);\nVAR8 = &VAR4->VAR22;\nif (VAR9 & VAR26) {\nstruct sockaddr_in VAR27;\nif (FUN4(VAR1)) {\nVAR10 = -VAR28;\ngoto VAR29;\n}\nVAR27.VAR30 = VAR13;\nVAR27.VAR31.VAR32 = VAR8->VAR33[3];\nVAR27.VAR34 = VAR4->VAR35;\nVAR10 = FUN5(VAR1,\n(struct VAR36 *) &VAR27,\nsizeof(VAR27));\nVAR15:\nif (VAR10)\ngoto VAR29;\nFUN8(VAR6->VAR37, &VAR1->VAR38);\nif (FUN6(&VAR7->VAR39) ||\nFUN11(&VAR7->VAR39))\nFUN8(VAR6->VAR40, &VAR7->VAR39);\nif (FUN6(&VAR1->VAR23) ||\nFUN11(&VAR1->VAR23)) {\nFUN8(VAR6->VAR41,\n&VAR1->VAR23);\nif (VAR1->VAR42->VAR43)\nVAR1->VAR42->FUN12(VAR1);\n}\ngoto VAR29;\n}\nif (FUN13(VAR9)) {\nif (VAR3 >= sizeof(struct VAR5) &&\nVAR4->VAR44) {\nif (!FUN14(VAR1, VAR4->VAR44)) {\nVAR10 = -VAR17;\ngoto VAR29;\n}\nVAR1->VAR45 = VAR4->VAR44;\n}\nif (!VAR1->VAR45 && (VAR9 & VAR46))\nVAR1->VAR45 = VAR7->VAR47;\nif (!VAR1->VAR45) {\nVAR10 = -VAR17;\ngoto VAR29;\n}\n}\nVAR1->VAR38 = *VAR8;\nVAR7->VAR48 = VAR11;\nVAR6->VAR49 = VAR4->VAR35;\nVAR10 = FUN15(VAR1, true);\nif (VAR10) {\nFUN16(&VAR1->VAR38, 0, sizeof(VAR1->VAR38));\nVAR6->VAR49 = 0;\ngoto VAR29;\n}\nVAR1->VAR50 = VAR51;\nFUN17(VAR1);\nVAR29:\nreturn VAR10;\n}\n",
      "code_after_change_raw": "int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\nint addr_len)\n{\nstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\nstruct inet_sock\t*inet = inet_sk(sk);\nstruct ipv6_pinfo\t*np = inet6_sk(sk);\nstruct in6_addr\t\t*daddr, old_daddr;\n__be32\t\t\tfl6_flowlabel = 0;\n__be32\t\t\told_fl6_flowlabel;\n__be32\t\t\told_dport;\nint\t\t\taddr_type;\nint\t\t\terr;\nif (usin->sin6_family == AF_INET) {\nif (__ipv6_only_sock(sk))\nreturn -EAFNOSUPPORT;\nerr = __ip4_datagram_connect(sk, uaddr, addr_len);\ngoto ipv4_connected;\n}\nif (addr_len < SIN6_LEN_RFC2133)\nreturn -EINVAL;\nif (usin->sin6_family != AF_INET6)\nreturn -EAFNOSUPPORT;\nif (np->sndflow)\nfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\nif (ipv6_addr_any(&usin->sin6_addr)) {\nif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\nipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n&usin->sin6_addr);\nelse\nusin->sin6_addr = in6addr_loopback;\n}\naddr_type = ipv6_addr_type(&usin->sin6_addr);\ndaddr = &usin->sin6_addr;\nif (addr_type & IPV6_ADDR_MAPPED) {\nstruct sockaddr_in sin;\nif (__ipv6_only_sock(sk)) {\nerr = -ENETUNREACH;\ngoto out;\n}\nsin.sin_family = AF_INET;\nsin.sin_addr.s_addr = daddr->s6_addr32[3];\nsin.sin_port = usin->sin6_port;\nerr = __ip4_datagram_connect(sk,\n(struct sockaddr *) &sin,\nsizeof(sin));\nipv4_connected:\nif (err)\ngoto out;\nipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\nif (ipv6_addr_any(&np->saddr) ||\nipv6_mapped_addr_any(&np->saddr))\nipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\nif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\nipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\nipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n&sk->sk_v6_rcv_saddr);\nif (sk->sk_prot->rehash)\nsk->sk_prot->rehash(sk);\n}\ngoto out;\n}\nif (__ipv6_addr_needs_scope_id(addr_type)) {\nif (addr_len >= sizeof(struct sockaddr_in6) &&\nusin->sin6_scope_id) {\nif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\nerr = -EINVAL;\ngoto out;\n}\nsk->sk_bound_dev_if = usin->sin6_scope_id;\n}\nif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\nsk->sk_bound_dev_if = np->mcast_oif;\nif (!sk->sk_bound_dev_if) {\nerr = -EINVAL;\ngoto out;\n}\n}\nold_daddr = sk->sk_v6_daddr;\nold_fl6_flowlabel = np->flow_label;\nold_dport = inet->inet_dport;\nsk->sk_v6_daddr = *daddr;\nnp->flow_label = fl6_flowlabel;\ninet->inet_dport = usin->sin6_port;\nerr = ip6_datagram_dst_update(sk, true);\nif (err) {\nsk->sk_v6_daddr = old_daddr;\nnp->flow_label = old_fl6_flowlabel;\ninet->inet_dport = old_dport;\ngoto out;\n}\nsk->sk_state = TCP_ESTABLISHED;\nsk_set_txhash(sk);\nout:\nreturn err;\n}\n",
      "code_before_change_raw": "int __ip6_datagram_connect(struct sock *sk, struct sockaddr *uaddr,\nint addr_len)\n{\nstruct sockaddr_in6\t*usin = (struct sockaddr_in6 *) uaddr;\nstruct inet_sock\t*inet = inet_sk(sk);\nstruct ipv6_pinfo\t*np = inet6_sk(sk);\nstruct in6_addr\t\t*daddr;\nint\t\t\taddr_type;\nint\t\t\terr;\n__be32\t\t\tfl6_flowlabel = 0;\nif (usin->sin6_family == AF_INET) {\nif (__ipv6_only_sock(sk))\nreturn -EAFNOSUPPORT;\nerr = __ip4_datagram_connect(sk, uaddr, addr_len);\ngoto ipv4_connected;\n}\nif (addr_len < SIN6_LEN_RFC2133)\nreturn -EINVAL;\nif (usin->sin6_family != AF_INET6)\nreturn -EAFNOSUPPORT;\nif (np->sndflow)\nfl6_flowlabel = usin->sin6_flowinfo & IPV6_FLOWINFO_MASK;\nif (ipv6_addr_any(&usin->sin6_addr)) {\nif (ipv6_addr_v4mapped(&sk->sk_v6_rcv_saddr))\nipv6_addr_set_v4mapped(htonl(INADDR_LOOPBACK),\n&usin->sin6_addr);\nelse\nusin->sin6_addr = in6addr_loopback;\n}\naddr_type = ipv6_addr_type(&usin->sin6_addr);\ndaddr = &usin->sin6_addr;\nif (addr_type & IPV6_ADDR_MAPPED) {\nstruct sockaddr_in sin;\nif (__ipv6_only_sock(sk)) {\nerr = -ENETUNREACH;\ngoto out;\n}\nsin.sin_family = AF_INET;\nsin.sin_addr.s_addr = daddr->s6_addr32[3];\nsin.sin_port = usin->sin6_port;\nerr = __ip4_datagram_connect(sk,\n(struct sockaddr *) &sin,\nsizeof(sin));\nipv4_connected:\nif (err)\ngoto out;\nipv6_addr_set_v4mapped(inet->inet_daddr, &sk->sk_v6_daddr);\nif (ipv6_addr_any(&np->saddr) ||\nipv6_mapped_addr_any(&np->saddr))\nipv6_addr_set_v4mapped(inet->inet_saddr, &np->saddr);\nif (ipv6_addr_any(&sk->sk_v6_rcv_saddr) ||\nipv6_mapped_addr_any(&sk->sk_v6_rcv_saddr)) {\nipv6_addr_set_v4mapped(inet->inet_rcv_saddr,\n&sk->sk_v6_rcv_saddr);\nif (sk->sk_prot->rehash)\nsk->sk_prot->rehash(sk);\n}\ngoto out;\n}\nif (__ipv6_addr_needs_scope_id(addr_type)) {\nif (addr_len >= sizeof(struct sockaddr_in6) &&\nusin->sin6_scope_id) {\nif (!sk_dev_equal_l3scope(sk, usin->sin6_scope_id)) {\nerr = -EINVAL;\ngoto out;\n}\nsk->sk_bound_dev_if = usin->sin6_scope_id;\n}\nif (!sk->sk_bound_dev_if && (addr_type & IPV6_ADDR_MULTICAST))\nsk->sk_bound_dev_if = np->mcast_oif;\nif (!sk->sk_bound_dev_if) {\nerr = -EINVAL;\ngoto out;\n}\n}\nsk->sk_v6_daddr = *daddr;\nnp->flow_label = fl6_flowlabel;\ninet->inet_dport = usin->sin6_port;\nerr = ip6_datagram_dst_update(sk, true);\nif (err) {\nmemset(&sk->sk_v6_daddr, 0, sizeof(sk->sk_v6_daddr));\ninet->inet_dport = 0;\ngoto out;\n}\nsk->sk_state = TCP_ESTABLISHED;\nsk_set_txhash(sk);\nout:\nreturn err;\n}\n"
    }
  ],
  "CVE-2021-22555": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper initialization of memory allocated for a data structure in the code.",
        "trigger_condition": "The code writes data to the allocated memory without ensuring it is properly initialized, leading to a heap out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code fails to initialize the memory allocated for 'newinfo->entries' before writing data to it, which can result in heap memory corruption and potential exploitation by an attacker."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the memory allocated for 'newinfo->entries' is properly initialized before any data is written to it. This can be achieved by adding a line to initialize the memory to zero, thereby preventing out-of-bounds writes and potential heap memory corruption. The solution involves adding the line 'memset(newinfo->entries, 0, size);' before any data is written to 'newinfo->entries'.",
      "GPT_analysis": "The modification is necessary to fix a heap out-of-bounds write vulnerability in the code snippet. By adding the line `memset(newinfo->entries, 0, size);`, we ensure that the memory allocated for `newinfo->entries` is properly initialized to zero before any data is written to it. This helps prevent potential heap memory corruption that could be exploited by an attacker to gain privileges or cause a denial-of-service (DoS) through user namespace manipulation.\n\nInitializing the memory to zero ensures that the subsequent operations that write data to `newinfo->entries` do not inadvertently access or modify memory outside the allocated bounds, thus mitigating the risk of the vulnerability described in CVE-2021-22555.",
      "GPT_purpose": "Translate a compatibility table for ARP rules into a new format for netfilter rules.",
      "GPT_function": "\n1. Translate compatibility table entries from a compat_arpt_replace structure to a new xt_table_info structure.\n2. Check and validate offsets and sizes of entries.\n3. Allocate memory for a new xt_table_info structure and copy entries from the compat_arpt_replace structure.\n4. Update hook entries and underflow values in the new xt_table_info structure.\n5. Translate the table using the new xt_table_info structure and update the arpt_replace structure.\n6. Free memory if translation fails or encounters an error.",
      "CVE_id": "CVE-2021-22555",
      "code_before_change": "static int translate_compat_table(struct net *net,\n\t\t\t\t  struct xt_table_info **pinfo,\n\t\t\t\t  void **pentry0,\n\t\t\t\t  const struct compat_arpt_replace *compatr)\n{\n\tunsigned int i, j;\n\tstruct xt_table_info *newinfo, *info;\n\tvoid *pos, *entry0, *entry1;\n\tstruct compat_arpt_entry *iter0;\n\tstruct arpt_replace repl;\n\tunsigned int size;\n\tint ret;\n\n\tinfo = *pinfo;\n\tentry0 = *pentry0;\n\tsize = compatr->size;\n\tinfo->number = compatr->num_entries;\n\n\tj = 0;\n\txt_compat_lock(NFPROTO_ARP);\n\tret = xt_compat_init_offsets(NFPROTO_ARP, compatr->num_entries);\n\tif (ret)\n\t\tgoto out_unlock;\n\t/* Walk through entries, checking offsets. */\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tret = check_compat_entry_size_and_hooks(iter0, info, &size,\n\t\t\t\t\t\t\tentry0,\n\t\t\t\t\t\t\tentry0 + compatr->size);\n\t\tif (ret != 0)\n\t\t\tgoto out_unlock;\n\t\t++j;\n\t}\n\n\tret = -EINVAL;\n\tif (j != compatr->num_entries)\n\t\tgoto out_unlock;\n\n\tret = -ENOMEM;\n\tnewinfo = xt_alloc_table_info(size);\n\tif (!newinfo)\n\t\tgoto out_unlock;\n\n\tnewinfo->number = compatr->num_entries;\n\tfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\n\t\tnewinfo->hook_entry[i] = compatr->hook_entry[i];\n\t\tnewinfo->underflow[i] = compatr->underflow[i];\n\t}\n\tentry1 = newinfo->entries;\n\tpos = entry1;\n\tsize = compatr->size;\n\txt_entry_foreach(iter0, entry0, compatr->size)\n\t\tcompat_copy_entry_from_user(iter0, &pos, &size,\n\t\t\t\t\t    newinfo, entry1);\n\n\t/* all module references in entry0 are now gone */\n\n\txt_compat_flush_offsets(NFPROTO_ARP);\n\txt_compat_unlock(NFPROTO_ARP);\n\n\tmemcpy(&repl, compatr, sizeof(*compatr));\n\n\tfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\n\t\trepl.hook_entry[i] = newinfo->hook_entry[i];\n\t\trepl.underflow[i] = newinfo->underflow[i];\n\t}\n\n\trepl.num_counters = 0;\n\trepl.counters = NULL;\n\trepl.size = newinfo->size;\n\tret = translate_table(net, newinfo, entry1, &repl);\n\tif (ret)\n\t\tgoto free_newinfo;\n\n\t*pinfo = newinfo;\n\t*pentry0 = entry1;\n\txt_free_table_info(info);\n\treturn 0;\n\nfree_newinfo:\n\txt_free_table_info(newinfo);\n\treturn ret;\nout_unlock:\n\txt_compat_flush_offsets(NFPROTO_ARP);\n\txt_compat_unlock(NFPROTO_ARP);\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tcompat_release_entry(iter0);\n\t}\n\treturn ret;\n}",
      "code_after_change": "static int translate_compat_table(struct net *net,\n\t\t\t\t  struct xt_table_info **pinfo,\n\t\t\t\t  void **pentry0,\n\t\t\t\t  const struct compat_arpt_replace *compatr)\n{\n\tunsigned int i, j;\n\tstruct xt_table_info *newinfo, *info;\n\tvoid *pos, *entry0, *entry1;\n\tstruct compat_arpt_entry *iter0;\n\tstruct arpt_replace repl;\n\tunsigned int size;\n\tint ret;\n\n\tinfo = *pinfo;\n\tentry0 = *pentry0;\n\tsize = compatr->size;\n\tinfo->number = compatr->num_entries;\n\n\tj = 0;\n\txt_compat_lock(NFPROTO_ARP);\n\tret = xt_compat_init_offsets(NFPROTO_ARP, compatr->num_entries);\n\tif (ret)\n\t\tgoto out_unlock;\n\t/* Walk through entries, checking offsets. */\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tret = check_compat_entry_size_and_hooks(iter0, info, &size,\n\t\t\t\t\t\t\tentry0,\n\t\t\t\t\t\t\tentry0 + compatr->size);\n\t\tif (ret != 0)\n\t\t\tgoto out_unlock;\n\t\t++j;\n\t}\n\n\tret = -EINVAL;\n\tif (j != compatr->num_entries)\n\t\tgoto out_unlock;\n\n\tret = -ENOMEM;\n\tnewinfo = xt_alloc_table_info(size);\n\tif (!newinfo)\n\t\tgoto out_unlock;\n\n\tmemset(newinfo->entries, 0, size);\n\n\tnewinfo->number = compatr->num_entries;\n\tfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\n\t\tnewinfo->hook_entry[i] = compatr->hook_entry[i];\n\t\tnewinfo->underflow[i] = compatr->underflow[i];\n\t}\n\tentry1 = newinfo->entries;\n\tpos = entry1;\n\tsize = compatr->size;\n\txt_entry_foreach(iter0, entry0, compatr->size)\n\t\tcompat_copy_entry_from_user(iter0, &pos, &size,\n\t\t\t\t\t    newinfo, entry1);\n\n\t/* all module references in entry0 are now gone */\n\n\txt_compat_flush_offsets(NFPROTO_ARP);\n\txt_compat_unlock(NFPROTO_ARP);\n\n\tmemcpy(&repl, compatr, sizeof(*compatr));\n\n\tfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\n\t\trepl.hook_entry[i] = newinfo->hook_entry[i];\n\t\trepl.underflow[i] = newinfo->underflow[i];\n\t}\n\n\trepl.num_counters = 0;\n\trepl.counters = NULL;\n\trepl.size = newinfo->size;\n\tret = translate_table(net, newinfo, entry1, &repl);\n\tif (ret)\n\t\tgoto free_newinfo;\n\n\t*pinfo = newinfo;\n\t*pentry0 = entry1;\n\txt_free_table_info(info);\n\treturn 0;\n\nfree_newinfo:\n\txt_free_table_info(newinfo);\n\treturn ret;\nout_unlock:\n\txt_compat_flush_offsets(NFPROTO_ARP);\n\txt_compat_unlock(NFPROTO_ARP);\n\txt_entry_foreach(iter0, entry0, compatr->size) {\n\t\tif (j-- == 0)\n\t\t\tbreak;\n\t\tcompat_release_entry(iter0);\n\t}\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tmemset(newinfo->entries, 0, size);"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper initialization of memory allocated for a data structure in the code.",
      "trigger_condition": "The code writes data to the allocated memory without ensuring it is properly initialized, leading to a heap out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code fails to initialize the memory allocated for 'newinfo->entries' before writing data to it, which can result in heap memory corruption and potential exploitation by an attacker.",
      "id": 96,
      "code_after_change_normalized": "static int FUN1(struct VAR1 *VAR1,\nstruct xt_table_info **VAR2,\nvoid **VAR3,\nconst struct compat_arpt_replace *VAR4)\n{\nunsigned int VAR5, VAR6;\nstruct xt_table_info *VAR7, *VAR8;\nvoid *VAR9, *VAR10, *VAR11;\nstruct compat_arpt_entry *VAR12;\nstruct arpt_replace VAR13;\nunsigned int VAR14;\nint VAR15;\nVAR8 = *VAR2;\nVAR10 = *VAR3;\nVAR14 = VAR4->VAR14;\nVAR8->VAR16 = VAR4->VAR17;\nVAR6 = 0;\nFUN2(VAR18);\nVAR15 = FUN3(VAR18, VAR4->VAR17);\nif (VAR15)\ngoto VAR19;\nFUN4(VAR12, VAR10, VAR4->VAR14) {\nVAR15 = FUN5(VAR12, VAR8, &VAR14,\nVAR10,\nVAR10 + VAR4->VAR14);\nif (VAR15 != 0)\ngoto VAR19;\n++VAR6;\n}\nVAR15 = -VAR20;\nif (VAR6 != VAR4->VAR17)\ngoto VAR19;\nVAR15 = -VAR21;\nVAR7 = FUN6(VAR14);\nif (!VAR7)\ngoto VAR19;\nFUN7(VAR7->VAR22, 0, VAR14);\nVAR7->VAR16 = VAR4->VAR17;\nfor (VAR5 = 0; VAR5 < VAR23; VAR5++) {\nVAR7->VAR24[VAR5] = VAR4->VAR24[VAR5];\nVAR7->VAR25[VAR5] = VAR4->VAR25[VAR5];\n}\nVAR11 = VAR7->VAR22;\nVAR9 = VAR11;\nVAR14 = VAR4->VAR14;\nFUN4(VAR12, VAR10, VAR4->VAR14)\nFUN8(VAR12, &VAR9, &VAR14,\nVAR7, VAR11);\nFUN9(VAR18);\nFUN10(VAR18);\nFUN11(&VAR13, VAR4, sizeof(*VAR4));\nfor (VAR5 = 0; VAR5 < VAR23; VAR5++) {\nVAR13.VAR24[VAR5] = VAR7->VAR24[VAR5];\nVAR13.VAR25[VAR5] = VAR7->VAR25[VAR5];\n}\nVAR13.VAR26 = 0;\nVAR13.VAR27 = NULL;\nVAR13.VAR14 = VAR7->VAR14;\nVAR15 = FUN12(VAR1, VAR7, VAR11, &VAR13);\nif (VAR15)\ngoto VAR28;\n*VAR2 = VAR7;\n*VAR3 = VAR11;\nFUN13(VAR8);\nreturn 0;\nVAR28:\nFUN13(VAR7);\nreturn VAR15;\nVAR19:\nFUN9(VAR18);\nFUN10(VAR18);\nFUN4(VAR12, VAR10, VAR4->VAR14) {\nif (VAR6-- == 0)\nbreak;\nFUN14(VAR12);\n}\nreturn VAR15;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct VAR1 *VAR1,\nstruct xt_table_info **VAR2,\nvoid **VAR3,\nconst struct compat_arpt_replace *VAR4)\n{\nunsigned int VAR5, VAR6;\nstruct xt_table_info *VAR7, *VAR8;\nvoid *VAR9, *VAR10, *VAR11;\nstruct compat_arpt_entry *VAR12;\nstruct arpt_replace VAR13;\nunsigned int VAR14;\nint VAR15;\nVAR8 = *VAR2;\nVAR10 = *VAR3;\nVAR14 = VAR4->VAR14;\nVAR8->VAR16 = VAR4->VAR17;\nVAR6 = 0;\nFUN2(VAR18);\nVAR15 = FUN3(VAR18, VAR4->VAR17);\nif (VAR15)\ngoto VAR19;\nFUN4(VAR12, VAR10, VAR4->VAR14) {\nVAR15 = FUN5(VAR12, VAR8, &VAR14,\nVAR10,\nVAR10 + VAR4->VAR14);\nif (VAR15 != 0)\ngoto VAR19;\n++VAR6;\n}\nVAR15 = -VAR20;\nif (VAR6 != VAR4->VAR17)\ngoto VAR19;\nVAR15 = -VAR21;\nVAR7 = FUN6(VAR14);\nif (!VAR7)\ngoto VAR19;\nVAR7->VAR16 = VAR4->VAR17;\nfor (VAR5 = 0; VAR5 < VAR22; VAR5++) {\nVAR7->VAR23[VAR5] = VAR4->VAR23[VAR5];\nVAR7->VAR24[VAR5] = VAR4->VAR24[VAR5];\n}\nVAR11 = VAR7->VAR25;\nVAR9 = VAR11;\nVAR14 = VAR4->VAR14;\nFUN4(VAR12, VAR10, VAR4->VAR14)\nFUN7(VAR12, &VAR9, &VAR14,\nVAR7, VAR11);\nFUN8(VAR18);\nFUN9(VAR18);\nFUN10(&VAR13, VAR4, sizeof(*VAR4));\nfor (VAR5 = 0; VAR5 < VAR22; VAR5++) {\nVAR13.VAR23[VAR5] = VAR7->VAR23[VAR5];\nVAR13.VAR24[VAR5] = VAR7->VAR24[VAR5];\n}\nVAR13.VAR26 = 0;\nVAR13.VAR27 = NULL;\nVAR13.VAR14 = VAR7->VAR14;\nVAR15 = FUN11(VAR1, VAR7, VAR11, &VAR13);\nif (VAR15)\ngoto VAR28;\n*VAR2 = VAR7;\n*VAR3 = VAR11;\nFUN12(VAR8);\nreturn 0;\nVAR28:\nFUN12(VAR7);\nreturn VAR15;\nVAR19:\nFUN8(VAR18);\nFUN9(VAR18);\nFUN4(VAR12, VAR10, VAR4->VAR14) {\nif (VAR6-- == 0)\nbreak;\nFUN13(VAR12);\n}\nreturn VAR15;\n}\n",
      "code_after_change_raw": "static int translate_compat_table(struct net *net,\nstruct xt_table_info **pinfo,\nvoid **pentry0,\nconst struct compat_arpt_replace *compatr)\n{\nunsigned int i, j;\nstruct xt_table_info *newinfo, *info;\nvoid *pos, *entry0, *entry1;\nstruct compat_arpt_entry *iter0;\nstruct arpt_replace repl;\nunsigned int size;\nint ret;\ninfo = *pinfo;\nentry0 = *pentry0;\nsize = compatr->size;\ninfo->number = compatr->num_entries;\nj = 0;\nxt_compat_lock(NFPROTO_ARP);\nret = xt_compat_init_offsets(NFPROTO_ARP, compatr->num_entries);\nif (ret)\ngoto out_unlock;\nxt_entry_foreach(iter0, entry0, compatr->size) {\nret = check_compat_entry_size_and_hooks(iter0, info, &size,\nentry0,\nentry0 + compatr->size);\nif (ret != 0)\ngoto out_unlock;\n++j;\n}\nret = -EINVAL;\nif (j != compatr->num_entries)\ngoto out_unlock;\nret = -ENOMEM;\nnewinfo = xt_alloc_table_info(size);\nif (!newinfo)\ngoto out_unlock;\nmemset(newinfo->entries, 0, size);\nnewinfo->number = compatr->num_entries;\nfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\nnewinfo->hook_entry[i] = compatr->hook_entry[i];\nnewinfo->underflow[i] = compatr->underflow[i];\n}\nentry1 = newinfo->entries;\npos = entry1;\nsize = compatr->size;\nxt_entry_foreach(iter0, entry0, compatr->size)\ncompat_copy_entry_from_user(iter0, &pos, &size,\nnewinfo, entry1);\nxt_compat_flush_offsets(NFPROTO_ARP);\nxt_compat_unlock(NFPROTO_ARP);\nmemcpy(&repl, compatr, sizeof(*compatr));\nfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\nrepl.hook_entry[i] = newinfo->hook_entry[i];\nrepl.underflow[i] = newinfo->underflow[i];\n}\nrepl.num_counters = 0;\nrepl.counters = NULL;\nrepl.size = newinfo->size;\nret = translate_table(net, newinfo, entry1, &repl);\nif (ret)\ngoto free_newinfo;\n*pinfo = newinfo;\n*pentry0 = entry1;\nxt_free_table_info(info);\nreturn 0;\nfree_newinfo:\nxt_free_table_info(newinfo);\nreturn ret;\nout_unlock:\nxt_compat_flush_offsets(NFPROTO_ARP);\nxt_compat_unlock(NFPROTO_ARP);\nxt_entry_foreach(iter0, entry0, compatr->size) {\nif (j-- == 0)\nbreak;\ncompat_release_entry(iter0);\n}\nreturn ret;\n}\n",
      "code_before_change_raw": "static int translate_compat_table(struct net *net,\nstruct xt_table_info **pinfo,\nvoid **pentry0,\nconst struct compat_arpt_replace *compatr)\n{\nunsigned int i, j;\nstruct xt_table_info *newinfo, *info;\nvoid *pos, *entry0, *entry1;\nstruct compat_arpt_entry *iter0;\nstruct arpt_replace repl;\nunsigned int size;\nint ret;\ninfo = *pinfo;\nentry0 = *pentry0;\nsize = compatr->size;\ninfo->number = compatr->num_entries;\nj = 0;\nxt_compat_lock(NFPROTO_ARP);\nret = xt_compat_init_offsets(NFPROTO_ARP, compatr->num_entries);\nif (ret)\ngoto out_unlock;\nxt_entry_foreach(iter0, entry0, compatr->size) {\nret = check_compat_entry_size_and_hooks(iter0, info, &size,\nentry0,\nentry0 + compatr->size);\nif (ret != 0)\ngoto out_unlock;\n++j;\n}\nret = -EINVAL;\nif (j != compatr->num_entries)\ngoto out_unlock;\nret = -ENOMEM;\nnewinfo = xt_alloc_table_info(size);\nif (!newinfo)\ngoto out_unlock;\nnewinfo->number = compatr->num_entries;\nfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\nnewinfo->hook_entry[i] = compatr->hook_entry[i];\nnewinfo->underflow[i] = compatr->underflow[i];\n}\nentry1 = newinfo->entries;\npos = entry1;\nsize = compatr->size;\nxt_entry_foreach(iter0, entry0, compatr->size)\ncompat_copy_entry_from_user(iter0, &pos, &size,\nnewinfo, entry1);\nxt_compat_flush_offsets(NFPROTO_ARP);\nxt_compat_unlock(NFPROTO_ARP);\nmemcpy(&repl, compatr, sizeof(*compatr));\nfor (i = 0; i < NF_ARP_NUMHOOKS; i++) {\nrepl.hook_entry[i] = newinfo->hook_entry[i];\nrepl.underflow[i] = newinfo->underflow[i];\n}\nrepl.num_counters = 0;\nrepl.counters = NULL;\nrepl.size = newinfo->size;\nret = translate_table(net, newinfo, entry1, &repl);\nif (ret)\ngoto free_newinfo;\n*pinfo = newinfo;\n*pentry0 = entry1;\nxt_free_table_info(info);\nreturn 0;\nfree_newinfo:\nxt_free_table_info(newinfo);\nreturn ret;\nout_unlock:\nxt_compat_flush_offsets(NFPROTO_ARP);\nxt_compat_unlock(NFPROTO_ARP);\nxt_entry_foreach(iter0, entry0, compatr->size) {\nif (j-- == 0)\nbreak;\ncompat_release_entry(iter0);\n}\nreturn ret;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and handling of input data from user space.",
        "trigger_condition": "An attacker provides malicious input data that exceeds the expected size, leading to a heap out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code calculates and sets padding based on the match size, which can result in writing beyond the allocated memory if the padding calculation is incorrect or manipulated by malicious input data."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the unnecessary padding calculation and setting in the code. By simplifying the code and avoiding unnecessary operations related to padding, the risk of heap out-of-bounds writes can be reduced. This solution ensures that the code does not perform calculations that could lead to memory corruption and helps prevent potential exploitation by attackers.",
      "GPT_analysis": "The modification made to the code snippet is necessary to address the vulnerability CVE-2021-22555, which involves a heap out-of-bounds write in the `xt_compat_match_from_user` function. The vulnerability allows an attacker to gain privileges or cause a denial of service (DoS) by corrupting heap memory through the user namespace.\n\nThe modification involves removing the calculation and setting of the `pad` variable, which was used to align the match size. By removing this calculation and setting of `pad`, the code avoids performing unnecessary operations that could lead to heap out-of-bounds writes.\n\nTherefore, by simplifying the code and removing the unnecessary padding calculation, the modified version of the function helps mitigate the vulnerability and reduces the risk of heap memory corruption.",
      "GPT_purpose": "Copy and convert a compatibility match entry from user space to kernel space in the netfilter/x_tables module.",
      "GPT_function": "\n1. Copy data from a compat_xt_entry_match structure to an xt_entry_match structure.\n2. Handle compatibility issues between different versions of the xt_entry_match structure.\n3. Update the size and name fields of the xt_entry_match structure.",
      "CVE_id": "CVE-2021-22555",
      "code_before_change": "void xt_compat_match_from_user(struct xt_entry_match *m, void **dstptr,\n\t\t\t       unsigned int *size)\n{\n\tconst struct xt_match *match = m->u.kernel.match;\n\tstruct compat_xt_entry_match *cm = (struct compat_xt_entry_match *)m;\n\tint pad, off = xt_compat_match_offset(match);\n\tu_int16_t msize = cm->u.user.match_size;\n\tchar name[sizeof(m->u.user.name)];\n\n\tm = *dstptr;\n\tmemcpy(m, cm, sizeof(*cm));\n\tif (match->compat_from_user)\n\t\tmatch->compat_from_user(m->data, cm->data);\n\telse\n\t\tmemcpy(m->data, cm->data, msize - sizeof(*cm));\n\tpad = XT_ALIGN(match->matchsize) - match->matchsize;\n\tif (pad > 0)\n\t\tmemset(m->data + match->matchsize, 0, pad);\n\n\tmsize += off;\n\tm->u.user.match_size = msize;\n\tstrlcpy(name, match->name, sizeof(name));\n\tmodule_put(match->me);\n\tstrncpy(m->u.user.name, name, sizeof(m->u.user.name));\n\n\t*size += off;\n\t*dstptr += msize;\n}",
      "code_after_change": "void xt_compat_match_from_user(struct xt_entry_match *m, void **dstptr,\n\t\t\t       unsigned int *size)\n{\n\tconst struct xt_match *match = m->u.kernel.match;\n\tstruct compat_xt_entry_match *cm = (struct compat_xt_entry_match *)m;\n\tint off = xt_compat_match_offset(match);\n\tu_int16_t msize = cm->u.user.match_size;\n\tchar name[sizeof(m->u.user.name)];\n\n\tm = *dstptr;\n\tmemcpy(m, cm, sizeof(*cm));\n\tif (match->compat_from_user)\n\t\tmatch->compat_from_user(m->data, cm->data);\n\telse\n\t\tmemcpy(m->data, cm->data, msize - sizeof(*cm));\n\n\tmsize += off;\n\tm->u.user.match_size = msize;\n\tstrlcpy(name, match->name, sizeof(name));\n\tmodule_put(match->me);\n\tstrncpy(m->u.user.name, name, sizeof(m->u.user.name));\n\n\t*size += off;\n\t*dstptr += msize;\n}",
      "modified_lines": {
        "added": [
          "\tint off = xt_compat_match_offset(match);"
        ],
        "deleted": [
          "\tint pad, off = xt_compat_match_offset(match);",
          "\tpad = XT_ALIGN(match->matchsize) - match->matchsize;",
          "\tif (pad > 0)",
          "\t\tmemset(m->data + match->matchsize, 0, pad);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and handling of input data from user space.",
      "trigger_condition": "An attacker provides malicious input data that exceeds the expected size, leading to a heap out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code calculates and sets padding based on the match size, which can result in writing beyond the allocated memory if the padding calculation is incorrect or manipulated by malicious input data.",
      "id": 97,
      "code_after_change_normalized": "void FUN1(struct xt_entry_match *VAR1, void **VAR2,\nunsigned int *VAR3)\n{\nconst struct xt_match *VAR4 = VAR1->VAR5.VAR6.VAR4;\nstruct VAR8 *VAR7 = (struct VAR8 *)VAR1;\nint VAR9 = FUN2(VAR4);\nu_int16_t VAR10 = VAR7->VAR5.VAR11.VAR12;\nchar VAR13[sizeof(VAR1->VAR5.VAR11.VAR13)];\nVAR1 = *VAR2;\nFUN3(VAR1, VAR7, sizeof(*VAR7));\nif (VAR4->VAR14)\nVAR4->FUN4(VAR1->VAR15, VAR7->VAR15);\nelse\nFUN3(VAR1->VAR15, VAR7->VAR15, VAR10 - sizeof(*VAR7));\nVAR10 += VAR9;\nVAR1->VAR5.VAR11.VAR12 = VAR10;\nFUN5(VAR13, VAR4->VAR13, sizeof(VAR13));\nFUN6(VAR4->VAR16);\nFUN7(VAR1->VAR5.VAR11.VAR13, VAR13, sizeof(VAR1->VAR5.VAR11.VAR13));\n*VAR3 += VAR9;\n*VAR2 += VAR10;\n}\n",
      "code_before_change_normalized": "void FUN1(struct xt_entry_match *VAR1, void **VAR2,\nunsigned int *VAR3)\n{\nconst struct xt_match *VAR4 = VAR1->VAR5.VAR6.VAR4;\nstruct VAR8 *VAR7 = (struct VAR8 *)VAR1;\nint VAR9, VAR10 = FUN2(VAR4);\nu_int16_t VAR11 = VAR7->VAR5.VAR12.VAR13;\nchar VAR14[sizeof(VAR1->VAR5.VAR12.VAR14)];\nVAR1 = *VAR2;\nFUN3(VAR1, VAR7, sizeof(*VAR7));\nif (VAR4->VAR15)\nVAR4->FUN4(VAR1->VAR16, VAR7->VAR16);\nelse\nFUN3(VAR1->VAR16, VAR7->VAR16, VAR11 - sizeof(*VAR7));\nVAR9 = FUN5(VAR4->VAR17) - VAR4->VAR17;\nif (VAR9 > 0)\nFUN6(VAR1->VAR16 + VAR4->VAR17, 0, VAR9);\nVAR11 += VAR10;\nVAR1->VAR5.VAR12.VAR13 = VAR11;\nFUN7(VAR14, VAR4->VAR14, sizeof(VAR14));\nFUN8(VAR4->VAR18);\nFUN9(VAR1->VAR5.VAR12.VAR14, VAR14, sizeof(VAR1->VAR5.VAR12.VAR14));\n*VAR3 += VAR10;\n*VAR2 += VAR11;\n}\n",
      "code_after_change_raw": "void xt_compat_match_from_user(struct xt_entry_match *m, void **dstptr,\nunsigned int *size)\n{\nconst struct xt_match *match = m->u.kernel.match;\nstruct compat_xt_entry_match *cm = (struct compat_xt_entry_match *)m;\nint off = xt_compat_match_offset(match);\nu_int16_t msize = cm->u.user.match_size;\nchar name[sizeof(m->u.user.name)];\nm = *dstptr;\nmemcpy(m, cm, sizeof(*cm));\nif (match->compat_from_user)\nmatch->compat_from_user(m->data, cm->data);\nelse\nmemcpy(m->data, cm->data, msize - sizeof(*cm));\nmsize += off;\nm->u.user.match_size = msize;\nstrlcpy(name, match->name, sizeof(name));\nmodule_put(match->me);\nstrncpy(m->u.user.name, name, sizeof(m->u.user.name));\n*size += off;\n*dstptr += msize;\n}\n",
      "code_before_change_raw": "void xt_compat_match_from_user(struct xt_entry_match *m, void **dstptr,\nunsigned int *size)\n{\nconst struct xt_match *match = m->u.kernel.match;\nstruct compat_xt_entry_match *cm = (struct compat_xt_entry_match *)m;\nint pad, off = xt_compat_match_offset(match);\nu_int16_t msize = cm->u.user.match_size;\nchar name[sizeof(m->u.user.name)];\nm = *dstptr;\nmemcpy(m, cm, sizeof(*cm));\nif (match->compat_from_user)\nmatch->compat_from_user(m->data, cm->data);\nelse\nmemcpy(m->data, cm->data, msize - sizeof(*cm));\npad = XT_ALIGN(match->matchsize) - match->matchsize;\nif (pad > 0)\nmemset(m->data + match->matchsize, 0, pad);\nmsize += off;\nm->u.user.match_size = msize;\nstrlcpy(name, match->name, sizeof(name));\nmodule_put(match->me);\nstrncpy(m->u.user.name, name, sizeof(m->u.user.name));\n*size += off;\n*dstptr += msize;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for target size alignment in the xt_compat_target_from_user function.",
        "trigger_condition": "An attacker manipulates the target size alignment calculation, leading to a heap out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code calculates and uses an unnecessary padding value to align the target size, which can result in heap out-of-bounds writes if manipulated by an attacker."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the unnecessary padding calculation and usage for target size alignment in the xt_compat_target_from_user function. By eliminating the padding calculation, the code prevents the possibility of heap out-of-bounds writes and ensures the function operates securely without introducing memory corruption issues.",
      "GPT_analysis": "The modification made to the code snippet is necessary to address the vulnerability CVE-2021-22555, which involves a heap out-of-bounds write in the `xt_compat_target_from_user` function. The vulnerability allows an attacker to gain privileges or cause a denial of service (DoS) by corrupting heap memory through the user namespace.\n\nThe modification involves removing the calculation and usage of the `pad` variable, which is used to align the target size. The `pad` calculation is not necessary and can lead to heap out-of-bounds writes, which is the root cause of the vulnerability.\n\nBy removing the `pad` calculation and usage, the modified code snippet eliminates the possibility of heap out-of-bounds writes and helps prevent the exploitation of the vulnerability. This modification ensures that the function operates correctly and securely without introducing potential memory corruption issues.",
      "GPT_purpose": "Copy and convert a compatibility target from user space to kernel space in the netfilter/x_tables module.",
      "GPT_function": "\n1. Copy data from a compat_xt_entry_target structure to an xt_entry_target structure.\n2. Handle compatibility issues between different target structures.\n3. Update target size and name fields in the xt_entry_target structure.",
      "CVE_id": "CVE-2021-22555",
      "code_before_change": "void xt_compat_target_from_user(struct xt_entry_target *t, void **dstptr,\n\t\t\t\tunsigned int *size)\n{\n\tconst struct xt_target *target = t->u.kernel.target;\n\tstruct compat_xt_entry_target *ct = (struct compat_xt_entry_target *)t;\n\tint pad, off = xt_compat_target_offset(target);\n\tu_int16_t tsize = ct->u.user.target_size;\n\tchar name[sizeof(t->u.user.name)];\n\n\tt = *dstptr;\n\tmemcpy(t, ct, sizeof(*ct));\n\tif (target->compat_from_user)\n\t\ttarget->compat_from_user(t->data, ct->data);\n\telse\n\t\tmemcpy(t->data, ct->data, tsize - sizeof(*ct));\n\tpad = XT_ALIGN(target->targetsize) - target->targetsize;\n\tif (pad > 0)\n\t\tmemset(t->data + target->targetsize, 0, pad);\n\n\ttsize += off;\n\tt->u.user.target_size = tsize;\n\tstrlcpy(name, target->name, sizeof(name));\n\tmodule_put(target->me);\n\tstrncpy(t->u.user.name, name, sizeof(t->u.user.name));\n\n\t*size += off;\n\t*dstptr += tsize;\n}",
      "code_after_change": "void xt_compat_target_from_user(struct xt_entry_target *t, void **dstptr,\n\t\t\t\tunsigned int *size)\n{\n\tconst struct xt_target *target = t->u.kernel.target;\n\tstruct compat_xt_entry_target *ct = (struct compat_xt_entry_target *)t;\n\tint off = xt_compat_target_offset(target);\n\tu_int16_t tsize = ct->u.user.target_size;\n\tchar name[sizeof(t->u.user.name)];\n\n\tt = *dstptr;\n\tmemcpy(t, ct, sizeof(*ct));\n\tif (target->compat_from_user)\n\t\ttarget->compat_from_user(t->data, ct->data);\n\telse\n\t\tmemcpy(t->data, ct->data, tsize - sizeof(*ct));\n\n\ttsize += off;\n\tt->u.user.target_size = tsize;\n\tstrlcpy(name, target->name, sizeof(name));\n\tmodule_put(target->me);\n\tstrncpy(t->u.user.name, name, sizeof(t->u.user.name));\n\n\t*size += off;\n\t*dstptr += tsize;\n}",
      "modified_lines": {
        "added": [
          "\tint off = xt_compat_target_offset(target);"
        ],
        "deleted": [
          "\tint pad, off = xt_compat_target_offset(target);",
          "\tpad = XT_ALIGN(target->targetsize) - target->targetsize;",
          "\tif (pad > 0)",
          "\t\tmemset(t->data + target->targetsize, 0, pad);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper handling for target size alignment in the xt_compat_target_from_user function.",
      "trigger_condition": "An attacker manipulates the target size alignment calculation, leading to a heap out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code calculates and uses an unnecessary padding value to align the target size, which can result in heap out-of-bounds writes if manipulated by an attacker.",
      "id": 98,
      "code_after_change_normalized": "void FUN1(struct xt_entry_target *VAR1, void **VAR2,\nunsigned int *VAR3)\n{\nconst struct xt_target *VAR4 = VAR1->VAR5.VAR6.VAR4;\nstruct VAR8 *VAR7 = (struct VAR8 *)VAR1;\nint VAR9 = FUN2(VAR4);\nu_int16_t VAR10 = VAR7->VAR5.VAR11.VAR12;\nchar VAR13[sizeof(VAR1->VAR5.VAR11.VAR13)];\nVAR1 = *VAR2;\nFUN3(VAR1, VAR7, sizeof(*VAR7));\nif (VAR4->VAR14)\nVAR4->FUN4(VAR1->VAR15, VAR7->VAR15);\nelse\nFUN3(VAR1->VAR15, VAR7->VAR15, VAR10 - sizeof(*VAR7));\nVAR10 += VAR9;\nVAR1->VAR5.VAR11.VAR12 = VAR10;\nFUN5(VAR13, VAR4->VAR13, sizeof(VAR13));\nFUN6(VAR4->VAR16);\nFUN7(VAR1->VAR5.VAR11.VAR13, VAR13, sizeof(VAR1->VAR5.VAR11.VAR13));\n*VAR3 += VAR9;\n*VAR2 += VAR10;\n}\n",
      "code_before_change_normalized": "void FUN1(struct xt_entry_target *VAR1, void **VAR2,\nunsigned int *VAR3)\n{\nconst struct xt_target *VAR4 = VAR1->VAR5.VAR6.VAR4;\nstruct VAR8 *VAR7 = (struct VAR8 *)VAR1;\nint VAR9, VAR10 = FUN2(VAR4);\nu_int16_t VAR11 = VAR7->VAR5.VAR12.VAR13;\nchar VAR14[sizeof(VAR1->VAR5.VAR12.VAR14)];\nVAR1 = *VAR2;\nFUN3(VAR1, VAR7, sizeof(*VAR7));\nif (VAR4->VAR15)\nVAR4->FUN4(VAR1->VAR16, VAR7->VAR16);\nelse\nFUN3(VAR1->VAR16, VAR7->VAR16, VAR11 - sizeof(*VAR7));\nVAR9 = FUN5(VAR4->VAR17) - VAR4->VAR17;\nif (VAR9 > 0)\nFUN6(VAR1->VAR16 + VAR4->VAR17, 0, VAR9);\nVAR11 += VAR10;\nVAR1->VAR5.VAR12.VAR13 = VAR11;\nFUN7(VAR14, VAR4->VAR14, sizeof(VAR14));\nFUN8(VAR4->VAR18);\nFUN9(VAR1->VAR5.VAR12.VAR14, VAR14, sizeof(VAR1->VAR5.VAR12.VAR14));\n*VAR3 += VAR10;\n*VAR2 += VAR11;\n}\n",
      "code_after_change_raw": "void xt_compat_target_from_user(struct xt_entry_target *t, void **dstptr,\nunsigned int *size)\n{\nconst struct xt_target *target = t->u.kernel.target;\nstruct compat_xt_entry_target *ct = (struct compat_xt_entry_target *)t;\nint off = xt_compat_target_offset(target);\nu_int16_t tsize = ct->u.user.target_size;\nchar name[sizeof(t->u.user.name)];\nt = *dstptr;\nmemcpy(t, ct, sizeof(*ct));\nif (target->compat_from_user)\ntarget->compat_from_user(t->data, ct->data);\nelse\nmemcpy(t->data, ct->data, tsize - sizeof(*ct));\ntsize += off;\nt->u.user.target_size = tsize;\nstrlcpy(name, target->name, sizeof(name));\nmodule_put(target->me);\nstrncpy(t->u.user.name, name, sizeof(t->u.user.name));\n*size += off;\n*dstptr += tsize;\n}\n",
      "code_before_change_raw": "void xt_compat_target_from_user(struct xt_entry_target *t, void **dstptr,\nunsigned int *size)\n{\nconst struct xt_target *target = t->u.kernel.target;\nstruct compat_xt_entry_target *ct = (struct compat_xt_entry_target *)t;\nint pad, off = xt_compat_target_offset(target);\nu_int16_t tsize = ct->u.user.target_size;\nchar name[sizeof(t->u.user.name)];\nt = *dstptr;\nmemcpy(t, ct, sizeof(*ct));\nif (target->compat_from_user)\ntarget->compat_from_user(t->data, ct->data);\nelse\nmemcpy(t->data, ct->data, tsize - sizeof(*ct));\npad = XT_ALIGN(target->targetsize) - target->targetsize;\nif (pad > 0)\nmemset(t->data + target->targetsize, 0, pad);\ntsize += off;\nt->u.user.target_size = tsize;\nstrlcpy(name, target->name, sizeof(name));\nmodule_put(target->me);\nstrncpy(t->u.user.name, name, sizeof(t->u.user.name));\n*size += off;\n*dstptr += tsize;\n}\n"
    }
  ],
  "CVE-2021-27365": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
        "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message.",
        "specific_code_behavior_causing_vulnerability": "The code uses the sprintf function to format and write data to a buffer without ensuring that the output size is properly constrained. This can lead to buffer overflows if the data exceeds the buffer size, potentially allowing an attacker to exploit the vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to replace the sprintf function with sysfs_emit, which provides a safer way to write data to a buffer by ensuring that the output does not exceed the specified buffer size. This change helps prevent buffer overflows and enhances the security of the system by properly handling the length of the output data.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2021-27365 related to iSCSI data structures not having appropriate length constraints or checks. By replacing the `sprintf` function with `sysfs_emit`, the code ensures that the output buffer size is properly handled and limited to prevent potential buffer overflows.\n\nThe `sysfs_emit` function is designed to safely handle writing data to a buffer by ensuring that the output does not exceed the specified buffer size. This helps prevent scenarios where unprivileged users could send Netlink messages associated with iSCSI that exceed the maximum length of a Netlink message, potentially leading to security vulnerabilities like buffer overflows.\n\nBy using `sysfs_emit` instead of `sprintf`, the modified code snippet provides a more secure and robust solution to mitigate the vulnerability and ensure that the length of the output data is properly constrained, thus enhancing the overall security of the system.",
      "GPT_purpose": "This function retrieves address parameters for an iSCSI connection.",
      "GPT_function": "\n1. Extracts the address parameters from a sockaddr_storage structure based on the address family.\n2. Formats the address and port information based on the specified parameter.\n3. Returns the length of the formatted string.",
      "CVE_id": "CVE-2021-27365",
      "code_before_change": "int iscsi_conn_get_addr_param(struct sockaddr_storage *addr,\n\t\t\t      enum iscsi_param param, char *buf)\n{\n\tstruct sockaddr_in6 *sin6 = NULL;\n\tstruct sockaddr_in *sin = NULL;\n\tint len;\n\n\tswitch (addr->ss_family) {\n\tcase AF_INET:\n\t\tsin = (struct sockaddr_in *)addr;\n\t\tbreak;\n\tcase AF_INET6:\n\t\tsin6 = (struct sockaddr_in6 *)addr;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (param) {\n\tcase ISCSI_PARAM_CONN_ADDRESS:\n\tcase ISCSI_HOST_PARAM_IPADDRESS:\n\t\tif (sin)\n\t\t\tlen = sprintf(buf, \"%pI4\\n\", &sin->sin_addr.s_addr);\n\t\telse\n\t\t\tlen = sprintf(buf, \"%pI6\\n\", &sin6->sin6_addr);\n\t\tbreak;\n\tcase ISCSI_PARAM_CONN_PORT:\n\tcase ISCSI_PARAM_LOCAL_PORT:\n\t\tif (sin)\n\t\t\tlen = sprintf(buf, \"%hu\\n\", be16_to_cpu(sin->sin_port));\n\t\telse\n\t\t\tlen = sprintf(buf, \"%hu\\n\",\n\t\t\t\t      be16_to_cpu(sin6->sin6_port));\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn len;\n}",
      "code_after_change": "int iscsi_conn_get_addr_param(struct sockaddr_storage *addr,\n\t\t\t      enum iscsi_param param, char *buf)\n{\n\tstruct sockaddr_in6 *sin6 = NULL;\n\tstruct sockaddr_in *sin = NULL;\n\tint len;\n\n\tswitch (addr->ss_family) {\n\tcase AF_INET:\n\t\tsin = (struct sockaddr_in *)addr;\n\t\tbreak;\n\tcase AF_INET6:\n\t\tsin6 = (struct sockaddr_in6 *)addr;\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (param) {\n\tcase ISCSI_PARAM_CONN_ADDRESS:\n\tcase ISCSI_HOST_PARAM_IPADDRESS:\n\t\tif (sin)\n\t\t\tlen = sysfs_emit(buf, \"%pI4\\n\", &sin->sin_addr.s_addr);\n\t\telse\n\t\t\tlen = sysfs_emit(buf, \"%pI6\\n\", &sin6->sin6_addr);\n\t\tbreak;\n\tcase ISCSI_PARAM_CONN_PORT:\n\tcase ISCSI_PARAM_LOCAL_PORT:\n\t\tif (sin)\n\t\t\tlen = sysfs_emit(buf, \"%hu\\n\", be16_to_cpu(sin->sin_port));\n\t\telse\n\t\t\tlen = sysfs_emit(buf, \"%hu\\n\",\n\t\t\t\t      be16_to_cpu(sin6->sin6_port));\n\t\tbreak;\n\tdefault:\n\t\treturn -EINVAL;\n\t}\n\n\treturn len;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tlen = sysfs_emit(buf, \"%pI4\\n\", &sin->sin_addr.s_addr);",
          "\t\t\tlen = sysfs_emit(buf, \"%pI6\\n\", &sin6->sin6_addr);",
          "\t\t\tlen = sysfs_emit(buf, \"%hu\\n\", be16_to_cpu(sin->sin_port));",
          "\t\t\tlen = sysfs_emit(buf, \"%hu\\n\","
        ],
        "deleted": [
          "\t\t\tlen = sprintf(buf, \"%pI4\\n\", &sin->sin_addr.s_addr);",
          "\t\t\tlen = sprintf(buf, \"%pI6\\n\", &sin6->sin6_addr);",
          "\t\t\tlen = sprintf(buf, \"%hu\\n\", be16_to_cpu(sin->sin_port));",
          "\t\t\tlen = sprintf(buf, \"%hu\\n\","
        ]
      },
      "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
      "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message.",
      "specific_code_behavior_causing_vulnerability": "The code uses the sprintf function to format and write data to a buffer without ensuring that the output size is properly constrained. This can lead to buffer overflows if the data exceeds the buffer size, potentially allowing an attacker to exploit the vulnerability.",
      "id": 99,
      "code_after_change_normalized": "int FUN1(struct sockaddr_storage *VAR1,\nenum iscsi_param VAR2, char *VAR3)\n{\nstruct sockaddr_in6 *VAR4 = NULL;\nstruct sockaddr_in *VAR5 = NULL;\nint VAR6;\nswitch (VAR1->VAR7) {\ncase VAR8:\nVAR5 = (struct VAR9 *)VAR1;\nbreak;\ncase VAR10:\nVAR4 = (struct VAR11 *)VAR1;\nbreak;\ndefault:\nreturn -VAR12;\n}\nswitch (VAR2) {\ncase VAR13:\ncase VAR14:\nif (VAR5)\nVAR6 = FUN2(VAR3, \"STR\", &VAR5->VAR15.VAR16);\nelse\nVAR6 = FUN2(VAR3, \"STR\", &VAR4->VAR17);\nbreak;\ncase VAR18:\ncase VAR19:\nif (VAR5)\nVAR6 = FUN2(VAR3, \"STR\", FUN3(VAR5->VAR20));\nelse\nVAR6 = FUN2(VAR3, \"STR\",\nFUN3(VAR4->VAR21));\nbreak;\ndefault:\nreturn -VAR12;\n}\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "int FUN1(struct sockaddr_storage *VAR1,\nenum iscsi_param VAR2, char *VAR3)\n{\nstruct sockaddr_in6 *VAR4 = NULL;\nstruct sockaddr_in *VAR5 = NULL;\nint VAR6;\nswitch (VAR1->VAR7) {\ncase VAR8:\nVAR5 = (struct VAR9 *)VAR1;\nbreak;\ncase VAR10:\nVAR4 = (struct VAR11 *)VAR1;\nbreak;\ndefault:\nreturn -VAR12;\n}\nswitch (VAR2) {\ncase VAR13:\ncase VAR14:\nif (VAR5)\nVAR6 = FUN2(VAR3, \"STR\", &VAR5->VAR15.VAR16);\nelse\nVAR6 = FUN2(VAR3, \"STR\", &VAR4->VAR17);\nbreak;\ncase VAR18:\ncase VAR19:\nif (VAR5)\nVAR6 = FUN2(VAR3, \"STR\", FUN3(VAR5->VAR20));\nelse\nVAR6 = FUN2(VAR3, \"STR\",\nFUN3(VAR4->VAR21));\nbreak;\ndefault:\nreturn -VAR12;\n}\nreturn VAR6;\n}\n",
      "code_after_change_raw": "int iscsi_conn_get_addr_param(struct sockaddr_storage *addr,\nenum iscsi_param param, char *buf)\n{\nstruct sockaddr_in6 *sin6 = NULL;\nstruct sockaddr_in *sin = NULL;\nint len;\nswitch (addr->ss_family) {\ncase AF_INET:\nsin = (struct sockaddr_in *)addr;\nbreak;\ncase AF_INET6:\nsin6 = (struct sockaddr_in6 *)addr;\nbreak;\ndefault:\nreturn -EINVAL;\n}\nswitch (param) {\ncase ISCSI_PARAM_CONN_ADDRESS:\ncase ISCSI_HOST_PARAM_IPADDRESS:\nif (sin)\nlen = sysfs_emit(buf, \"%pI4\\n\", &sin->sin_addr.s_addr);\nelse\nlen = sysfs_emit(buf, \"%pI6\\n\", &sin6->sin6_addr);\nbreak;\ncase ISCSI_PARAM_CONN_PORT:\ncase ISCSI_PARAM_LOCAL_PORT:\nif (sin)\nlen = sysfs_emit(buf, \"%hu\\n\", be16_to_cpu(sin->sin_port));\nelse\nlen = sysfs_emit(buf, \"%hu\\n\",\nbe16_to_cpu(sin6->sin6_port));\nbreak;\ndefault:\nreturn -EINVAL;\n}\nreturn len;\n}\n",
      "code_before_change_raw": "int iscsi_conn_get_addr_param(struct sockaddr_storage *addr,\nenum iscsi_param param, char *buf)\n{\nstruct sockaddr_in6 *sin6 = NULL;\nstruct sockaddr_in *sin = NULL;\nint len;\nswitch (addr->ss_family) {\ncase AF_INET:\nsin = (struct sockaddr_in *)addr;\nbreak;\ncase AF_INET6:\nsin6 = (struct sockaddr_in6 *)addr;\nbreak;\ndefault:\nreturn -EINVAL;\n}\nswitch (param) {\ncase ISCSI_PARAM_CONN_ADDRESS:\ncase ISCSI_HOST_PARAM_IPADDRESS:\nif (sin)\nlen = sprintf(buf, \"%pI4\\n\", &sin->sin_addr.s_addr);\nelse\nlen = sprintf(buf, \"%pI6\\n\", &sin6->sin6_addr);\nbreak;\ncase ISCSI_PARAM_CONN_PORT:\ncase ISCSI_PARAM_LOCAL_PORT:\nif (sin)\nlen = sprintf(buf, \"%hu\\n\", be16_to_cpu(sin->sin_port));\nelse\nlen = sprintf(buf, \"%hu\\n\",\nbe16_to_cpu(sin6->sin6_port));\nbreak;\ndefault:\nreturn -EINVAL;\n}\nreturn len;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks on certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
        "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length exceeding the maximum length of a Netlink message.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the Netlink message associated with iSCSI, allowing it to exceed the PAGE_SIZE value. This can lead to buffer overflows and potential security vulnerabilities."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the length of the Netlink message does not exceed the maximum allowed size (PAGE_SIZE). By validating the length of the message, the code can prevent buffer overflows and enforce appropriate constraints on data structures. In this case, the solution involves adding a check at the beginning of the function to verify that the length of the Netlink message is within acceptable limits.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-27365, which allows an unprivileged user to send a Netlink message associated with iSCSI that can exceed the PAGE_SIZE value. By adding the check `if (ev->u.set_param.len > PAGE_SIZE) return -EINVAL;`, the code ensures that the length of the Netlink message does not exceed the maximum allowed size (PAGE_SIZE). This check helps prevent buffer overflows and ensures that the data structures do not exceed the expected size, thereby mitigating the vulnerability and improving the security of the code.",
      "GPT_purpose": "Set parameters for iSCSI connections and sessions, handling different parameters including session recovery timeout.",
      "GPT_function": "\n1. Set iSCSI parameters for a given transport and event.\n2. Look up the iSCSI session and connection based on the provided session ID and connection ID.\n3. Handle setting specific iSCSI parameters, such as session recovery timeout, or delegate to the transport's set_param function for other parameters.",
      "CVE_id": "CVE-2021-27365",
      "code_before_change": "static int\niscsi_set_param(struct iscsi_transport *transport, struct iscsi_uevent *ev)\n{\n\tchar *data = (char*)ev + sizeof(*ev);\n\tstruct iscsi_cls_conn *conn;\n\tstruct iscsi_cls_session *session;\n\tint err = 0, value = 0;\n\n\tsession = iscsi_session_lookup(ev->u.set_param.sid);\n\tconn = iscsi_conn_lookup(ev->u.set_param.sid, ev->u.set_param.cid);\n\tif (!conn || !session)\n\t\treturn -EINVAL;\n\n\tswitch (ev->u.set_param.param) {\n\tcase ISCSI_PARAM_SESS_RECOVERY_TMO:\n\t\tsscanf(data, \"%d\", &value);\n\t\tif (!session->recovery_tmo_sysfs_override)\n\t\t\tsession->recovery_tmo = value;\n\t\tbreak;\n\tdefault:\n\t\terr = transport->set_param(conn, ev->u.set_param.param,\n\t\t\t\t\t   data, ev->u.set_param.len);\n\t}\n\n\treturn err;\n}",
      "code_after_change": "static int\niscsi_set_param(struct iscsi_transport *transport, struct iscsi_uevent *ev)\n{\n\tchar *data = (char*)ev + sizeof(*ev);\n\tstruct iscsi_cls_conn *conn;\n\tstruct iscsi_cls_session *session;\n\tint err = 0, value = 0;\n\n\tif (ev->u.set_param.len > PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tsession = iscsi_session_lookup(ev->u.set_param.sid);\n\tconn = iscsi_conn_lookup(ev->u.set_param.sid, ev->u.set_param.cid);\n\tif (!conn || !session)\n\t\treturn -EINVAL;\n\n\tswitch (ev->u.set_param.param) {\n\tcase ISCSI_PARAM_SESS_RECOVERY_TMO:\n\t\tsscanf(data, \"%d\", &value);\n\t\tif (!session->recovery_tmo_sysfs_override)\n\t\t\tsession->recovery_tmo = value;\n\t\tbreak;\n\tdefault:\n\t\terr = transport->set_param(conn, ev->u.set_param.param,\n\t\t\t\t\t   data, ev->u.set_param.len);\n\t}\n\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (ev->u.set_param.len > PAGE_SIZE)",
          "\t\treturn -EINVAL;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks on certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
      "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length exceeding the maximum length of a Netlink message.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the Netlink message associated with iSCSI, allowing it to exceed the PAGE_SIZE value. This can lead to buffer overflows and potential security vulnerabilities.",
      "id": 100,
      "code_after_change_normalized": "static int\nFUN1(struct iscsi_transport *VAR1, struct iscsi_uevent *VAR2)\n{\nchar *VAR3 = (char*)VAR2 + sizeof(*VAR2);\nstruct iscsi_cls_conn *VAR4;\nstruct iscsi_cls_session *VAR5;\nint VAR6 = 0, VAR7 = 0;\nif (VAR2->VAR8.VAR9.VAR10 > VAR11)\nreturn -VAR12;\nVAR5 = FUN2(VAR2->VAR8.VAR9.VAR13);\nVAR4 = FUN3(VAR2->VAR8.VAR9.VAR13, VAR2->VAR8.VAR9.VAR14);\nif (!VAR4 || !VAR5)\nreturn -VAR12;\nswitch (VAR2->VAR8.VAR9.VAR15) {\ncase VAR16:\nFUN4(VAR3, \"STR\", &VAR7);\nif (!VAR5->VAR17)\nVAR5->VAR18 = VAR7;\nbreak;\ndefault:\nVAR6 = VAR1->FUN5(VAR4, VAR2->VAR8.VAR9.VAR15,\nVAR3, VAR2->VAR8.VAR9.VAR10);\n}\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static int\nFUN1(struct iscsi_transport *VAR1, struct iscsi_uevent *VAR2)\n{\nchar *VAR3 = (char*)VAR2 + sizeof(*VAR2);\nstruct iscsi_cls_conn *VAR4;\nstruct iscsi_cls_session *VAR5;\nint VAR6 = 0, VAR7 = 0;\nVAR5 = FUN2(VAR2->VAR8.VAR9.VAR10);\nVAR4 = FUN3(VAR2->VAR8.VAR9.VAR10, VAR2->VAR8.VAR9.VAR11);\nif (!VAR4 || !VAR5)\nreturn -VAR12;\nswitch (VAR2->VAR8.VAR9.VAR13) {\ncase VAR14:\nFUN4(VAR3, \"STR\", &VAR7);\nif (!VAR5->VAR15)\nVAR5->VAR16 = VAR7;\nbreak;\ndefault:\nVAR6 = VAR1->FUN5(VAR4, VAR2->VAR8.VAR9.VAR13,\nVAR3, VAR2->VAR8.VAR9.VAR17);\n}\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static int\niscsi_set_param(struct iscsi_transport *transport, struct iscsi_uevent *ev)\n{\nchar *data = (char*)ev + sizeof(*ev);\nstruct iscsi_cls_conn *conn;\nstruct iscsi_cls_session *session;\nint err = 0, value = 0;\nif (ev->u.set_param.len > PAGE_SIZE)\nreturn -EINVAL;\nsession = iscsi_session_lookup(ev->u.set_param.sid);\nconn = iscsi_conn_lookup(ev->u.set_param.sid, ev->u.set_param.cid);\nif (!conn || !session)\nreturn -EINVAL;\nswitch (ev->u.set_param.param) {\ncase ISCSI_PARAM_SESS_RECOVERY_TMO:\nsscanf(data, \"%d\", &value);\nif (!session->recovery_tmo_sysfs_override)\nsession->recovery_tmo = value;\nbreak;\ndefault:\nerr = transport->set_param(conn, ev->u.set_param.param,\ndata, ev->u.set_param.len);\n}\nreturn err;\n}\n",
      "code_before_change_raw": "static int\niscsi_set_param(struct iscsi_transport *transport, struct iscsi_uevent *ev)\n{\nchar *data = (char*)ev + sizeof(*ev);\nstruct iscsi_cls_conn *conn;\nstruct iscsi_cls_session *session;\nint err = 0, value = 0;\nsession = iscsi_session_lookup(ev->u.set_param.sid);\nconn = iscsi_conn_lookup(ev->u.set_param.sid, ev->u.set_param.cid);\nif (!conn || !session)\nreturn -EINVAL;\nswitch (ev->u.set_param.param) {\ncase ISCSI_PARAM_SESS_RECOVERY_TMO:\nsscanf(data, \"%d\", &value);\nif (!session->recovery_tmo_sysfs_override)\nsession->recovery_tmo = value;\nbreak;\ndefault:\nerr = transport->set_param(conn, ev->u.set_param.param,\ndata, ev->u.set_param.len);\n}\nreturn err;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
        "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message, triggering a potential buffer overflow.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the data being passed in the structure associated with iSCSI operations, which can lead to a buffer overflow if the data exceeds the PAGE_SIZE value."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a length check to ensure that the data being processed does not exceed the PAGE_SIZE value. This prevents potential buffer overflows caused by unvalidated data lengths. In this case, the solution involves adding a line to check if the length of the data exceeds PAGE_SIZE and returning an error if it does.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-27365 by adding a length check to prevent a potential buffer overflow. \n\nIn the original code snippet, the function `iscsi_set_host_param` does not validate the length of the data being passed in the `ev` structure. This lack of length constraint checking can lead to a situation where an unprivileged user can send a Netlink message associated with iSCSI that exceeds the `PAGE_SIZE` value, potentially causing a buffer overflow.\n\nBy adding the line `if (ev->u.set_host_param.len > PAGE_SIZE) return -EINVAL;`, the modified code snippet ensures that the length of the data being passed is checked against the `PAGE_SIZE` value. If the length exceeds `PAGE_SIZE`, the function will return an error (`-EINVAL`), preventing the buffer overflow vulnerability.\n\nTherefore, the modification is necessary to enforce a length constraint on the data being processed, thereby mitigating the vulnerability and enhancing the security of the code.",
      "GPT_purpose": "Set a host parameter for iSCSI transport.",
      "GPT_function": "\n1. Set a host parameter for an iSCSI transport.\n2. Look up the SCSI host based on the host number provided in the event.\n3. Call the set_host_param function of the transport with the SCSI host, parameter, data, and length provided in the event.",
      "CVE_id": "CVE-2021-27365",
      "code_before_change": "static int\niscsi_set_host_param(struct iscsi_transport *transport,\n\t\t     struct iscsi_uevent *ev)\n{\n\tchar *data = (char*)ev + sizeof(*ev);\n\tstruct Scsi_Host *shost;\n\tint err;\n\n\tif (!transport->set_host_param)\n\t\treturn -ENOSYS;\n\n\tshost = scsi_host_lookup(ev->u.set_host_param.host_no);\n\tif (!shost) {\n\t\tprintk(KERN_ERR \"set_host_param could not find host no %u\\n\",\n\t\t       ev->u.set_host_param.host_no);\n\t\treturn -ENODEV;\n\t}\n\n\terr = transport->set_host_param(shost, ev->u.set_host_param.param,\n\t\t\t\t\tdata, ev->u.set_host_param.len);\n\tscsi_host_put(shost);\n\treturn err;\n}",
      "code_after_change": "static int\niscsi_set_host_param(struct iscsi_transport *transport,\n\t\t     struct iscsi_uevent *ev)\n{\n\tchar *data = (char*)ev + sizeof(*ev);\n\tstruct Scsi_Host *shost;\n\tint err;\n\n\tif (!transport->set_host_param)\n\t\treturn -ENOSYS;\n\n\tif (ev->u.set_host_param.len > PAGE_SIZE)\n\t\treturn -EINVAL;\n\n\tshost = scsi_host_lookup(ev->u.set_host_param.host_no);\n\tif (!shost) {\n\t\tprintk(KERN_ERR \"set_host_param could not find host no %u\\n\",\n\t\t       ev->u.set_host_param.host_no);\n\t\treturn -ENODEV;\n\t}\n\n\terr = transport->set_host_param(shost, ev->u.set_host_param.param,\n\t\t\t\t\tdata, ev->u.set_host_param.len);\n\tscsi_host_put(shost);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (ev->u.set_host_param.len > PAGE_SIZE)",
          "\t\treturn -EINVAL;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
      "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message, triggering a potential buffer overflow.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the data being passed in the structure associated with iSCSI operations, which can lead to a buffer overflow if the data exceeds the PAGE_SIZE value.",
      "id": 101,
      "code_after_change_normalized": "static int\nFUN1(struct iscsi_transport *VAR1,\nstruct iscsi_uevent *VAR2)\n{\nchar *VAR3 = (char*)VAR2 + sizeof(*VAR2);\nstruct Scsi_Host *VAR4;\nint VAR5;\nif (!VAR1->VAR6)\nreturn -VAR7;\nif (VAR2->VAR8.VAR6.VAR9 > VAR10)\nreturn -VAR11;\nVAR4 = FUN2(VAR2->VAR8.VAR6.VAR12);\nif (!VAR4) {\nFUN3(VAR13 \"STR\",\nVAR2->VAR8.VAR6.VAR12);\nreturn -VAR14;\n}\nVAR5 = VAR1->FUN4(VAR4, VAR2->VAR8.VAR6.VAR15,\nVAR3, VAR2->VAR8.VAR6.VAR9);\nFUN5(VAR4);\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "static int\nFUN1(struct iscsi_transport *VAR1,\nstruct iscsi_uevent *VAR2)\n{\nchar *VAR3 = (char*)VAR2 + sizeof(*VAR2);\nstruct Scsi_Host *VAR4;\nint VAR5;\nif (!VAR1->VAR6)\nreturn -VAR7;\nVAR4 = FUN2(VAR2->VAR8.VAR6.VAR9);\nif (!VAR4) {\nFUN3(VAR10 \"STR\",\nVAR2->VAR8.VAR6.VAR9);\nreturn -VAR11;\n}\nVAR5 = VAR1->FUN4(VAR4, VAR2->VAR8.VAR6.VAR12,\nVAR3, VAR2->VAR8.VAR6.VAR13);\nFUN5(VAR4);\nreturn VAR5;\n}\n",
      "code_after_change_raw": "static int\niscsi_set_host_param(struct iscsi_transport *transport,\nstruct iscsi_uevent *ev)\n{\nchar *data = (char*)ev + sizeof(*ev);\nstruct Scsi_Host *shost;\nint err;\nif (!transport->set_host_param)\nreturn -ENOSYS;\nif (ev->u.set_host_param.len > PAGE_SIZE)\nreturn -EINVAL;\nshost = scsi_host_lookup(ev->u.set_host_param.host_no);\nif (!shost) {\nprintk(KERN_ERR \"set_host_param could not find host no %u\\n\",\nev->u.set_host_param.host_no);\nreturn -ENODEV;\n}\nerr = transport->set_host_param(shost, ev->u.set_host_param.param,\ndata, ev->u.set_host_param.len);\nscsi_host_put(shost);\nreturn err;\n}\n",
      "code_before_change_raw": "static int\niscsi_set_host_param(struct iscsi_transport *transport,\nstruct iscsi_uevent *ev)\n{\nchar *data = (char*)ev + sizeof(*ev);\nstruct Scsi_Host *shost;\nint err;\nif (!transport->set_host_param)\nreturn -ENOSYS;\nshost = scsi_host_lookup(ev->u.set_host_param.host_no);\nif (!shost) {\nprintk(KERN_ERR \"set_host_param could not find host no %u\\n\",\nev->u.set_host_param.host_no);\nreturn -ENODEV;\n}\nerr = transport->set_host_param(shost, ev->u.set_host_param.param,\ndata, ev->u.set_host_param.len);\nscsi_host_put(shost);\nreturn err;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
        "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message.",
        "specific_code_behavior_causing_vulnerability": "The code uses the sprintf function to write data to a buffer without ensuring that the data does not exceed the maximum length allowed for a Netlink message. This lack of length constraints can lead to a buffer overflow vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to replace the sprintf function with the sysfs_emit function. The sysfs_emit function ensures that the data being written to the buffer does not exceed the maximum length allowed for a Netlink message, thereby preventing potential buffer overflow vulnerabilities. By using sysfs_emit, the code enforces appropriate length constraints and enhances the security of the data writing process.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-27365 by replacing the `sprintf` function with `sysfs_emit` function. The vulnerability arises from the fact that the `sprintf` function does not have appropriate length constraints or checks, which can lead to a buffer overflow if the data exceeds the PAGE_SIZE value.\n\nBy using the `sysfs_emit` function instead of `sprintf`, the output is written to the buffer in a safer manner. The `sysfs_emit` function ensures that the data being written to the buffer does not exceed the maximum length allowed for a Netlink message, thus preventing potential buffer overflow vulnerabilities.\n\nTherefore, the modification helps to mitigate the risk of buffer overflow attacks by using a safer alternative function for writing data to the buffer.",
      "GPT_purpose": "Display the handle associated with the iSCSI transport for a given device.",
      "GPT_function": "\n1. Show the transport handle for a device.\n2. Check if the user has CAP_SYS_ADMIN capability.\n3. Format and return the transport handle value as a string.",
      "CVE_id": "CVE-2021-27365",
      "code_before_change": "static ssize_t\nshow_transport_handle(struct device *dev, struct device_attribute *attr,\n\t\t      char *buf)\n{\n\tstruct iscsi_internal *priv = dev_to_iscsi_internal(dev);\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\treturn sprintf(buf, \"%llu\\n\", (unsigned long long)iscsi_handle(priv->iscsi_transport));\n}",
      "code_after_change": "static ssize_t\nshow_transport_handle(struct device *dev, struct device_attribute *attr,\n\t\t      char *buf)\n{\n\tstruct iscsi_internal *priv = dev_to_iscsi_internal(dev);\n\n\tif (!capable(CAP_SYS_ADMIN))\n\t\treturn -EACCES;\n\treturn sysfs_emit(buf, \"%llu\\n\",\n\t\t  (unsigned long long)iscsi_handle(priv->iscsi_transport));\n}",
      "modified_lines": {
        "added": [
          "\treturn sysfs_emit(buf, \"%llu\\n\",",
          "\t\t  (unsigned long long)iscsi_handle(priv->iscsi_transport));"
        ],
        "deleted": [
          "\treturn sprintf(buf, \"%llu\\n\", (unsigned long long)iscsi_handle(priv->iscsi_transport));"
        ]
      },
      "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
      "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message.",
      "specific_code_behavior_causing_vulnerability": "The code uses the sprintf function to write data to a buffer without ensuring that the data does not exceed the maximum length allowed for a Netlink message. This lack of length constraints can lead to a buffer overflow vulnerability.",
      "id": 102,
      "code_after_change_normalized": "static VAR1\nFUN1(struct device *VAR2, struct device_attribute *VAR3,\nchar *VAR4)\n{\nstruct iscsi_internal *VAR5 = FUN2(VAR2);\nif (!FUN3(VAR6))\nreturn -VAR7;\nreturn FUN4(VAR4, \"STR\",\n(unsigned long long)FUN5(VAR5->VAR8));\n}\n",
      "code_before_change_normalized": "static VAR1\nFUN1(struct device *VAR2, struct device_attribute *VAR3,\nchar *VAR4)\n{\nstruct iscsi_internal *VAR5 = FUN2(VAR2);\nif (!FUN3(VAR6))\nreturn -VAR7;\nreturn FUN4(VAR4, \"STR\", (unsigned long long)FUN5(VAR5->VAR8));\n}\n",
      "code_after_change_raw": "static ssize_t\nshow_transport_handle(struct device *dev, struct device_attribute *attr,\nchar *buf)\n{\nstruct iscsi_internal *priv = dev_to_iscsi_internal(dev);\nif (!capable(CAP_SYS_ADMIN))\nreturn -EACCES;\nreturn sysfs_emit(buf, \"%llu\\n\",\n(unsigned long long)iscsi_handle(priv->iscsi_transport));\n}\n",
      "code_before_change_raw": "static ssize_t\nshow_transport_handle(struct device *dev, struct device_attribute *attr,\nchar *buf)\n{\nstruct iscsi_internal *priv = dev_to_iscsi_internal(dev);\nif (!capable(CAP_SYS_ADMIN))\nreturn -EACCES;\nreturn sprintf(buf, \"%llu\\n\", (unsigned long long)iscsi_handle(priv->iscsi_transport));\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
        "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message, triggering a buffer overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code uses the sprintf function to format data into a buffer without ensuring that the data fits within the buffer size, potentially leading to a buffer overflow if the data exceeds the buffer capacity."
      },
      "solution": "To mitigate the vulnerability, it is necessary to replace the sprintf function with a safer alternative that enforces proper length constraints and checks. In this case, the solution involves replacing sprintf with sysfs_emit, which ensures that the data being written to the buffer does not exceed its size, thereby preventing buffer overflow vulnerabilities.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-27365 by replacing the `sprintf` function with `sysfs_emit` function. \n\nThe vulnerability arises from the use of `sprintf` in the original code snippet, which does not have appropriate length constraints or checks. This can lead to a buffer overflow if the data being formatted exceeds the size of the buffer `buf`. \n\nBy replacing `sprintf` with `sysfs_emit`, the `sysfs_emit` function ensures that the data being written to `buf` is properly handled and does not exceed the buffer size. This helps prevent buffer overflow vulnerabilities and ensures the safe handling of data within the specified buffer limits. \n\nTherefore, the modification is necessary to mitigate the vulnerability and improve the security of the code.",
      "GPT_purpose": "Display the handle of an iSCSI endpoint.",
      "GPT_function": "\n1. Retrieve the handle of an iSCSI endpoint associated with a device.\n2. Format and output the endpoint handle as an unsigned long long integer.",
      "CVE_id": "CVE-2021-27365",
      "code_before_change": "static ssize_t\nshow_ep_handle(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct iscsi_endpoint *ep = iscsi_dev_to_endpoint(dev);\n\treturn sprintf(buf, \"%llu\\n\", (unsigned long long) ep->id);\n}",
      "code_after_change": "static ssize_t\nshow_ep_handle(struct device *dev, struct device_attribute *attr, char *buf)\n{\n\tstruct iscsi_endpoint *ep = iscsi_dev_to_endpoint(dev);\n\treturn sysfs_emit(buf, \"%llu\\n\", (unsigned long long) ep->id);\n}",
      "modified_lines": {
        "added": [
          "\treturn sysfs_emit(buf, \"%llu\\n\", (unsigned long long) ep->id);"
        ],
        "deleted": [
          "\treturn sprintf(buf, \"%llu\\n\", (unsigned long long) ep->id);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of appropriate length constraints or checks for certain iSCSI data structures, allowing them to exceed the PAGE_SIZE value.",
      "trigger_condition": "An unprivileged user sends a Netlink message associated with iSCSI, with a length up to the maximum length of a Netlink message, triggering a buffer overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code uses the sprintf function to format data into a buffer without ensuring that the data fits within the buffer size, potentially leading to a buffer overflow if the data exceeds the buffer capacity.",
      "id": 103,
      "code_after_change_normalized": "static VAR1\nFUN1(struct device *VAR2, struct device_attribute *VAR3, char *VAR4)\n{\nstruct iscsi_endpoint *VAR5 = FUN2(VAR2);\nreturn FUN3(VAR4, \"STR\", (unsigned long long) VAR5->VAR6);\n}\n",
      "code_before_change_normalized": "static VAR1\nFUN1(struct device *VAR2, struct device_attribute *VAR3, char *VAR4)\n{\nstruct iscsi_endpoint *VAR5 = FUN2(VAR2);\nreturn FUN3(VAR4, \"STR\", (unsigned long long) VAR5->VAR6);\n}\n",
      "code_after_change_raw": "static ssize_t\nshow_ep_handle(struct device *dev, struct device_attribute *attr, char *buf)\n{\nstruct iscsi_endpoint *ep = iscsi_dev_to_endpoint(dev);\nreturn sysfs_emit(buf, \"%llu\\n\", (unsigned long long) ep->id);\n}\n",
      "code_before_change_raw": "static ssize_t\nshow_ep_handle(struct device *dev, struct device_attribute *attr, char *buf)\n{\nstruct iscsi_endpoint *ep = iscsi_dev_to_endpoint(dev);\nreturn sprintf(buf, \"%llu\\n\", (unsigned long long) ep->id);\n}\n"
    }
  ],
  "CVE-2021-28660": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when accessing and writing data into the ssid array.",
        "trigger_condition": "Accessing and writing data into the ssid array without validating the length of the data being copied, leading to a buffer overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the data being copied into the ssid array, allowing potential buffer overflow by writing beyond the end of the array."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add proper bounds checking to ensure that the length of the data being copied into the ssid array is within a valid range. Specifically, validating the length of the data before copying it into the array and ensuring that it does not exceed the array size prevents buffer overflow vulnerabilities. In this case, the solution involves checking and validating the sec_len value before copying data into the ssid array.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2021-28660, which allows writing beyond the end of the `ssid[]` array. The vulnerability occurs when the `ssid` array is accessed and written to without proper bounds checking, potentially leading to a buffer overflow.\n\nIn the modified code snippet, the following changes were made to mitigate the vulnerability:\n1. Added a check to ensure that the `sec_len` value is within a valid range (greater than 0, less than or equal to the remaining length, and less than or equal to 32) before copying data into the `ssid` array.\n2. Updated the `memcpy` call to copy data based on the validated `sec_len` value to prevent writing beyond the bounds of the `ssid` array.\n\nBy making these modifications, the code now performs proper bounds checking and ensures that data is copied safely into the `ssid` array, preventing the buffer overflow vulnerability described in CVE-2021-28660.",
      "GPT_purpose": "Set up a scan for available wireless networks on a network device.",
      "GPT_function": "\n1. Set up a scan for available wireless networks.\n2. Check various conditions before initiating the scan.\n3. Handle different types of scan requests based on input parameters.\n4. Perform scanning based on the provided SSID information.\n5. Handle different sections of scan parameters for custom scan requests.\n6. Return an error code if the scanning process fails.",
      "CVE_id": "CVE-2021-28660",
      "code_before_change": "static int rtw_wx_set_scan(struct net_device *dev, struct iw_request_info *a,\n\t\t\t   union iwreq_data *wrqu, char *extra)\n{\n\tu8 _status = false;\n\tint ret = 0;\n\tstruct adapter *padapter = rtw_netdev_priv(dev);\n\tstruct mlme_priv *pmlmepriv = &padapter->mlmepriv;\n\tstruct ndis_802_11_ssid ssid[RTW_SSID_SCAN_AMOUNT];\n\n\tRT_TRACE(_module_rtl871x_mlme_c_, _drv_info_, (\"%s\\n\", __func__));\n\n\tif (!rtw_pwr_wakeup(padapter)) {\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\tif (padapter->bDriverStopped) {\n\t\tDBG_88E(\"bDriverStopped =%d\\n\", padapter->bDriverStopped);\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\tif (!padapter->bup) {\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\tif (!padapter->hw_init_completed) {\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\t/*  When Busy Traffic, driver do not site survey. So driver return success. */\n\t/*  wpa_supplicant will not issue SIOCSIWSCAN cmd again after scan timeout. */\n\t/*  modify by thomas 2011-02-22. */\n\tif (pmlmepriv->LinkDetectInfo.bBusyTraffic) {\n\t\tindicate_wx_scan_complete_event(padapter);\n\t\tgoto exit;\n\t}\n\n\tif (check_fwstate(pmlmepriv, _FW_UNDER_SURVEY | _FW_UNDER_LINKING)) {\n\t\tindicate_wx_scan_complete_event(padapter);\n\t\tgoto exit;\n\t}\n\n/*\tFor the DMP WiFi Display project, the driver won't to scan because */\n/*\tthe pmlmepriv->scan_interval is always equal to 3. */\n/*\tSo, the wpa_supplicant won't find out the WPS SoftAP. */\n\n\tmemset(ssid, 0, sizeof(struct ndis_802_11_ssid) * RTW_SSID_SCAN_AMOUNT);\n\n\tif (wrqu->data.length == sizeof(struct iw_scan_req)) {\n\t\tstruct iw_scan_req *req = (struct iw_scan_req *)extra;\n\n\t\tif (wrqu->data.flags & IW_SCAN_THIS_ESSID) {\n\t\t\tint len = min_t(int, req->essid_len,\n\t\t\t\t\tIW_ESSID_MAX_SIZE);\n\n\t\t\tmemcpy(ssid[0].ssid, req->essid, len);\n\t\t\tssid[0].ssid_length = len;\n\n\t\t\tDBG_88E(\"IW_SCAN_THIS_ESSID, ssid =%s, len =%d\\n\", req->essid, req->essid_len);\n\n\t\t\tspin_lock_bh(&pmlmepriv->lock);\n\n\t\t\t_status = rtw_sitesurvey_cmd(padapter, ssid, 1, NULL, 0);\n\n\t\t\tspin_unlock_bh(&pmlmepriv->lock);\n\t\t} else if (req->scan_type == IW_SCAN_TYPE_PASSIVE) {\n\t\t\tDBG_88E(\"%s, req->scan_type == IW_SCAN_TYPE_PASSIVE\\n\", __func__);\n\t\t}\n\t} else {\n\t\tif (wrqu->data.length >= WEXT_CSCAN_HEADER_SIZE &&\n\t\t    !memcmp(extra, WEXT_CSCAN_HEADER, WEXT_CSCAN_HEADER_SIZE)) {\n\t\t\tint len = wrqu->data.length - WEXT_CSCAN_HEADER_SIZE;\n\t\t\tchar *pos = extra + WEXT_CSCAN_HEADER_SIZE;\n\t\t\tchar section;\n\t\t\tchar sec_len;\n\t\t\tint ssid_index = 0;\n\n\t\t\twhile (len >= 1) {\n\t\t\t\tsection = *(pos++);\n\t\t\t\tlen -= 1;\n\n\t\t\t\tswitch (section) {\n\t\t\t\tcase WEXT_CSCAN_SSID_SECTION:\n\t\t\t\t\tif (len < 1) {\n\t\t\t\t\t\tlen = 0;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tsec_len = *(pos++); len -= 1;\n\t\t\t\t\tif (sec_len > 0 && sec_len <= len) {\n\t\t\t\t\t\tssid[ssid_index].ssid_length = sec_len;\n\t\t\t\t\t\tmemcpy(ssid[ssid_index].ssid, pos, ssid[ssid_index].ssid_length);\n\t\t\t\t\t\tssid_index++;\n\t\t\t\t\t}\n\t\t\t\t\tpos += sec_len;\n\t\t\t\t\tlen -= sec_len;\n\t\t\t\t\tbreak;\n\t\t\t\tcase WEXT_CSCAN_TYPE_SECTION:\n\t\t\t\tcase WEXT_CSCAN_CHANNEL_SECTION:\n\t\t\t\t\tpos += 1;\n\t\t\t\t\tlen -= 1;\n\t\t\t\t\tbreak;\n\t\t\t\tcase WEXT_CSCAN_PASV_DWELL_SECTION:\n\t\t\t\tcase WEXT_CSCAN_HOME_DWELL_SECTION:\n\t\t\t\tcase WEXT_CSCAN_ACTV_DWELL_SECTION:\n\t\t\t\t\tpos += 2;\n\t\t\t\t\tlen -= 2;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tlen = 0; /*  stop parsing */\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* it has still some scan parameter to parse, we only do this now... */\n\t\t\t_status = rtw_set_802_11_bssid_list_scan(padapter, ssid, RTW_SSID_SCAN_AMOUNT);\n\t\t} else {\n\t\t\t_status = rtw_set_802_11_bssid_list_scan(padapter, NULL, 0);\n\t\t}\n\t}\n\n\tif (!_status)\n\t\tret = -1;\n\nexit:\n\n\treturn ret;\n}",
      "code_after_change": "static int rtw_wx_set_scan(struct net_device *dev, struct iw_request_info *a,\n\t\t\t   union iwreq_data *wrqu, char *extra)\n{\n\tu8 _status = false;\n\tint ret = 0;\n\tstruct adapter *padapter = rtw_netdev_priv(dev);\n\tstruct mlme_priv *pmlmepriv = &padapter->mlmepriv;\n\tstruct ndis_802_11_ssid ssid[RTW_SSID_SCAN_AMOUNT];\n\n\tRT_TRACE(_module_rtl871x_mlme_c_, _drv_info_, (\"%s\\n\", __func__));\n\n\tif (!rtw_pwr_wakeup(padapter)) {\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\tif (padapter->bDriverStopped) {\n\t\tDBG_88E(\"bDriverStopped =%d\\n\", padapter->bDriverStopped);\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\tif (!padapter->bup) {\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\tif (!padapter->hw_init_completed) {\n\t\tret = -1;\n\t\tgoto exit;\n\t}\n\n\t/*  When Busy Traffic, driver do not site survey. So driver return success. */\n\t/*  wpa_supplicant will not issue SIOCSIWSCAN cmd again after scan timeout. */\n\t/*  modify by thomas 2011-02-22. */\n\tif (pmlmepriv->LinkDetectInfo.bBusyTraffic) {\n\t\tindicate_wx_scan_complete_event(padapter);\n\t\tgoto exit;\n\t}\n\n\tif (check_fwstate(pmlmepriv, _FW_UNDER_SURVEY | _FW_UNDER_LINKING)) {\n\t\tindicate_wx_scan_complete_event(padapter);\n\t\tgoto exit;\n\t}\n\n/*\tFor the DMP WiFi Display project, the driver won't to scan because */\n/*\tthe pmlmepriv->scan_interval is always equal to 3. */\n/*\tSo, the wpa_supplicant won't find out the WPS SoftAP. */\n\n\tmemset(ssid, 0, sizeof(struct ndis_802_11_ssid) * RTW_SSID_SCAN_AMOUNT);\n\n\tif (wrqu->data.length == sizeof(struct iw_scan_req)) {\n\t\tstruct iw_scan_req *req = (struct iw_scan_req *)extra;\n\n\t\tif (wrqu->data.flags & IW_SCAN_THIS_ESSID) {\n\t\t\tint len = min_t(int, req->essid_len,\n\t\t\t\t\tIW_ESSID_MAX_SIZE);\n\n\t\t\tmemcpy(ssid[0].ssid, req->essid, len);\n\t\t\tssid[0].ssid_length = len;\n\n\t\t\tDBG_88E(\"IW_SCAN_THIS_ESSID, ssid =%s, len =%d\\n\", req->essid, req->essid_len);\n\n\t\t\tspin_lock_bh(&pmlmepriv->lock);\n\n\t\t\t_status = rtw_sitesurvey_cmd(padapter, ssid, 1, NULL, 0);\n\n\t\t\tspin_unlock_bh(&pmlmepriv->lock);\n\t\t} else if (req->scan_type == IW_SCAN_TYPE_PASSIVE) {\n\t\t\tDBG_88E(\"%s, req->scan_type == IW_SCAN_TYPE_PASSIVE\\n\", __func__);\n\t\t}\n\t} else {\n\t\tif (wrqu->data.length >= WEXT_CSCAN_HEADER_SIZE &&\n\t\t    !memcmp(extra, WEXT_CSCAN_HEADER, WEXT_CSCAN_HEADER_SIZE)) {\n\t\t\tint len = wrqu->data.length - WEXT_CSCAN_HEADER_SIZE;\n\t\t\tchar *pos = extra + WEXT_CSCAN_HEADER_SIZE;\n\t\t\tchar section;\n\t\t\tchar sec_len;\n\t\t\tint ssid_index = 0;\n\n\t\t\twhile (len >= 1) {\n\t\t\t\tsection = *(pos++);\n\t\t\t\tlen -= 1;\n\n\t\t\t\tswitch (section) {\n\t\t\t\tcase WEXT_CSCAN_SSID_SECTION:\n\t\t\t\t\tif (len < 1) {\n\t\t\t\t\t\tlen = 0;\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tsec_len = *(pos++); len -= 1;\n\t\t\t\t\tif (sec_len > 0 &&\n\t\t\t\t\t    sec_len <= len &&\n\t\t\t\t\t    sec_len <= 32) {\n\t\t\t\t\t\tssid[ssid_index].ssid_length = sec_len;\n\t\t\t\t\t\tmemcpy(ssid[ssid_index].ssid, pos, sec_len);\n\t\t\t\t\t\tssid_index++;\n\t\t\t\t\t}\n\t\t\t\t\tpos += sec_len;\n\t\t\t\t\tlen -= sec_len;\n\t\t\t\t\tbreak;\n\t\t\t\tcase WEXT_CSCAN_TYPE_SECTION:\n\t\t\t\tcase WEXT_CSCAN_CHANNEL_SECTION:\n\t\t\t\t\tpos += 1;\n\t\t\t\t\tlen -= 1;\n\t\t\t\t\tbreak;\n\t\t\t\tcase WEXT_CSCAN_PASV_DWELL_SECTION:\n\t\t\t\tcase WEXT_CSCAN_HOME_DWELL_SECTION:\n\t\t\t\tcase WEXT_CSCAN_ACTV_DWELL_SECTION:\n\t\t\t\t\tpos += 2;\n\t\t\t\t\tlen -= 2;\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tlen = 0; /*  stop parsing */\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t/* it has still some scan parameter to parse, we only do this now... */\n\t\t\t_status = rtw_set_802_11_bssid_list_scan(padapter, ssid, RTW_SSID_SCAN_AMOUNT);\n\t\t} else {\n\t\t\t_status = rtw_set_802_11_bssid_list_scan(padapter, NULL, 0);\n\t\t}\n\t}\n\n\tif (!_status)\n\t\tret = -1;\n\nexit:\n\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t\t\tif (sec_len > 0 &&",
          "\t\t\t\t\t    sec_len <= len &&",
          "\t\t\t\t\t    sec_len <= 32) {",
          "\t\t\t\t\t\tmemcpy(ssid[ssid_index].ssid, pos, sec_len);"
        ],
        "deleted": [
          "\t\t\t\t\tif (sec_len > 0 && sec_len <= len) {",
          "\t\t\t\t\t\tmemcpy(ssid[ssid_index].ssid, pos, ssid[ssid_index].ssid_length);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when accessing and writing data into the ssid array.",
      "trigger_condition": "Accessing and writing data into the ssid array without validating the length of the data being copied, leading to a buffer overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the data being copied into the ssid array, allowing potential buffer overflow by writing beyond the end of the array.",
      "id": 104,
      "code_after_change_normalized": "static int FUN1(struct net_device *VAR1, struct iw_request_info *VAR2,\nunion iwreq_data *VAR3, char *VAR4)\n{\nu8 VAR5 = false;\nint VAR6 = 0;\nstruct adapter *VAR7 = FUN2(VAR1);\nstruct mlme_priv *VAR8 = &VAR7->VAR9;\nstruct ndis_802_11_ssid VAR10[VAR11];\nFUN3(VAR12, VAR13, (\"STR\", VAR14));\nif (!FUN4(VAR7)) {\nVAR6 = -1;\ngoto VAR15;\n}\nif (VAR7->VAR16) {\nFUN5(\"STR\", VAR7->VAR16);\nVAR6 = -1;\ngoto VAR15;\n}\nif (!VAR7->VAR17) {\nVAR6 = -1;\ngoto VAR15;\n}\nif (!VAR7->VAR18) {\nVAR6 = -1;\ngoto VAR15;\n}\nif (VAR8->VAR19.VAR20) {\nFUN6(VAR7);\ngoto VAR15;\n}\nif (FUN7(VAR8, VAR21 | VAR22)) {\nFUN6(VAR7);\ngoto VAR15;\n}\nFUN8(VAR10, 0, sizeof(struct VAR23) * VAR11);\nif (VAR3->VAR24.VAR25 == sizeof(struct VAR26)) {\nstruct VAR26 *VAR27 = (struct VAR26 *)VAR4;\nif (VAR3->VAR24.VAR28 & VAR29) {\nint VAR30 = FUN9(int, VAR27->VAR31,\nVAR32);\nFUN10(VAR10[0].VAR10, VAR27->VAR33, VAR30);\nVAR10[0].VAR34 = VAR30;\nFUN5(\"STR\", VAR27->VAR33, VAR27->VAR31);\nFUN11(&VAR8->VAR35);\nVAR5 = FUN12(VAR7, VAR10, 1, NULL, 0);\nFUN13(&VAR8->VAR35);\n} else if (VAR27->VAR36 == VAR37) {\nFUN5(\"STR\", VAR14);\n}\n} else {\nif (VAR3->VAR24.VAR25 >= VAR38 &&\n!FUN14(VAR4, VAR39, VAR38)) {\nint VAR30 = VAR3->VAR24.VAR25 - VAR38;\nchar *VAR40 = VAR4 + VAR38;\nchar VAR41;\nchar VAR42;\nint VAR43 = 0;\nwhile (VAR30 >= 1) {\nVAR41 = *(VAR40++);\nVAR30 -= 1;\nswitch (VAR41) {\ncase VAR44:\nif (VAR30 < 1) {\nVAR30 = 0;\nbreak;\n}\nVAR42 = *(VAR40++); VAR30 -= 1;\nif (VAR42 > 0 &&\nVAR42 <= VAR30 &&\nVAR42 <= 32) {\nVAR10[VAR43].VAR34 = VAR42;\nFUN10(VAR10[VAR43].VAR10, VAR40, VAR42);\nVAR43++;\n}\nVAR40 += VAR42;\nVAR30 -= VAR42;\nbreak;\ncase VAR45:\ncase VAR46:\nVAR40 += 1;\nVAR30 -= 1;\nbreak;\ncase VAR47:\ncase VAR48:\ncase VAR49:\nVAR40 += 2;\nVAR30 -= 2;\nbreak;\ndefault:\nVAR30 = 0; \n}\n}\nVAR5 = FUN15(VAR7, VAR10, VAR11);\n} else {\nVAR5 = FUN15(VAR7, NULL, 0);\n}\n}\nif (!VAR5)\nVAR6 = -1;\nVAR15:\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct net_device *VAR1, struct iw_request_info *VAR2,\nunion iwreq_data *VAR3, char *VAR4)\n{\nu8 VAR5 = false;\nint VAR6 = 0;\nstruct adapter *VAR7 = FUN2(VAR1);\nstruct mlme_priv *VAR8 = &VAR7->VAR9;\nstruct ndis_802_11_ssid VAR10[VAR11];\nFUN3(VAR12, VAR13, (\"STR\", VAR14));\nif (!FUN4(VAR7)) {\nVAR6 = -1;\ngoto VAR15;\n}\nif (VAR7->VAR16) {\nFUN5(\"STR\", VAR7->VAR16);\nVAR6 = -1;\ngoto VAR15;\n}\nif (!VAR7->VAR17) {\nVAR6 = -1;\ngoto VAR15;\n}\nif (!VAR7->VAR18) {\nVAR6 = -1;\ngoto VAR15;\n}\nif (VAR8->VAR19.VAR20) {\nFUN6(VAR7);\ngoto VAR15;\n}\nif (FUN7(VAR8, VAR21 | VAR22)) {\nFUN6(VAR7);\ngoto VAR15;\n}\nFUN8(VAR10, 0, sizeof(struct VAR23) * VAR11);\nif (VAR3->VAR24.VAR25 == sizeof(struct VAR26)) {\nstruct VAR26 *VAR27 = (struct VAR26 *)VAR4;\nif (VAR3->VAR24.VAR28 & VAR29) {\nint VAR30 = FUN9(int, VAR27->VAR31,\nVAR32);\nFUN10(VAR10[0].VAR10, VAR27->VAR33, VAR30);\nVAR10[0].VAR34 = VAR30;\nFUN5(\"STR\", VAR27->VAR33, VAR27->VAR31);\nFUN11(&VAR8->VAR35);\nVAR5 = FUN12(VAR7, VAR10, 1, NULL, 0);\nFUN13(&VAR8->VAR35);\n} else if (VAR27->VAR36 == VAR37) {\nFUN5(\"STR\", VAR14);\n}\n} else {\nif (VAR3->VAR24.VAR25 >= VAR38 &&\n!FUN14(VAR4, VAR39, VAR38)) {\nint VAR30 = VAR3->VAR24.VAR25 - VAR38;\nchar *VAR40 = VAR4 + VAR38;\nchar VAR41;\nchar VAR42;\nint VAR43 = 0;\nwhile (VAR30 >= 1) {\nVAR41 = *(VAR40++);\nVAR30 -= 1;\nswitch (VAR41) {\ncase VAR44:\nif (VAR30 < 1) {\nVAR30 = 0;\nbreak;\n}\nVAR42 = *(VAR40++); VAR30 -= 1;\nif (VAR42 > 0 && VAR42 <= VAR30) {\nVAR10[VAR43].VAR34 = VAR42;\nFUN10(VAR10[VAR43].VAR10, VAR40, VAR10[VAR43].VAR34);\nVAR43++;\n}\nVAR40 += VAR42;\nVAR30 -= VAR42;\nbreak;\ncase VAR45:\ncase VAR46:\nVAR40 += 1;\nVAR30 -= 1;\nbreak;\ncase VAR47:\ncase VAR48:\ncase VAR49:\nVAR40 += 2;\nVAR30 -= 2;\nbreak;\ndefault:\nVAR30 = 0; \n}\n}\nVAR5 = FUN15(VAR7, VAR10, VAR11);\n} else {\nVAR5 = FUN15(VAR7, NULL, 0);\n}\n}\nif (!VAR5)\nVAR6 = -1;\nVAR15:\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static int rtw_wx_set_scan(struct net_device *dev, struct iw_request_info *a,\nunion iwreq_data *wrqu, char *extra)\n{\nu8 _status = false;\nint ret = 0;\nstruct adapter *padapter = rtw_netdev_priv(dev);\nstruct mlme_priv *pmlmepriv = &padapter->mlmepriv;\nstruct ndis_802_11_ssid ssid[RTW_SSID_SCAN_AMOUNT];\nRT_TRACE(_module_rtl871x_mlme_c_, _drv_info_, (\"%s\\n\", __func__));\nif (!rtw_pwr_wakeup(padapter)) {\nret = -1;\ngoto exit;\n}\nif (padapter->bDriverStopped) {\nDBG_88E(\"bDriverStopped =%d\\n\", padapter->bDriverStopped);\nret = -1;\ngoto exit;\n}\nif (!padapter->bup) {\nret = -1;\ngoto exit;\n}\nif (!padapter->hw_init_completed) {\nret = -1;\ngoto exit;\n}\nif (pmlmepriv->LinkDetectInfo.bBusyTraffic) {\nindicate_wx_scan_complete_event(padapter);\ngoto exit;\n}\nif (check_fwstate(pmlmepriv, _FW_UNDER_SURVEY | _FW_UNDER_LINKING)) {\nindicate_wx_scan_complete_event(padapter);\ngoto exit;\n}\nmemset(ssid, 0, sizeof(struct ndis_802_11_ssid) * RTW_SSID_SCAN_AMOUNT);\nif (wrqu->data.length == sizeof(struct iw_scan_req)) {\nstruct iw_scan_req *req = (struct iw_scan_req *)extra;\nif (wrqu->data.flags & IW_SCAN_THIS_ESSID) {\nint len = min_t(int, req->essid_len,\nIW_ESSID_MAX_SIZE);\nmemcpy(ssid[0].ssid, req->essid, len);\nssid[0].ssid_length = len;\nDBG_88E(\"IW_SCAN_THIS_ESSID, ssid =%s, len =%d\\n\", req->essid, req->essid_len);\nspin_lock_bh(&pmlmepriv->lock);\n_status = rtw_sitesurvey_cmd(padapter, ssid, 1, NULL, 0);\nspin_unlock_bh(&pmlmepriv->lock);\n} else if (req->scan_type == IW_SCAN_TYPE_PASSIVE) {\nDBG_88E(\"%s, req->scan_type == IW_SCAN_TYPE_PASSIVE\\n\", __func__);\n}\n} else {\nif (wrqu->data.length >= WEXT_CSCAN_HEADER_SIZE &&\n!memcmp(extra, WEXT_CSCAN_HEADER, WEXT_CSCAN_HEADER_SIZE)) {\nint len = wrqu->data.length - WEXT_CSCAN_HEADER_SIZE;\nchar *pos = extra + WEXT_CSCAN_HEADER_SIZE;\nchar section;\nchar sec_len;\nint ssid_index = 0;\nwhile (len >= 1) {\nsection = *(pos++);\nlen -= 1;\nswitch (section) {\ncase WEXT_CSCAN_SSID_SECTION:\nif (len < 1) {\nlen = 0;\nbreak;\n}\nsec_len = *(pos++); len -= 1;\nif (sec_len > 0 &&\nsec_len <= len &&\nsec_len <= 32) {\nssid[ssid_index].ssid_length = sec_len;\nmemcpy(ssid[ssid_index].ssid, pos, sec_len);\nssid_index++;\n}\npos += sec_len;\nlen -= sec_len;\nbreak;\ncase WEXT_CSCAN_TYPE_SECTION:\ncase WEXT_CSCAN_CHANNEL_SECTION:\npos += 1;\nlen -= 1;\nbreak;\ncase WEXT_CSCAN_PASV_DWELL_SECTION:\ncase WEXT_CSCAN_HOME_DWELL_SECTION:\ncase WEXT_CSCAN_ACTV_DWELL_SECTION:\npos += 2;\nlen -= 2;\nbreak;\ndefault:\nlen = 0; \n}\n}\n_status = rtw_set_802_11_bssid_list_scan(padapter, ssid, RTW_SSID_SCAN_AMOUNT);\n} else {\n_status = rtw_set_802_11_bssid_list_scan(padapter, NULL, 0);\n}\n}\nif (!_status)\nret = -1;\nexit:\nreturn ret;\n}\n",
      "code_before_change_raw": "static int rtw_wx_set_scan(struct net_device *dev, struct iw_request_info *a,\nunion iwreq_data *wrqu, char *extra)\n{\nu8 _status = false;\nint ret = 0;\nstruct adapter *padapter = rtw_netdev_priv(dev);\nstruct mlme_priv *pmlmepriv = &padapter->mlmepriv;\nstruct ndis_802_11_ssid ssid[RTW_SSID_SCAN_AMOUNT];\nRT_TRACE(_module_rtl871x_mlme_c_, _drv_info_, (\"%s\\n\", __func__));\nif (!rtw_pwr_wakeup(padapter)) {\nret = -1;\ngoto exit;\n}\nif (padapter->bDriverStopped) {\nDBG_88E(\"bDriverStopped =%d\\n\", padapter->bDriverStopped);\nret = -1;\ngoto exit;\n}\nif (!padapter->bup) {\nret = -1;\ngoto exit;\n}\nif (!padapter->hw_init_completed) {\nret = -1;\ngoto exit;\n}\nif (pmlmepriv->LinkDetectInfo.bBusyTraffic) {\nindicate_wx_scan_complete_event(padapter);\ngoto exit;\n}\nif (check_fwstate(pmlmepriv, _FW_UNDER_SURVEY | _FW_UNDER_LINKING)) {\nindicate_wx_scan_complete_event(padapter);\ngoto exit;\n}\nmemset(ssid, 0, sizeof(struct ndis_802_11_ssid) * RTW_SSID_SCAN_AMOUNT);\nif (wrqu->data.length == sizeof(struct iw_scan_req)) {\nstruct iw_scan_req *req = (struct iw_scan_req *)extra;\nif (wrqu->data.flags & IW_SCAN_THIS_ESSID) {\nint len = min_t(int, req->essid_len,\nIW_ESSID_MAX_SIZE);\nmemcpy(ssid[0].ssid, req->essid, len);\nssid[0].ssid_length = len;\nDBG_88E(\"IW_SCAN_THIS_ESSID, ssid =%s, len =%d\\n\", req->essid, req->essid_len);\nspin_lock_bh(&pmlmepriv->lock);\n_status = rtw_sitesurvey_cmd(padapter, ssid, 1, NULL, 0);\nspin_unlock_bh(&pmlmepriv->lock);\n} else if (req->scan_type == IW_SCAN_TYPE_PASSIVE) {\nDBG_88E(\"%s, req->scan_type == IW_SCAN_TYPE_PASSIVE\\n\", __func__);\n}\n} else {\nif (wrqu->data.length >= WEXT_CSCAN_HEADER_SIZE &&\n!memcmp(extra, WEXT_CSCAN_HEADER, WEXT_CSCAN_HEADER_SIZE)) {\nint len = wrqu->data.length - WEXT_CSCAN_HEADER_SIZE;\nchar *pos = extra + WEXT_CSCAN_HEADER_SIZE;\nchar section;\nchar sec_len;\nint ssid_index = 0;\nwhile (len >= 1) {\nsection = *(pos++);\nlen -= 1;\nswitch (section) {\ncase WEXT_CSCAN_SSID_SECTION:\nif (len < 1) {\nlen = 0;\nbreak;\n}\nsec_len = *(pos++); len -= 1;\nif (sec_len > 0 && sec_len <= len) {\nssid[ssid_index].ssid_length = sec_len;\nmemcpy(ssid[ssid_index].ssid, pos, ssid[ssid_index].ssid_length);\nssid_index++;\n}\npos += sec_len;\nlen -= sec_len;\nbreak;\ncase WEXT_CSCAN_TYPE_SECTION:\ncase WEXT_CSCAN_CHANNEL_SECTION:\npos += 1;\nlen -= 1;\nbreak;\ncase WEXT_CSCAN_PASV_DWELL_SECTION:\ncase WEXT_CSCAN_HOME_DWELL_SECTION:\ncase WEXT_CSCAN_ACTV_DWELL_SECTION:\npos += 2;\nlen -= 2;\nbreak;\ndefault:\nlen = 0; \n}\n}\n_status = rtw_set_802_11_bssid_list_scan(padapter, ssid, RTW_SSID_SCAN_AMOUNT);\n} else {\n_status = rtw_set_802_11_bssid_list_scan(padapter, NULL, 0);\n}\n}\nif (!_status)\nret = -1;\nexit:\nreturn ret;\n}\n"
    }
  ],
  "CVE-2021-31916": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper buffer size validation before writing data to the buffer.",
        "trigger_condition": "An attacker with special user privilege triggers the vulnerable code path, leading to an out-of-bounds memory write.",
        "specific_code_behavior_causing_vulnerability": "The code writes data to the buffer without ensuring that the buffer size is sufficient, allowing an attacker to access out-of-bounds memory and potentially crash the system or leak internal kernel information."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to validate the buffer size before writing data to it. By ensuring that the buffer size is large enough to accommodate the data being written, the code can prevent out-of-bounds memory writes and enhance system security. In this case, the solution involves adding a check to verify that the buffer size is at least as large as the size of the field being written to, thereby preventing potential buffer overflow vulnerabilities.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-31916, which involves an out-of-bounds (OOB) memory write flaw in the `list_devices` function. By adding the check `len < sizeof(nl->dev)` in the code snippet, we are ensuring that the length of the buffer `nl` is sufficient to hold the `dev` field before writing to it. This check helps prevent an attacker with special user privilege from gaining access to out-of-bounds memory, which could lead to a system crash or a leak of internal kernel information.\n\nBy ensuring that the buffer is large enough to hold the data being written to it, we can mitigate the risk of buffer overflow vulnerabilities and enhance the security of the system. This modification helps to prevent potential exploitation of the vulnerability and improves the overall robustness of the code.",
      "GPT_purpose": "List all devices and their associated information for the Multi-device driver module in the Linux kernel.",
      "GPT_function": "\n1. List all devices and calculate the required buffer space.\n2. Fill out the names and device information in the output buffer.\n3. Check for buffer overflow and handle the situation accordingly.",
      "CVE_id": "CVE-2021-31916",
      "code_before_change": "static int list_devices(struct file *filp, struct dm_ioctl *param, size_t param_size)\n{\n\tunsigned int i;\n\tstruct hash_cell *hc;\n\tsize_t len, needed = 0;\n\tstruct gendisk *disk;\n\tstruct dm_name_list *orig_nl, *nl, *old_nl = NULL;\n\tuint32_t *event_nr;\n\n\tdown_write(&_hash_lock);\n\n\t/*\n\t * Loop through all the devices working out how much\n\t * space we need.\n\t */\n\tfor (i = 0; i < NUM_BUCKETS; i++) {\n\t\tlist_for_each_entry (hc, _name_buckets + i, name_list) {\n\t\t\tneeded += align_val(offsetof(struct dm_name_list, name) + strlen(hc->name) + 1);\n\t\t\tneeded += align_val(sizeof(uint32_t));\n\t\t}\n\t}\n\n\t/*\n\t * Grab our output buffer.\n\t */\n\tnl = orig_nl = get_result_buffer(param, param_size, &len);\n\tif (len < needed) {\n\t\tparam->flags |= DM_BUFFER_FULL_FLAG;\n\t\tgoto out;\n\t}\n\tparam->data_size = param->data_start + needed;\n\n\tnl->dev = 0;\t/* Flags no data */\n\n\t/*\n\t * Now loop through filling out the names.\n\t */\n\tfor (i = 0; i < NUM_BUCKETS; i++) {\n\t\tlist_for_each_entry (hc, _name_buckets + i, name_list) {\n\t\t\tif (old_nl)\n\t\t\t\told_nl->next = (uint32_t) ((void *) nl -\n\t\t\t\t\t\t\t   (void *) old_nl);\n\t\t\tdisk = dm_disk(hc->md);\n\t\t\tnl->dev = huge_encode_dev(disk_devt(disk));\n\t\t\tnl->next = 0;\n\t\t\tstrcpy(nl->name, hc->name);\n\n\t\t\told_nl = nl;\n\t\t\tevent_nr = align_ptr(nl->name + strlen(hc->name) + 1);\n\t\t\t*event_nr = dm_get_event_nr(hc->md);\n\t\t\tnl = align_ptr(event_nr + 1);\n\t\t}\n\t}\n\t/*\n\t * If mismatch happens, security may be compromised due to buffer\n\t * overflow, so it's better to crash.\n\t */\n\tBUG_ON((char *)nl - (char *)orig_nl != needed);\n\n out:\n\tup_write(&_hash_lock);\n\treturn 0;\n}",
      "code_after_change": "static int list_devices(struct file *filp, struct dm_ioctl *param, size_t param_size)\n{\n\tunsigned int i;\n\tstruct hash_cell *hc;\n\tsize_t len, needed = 0;\n\tstruct gendisk *disk;\n\tstruct dm_name_list *orig_nl, *nl, *old_nl = NULL;\n\tuint32_t *event_nr;\n\n\tdown_write(&_hash_lock);\n\n\t/*\n\t * Loop through all the devices working out how much\n\t * space we need.\n\t */\n\tfor (i = 0; i < NUM_BUCKETS; i++) {\n\t\tlist_for_each_entry (hc, _name_buckets + i, name_list) {\n\t\t\tneeded += align_val(offsetof(struct dm_name_list, name) + strlen(hc->name) + 1);\n\t\t\tneeded += align_val(sizeof(uint32_t));\n\t\t}\n\t}\n\n\t/*\n\t * Grab our output buffer.\n\t */\n\tnl = orig_nl = get_result_buffer(param, param_size, &len);\n\tif (len < needed || len < sizeof(nl->dev)) {\n\t\tparam->flags |= DM_BUFFER_FULL_FLAG;\n\t\tgoto out;\n\t}\n\tparam->data_size = param->data_start + needed;\n\n\tnl->dev = 0;\t/* Flags no data */\n\n\t/*\n\t * Now loop through filling out the names.\n\t */\n\tfor (i = 0; i < NUM_BUCKETS; i++) {\n\t\tlist_for_each_entry (hc, _name_buckets + i, name_list) {\n\t\t\tif (old_nl)\n\t\t\t\told_nl->next = (uint32_t) ((void *) nl -\n\t\t\t\t\t\t\t   (void *) old_nl);\n\t\t\tdisk = dm_disk(hc->md);\n\t\t\tnl->dev = huge_encode_dev(disk_devt(disk));\n\t\t\tnl->next = 0;\n\t\t\tstrcpy(nl->name, hc->name);\n\n\t\t\told_nl = nl;\n\t\t\tevent_nr = align_ptr(nl->name + strlen(hc->name) + 1);\n\t\t\t*event_nr = dm_get_event_nr(hc->md);\n\t\t\tnl = align_ptr(event_nr + 1);\n\t\t}\n\t}\n\t/*\n\t * If mismatch happens, security may be compromised due to buffer\n\t * overflow, so it's better to crash.\n\t */\n\tBUG_ON((char *)nl - (char *)orig_nl != needed);\n\n out:\n\tup_write(&_hash_lock);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tif (len < needed || len < sizeof(nl->dev)) {"
        ],
        "deleted": [
          "\tif (len < needed) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper buffer size validation before writing data to the buffer.",
      "trigger_condition": "An attacker with special user privilege triggers the vulnerable code path, leading to an out-of-bounds memory write.",
      "specific_code_behavior_causing_vulnerability": "The code writes data to the buffer without ensuring that the buffer size is sufficient, allowing an attacker to access out-of-bounds memory and potentially crash the system or leak internal kernel information.",
      "id": 105,
      "code_after_change_normalized": "static int FUN1(struct file *VAR1, struct dm_ioctl *VAR2, size_t VAR3)\n{\nunsigned int VAR4;\nstruct hash_cell *VAR5;\nsize_t VAR6, VAR7 = 0;\nstruct gendisk *VAR8;\nstruct dm_name_list *VAR9, *VAR10, *VAR11 = NULL;\nuint32_t *VAR12;\nFUN2(&VAR13);\nfor (VAR4 = 0; VAR4 < VAR14; VAR4++) {\nFUN3 (VAR5, VAR15 + VAR4, VAR16) {\nVAR7 += FUN4(FUN5(struct VAR17, VAR18) + FUN6(VAR5->VAR18) + 1);\nVAR7 += FUN4(sizeof(VAR19));\n}\n}\nVAR10 = VAR9 = FUN7(VAR2, VAR3, &VAR6);\nif (VAR6 < VAR7 || VAR6 < sizeof(VAR10->VAR20)) {\nVAR2->VAR21 |= VAR22;\ngoto VAR23;\n}\nVAR2->VAR24 = VAR2->VAR25 + VAR7;\nVAR10->VAR20 = 0;\t\nfor (VAR4 = 0; VAR4 < VAR14; VAR4++) {\nFUN3 (VAR5, VAR15 + VAR4, VAR16) {\nif (VAR11)\nVAR11->VAR26 = (VAR19) ((void *) VAR10 -\n(void *) VAR11);\nVAR8 = FUN8(VAR5->VAR27);\nVAR10->VAR20 = FUN9(FUN10(VAR8));\nVAR10->VAR26 = 0;\nFUN11(VAR10->VAR18, VAR5->VAR18);\nVAR11 = VAR10;\nVAR12 = FUN12(VAR10->VAR18 + FUN6(VAR5->VAR18) + 1);\n*VAR12 = FUN13(VAR5->VAR27);\nVAR10 = FUN12(VAR12 + 1);\n}\n}\nFUN14((char *)VAR10 - (char *)VAR9 != VAR7);\nVAR23:\nFUN15(&VAR13);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct file *VAR1, struct dm_ioctl *VAR2, size_t VAR3)\n{\nunsigned int VAR4;\nstruct hash_cell *VAR5;\nsize_t VAR6, VAR7 = 0;\nstruct gendisk *VAR8;\nstruct dm_name_list *VAR9, *VAR10, *VAR11 = NULL;\nuint32_t *VAR12;\nFUN2(&VAR13);\nfor (VAR4 = 0; VAR4 < VAR14; VAR4++) {\nFUN3 (VAR5, VAR15 + VAR4, VAR16) {\nVAR7 += FUN4(FUN5(struct VAR17, VAR18) + FUN6(VAR5->VAR18) + 1);\nVAR7 += FUN4(sizeof(VAR19));\n}\n}\nVAR10 = VAR9 = FUN7(VAR2, VAR3, &VAR6);\nif (VAR6 < VAR7) {\nVAR2->VAR20 |= VAR21;\ngoto VAR22;\n}\nVAR2->VAR23 = VAR2->VAR24 + VAR7;\nVAR10->VAR25 = 0;\t\nfor (VAR4 = 0; VAR4 < VAR14; VAR4++) {\nFUN3 (VAR5, VAR15 + VAR4, VAR16) {\nif (VAR11)\nVAR11->VAR26 = (VAR19) ((void *) VAR10 -\n(void *) VAR11);\nVAR8 = FUN8(VAR5->VAR27);\nVAR10->VAR25 = FUN9(FUN10(VAR8));\nVAR10->VAR26 = 0;\nFUN11(VAR10->VAR18, VAR5->VAR18);\nVAR11 = VAR10;\nVAR12 = FUN12(VAR10->VAR18 + FUN6(VAR5->VAR18) + 1);\n*VAR12 = FUN13(VAR5->VAR27);\nVAR10 = FUN12(VAR12 + 1);\n}\n}\nFUN14((char *)VAR10 - (char *)VAR9 != VAR7);\nVAR22:\nFUN15(&VAR13);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int list_devices(struct file *filp, struct dm_ioctl *param, size_t param_size)\n{\nunsigned int i;\nstruct hash_cell *hc;\nsize_t len, needed = 0;\nstruct gendisk *disk;\nstruct dm_name_list *orig_nl, *nl, *old_nl = NULL;\nuint32_t *event_nr;\ndown_write(&_hash_lock);\nfor (i = 0; i < NUM_BUCKETS; i++) {\nlist_for_each_entry (hc, _name_buckets + i, name_list) {\nneeded += align_val(offsetof(struct dm_name_list, name) + strlen(hc->name) + 1);\nneeded += align_val(sizeof(uint32_t));\n}\n}\nnl = orig_nl = get_result_buffer(param, param_size, &len);\nif (len < needed || len < sizeof(nl->dev)) {\nparam->flags |= DM_BUFFER_FULL_FLAG;\ngoto out;\n}\nparam->data_size = param->data_start + needed;\nnl->dev = 0;\t\nfor (i = 0; i < NUM_BUCKETS; i++) {\nlist_for_each_entry (hc, _name_buckets + i, name_list) {\nif (old_nl)\nold_nl->next = (uint32_t) ((void *) nl -\n(void *) old_nl);\ndisk = dm_disk(hc->md);\nnl->dev = huge_encode_dev(disk_devt(disk));\nnl->next = 0;\nstrcpy(nl->name, hc->name);\nold_nl = nl;\nevent_nr = align_ptr(nl->name + strlen(hc->name) + 1);\n*event_nr = dm_get_event_nr(hc->md);\nnl = align_ptr(event_nr + 1);\n}\n}\nBUG_ON((char *)nl - (char *)orig_nl != needed);\nout:\nup_write(&_hash_lock);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int list_devices(struct file *filp, struct dm_ioctl *param, size_t param_size)\n{\nunsigned int i;\nstruct hash_cell *hc;\nsize_t len, needed = 0;\nstruct gendisk *disk;\nstruct dm_name_list *orig_nl, *nl, *old_nl = NULL;\nuint32_t *event_nr;\ndown_write(&_hash_lock);\nfor (i = 0; i < NUM_BUCKETS; i++) {\nlist_for_each_entry (hc, _name_buckets + i, name_list) {\nneeded += align_val(offsetof(struct dm_name_list, name) + strlen(hc->name) + 1);\nneeded += align_val(sizeof(uint32_t));\n}\n}\nnl = orig_nl = get_result_buffer(param, param_size, &len);\nif (len < needed) {\nparam->flags |= DM_BUFFER_FULL_FLAG;\ngoto out;\n}\nparam->data_size = param->data_start + needed;\nnl->dev = 0;\t\nfor (i = 0; i < NUM_BUCKETS; i++) {\nlist_for_each_entry (hc, _name_buckets + i, name_list) {\nif (old_nl)\nold_nl->next = (uint32_t) ((void *) nl -\n(void *) old_nl);\ndisk = dm_disk(hc->md);\nnl->dev = huge_encode_dev(disk_devt(disk));\nnl->next = 0;\nstrcpy(nl->name, hc->name);\nold_nl = nl;\nevent_nr = align_ptr(nl->name + strlen(hc->name) + 1);\n*event_nr = dm_get_event_nr(hc->md);\nnl = align_ptr(event_nr + 1);\n}\n}\nBUG_ON((char *)nl - (char *)orig_nl != needed);\nout:\nup_write(&_hash_lock);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2021-33655": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for font dimensions against screen resolution, allowing font dimensions to exceed screen boundaries.",
        "trigger_condition": "Malicious data sent to the kernel via ioctl command FBIOPUT_VSCREENINFO triggers a memory out-of-bounds write due to unchecked font dimensions.",
        "specific_code_behavior_causing_vulnerability": "The code fails to validate font dimensions against the screen resolution, potentially leading to memory corruption when writing data outside the allocated memory space."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that font dimensions do not exceed the screen resolution. This prevents potential memory out-of-bounds writes and helps safeguard against memory corruption issues. In this case, the solution involves adding a check for font dimensions against the screen resolution to prevent font sizes from exceeding the screen boundaries.",
      "GPT_analysis": "The modification is necessary to address a vulnerability in the code that could lead to a memory out-of-bounds write when malicious data is sent to the kernel via the ioctl command FBIOPUT_VSCREENINFO. By adding the check for the font size against the screen resolution, the code ensures that the font dimensions do not exceed the screen dimensions, preventing potential memory corruption issues.\n\nThe added check `if (w > FBCON_SWAP(info->var.rotate, info->var.xres, info->var.yres) || h > FBCON_SWAP(info->var.rotate, info->var.yres, info->var.xres))` ensures that the font width and height are within the bounds of the screen resolution based on the rotation of the screen. This helps prevent writing data outside the allocated memory space and mitigates the vulnerability described in CVE-2021-33655.",
      "GPT_purpose": "Set the font for the framebuffer console, ensuring the font data is valid and compatible with the driver.",
      "GPT_function": "\n1. Set the font for the framebuffer console.\n2. Check if the charcount is either 256 or 512.\n3. Verify if the drawing engine can handle the font.\n4. Check if the driver can handle the font length.\n5. Allocate memory for the new font data.\n6. Copy font data and calculate checksum.\n7. Check if the same font is already in use on another console.\n8. Set the new font data for the framebuffer console.",
      "CVE_id": "CVE-2021-33655",
      "code_before_change": "static int fbcon_set_font(struct vc_data *vc, struct console_font *font,\n\t\t\t  unsigned int flags)\n{\n\tstruct fb_info *info = fbcon_info_from_console(vc->vc_num);\n\tunsigned charcount = font->charcount;\n\tint w = font->width;\n\tint h = font->height;\n\tint size;\n\tint i, csum;\n\tu8 *new_data, *data = font->data;\n\tint pitch = PITCH(font->width);\n\n\t/* Is there a reason why fbconsole couldn't handle any charcount >256?\n\t * If not this check should be changed to charcount < 256 */\n\tif (charcount != 256 && charcount != 512)\n\t\treturn -EINVAL;\n\n\t/* Make sure drawing engine can handle the font */\n\tif (!(info->pixmap.blit_x & (1 << (font->width - 1))) ||\n\t    !(info->pixmap.blit_y & (1 << (font->height - 1))))\n\t\treturn -EINVAL;\n\n\t/* Make sure driver can handle the font length */\n\tif (fbcon_invalid_charcount(info, charcount))\n\t\treturn -EINVAL;\n\n\tsize = CALC_FONTSZ(h, pitch, charcount);\n\n\tnew_data = kmalloc(FONT_EXTRA_WORDS * sizeof(int) + size, GFP_USER);\n\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\n\tmemset(new_data, 0, FONT_EXTRA_WORDS * sizeof(int));\n\n\tnew_data += FONT_EXTRA_WORDS * sizeof(int);\n\tFNTSIZE(new_data) = size;\n\tREFCOUNT(new_data) = 0;\t/* usage counter */\n\tfor (i=0; i< charcount; i++) {\n\t\tmemcpy(new_data + i*h*pitch, data +  i*32*pitch, h*pitch);\n\t}\n\n\t/* Since linux has a nice crc32 function use it for counting font\n\t * checksums. */\n\tcsum = crc32(0, new_data, size);\n\n\tFNTSUM(new_data) = csum;\n\t/* Check if the same font is on some other console already */\n\tfor (i = first_fb_vc; i <= last_fb_vc; i++) {\n\t\tstruct vc_data *tmp = vc_cons[i].d;\n\t\t\n\t\tif (fb_display[i].userfont &&\n\t\t    fb_display[i].fontdata &&\n\t\t    FNTSUM(fb_display[i].fontdata) == csum &&\n\t\t    FNTSIZE(fb_display[i].fontdata) == size &&\n\t\t    tmp->vc_font.width == w &&\n\t\t    !memcmp(fb_display[i].fontdata, new_data, size)) {\n\t\t\tkfree(new_data - FONT_EXTRA_WORDS * sizeof(int));\n\t\t\tnew_data = (u8 *)fb_display[i].fontdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn fbcon_do_set_font(vc, font->width, font->height, charcount, new_data, 1);\n}",
      "code_after_change": "static int fbcon_set_font(struct vc_data *vc, struct console_font *font,\n\t\t\t  unsigned int flags)\n{\n\tstruct fb_info *info = fbcon_info_from_console(vc->vc_num);\n\tunsigned charcount = font->charcount;\n\tint w = font->width;\n\tint h = font->height;\n\tint size;\n\tint i, csum;\n\tu8 *new_data, *data = font->data;\n\tint pitch = PITCH(font->width);\n\n\t/* Is there a reason why fbconsole couldn't handle any charcount >256?\n\t * If not this check should be changed to charcount < 256 */\n\tif (charcount != 256 && charcount != 512)\n\t\treturn -EINVAL;\n\n\t/* font bigger than screen resolution ? */\n\tif (w > FBCON_SWAP(info->var.rotate, info->var.xres, info->var.yres) ||\n\t    h > FBCON_SWAP(info->var.rotate, info->var.yres, info->var.xres))\n\t\treturn -EINVAL;\n\n\t/* Make sure drawing engine can handle the font */\n\tif (!(info->pixmap.blit_x & (1 << (font->width - 1))) ||\n\t    !(info->pixmap.blit_y & (1 << (font->height - 1))))\n\t\treturn -EINVAL;\n\n\t/* Make sure driver can handle the font length */\n\tif (fbcon_invalid_charcount(info, charcount))\n\t\treturn -EINVAL;\n\n\tsize = CALC_FONTSZ(h, pitch, charcount);\n\n\tnew_data = kmalloc(FONT_EXTRA_WORDS * sizeof(int) + size, GFP_USER);\n\n\tif (!new_data)\n\t\treturn -ENOMEM;\n\n\tmemset(new_data, 0, FONT_EXTRA_WORDS * sizeof(int));\n\n\tnew_data += FONT_EXTRA_WORDS * sizeof(int);\n\tFNTSIZE(new_data) = size;\n\tREFCOUNT(new_data) = 0;\t/* usage counter */\n\tfor (i=0; i< charcount; i++) {\n\t\tmemcpy(new_data + i*h*pitch, data +  i*32*pitch, h*pitch);\n\t}\n\n\t/* Since linux has a nice crc32 function use it for counting font\n\t * checksums. */\n\tcsum = crc32(0, new_data, size);\n\n\tFNTSUM(new_data) = csum;\n\t/* Check if the same font is on some other console already */\n\tfor (i = first_fb_vc; i <= last_fb_vc; i++) {\n\t\tstruct vc_data *tmp = vc_cons[i].d;\n\t\t\n\t\tif (fb_display[i].userfont &&\n\t\t    fb_display[i].fontdata &&\n\t\t    FNTSUM(fb_display[i].fontdata) == csum &&\n\t\t    FNTSIZE(fb_display[i].fontdata) == size &&\n\t\t    tmp->vc_font.width == w &&\n\t\t    !memcmp(fb_display[i].fontdata, new_data, size)) {\n\t\t\tkfree(new_data - FONT_EXTRA_WORDS * sizeof(int));\n\t\t\tnew_data = (u8 *)fb_display[i].fontdata;\n\t\t\tbreak;\n\t\t}\n\t}\n\treturn fbcon_do_set_font(vc, font->width, font->height, charcount, new_data, 1);\n}",
      "modified_lines": {
        "added": [
          "\t\treturn -EINVAL;",
          "",
          "\t/* font bigger than screen resolution ? */",
          "\tif (w > FBCON_SWAP(info->var.rotate, info->var.xres, info->var.yres) ||",
          "\t    h > FBCON_SWAP(info->var.rotate, info->var.yres, info->var.xres))"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation for font dimensions against screen resolution, allowing font dimensions to exceed screen boundaries.",
      "trigger_condition": "Malicious data sent to the kernel via ioctl command FBIOPUT_VSCREENINFO triggers a memory out-of-bounds write due to unchecked font dimensions.",
      "specific_code_behavior_causing_vulnerability": "The code fails to validate font dimensions against the screen resolution, potentially leading to memory corruption when writing data outside the allocated memory space.",
      "id": 106,
      "code_after_change_normalized": "static int FUN1(struct vc_data *VAR1, struct console_font *VAR2,\nunsigned int VAR3)\n{\nstruct fb_info *VAR4 = FUN2(VAR1->VAR5);\nunsigned VAR6 = VAR2->VAR6;\nint VAR7 = VAR2->VAR8;\nint VAR9 = VAR2->VAR10;\nint VAR11;\nint VAR12, VAR13;\nu8 *VAR14, *VAR15 = VAR2->VAR15;\nint VAR16 = FUN3(VAR2->VAR8);\nif (VAR6 != 256 && VAR6 != 512)\nreturn -VAR17;\nif (VAR7 > FUN4(VAR4->VAR18.VAR19, VAR4->VAR18.VAR20, VAR4->VAR18.VAR21) ||\nVAR9 > FUN4(VAR4->VAR18.VAR19, VAR4->VAR18.VAR21, VAR4->VAR18.VAR20))\nreturn -VAR17;\nif (!(VAR4->VAR22.VAR23 & (1 << (VAR2->VAR8 - 1))) ||\n!(VAR4->VAR22.VAR24 & (1 << (VAR2->VAR10 - 1))))\nreturn -VAR17;\nif (FUN5(VAR4, VAR6))\nreturn -VAR17;\nVAR11 = FUN6(VAR9, VAR16, VAR6);\nVAR14 = FUN7(VAR25 * sizeof(int) + VAR11, VAR26);\nif (!VAR14)\nreturn -VAR27;\nFUN8(VAR14, 0, VAR25 * sizeof(int));\nVAR14 += VAR25 * sizeof(int);\nFUN9(VAR14) = VAR11;\nFUN10(VAR14) = 0;\t\nfor (VAR12=0; VAR12< VAR6; VAR12++) {\nFUN11(VAR14 + i*h*VAR16, VAR15 +  i*32*VAR16, h*VAR16);\n}\nVAR13 = FUN12(0, VAR14, VAR11);\nFUN13(VAR14) = VAR13;\nfor (VAR12 = VAR28; VAR12 <= VAR29; VAR12++) {\nstruct vc_data *VAR30 = VAR31[VAR12].VAR32;\nif (VAR33[VAR12].VAR34 &&\nVAR33[VAR12].VAR35 &&\nFUN13(VAR33[VAR12].VAR35) == VAR13 &&\nFUN9(VAR33[VAR12].VAR35) == VAR11 &&\nVAR30->VAR36.VAR8 == VAR7 &&\n!FUN14(VAR33[VAR12].VAR35, VAR14, VAR11)) {\nFUN15(VAR14 - VAR25 * sizeof(int));\nVAR14 = (VAR37 *)VAR33[VAR12].VAR35;\nbreak;\n}\n}\nreturn FUN16(VAR1, VAR2->VAR8, VAR2->VAR10, VAR6, VAR14, 1);\n}\n",
      "code_before_change_normalized": "static int FUN1(struct vc_data *VAR1, struct console_font *VAR2,\nunsigned int VAR3)\n{\nstruct fb_info *VAR4 = FUN2(VAR1->VAR5);\nunsigned VAR6 = VAR2->VAR6;\nint VAR7 = VAR2->VAR8;\nint VAR9 = VAR2->VAR10;\nint VAR11;\nint VAR12, VAR13;\nu8 *VAR14, *VAR15 = VAR2->VAR15;\nint VAR16 = FUN3(VAR2->VAR8);\nif (VAR6 != 256 && VAR6 != 512)\nreturn -VAR17;\nif (!(VAR4->VAR18.VAR19 & (1 << (VAR2->VAR8 - 1))) ||\n!(VAR4->VAR18.VAR20 & (1 << (VAR2->VAR10 - 1))))\nreturn -VAR17;\nif (FUN4(VAR4, VAR6))\nreturn -VAR17;\nVAR11 = FUN5(VAR9, VAR16, VAR6);\nVAR14 = FUN6(VAR21 * sizeof(int) + VAR11, VAR22);\nif (!VAR14)\nreturn -VAR23;\nFUN7(VAR14, 0, VAR21 * sizeof(int));\nVAR14 += VAR21 * sizeof(int);\nFUN8(VAR14) = VAR11;\nFUN9(VAR14) = 0;\t\nfor (VAR12=0; VAR12< VAR6; VAR12++) {\nFUN10(VAR14 + i*h*VAR16, VAR15 +  i*32*VAR16, h*VAR16);\n}\nVAR13 = FUN11(0, VAR14, VAR11);\nFUN12(VAR14) = VAR13;\nfor (VAR12 = VAR24; VAR12 <= VAR25; VAR12++) {\nstruct vc_data *VAR26 = VAR27[VAR12].VAR28;\nif (VAR29[VAR12].VAR30 &&\nVAR29[VAR12].VAR31 &&\nFUN12(VAR29[VAR12].VAR31) == VAR13 &&\nFUN8(VAR29[VAR12].VAR31) == VAR11 &&\nVAR26->VAR32.VAR8 == VAR7 &&\n!FUN13(VAR29[VAR12].VAR31, VAR14, VAR11)) {\nFUN14(VAR14 - VAR21 * sizeof(int));\nVAR14 = (VAR33 *)VAR29[VAR12].VAR31;\nbreak;\n}\n}\nreturn FUN15(VAR1, VAR2->VAR8, VAR2->VAR10, VAR6, VAR14, 1);\n}\n",
      "code_after_change_raw": "static int fbcon_set_font(struct vc_data *vc, struct console_font *font,\nunsigned int flags)\n{\nstruct fb_info *info = fbcon_info_from_console(vc->vc_num);\nunsigned charcount = font->charcount;\nint w = font->width;\nint h = font->height;\nint size;\nint i, csum;\nu8 *new_data, *data = font->data;\nint pitch = PITCH(font->width);\nif (charcount != 256 && charcount != 512)\nreturn -EINVAL;\nif (w > FBCON_SWAP(info->var.rotate, info->var.xres, info->var.yres) ||\nh > FBCON_SWAP(info->var.rotate, info->var.yres, info->var.xres))\nreturn -EINVAL;\nif (!(info->pixmap.blit_x & (1 << (font->width - 1))) ||\n!(info->pixmap.blit_y & (1 << (font->height - 1))))\nreturn -EINVAL;\nif (fbcon_invalid_charcount(info, charcount))\nreturn -EINVAL;\nsize = CALC_FONTSZ(h, pitch, charcount);\nnew_data = kmalloc(FONT_EXTRA_WORDS * sizeof(int) + size, GFP_USER);\nif (!new_data)\nreturn -ENOMEM;\nmemset(new_data, 0, FONT_EXTRA_WORDS * sizeof(int));\nnew_data += FONT_EXTRA_WORDS * sizeof(int);\nFNTSIZE(new_data) = size;\nREFCOUNT(new_data) = 0;\t\nfor (i=0; i< charcount; i++) {\nmemcpy(new_data + i*h*pitch, data +  i*32*pitch, h*pitch);\n}\ncsum = crc32(0, new_data, size);\nFNTSUM(new_data) = csum;\nfor (i = first_fb_vc; i <= last_fb_vc; i++) {\nstruct vc_data *tmp = vc_cons[i].d;\nif (fb_display[i].userfont &&\nfb_display[i].fontdata &&\nFNTSUM(fb_display[i].fontdata) == csum &&\nFNTSIZE(fb_display[i].fontdata) == size &&\ntmp->vc_font.width == w &&\n!memcmp(fb_display[i].fontdata, new_data, size)) {\nkfree(new_data - FONT_EXTRA_WORDS * sizeof(int));\nnew_data = (u8 *)fb_display[i].fontdata;\nbreak;\n}\n}\nreturn fbcon_do_set_font(vc, font->width, font->height, charcount, new_data, 1);\n}\n",
      "code_before_change_raw": "static int fbcon_set_font(struct vc_data *vc, struct console_font *font,\nunsigned int flags)\n{\nstruct fb_info *info = fbcon_info_from_console(vc->vc_num);\nunsigned charcount = font->charcount;\nint w = font->width;\nint h = font->height;\nint size;\nint i, csum;\nu8 *new_data, *data = font->data;\nint pitch = PITCH(font->width);\nif (charcount != 256 && charcount != 512)\nreturn -EINVAL;\nif (!(info->pixmap.blit_x & (1 << (font->width - 1))) ||\n!(info->pixmap.blit_y & (1 << (font->height - 1))))\nreturn -EINVAL;\nif (fbcon_invalid_charcount(info, charcount))\nreturn -EINVAL;\nsize = CALC_FONTSZ(h, pitch, charcount);\nnew_data = kmalloc(FONT_EXTRA_WORDS * sizeof(int) + size, GFP_USER);\nif (!new_data)\nreturn -ENOMEM;\nmemset(new_data, 0, FONT_EXTRA_WORDS * sizeof(int));\nnew_data += FONT_EXTRA_WORDS * sizeof(int);\nFNTSIZE(new_data) = size;\nREFCOUNT(new_data) = 0;\t\nfor (i=0; i< charcount; i++) {\nmemcpy(new_data + i*h*pitch, data +  i*32*pitch, h*pitch);\n}\ncsum = crc32(0, new_data, size);\nFNTSUM(new_data) = csum;\nfor (i = first_fb_vc; i <= last_fb_vc; i++) {\nstruct vc_data *tmp = vc_cons[i].d;\nif (fb_display[i].userfont &&\nfb_display[i].fontdata &&\nFNTSUM(fb_display[i].fontdata) == csum &&\nFNTSIZE(fb_display[i].fontdata) == size &&\ntmp->vc_font.width == w &&\n!memcmp(fb_display[i].fontdata, new_data, size)) {\nkfree(new_data - FONT_EXTRA_WORDS * sizeof(int));\nnew_data = (u8 *)fb_display[i].fontdata;\nbreak;\n}\n}\nreturn fbcon_do_set_font(vc, font->width, font->height, charcount, new_data, 1);\n}\n"
    }
  ],
  "CVE-2021-33656": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when calculating memory allocation size based on font width, charcount, and a fixed value.",
        "trigger_condition": "Setting font with malicious data using the ioctl command PIO_FONT can lead to out-of-bounds memory write due to incorrect calculation of memory allocation size.",
        "specific_code_behavior_causing_vulnerability": "The code calculates the memory allocation size 'c' based on font.width, font.charcount, and a fixed value of 32 without proper bounds checking, which can result in writing memory out of bounds if the font data is maliciously crafted."
      },
      "solution": "To mitigate the vulnerability, it is necessary to remove the problematic bounds check on the memory allocation size 'c' and focus on checking the font width and height against the specified op width and height. By ensuring that the font width and height do not exceed the specified limits, the code prevents potential out-of-bounds memory writes when copying font data to user space. The solution involves adjusting the bounds checks for font width and height to ensure proper memory access and prevent memory corruption issues.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-33656, which involves a potential out-of-bounds memory write when setting the font with malicious data using the ioctl command PIO_FONT. \n\nIn the original code snippet, the vulnerability arises from the calculation of the variable 'c' without proper bounds checking. Specifically, the calculation of 'c' is based on font.width, font.charcount, and a fixed value of 32, which can lead to writing memory out of bounds if the provided font data is maliciously crafted.\n\nThe modification addresses this vulnerability by removing the problematic bounds check on 'c' and instead focusing on checking the font width and height against the specified op width and height. By ensuring that the font width and height do not exceed the specified limits, the code mitigates the risk of out-of-bounds memory writes when copying font data to user space.\n\nTherefore, the modified code snippet provides a more secure implementation by focusing on the correct bounds checks for font width and height, thus preventing potential memory corruption issues.",
      "GPT_purpose": "Retrieve console font information and potentially set font data.",
      "GPT_function": "\n1. Get console font information.\n2. Check if the console mode is text mode.\n3. Check for available space for font data.\n4. Handle different font width and height scenarios.\n5. Copy font data to user space.\n6. Free allocated font data memory.",
      "CVE_id": "CVE-2021-33656",
      "code_before_change": "static int con_font_get(struct vc_data *vc, struct console_font_op *op)\n{\n\tstruct console_font font;\n\tint rc = -EINVAL;\n\tint c;\n\n\tif (op->data) {\n\t\tfont.data = kmalloc(max_font_size, GFP_KERNEL);\n\t\tif (!font.data)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tfont.data = NULL;\n\n\tconsole_lock();\n\tif (vc->vc_mode != KD_TEXT)\n\t\trc = -EINVAL;\n\telse if (vc->vc_sw->con_font_get)\n\t\trc = vc->vc_sw->con_font_get(vc, &font);\n\telse\n\t\trc = -ENOSYS;\n\tconsole_unlock();\n\n\tif (rc)\n\t\tgoto out;\n\n\tc = (font.width+7)/8 * 32 * font.charcount;\n\n\tif (op->data && font.charcount > op->charcount)\n\t\trc = -ENOSPC;\n\tif (!(op->flags & KD_FONT_FLAG_OLD)) {\n\t\tif (font.width > op->width || font.height > op->height)\n\t\t\trc = -ENOSPC;\n\t} else {\n\t\tif (font.width != 8)\n\t\t\trc = -EIO;\n\t\telse if ((op->height && font.height > op->height) ||\n\t\t\t font.height > 32)\n\t\t\trc = -ENOSPC;\n\t}\n\tif (rc)\n\t\tgoto out;\n\n\top->height = font.height;\n\top->width = font.width;\n\top->charcount = font.charcount;\n\n\tif (op->data && copy_to_user(op->data, font.data, c))\n\t\trc = -EFAULT;\n\nout:\n\tkfree(font.data);\n\treturn rc;\n}",
      "code_after_change": "static int con_font_get(struct vc_data *vc, struct console_font_op *op)\n{\n\tstruct console_font font;\n\tint rc = -EINVAL;\n\tint c;\n\n\tif (op->data) {\n\t\tfont.data = kmalloc(max_font_size, GFP_KERNEL);\n\t\tif (!font.data)\n\t\t\treturn -ENOMEM;\n\t} else\n\t\tfont.data = NULL;\n\n\tconsole_lock();\n\tif (vc->vc_mode != KD_TEXT)\n\t\trc = -EINVAL;\n\telse if (vc->vc_sw->con_font_get)\n\t\trc = vc->vc_sw->con_font_get(vc, &font);\n\telse\n\t\trc = -ENOSYS;\n\tconsole_unlock();\n\n\tif (rc)\n\t\tgoto out;\n\n\tc = (font.width+7)/8 * 32 * font.charcount;\n\n\tif (op->data && font.charcount > op->charcount)\n\t\trc = -ENOSPC;\n\tif (font.width > op->width || font.height > op->height)\n\t\trc = -ENOSPC;\n\tif (rc)\n\t\tgoto out;\n\n\top->height = font.height;\n\top->width = font.width;\n\top->charcount = font.charcount;\n\n\tif (op->data && copy_to_user(op->data, font.data, c))\n\t\trc = -EFAULT;\n\nout:\n\tkfree(font.data);\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tif (font.width > op->width || font.height > op->height)",
          "\t\trc = -ENOSPC;"
        ],
        "deleted": [
          "\tif (!(op->flags & KD_FONT_FLAG_OLD)) {",
          "\t\tif (font.width > op->width || font.height > op->height)",
          "\t\t\trc = -ENOSPC;",
          "\t} else {",
          "\t\tif (font.width != 8)",
          "\t\t\trc = -EIO;",
          "\t\telse if ((op->height && font.height > op->height) ||",
          "\t\t\t font.height > 32)",
          "\t\t\trc = -ENOSPC;",
          "\t}"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when calculating memory allocation size based on font width, charcount, and a fixed value.",
      "trigger_condition": "Setting font with malicious data using the ioctl command PIO_FONT can lead to out-of-bounds memory write due to incorrect calculation of memory allocation size.",
      "specific_code_behavior_causing_vulnerability": "The code calculates the memory allocation size 'c' based on font.width, font.charcount, and a fixed value of 32 without proper bounds checking, which can result in writing memory out of bounds if the font data is maliciously crafted.",
      "id": 107,
      "code_after_change_normalized": "static int FUN1(struct vc_data *VAR1, struct console_font_op *VAR2)\n{\nstruct console_font VAR3;\nint VAR4 = -VAR5;\nint VAR6;\nif (VAR2->VAR7) {\nVAR3.VAR7 = FUN2(VAR8, VAR9);\nif (!VAR3.VAR7)\nreturn -VAR10;\n} else\nVAR3.VAR7 = NULL;\nFUN3();\nif (VAR1->VAR11 != VAR12)\nVAR4 = -VAR5;\nelse if (VAR1->VAR13->VAR14)\nVAR4 = VAR1->VAR13->FUN1(VAR1, &VAR3);\nelse\nVAR4 = -VAR15;\nFUN4();\nif (VAR4)\ngoto VAR16;\nVAR6 = (VAR3.VAR17+7)/8 * 32 * VAR3.VAR18;\nif (VAR2->VAR7 && VAR3.VAR18 > VAR2->VAR18)\nVAR4 = -VAR19;\nif (VAR3.VAR17 > VAR2->VAR17 || VAR3.VAR20 > VAR2->VAR20)\nVAR4 = -VAR19;\nif (VAR4)\ngoto VAR16;\nVAR2->VAR20 = VAR3.VAR20;\nVAR2->VAR17 = VAR3.VAR17;\nVAR2->VAR18 = VAR3.VAR18;\nif (VAR2->VAR7 && FUN5(VAR2->VAR7, VAR3.VAR7, VAR6))\nVAR4 = -VAR21;\nVAR16:\nFUN6(VAR3.VAR7);\nreturn VAR4;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct vc_data *VAR1, struct console_font_op *VAR2)\n{\nstruct console_font VAR3;\nint VAR4 = -VAR5;\nint VAR6;\nif (VAR2->VAR7) {\nVAR3.VAR7 = FUN2(VAR8, VAR9);\nif (!VAR3.VAR7)\nreturn -VAR10;\n} else\nVAR3.VAR7 = NULL;\nFUN3();\nif (VAR1->VAR11 != VAR12)\nVAR4 = -VAR5;\nelse if (VAR1->VAR13->VAR14)\nVAR4 = VAR1->VAR13->FUN1(VAR1, &VAR3);\nelse\nVAR4 = -VAR15;\nFUN4();\nif (VAR4)\ngoto VAR16;\nVAR6 = (VAR3.VAR17+7)/8 * 32 * VAR3.VAR18;\nif (VAR2->VAR7 && VAR3.VAR18 > VAR2->VAR18)\nVAR4 = -VAR19;\nif (!(VAR2->VAR20 & VAR21)) {\nif (VAR3.VAR17 > VAR2->VAR17 || VAR3.VAR22 > VAR2->VAR22)\nVAR4 = -VAR19;\n} else {\nif (VAR3.VAR17 != 8)\nVAR4 = -VAR23;\nelse if ((VAR2->VAR22 && VAR3.VAR22 > VAR2->VAR22) ||\nVAR3.VAR22 > 32)\nVAR4 = -VAR19;\n}\nif (VAR4)\ngoto VAR16;\nVAR2->VAR22 = VAR3.VAR22;\nVAR2->VAR17 = VAR3.VAR17;\nVAR2->VAR18 = VAR3.VAR18;\nif (VAR2->VAR7 && FUN5(VAR2->VAR7, VAR3.VAR7, VAR6))\nVAR4 = -VAR24;\nVAR16:\nFUN6(VAR3.VAR7);\nreturn VAR4;\n}\n",
      "code_after_change_raw": "static int con_font_get(struct vc_data *vc, struct console_font_op *op)\n{\nstruct console_font font;\nint rc = -EINVAL;\nint c;\nif (op->data) {\nfont.data = kmalloc(max_font_size, GFP_KERNEL);\nif (!font.data)\nreturn -ENOMEM;\n} else\nfont.data = NULL;\nconsole_lock();\nif (vc->vc_mode != KD_TEXT)\nrc = -EINVAL;\nelse if (vc->vc_sw->con_font_get)\nrc = vc->vc_sw->con_font_get(vc, &font);\nelse\nrc = -ENOSYS;\nconsole_unlock();\nif (rc)\ngoto out;\nc = (font.width+7)/8 * 32 * font.charcount;\nif (op->data && font.charcount > op->charcount)\nrc = -ENOSPC;\nif (font.width > op->width || font.height > op->height)\nrc = -ENOSPC;\nif (rc)\ngoto out;\nop->height = font.height;\nop->width = font.width;\nop->charcount = font.charcount;\nif (op->data && copy_to_user(op->data, font.data, c))\nrc = -EFAULT;\nout:\nkfree(font.data);\nreturn rc;\n}\n",
      "code_before_change_raw": "static int con_font_get(struct vc_data *vc, struct console_font_op *op)\n{\nstruct console_font font;\nint rc = -EINVAL;\nint c;\nif (op->data) {\nfont.data = kmalloc(max_font_size, GFP_KERNEL);\nif (!font.data)\nreturn -ENOMEM;\n} else\nfont.data = NULL;\nconsole_lock();\nif (vc->vc_mode != KD_TEXT)\nrc = -EINVAL;\nelse if (vc->vc_sw->con_font_get)\nrc = vc->vc_sw->con_font_get(vc, &font);\nelse\nrc = -ENOSYS;\nconsole_unlock();\nif (rc)\ngoto out;\nc = (font.width+7)/8 * 32 * font.charcount;\nif (op->data && font.charcount > op->charcount)\nrc = -ENOSPC;\nif (!(op->flags & KD_FONT_FLAG_OLD)) {\nif (font.width > op->width || font.height > op->height)\nrc = -ENOSPC;\n} else {\nif (font.width != 8)\nrc = -EIO;\nelse if ((op->height && font.height > op->height) ||\nfont.height > 32)\nrc = -ENOSPC;\n}\nif (rc)\ngoto out;\nop->height = font.height;\nop->width = font.width;\nop->charcount = font.charcount;\nif (op->data && copy_to_user(op->data, font.data, c))\nrc = -EFAULT;\nout:\nkfree(font.data);\nreturn rc;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and initialization of user-supplied data before using it in a critical operation.",
        "trigger_condition": "When setting font with malicious data using the ioctl command PIO_FONT, the code directly assigns the user-supplied data to a structure without proper validation, leading to a potential out-of-bounds memory write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code snippet does not validate the user-supplied data before assigning it to the 'op' structure in the PIO_FONT case of the switch statement. This can allow an attacker to provide malicious data that exceeds the bounds of the 'op' structure, leading to memory corruption and potential security risks."
      },
      "solution": "To mitigate the vulnerability, it is necessary to validate and properly initialize user-supplied data before using it in critical operations. In this case, the solution involves adding a line to copy the user-supplied data into the 'op' structure using 'copy_from_user' before further processing. Additionally, the line 'op.data = up;' inside the PIO_FONT case should be removed to prevent direct assignment of potentially malicious data.",
      "GPT_analysis": "The modification is necessary to prevent a potential out-of-bounds memory write vulnerability in the code snippet. By adding the following lines before the `PIO_FONT` case in the switch statement:\n```\nif (copy_from_user(&op, up, sizeof(op)))\n    return -EFAULT;\n```\nand deleting the line `op.data = up;` inside the `PIO_FONT` case, you ensure that the `op` structure is properly initialized with user-supplied data before using it. This helps prevent a scenario where malicious user input could lead to writing beyond the bounds of the `op` structure, which could potentially be exploited to execute arbitrary code or crash the system. By copying the user-supplied data into the `op` structure using `copy_from_user`, you ensure that the kernel operates on valid and safe data, reducing the risk of security vulnerabilities.",
      "GPT_purpose": "Handle various input/output control operations for console fonts and mappings in the Linux kernel.",
      "GPT_function": "\n1. Handling IOCTL command PIO_FONT to set font data.\n2. Handling IOCTL command GIO_FONT to get font data.\n3. Handling IOCTL command PIO_CMAP to set color map data.\n4. Handling IOCTL command GIO_CMAP to get color map data.\n5. Handling IOCTL commands PIO_FONTX and GIO_FONTX to perform font-related operations.\n6. Handling IOCTL command PIO_FONTRESET to reset font data.\n7. Handling IOCTL command PIO_SCRNMAP to set screen map data.\n8. Handling IOCTL command GIO_SCRNMAP to get screen map data.\n9. Handling IOCTL command PIO_UNISCRNMAP to set Unicode screen map data.\n10. Handling IOCTL command GIO_UNISCRNMAP to get Unicode screen map data.\n11. Handling IOCTL command PIO_UNIMAPCLR to clear Unicode map data.\n12. Handling IOCTL commands PIO_UNIMAP and GIO_UNIMAP to perform Unicode map operations.\n13. Handling default case for unknown IOCTL commands.",
      "CVE_id": "CVE-2021-33656",
      "code_before_change": "static int vt_io_ioctl(struct vc_data *vc, unsigned int cmd, void __user *up,\n\t\tbool perm)\n{\n\tstruct console_font_op op;\t/* used in multiple places here */\n\n\tswitch (cmd) {\n\tcase PIO_FONT:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\top.op = KD_FONT_OP_SET;\n\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */\n\t\top.width = 8;\n\t\top.height = 0;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\treturn con_font_op(vc, &op);\n\n\tcase GIO_FONT:\n\t\top.op = KD_FONT_OP_GET;\n\t\top.flags = KD_FONT_FLAG_OLD;\n\t\top.width = 8;\n\t\top.height = 32;\n\t\top.charcount = 256;\n\t\top.data = up;\n\t\treturn con_font_op(vc, &op);\n\n\tcase PIO_CMAP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\treturn con_set_cmap(up);\n\n\tcase GIO_CMAP:\n\t\treturn con_get_cmap(up);\n\n\tcase PIO_FONTX:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\tfallthrough;\n\tcase GIO_FONTX:\n\t\treturn do_fontx_ioctl(vc, cmd, up, &op);\n\n\tcase PIO_FONTRESET:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\n\t\treturn vt_io_fontreset(vc, &op);\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\treturn con_set_trans_old(up);\n\n\tcase GIO_SCRNMAP:\n\t\treturn con_get_trans_old(up);\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\treturn con_set_trans_new(up);\n\n\tcase GIO_UNISCRNMAP:\n\t\treturn con_get_trans_new(up);\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\treturn do_unimap_ioctl(cmd, up, perm, vc);\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}",
      "code_after_change": "static int vt_io_ioctl(struct vc_data *vc, unsigned int cmd, void __user *up,\n\t\tbool perm)\n{\n\tswitch (cmd) {\n\tcase PIO_CMAP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\treturn con_set_cmap(up);\n\n\tcase GIO_CMAP:\n\t\treturn con_get_cmap(up);\n\n\tcase PIO_SCRNMAP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\treturn con_set_trans_old(up);\n\n\tcase GIO_SCRNMAP:\n\t\treturn con_get_trans_old(up);\n\n\tcase PIO_UNISCRNMAP:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\treturn con_set_trans_new(up);\n\n\tcase GIO_UNISCRNMAP:\n\t\treturn con_get_trans_new(up);\n\n\tcase PIO_UNIMAPCLR:\n\t\tif (!perm)\n\t\t\treturn -EPERM;\n\t\tcon_clear_unimap(vc);\n\t\tbreak;\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\treturn do_unimap_ioctl(cmd, up, perm, vc);\n\n\tdefault:\n\t\treturn -ENOIOCTLCMD;\n\t}\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tstruct console_font_op op;\t/* used in multiple places here */",
          "",
          "\tcase PIO_FONT:",
          "\t\tif (!perm)",
          "\t\t\treturn -EPERM;",
          "\t\top.op = KD_FONT_OP_SET;",
          "\t\top.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t/* Compatibility */",
          "\t\top.width = 8;",
          "\t\top.height = 0;",
          "\t\top.charcount = 256;",
          "\t\top.data = up;",
          "\t\treturn con_font_op(vc, &op);",
          "",
          "\tcase GIO_FONT:",
          "\t\top.op = KD_FONT_OP_GET;",
          "\t\top.flags = KD_FONT_FLAG_OLD;",
          "\t\top.width = 8;",
          "\t\top.height = 32;",
          "\t\top.charcount = 256;",
          "\t\top.data = up;",
          "\t\treturn con_font_op(vc, &op);",
          "",
          "",
          "\tcase PIO_FONTX:",
          "\t\tif (!perm)",
          "\t\t\treturn -EPERM;",
          "",
          "\t\tfallthrough;",
          "\tcase GIO_FONTX:",
          "\t\treturn do_fontx_ioctl(vc, cmd, up, &op);",
          "",
          "\tcase PIO_FONTRESET:",
          "\t\tif (!perm)",
          "\t\t\treturn -EPERM;",
          "",
          "\t\treturn vt_io_fontreset(vc, &op);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and initialization of user-supplied data before using it in a critical operation.",
      "trigger_condition": "When setting font with malicious data using the ioctl command PIO_FONT, the code directly assigns the user-supplied data to a structure without proper validation, leading to a potential out-of-bounds memory write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code snippet does not validate the user-supplied data before assigning it to the 'op' structure in the PIO_FONT case of the switch statement. This can allow an attacker to provide malicious data that exceeds the bounds of the 'op' structure, leading to memory corruption and potential security risks.",
      "id": 108,
      "code_after_change_normalized": "static int FUN1(struct vc_data *VAR1, unsigned int VAR2, void __user *VAR3,\nbool VAR4)\n{\nswitch (VAR2) {\ncase VAR5:\nif (!VAR4)\nreturn -VAR6;\nreturn FUN2(VAR3);\ncase VAR7:\nreturn FUN3(VAR3);\ncase VAR8:\nif (!VAR4)\nreturn -VAR6;\nreturn FUN4(VAR3);\ncase VAR9:\nreturn FUN5(VAR3);\ncase VAR10:\nif (!VAR4)\nreturn -VAR6;\nreturn FUN6(VAR3);\ncase VAR11:\nreturn FUN7(VAR3);\ncase VAR12:\nif (!VAR4)\nreturn -VAR6;\nFUN8(VAR1);\nbreak;\ncase VAR13:\ncase VAR14:\nreturn FUN9(VAR2, VAR3, VAR4, VAR1);\ndefault:\nreturn -VAR15;\n}\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct vc_data *VAR1, unsigned int VAR2, void __user *VAR3,\nbool VAR4)\n{\nstruct console_font_op VAR5;\t\nswitch (VAR2) {\ncase VAR6:\nif (!VAR4)\nreturn -VAR7;\nVAR5.VAR5 = VAR8;\nVAR5.VAR9 = VAR10 | VAR11;\t\nVAR5.VAR12 = 8;\nVAR5.VAR13 = 0;\nVAR5.VAR14 = 256;\nVAR5.VAR15 = VAR3;\nreturn FUN2(VAR1, &VAR5);\ncase VAR16:\nVAR5.VAR5 = VAR17;\nVAR5.VAR9 = VAR10;\nVAR5.VAR12 = 8;\nVAR5.VAR13 = 32;\nVAR5.VAR14 = 256;\nVAR5.VAR15 = VAR3;\nreturn FUN2(VAR1, &VAR5);\ncase VAR18:\nif (!VAR4)\nreturn -VAR7;\nreturn FUN3(VAR3);\ncase VAR19:\nreturn FUN4(VAR3);\ncase VAR20:\nif (!VAR4)\nreturn -VAR7;\nVAR21;\ncase VAR22:\nreturn FUN5(VAR1, VAR2, VAR3, &VAR5);\ncase VAR23:\nif (!VAR4)\nreturn -VAR7;\nreturn FUN6(VAR1, &VAR5);\ncase VAR24:\nif (!VAR4)\nreturn -VAR7;\nreturn FUN7(VAR3);\ncase VAR25:\nreturn FUN8(VAR3);\ncase VAR26:\nif (!VAR4)\nreturn -VAR7;\nreturn FUN9(VAR3);\ncase VAR27:\nreturn FUN10(VAR3);\ncase VAR28:\nif (!VAR4)\nreturn -VAR7;\nFUN11(VAR1);\nbreak;\ncase VAR29:\ncase VAR30:\nreturn FUN12(VAR2, VAR3, VAR4, VAR1);\ndefault:\nreturn -VAR31;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int vt_io_ioctl(struct vc_data *vc, unsigned int cmd, void __user *up,\nbool perm)\n{\nswitch (cmd) {\ncase PIO_CMAP:\nif (!perm)\nreturn -EPERM;\nreturn con_set_cmap(up);\ncase GIO_CMAP:\nreturn con_get_cmap(up);\ncase PIO_SCRNMAP:\nif (!perm)\nreturn -EPERM;\nreturn con_set_trans_old(up);\ncase GIO_SCRNMAP:\nreturn con_get_trans_old(up);\ncase PIO_UNISCRNMAP:\nif (!perm)\nreturn -EPERM;\nreturn con_set_trans_new(up);\ncase GIO_UNISCRNMAP:\nreturn con_get_trans_new(up);\ncase PIO_UNIMAPCLR:\nif (!perm)\nreturn -EPERM;\ncon_clear_unimap(vc);\nbreak;\ncase PIO_UNIMAP:\ncase GIO_UNIMAP:\nreturn do_unimap_ioctl(cmd, up, perm, vc);\ndefault:\nreturn -ENOIOCTLCMD;\n}\nreturn 0;\n}\n",
      "code_before_change_raw": "static int vt_io_ioctl(struct vc_data *vc, unsigned int cmd, void __user *up,\nbool perm)\n{\nstruct console_font_op op;\t\nswitch (cmd) {\ncase PIO_FONT:\nif (!perm)\nreturn -EPERM;\nop.op = KD_FONT_OP_SET;\nop.flags = KD_FONT_FLAG_OLD | KD_FONT_FLAG_DONT_RECALC;\t\nop.width = 8;\nop.height = 0;\nop.charcount = 256;\nop.data = up;\nreturn con_font_op(vc, &op);\ncase GIO_FONT:\nop.op = KD_FONT_OP_GET;\nop.flags = KD_FONT_FLAG_OLD;\nop.width = 8;\nop.height = 32;\nop.charcount = 256;\nop.data = up;\nreturn con_font_op(vc, &op);\ncase PIO_CMAP:\nif (!perm)\nreturn -EPERM;\nreturn con_set_cmap(up);\ncase GIO_CMAP:\nreturn con_get_cmap(up);\ncase PIO_FONTX:\nif (!perm)\nreturn -EPERM;\nfallthrough;\ncase GIO_FONTX:\nreturn do_fontx_ioctl(vc, cmd, up, &op);\ncase PIO_FONTRESET:\nif (!perm)\nreturn -EPERM;\nreturn vt_io_fontreset(vc, &op);\ncase PIO_SCRNMAP:\nif (!perm)\nreturn -EPERM;\nreturn con_set_trans_old(up);\ncase GIO_SCRNMAP:\nreturn con_get_trans_old(up);\ncase PIO_UNISCRNMAP:\nif (!perm)\nreturn -EPERM;\nreturn con_set_trans_new(up);\ncase GIO_UNISCRNMAP:\nreturn con_get_trans_new(up);\ncase PIO_UNIMAPCLR:\nif (!perm)\nreturn -EPERM;\ncon_clear_unimap(vc);\nbreak;\ncase PIO_UNIMAP:\ncase GIO_UNIMAP:\nreturn do_unimap_ioctl(cmd, up, perm, vc);\ndefault:\nreturn -ENOIOCTLCMD;\n}\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for user-provided data in ioctl commands related to font operations.",
        "trigger_condition": "A malicious user provides crafted data in an ioctl command, specifically PIO_FONTX, leading to out-of-bounds memory writes.",
        "specific_code_behavior_causing_vulnerability": "The code does not perform proper validation to ensure that the user-provided data is within bounds and of the correct size before processing font-related ioctl commands. This oversight allows a malicious user to trigger out-of-bounds memory writes, potentially leading to security vulnerabilities."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to validate the user-provided data before processing font-related ioctl commands. Specifically, adding a check using access_ok to verify the accessibility and size of the user-provided data can prevent out-of-bounds memory writes. By performing this validation step, the code can ensure that only valid and safe data is processed, reducing the risk of security vulnerabilities.",
      "GPT_analysis": "The modification you provided is necessary to fix the vulnerability described in CVE-2021-33656. By adding the following lines:\n```\nif (!access_ok(up, sizeof(struct console_font_op)))\n    return -EFAULT;\n```\nbefore the switch statement in the code snippet, you can ensure that the user-provided data pointed to by `up` is within the user space and of the correct size. This check helps prevent potential out-of-bounds memory writes when processing the font-related ioctl commands, such as PIO_FONTX and GIO_FONTX.\n\nBy performing the access_ok check, you verify that the user-provided data is accessible and valid before proceeding with the ioctl operations. This helps to mitigate the risk of a malicious user exploiting the vulnerability to write memory out of bounds, which could lead to security issues or system instability.\n\nIn summary, adding the access_ok check is a crucial step in enhancing the security and robustness of the code by ensuring that the input data is safe to use before processing it further.",
      "GPT_purpose": "Handle compatibility for various ioctl commands on virtual terminals.",
      "GPT_function": "\n1. Handle compatibility ioctl operations for virtual terminal fonts and data structures.\n2. Check permissions for various virtual terminal ioctl commands.\n3. Execute different actions based on the ioctl command provided.",
      "CVE_id": "CVE-2021-33656",
      "code_before_change": "long vt_compat_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tvoid __user *up = compat_ptr(arg);\n\tint perm;\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n\n\tswitch (cmd) {\n\t/*\n\t * these need special handlers for incompatible data structures\n\t */\n\tcase PIO_FONTX:\n\tcase GIO_FONTX:\n\t\treturn compat_fontx_ioctl(vc, cmd, up, perm, &op);\n\n\tcase KDFONTOP:\n\t\treturn compat_kdfontop_ioctl(up, perm, &op, vc);\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\treturn compat_unimap_ioctl(cmd, up, perm, vc);\n\n\t/*\n\t * all these treat 'arg' as an integer\n\t */\n\tcase KIOCSOUND:\n\tcase KDMKTONE:\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n#endif\n\tcase KDSETMODE:\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\tcase KDSKBMODE:\n\tcase KDSKBMETA:\n\tcase KDSKBLED:\n\tcase KDSETLED:\n\tcase KDSIGACCEPT:\n\tcase VT_ACTIVATE:\n\tcase VT_WAITACTIVE:\n\tcase VT_RELDISP:\n\tcase VT_DISALLOCATE:\n\tcase VT_RESIZE:\n\tcase VT_RESIZEX:\n\t\treturn vt_ioctl(tty, cmd, arg);\n\n\t/*\n\t * the rest has a compatible data structure behind arg,\n\t * but we have to convert it to a proper 64 bit pointer.\n\t */\n\tdefault:\n\t\treturn vt_ioctl(tty, cmd, (unsigned long)up);\n\t}\n}",
      "code_after_change": "long vt_compat_ioctl(struct tty_struct *tty,\n\t     unsigned int cmd, unsigned long arg)\n{\n\tstruct vc_data *vc = tty->driver_data;\n\tstruct console_font_op op;\t/* used in multiple places here */\n\tvoid __user *up = compat_ptr(arg);\n\tint perm;\n\n\t/*\n\t * To have permissions to do most of the vt ioctls, we either have\n\t * to be the owner of the tty, or have CAP_SYS_TTY_CONFIG.\n\t */\n\tperm = 0;\n\tif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\n\t\tperm = 1;\n\n\tswitch (cmd) {\n\t/*\n\t * these need special handlers for incompatible data structures\n\t */\n\n\tcase KDFONTOP:\n\t\treturn compat_kdfontop_ioctl(up, perm, &op, vc);\n\n\tcase PIO_UNIMAP:\n\tcase GIO_UNIMAP:\n\t\treturn compat_unimap_ioctl(cmd, up, perm, vc);\n\n\t/*\n\t * all these treat 'arg' as an integer\n\t */\n\tcase KIOCSOUND:\n\tcase KDMKTONE:\n#ifdef CONFIG_X86\n\tcase KDADDIO:\n\tcase KDDELIO:\n#endif\n\tcase KDSETMODE:\n\tcase KDMAPDISP:\n\tcase KDUNMAPDISP:\n\tcase KDSKBMODE:\n\tcase KDSKBMETA:\n\tcase KDSKBLED:\n\tcase KDSETLED:\n\tcase KDSIGACCEPT:\n\tcase VT_ACTIVATE:\n\tcase VT_WAITACTIVE:\n\tcase VT_RELDISP:\n\tcase VT_DISALLOCATE:\n\tcase VT_RESIZE:\n\tcase VT_RESIZEX:\n\t\treturn vt_ioctl(tty, cmd, arg);\n\n\t/*\n\t * the rest has a compatible data structure behind arg,\n\t * but we have to convert it to a proper 64 bit pointer.\n\t */\n\tdefault:\n\t\treturn vt_ioctl(tty, cmd, (unsigned long)up);\n\t}\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tcase PIO_FONTX:",
          "\tcase GIO_FONTX:",
          "\t\treturn compat_fontx_ioctl(vc, cmd, up, perm, &op);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for user-provided data in ioctl commands related to font operations.",
      "trigger_condition": "A malicious user provides crafted data in an ioctl command, specifically PIO_FONTX, leading to out-of-bounds memory writes.",
      "specific_code_behavior_causing_vulnerability": "The code does not perform proper validation to ensure that the user-provided data is within bounds and of the correct size before processing font-related ioctl commands. This oversight allows a malicious user to trigger out-of-bounds memory writes, potentially leading to security vulnerabilities.",
      "id": 109,
      "code_after_change_normalized": "long FUN1(struct tty_struct *VAR1,\nunsigned int VAR2, unsigned long VAR3)\n{\nstruct vc_data *VAR4 = VAR1->VAR5;\nstruct console_font_op VAR6;\t\nvoid __user *VAR7 = FUN2(VAR3);\nint VAR8;\nVAR8 = 0;\nif (VAR9->VAR10->VAR1 == VAR1 || FUN3(VAR11))\nVAR8 = 1;\nswitch (VAR2) {\ncase VAR12:\nreturn FUN4(VAR7, VAR8, &VAR6, VAR4);\ncase VAR13:\ncase VAR14:\nreturn FUN5(VAR2, VAR7, VAR8, VAR4);\ncase VAR15:\ncase VAR16:\n#ifdef VAR17\ncase VAR18:\ncase VAR19:\n#VAR20\ncase VAR21:\ncase VAR22:\ncase VAR23:\ncase VAR24:\ncase VAR25:\ncase VAR26:\ncase VAR27:\ncase VAR28:\ncase VAR29:\ncase VAR30:\ncase VAR31:\ncase VAR32:\ncase VAR33:\ncase VAR34:\nreturn FUN6(VAR1, VAR2, VAR3);\ndefault:\nreturn FUN6(VAR1, VAR2, (unsigned long)VAR7);\n}\n}\n",
      "code_before_change_normalized": "long FUN1(struct tty_struct *VAR1,\nunsigned int VAR2, unsigned long VAR3)\n{\nstruct vc_data *VAR4 = VAR1->VAR5;\nstruct console_font_op VAR6;\t\nvoid __user *VAR7 = FUN2(VAR3);\nint VAR8;\nVAR8 = 0;\nif (VAR9->VAR10->VAR1 == VAR1 || FUN3(VAR11))\nVAR8 = 1;\nswitch (VAR2) {\ncase VAR12:\ncase VAR13:\nreturn FUN4(VAR4, VAR2, VAR7, VAR8, &VAR6);\ncase VAR14:\nreturn FUN5(VAR7, VAR8, &VAR6, VAR4);\ncase VAR15:\ncase VAR16:\nreturn FUN6(VAR2, VAR7, VAR8, VAR4);\ncase VAR17:\ncase VAR18:\n#ifdef VAR19\ncase VAR20:\ncase VAR21:\n#VAR22\ncase VAR23:\ncase VAR24:\ncase VAR25:\ncase VAR26:\ncase VAR27:\ncase VAR28:\ncase VAR29:\ncase VAR30:\ncase VAR31:\ncase VAR32:\ncase VAR33:\ncase VAR34:\ncase VAR35:\ncase VAR36:\nreturn FUN7(VAR1, VAR2, VAR3);\ndefault:\nreturn FUN7(VAR1, VAR2, (unsigned long)VAR7);\n}\n}\n",
      "code_after_change_raw": "long vt_compat_ioctl(struct tty_struct *tty,\nunsigned int cmd, unsigned long arg)\n{\nstruct vc_data *vc = tty->driver_data;\nstruct console_font_op op;\t\nvoid __user *up = compat_ptr(arg);\nint perm;\nperm = 0;\nif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\nperm = 1;\nswitch (cmd) {\ncase KDFONTOP:\nreturn compat_kdfontop_ioctl(up, perm, &op, vc);\ncase PIO_UNIMAP:\ncase GIO_UNIMAP:\nreturn compat_unimap_ioctl(cmd, up, perm, vc);\ncase KIOCSOUND:\ncase KDMKTONE:\n#ifdef CONFIG_X86\ncase KDADDIO:\ncase KDDELIO:\n#endif\ncase KDSETMODE:\ncase KDMAPDISP:\ncase KDUNMAPDISP:\ncase KDSKBMODE:\ncase KDSKBMETA:\ncase KDSKBLED:\ncase KDSETLED:\ncase KDSIGACCEPT:\ncase VT_ACTIVATE:\ncase VT_WAITACTIVE:\ncase VT_RELDISP:\ncase VT_DISALLOCATE:\ncase VT_RESIZE:\ncase VT_RESIZEX:\nreturn vt_ioctl(tty, cmd, arg);\ndefault:\nreturn vt_ioctl(tty, cmd, (unsigned long)up);\n}\n}\n",
      "code_before_change_raw": "long vt_compat_ioctl(struct tty_struct *tty,\nunsigned int cmd, unsigned long arg)\n{\nstruct vc_data *vc = tty->driver_data;\nstruct console_font_op op;\t\nvoid __user *up = compat_ptr(arg);\nint perm;\nperm = 0;\nif (current->signal->tty == tty || capable(CAP_SYS_TTY_CONFIG))\nperm = 1;\nswitch (cmd) {\ncase PIO_FONTX:\ncase GIO_FONTX:\nreturn compat_fontx_ioctl(vc, cmd, up, perm, &op);\ncase KDFONTOP:\nreturn compat_kdfontop_ioctl(up, perm, &op, vc);\ncase PIO_UNIMAP:\ncase GIO_UNIMAP:\nreturn compat_unimap_ioctl(cmd, up, perm, vc);\ncase KIOCSOUND:\ncase KDMKTONE:\n#ifdef CONFIG_X86\ncase KDADDIO:\ncase KDDELIO:\n#endif\ncase KDSETMODE:\ncase KDMAPDISP:\ncase KDUNMAPDISP:\ncase KDSKBMODE:\ncase KDSKBMETA:\ncase KDSKBLED:\ncase KDSETLED:\ncase KDSIGACCEPT:\ncase VT_ACTIVATE:\ncase VT_WAITACTIVE:\ncase VT_RELDISP:\ncase VT_DISALLOCATE:\ncase VT_RESIZE:\ncase VT_RESIZEX:\nreturn vt_ioctl(tty, cmd, arg);\ndefault:\nreturn vt_ioctl(tty, cmd, (unsigned long)up);\n}\n}\n"
    }
  ],
  "CVE-2021-33909": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper size validation for buffer allocations in the code.",
        "trigger_condition": "An unprivileged user can trigger an integer overflow by requesting a large buffer allocation, leading to an Out-of-bounds Write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the size of the buffer being allocated, allowing an unprivileged user to request a buffer size that exceeds the system's capacity, leading to an integer overflow and potential escalation to root."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a size check before allocating memory for the buffer. By verifying that the size does not exceed a predefined maximum value, the code prevents the integer overflow vulnerability. In this case, the solution involves adding a check to ensure that the buffer size requested by the user is within a safe range before proceeding with memory allocation.",
      "GPT_analysis": "The modification is necessary to prevent the vulnerability described in CVE-2021-33909. By adding the check `if (unlikely(size > MAX_RW_COUNT))`, the code ensures that the size of the buffer being allocated does not exceed a certain maximum value (`MAX_RW_COUNT`). This check helps to prevent an integer overflow vulnerability that could lead to an Out-of-bounds Write and potential escalation to root by an unprivileged user.\n\nBy verifying that the size of the buffer is within a safe range before allocating memory for it, the modified code mitigates the risk of the vulnerability being exploited. This proactive measure helps to enhance the security and stability of the system by preventing malicious actors from manipulating the allocation process to gain unauthorized access or cause system instability.",
      "GPT_purpose": "Allocate a sequence buffer with a specified size using kvmalloc.",
      "GPT_function": "\n1. Allocate memory for a sequence buffer.\n2. Use kvmalloc to allocate memory with the GFP_KERNEL_ACCOUNT flag.",
      "CVE_id": "CVE-2021-33909",
      "code_before_change": "static void *seq_buf_alloc(unsigned long size)\n{\n\treturn kvmalloc(size, GFP_KERNEL_ACCOUNT);\n}",
      "code_after_change": "static void *seq_buf_alloc(unsigned long size)\n{\n\tif (unlikely(size > MAX_RW_COUNT))\n\t\treturn NULL;\n\n\treturn kvmalloc(size, GFP_KERNEL_ACCOUNT);\n}",
      "modified_lines": {
        "added": [
          "\tif (unlikely(size > MAX_RW_COUNT))",
          "\t\treturn NULL;",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper size validation for buffer allocations in the code.",
      "trigger_condition": "An unprivileged user can trigger an integer overflow by requesting a large buffer allocation, leading to an Out-of-bounds Write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the size of the buffer being allocated, allowing an unprivileged user to request a buffer size that exceeds the system's capacity, leading to an integer overflow and potential escalation to root.",
      "id": 110,
      "code_after_change_normalized": "static void *FUN1(unsigned long VAR1)\n{\nif (FUN2(VAR1 > VAR2))\nreturn NULL;\nreturn FUN3(VAR1, VAR3);\n}\n",
      "code_before_change_normalized": "static void *FUN1(unsigned long VAR1)\n{\nreturn FUN2(VAR1, VAR2);\n}\n",
      "code_after_change_raw": "static void *seq_buf_alloc(unsigned long size)\n{\nif (unlikely(size > MAX_RW_COUNT))\nreturn NULL;\nreturn kvmalloc(size, GFP_KERNEL_ACCOUNT);\n}\n",
      "code_before_change_raw": "static void *seq_buf_alloc(unsigned long size)\n{\nreturn kvmalloc(size, GFP_KERNEL_ACCOUNT);\n}\n"
    }
  ],
  "CVE-2021-3489": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper size validation for allocated buffers in the eBPF RINGBUF function.",
        "trigger_condition": "Allocated size exceeding the ringbuf size, leading to out-of-bounds writes within the kernel.",
        "specific_code_behavior_causing_vulnerability": "The code does not check that the allocated size is smaller than the ringbuf size, allowing the new_prod_pos value to exceed the ringbuf size and potentially write beyond the bounds of the ring buffer."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the allocated size does not exceed the ringbuf size. This prevents out-of-bounds writes and potential exploitation of the vulnerability. In this case, the solution involves adding a check to verify that the allocated size (len) is not greater than the ringbuf size (rb->mask) before proceeding with buffer reservation.",
      "GPT_analysis": "The modification in the code snippet is necessary to fix the vulnerability CVE-2021-3489, which allowed an attacker to perform out-of-bounds writes within the kernel and potentially achieve arbitrary code execution. \n\nThe vulnerability occurred because the original code did not properly check that the allocated size was smaller than the ringbuf size, which could lead to the new_prod_pos value exceeding the ringbuf size (rb->mask). This allowed an attacker to write beyond the bounds of the ring buffer, leading to potential security risks.\n\nBy adding the check `if (len > rb->mask + 1) return NULL;`, the modified code ensures that the allocated size (len) does not exceed the ringbuf size, preventing out-of-bounds writes and mitigating the vulnerability. This check ensures that the new_prod_pos value does not go beyond the ring buffer's boundaries, thus preventing potential exploitation of the vulnerability.",
      "GPT_purpose": "Allocate space in an eBPF RINGBUF for a new record while ensuring it does not exceed the ringbuf size.",
      "GPT_function": "\n1. Reserve space in the eBPF RINGBUF.\n2. Check for out-of-bounds writes within the ringbuf.\n3. Update producer position and header information.",
      "CVE_id": "CVE-2021-3489",
      "code_before_change": "static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size)\n{\n\tunsigned long cons_pos, prod_pos, new_prod_pos, flags;\n\tu32 len, pg_off;\n\tstruct bpf_ringbuf_hdr *hdr;\n\n\tif (unlikely(size > RINGBUF_MAX_RECORD_SZ))\n\t\treturn NULL;\n\n\tlen = round_up(size + BPF_RINGBUF_HDR_SZ, 8);\n\tcons_pos = smp_load_acquire(&rb->consumer_pos);\n\n\tif (in_nmi()) {\n\t\tif (!spin_trylock_irqsave(&rb->spinlock, flags))\n\t\t\treturn NULL;\n\t} else {\n\t\tspin_lock_irqsave(&rb->spinlock, flags);\n\t}\n\n\tprod_pos = rb->producer_pos;\n\tnew_prod_pos = prod_pos + len;\n\n\t/* check for out of ringbuf space by ensuring producer position\n\t * doesn't advance more than (ringbuf_size - 1) ahead\n\t */\n\tif (new_prod_pos - cons_pos > rb->mask) {\n\t\tspin_unlock_irqrestore(&rb->spinlock, flags);\n\t\treturn NULL;\n\t}\n\n\thdr = (void *)rb->data + (prod_pos & rb->mask);\n\tpg_off = bpf_ringbuf_rec_pg_off(rb, hdr);\n\thdr->len = size | BPF_RINGBUF_BUSY_BIT;\n\thdr->pg_off = pg_off;\n\n\t/* pairs with consumer's smp_load_acquire() */\n\tsmp_store_release(&rb->producer_pos, new_prod_pos);\n\n\tspin_unlock_irqrestore(&rb->spinlock, flags);\n\n\treturn (void *)hdr + BPF_RINGBUF_HDR_SZ;\n}",
      "code_after_change": "static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size)\n{\n\tunsigned long cons_pos, prod_pos, new_prod_pos, flags;\n\tu32 len, pg_off;\n\tstruct bpf_ringbuf_hdr *hdr;\n\n\tif (unlikely(size > RINGBUF_MAX_RECORD_SZ))\n\t\treturn NULL;\n\n\tlen = round_up(size + BPF_RINGBUF_HDR_SZ, 8);\n\tif (len > rb->mask + 1)\n\t\treturn NULL;\n\n\tcons_pos = smp_load_acquire(&rb->consumer_pos);\n\n\tif (in_nmi()) {\n\t\tif (!spin_trylock_irqsave(&rb->spinlock, flags))\n\t\t\treturn NULL;\n\t} else {\n\t\tspin_lock_irqsave(&rb->spinlock, flags);\n\t}\n\n\tprod_pos = rb->producer_pos;\n\tnew_prod_pos = prod_pos + len;\n\n\t/* check for out of ringbuf space by ensuring producer position\n\t * doesn't advance more than (ringbuf_size - 1) ahead\n\t */\n\tif (new_prod_pos - cons_pos > rb->mask) {\n\t\tspin_unlock_irqrestore(&rb->spinlock, flags);\n\t\treturn NULL;\n\t}\n\n\thdr = (void *)rb->data + (prod_pos & rb->mask);\n\tpg_off = bpf_ringbuf_rec_pg_off(rb, hdr);\n\thdr->len = size | BPF_RINGBUF_BUSY_BIT;\n\thdr->pg_off = pg_off;\n\n\t/* pairs with consumer's smp_load_acquire() */\n\tsmp_store_release(&rb->producer_pos, new_prod_pos);\n\n\tspin_unlock_irqrestore(&rb->spinlock, flags);\n\n\treturn (void *)hdr + BPF_RINGBUF_HDR_SZ;\n}",
      "modified_lines": {
        "added": [
          "\tif (len > rb->mask + 1)",
          "\t\treturn NULL;",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper size validation for allocated buffers in the eBPF RINGBUF function.",
      "trigger_condition": "Allocated size exceeding the ringbuf size, leading to out-of-bounds writes within the kernel.",
      "specific_code_behavior_causing_vulnerability": "The code does not check that the allocated size is smaller than the ringbuf size, allowing the new_prod_pos value to exceed the ringbuf size and potentially write beyond the bounds of the ring buffer.",
      "id": 111,
      "code_after_change_normalized": "static void *FUN1(struct bpf_ringbuf *VAR1, u64 VAR2)\n{\nunsigned long VAR3, VAR4, VAR5, VAR6;\nu32 VAR7, VAR8;\nstruct bpf_ringbuf_hdr *VAR9;\nif (FUN2(VAR2 > VAR10))\nreturn NULL;\nVAR7 = FUN3(VAR2 + VAR11, 8);\nif (VAR7 > VAR1->VAR12 + 1)\nreturn NULL;\nVAR3 = FUN4(&VAR1->VAR13);\nif (FUN5()) {\nif (!FUN6(&VAR1->VAR14, VAR6))\nreturn NULL;\n} else {\nFUN7(&VAR1->VAR14, VAR6);\n}\nVAR4 = VAR1->VAR15;\nVAR5 = VAR4 + VAR7;\nif (VAR5 - VAR3 > VAR1->VAR12) {\nFUN8(&VAR1->VAR14, VAR6);\nreturn NULL;\n}\nVAR9 = (void *)VAR1->VAR16 + (VAR4 & VAR1->VAR12);\nVAR8 = FUN9(VAR1, VAR9);\nVAR9->VAR7 = VAR2 | VAR17;\nVAR9->VAR8 = VAR8;\nFUN10(&VAR1->VAR15, VAR5);\nFUN8(&VAR1->VAR14, VAR6);\nreturn (void *)VAR9 + VAR11;\n}\n",
      "code_before_change_normalized": "static void *FUN1(struct bpf_ringbuf *VAR1, u64 VAR2)\n{\nunsigned long VAR3, VAR4, VAR5, VAR6;\nu32 VAR7, VAR8;\nstruct bpf_ringbuf_hdr *VAR9;\nif (FUN2(VAR2 > VAR10))\nreturn NULL;\nVAR7 = FUN3(VAR2 + VAR11, 8);\nVAR3 = FUN4(&VAR1->VAR12);\nif (FUN5()) {\nif (!FUN6(&VAR1->VAR13, VAR6))\nreturn NULL;\n} else {\nFUN7(&VAR1->VAR13, VAR6);\n}\nVAR4 = VAR1->VAR14;\nVAR5 = VAR4 + VAR7;\nif (VAR5 - VAR3 > VAR1->VAR15) {\nFUN8(&VAR1->VAR13, VAR6);\nreturn NULL;\n}\nVAR9 = (void *)VAR1->VAR16 + (VAR4 & VAR1->VAR15);\nVAR8 = FUN9(VAR1, VAR9);\nVAR9->VAR7 = VAR2 | VAR17;\nVAR9->VAR8 = VAR8;\nFUN10(&VAR1->VAR14, VAR5);\nFUN8(&VAR1->VAR13, VAR6);\nreturn (void *)VAR9 + VAR11;\n}\n",
      "code_after_change_raw": "static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size)\n{\nunsigned long cons_pos, prod_pos, new_prod_pos, flags;\nu32 len, pg_off;\nstruct bpf_ringbuf_hdr *hdr;\nif (unlikely(size > RINGBUF_MAX_RECORD_SZ))\nreturn NULL;\nlen = round_up(size + BPF_RINGBUF_HDR_SZ, 8);\nif (len > rb->mask + 1)\nreturn NULL;\ncons_pos = smp_load_acquire(&rb->consumer_pos);\nif (in_nmi()) {\nif (!spin_trylock_irqsave(&rb->spinlock, flags))\nreturn NULL;\n} else {\nspin_lock_irqsave(&rb->spinlock, flags);\n}\nprod_pos = rb->producer_pos;\nnew_prod_pos = prod_pos + len;\nif (new_prod_pos - cons_pos > rb->mask) {\nspin_unlock_irqrestore(&rb->spinlock, flags);\nreturn NULL;\n}\nhdr = (void *)rb->data + (prod_pos & rb->mask);\npg_off = bpf_ringbuf_rec_pg_off(rb, hdr);\nhdr->len = size | BPF_RINGBUF_BUSY_BIT;\nhdr->pg_off = pg_off;\nsmp_store_release(&rb->producer_pos, new_prod_pos);\nspin_unlock_irqrestore(&rb->spinlock, flags);\nreturn (void *)hdr + BPF_RINGBUF_HDR_SZ;\n}\n",
      "code_before_change_raw": "static void *__bpf_ringbuf_reserve(struct bpf_ringbuf *rb, u64 size)\n{\nunsigned long cons_pos, prod_pos, new_prod_pos, flags;\nu32 len, pg_off;\nstruct bpf_ringbuf_hdr *hdr;\nif (unlikely(size > RINGBUF_MAX_RECORD_SZ))\nreturn NULL;\nlen = round_up(size + BPF_RINGBUF_HDR_SZ, 8);\ncons_pos = smp_load_acquire(&rb->consumer_pos);\nif (in_nmi()) {\nif (!spin_trylock_irqsave(&rb->spinlock, flags))\nreturn NULL;\n} else {\nspin_lock_irqsave(&rb->spinlock, flags);\n}\nprod_pos = rb->producer_pos;\nnew_prod_pos = prod_pos + len;\nif (new_prod_pos - cons_pos > rb->mask) {\nspin_unlock_irqrestore(&rb->spinlock, flags);\nreturn NULL;\n}\nhdr = (void *)rb->data + (prod_pos & rb->mask);\npg_off = bpf_ringbuf_rec_pg_off(rb, hdr);\nhdr->len = size | BPF_RINGBUF_BUSY_BIT;\nhdr->pg_off = pg_off;\nsmp_store_release(&rb->producer_pos, new_prod_pos);\nspin_unlock_irqrestore(&rb->spinlock, flags);\nreturn (void *)hdr + BPF_RINGBUF_HDR_SZ;\n}\n"
    }
  ],
  "CVE-2021-3490": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper updating of 32-bit bounds during bitwise operations in the Linux kernel.",
        "trigger_condition": "The eBPF ALU32 bounds tracking for bitwise ops (AND, OR, and XOR) does not properly update 32-bit bounds, leading to out-of-bounds reads and writes.",
        "specific_code_behavior_causing_vulnerability": "The code fails to correctly update 32-bit bounds during bitwise operations, potentially allowing for out-of-bounds accesses and arbitrary code execution."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper handling of known register states during bitwise operations in the eBPF verifier. Specifically, when both the source and destination registers are known, mark the destination register as known to update the bounds tracking correctly. This prevents potential out-of-bounds accesses and addresses the security risks associated with the vulnerability.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-3490 related to the eBPF ALU32 bounds tracking for bitwise operations in the Linux kernel. Specifically, the vulnerability arises from the improper updating of 32-bit bounds during bitwise operations (AND, OR, and XOR), which could lead to out-of-bounds reads and writes in the kernel, potentially resulting in arbitrary code execution.\n\nThe modification includes the addition of the following line:\n```c\n__mark_reg32_known(dst_reg, var32_off.value);\n```\nThis line is added to properly mark the 32-bit register as known when both the source and destination registers are known. By marking the register as known, it ensures that the bounds tracking is correctly updated, addressing the vulnerability and preventing potential out-of-bounds accesses.\n\nTherefore, the modification is necessary to fix the vulnerability by ensuring proper handling of known register states during bitwise operations in the eBPF verifier, thereby preventing security risks associated with out-of-bounds accesses.",
      "GPT_purpose": "Perform XOR operation on 32-bit values while updating register bounds in eBPF ALU32 bounds tracking.",
      "GPT_function": "\n1. Perform XOR operation on 32-bit registers.\n2. Update the minimum and maximum values of the destination register based on the source register.\n3. Handle cases where both source and destination registers are known constants.",
      "CVE_id": "CVE-2021-3490",
      "code_before_change": "static void scalar32_min_max_xor(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\n\t/* Assuming scalar64_min_max_xor will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get both minimum and maximum from the var32_off. */\n\tdst_reg->u32_min_value = var32_off.value;\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\n\tif (dst_reg->s32_min_value >= 0 && smin_val >= 0) {\n\t\t/* XORing two positive sign numbers gives a positive,\n\t\t * so safe to cast u32 result into s32.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t} else {\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t}\n}",
      "code_after_change": "static void scalar32_min_max_xor(struct bpf_reg_state *dst_reg,\n\t\t\t\t struct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg32_known(dst_reg, var32_off.value);\n\t\treturn;\n\t}\n\n\t/* We get both minimum and maximum from the var32_off. */\n\tdst_reg->u32_min_value = var32_off.value;\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\n\tif (dst_reg->s32_min_value >= 0 && smin_val >= 0) {\n\t\t/* XORing two positive sign numbers gives a positive,\n\t\t * so safe to cast u32 result into s32.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t} else {\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t}\n}",
      "modified_lines": {
        "added": [
          "\tif (src_known && dst_known) {",
          "\t\t__mark_reg32_known(dst_reg, var32_off.value);",
          "\t}"
        ],
        "deleted": [
          "\t/* Assuming scalar64_min_max_xor will be called so it is safe",
          "\t * to skip updating register for known case.",
          "\t */",
          "\tif (src_known && dst_known)"
        ]
      },
      "preconditions_for_vulnerability": "Improper updating of 32-bit bounds during bitwise operations in the Linux kernel.",
      "trigger_condition": "The eBPF ALU32 bounds tracking for bitwise ops (AND, OR, and XOR) does not properly update 32-bit bounds, leading to out-of-bounds reads and writes.",
      "specific_code_behavior_causing_vulnerability": "The code fails to correctly update 32-bit bounds during bitwise operations, potentially allowing for out-of-bounds accesses and arbitrary code execution.",
      "id": 112,
      "code_after_change_normalized": "static void FUN1(struct bpf_reg_state *VAR1,\nstruct bpf_reg_state *VAR2)\n{\nbool VAR3 = FUN2(VAR2->VAR4);\nbool VAR5 = FUN2(VAR1->VAR4);\nstruct tnum VAR6 = FUN3(VAR1->VAR4);\ns32 VAR7 = VAR2->VAR8;\nif (VAR3 && VAR5) {\nFUN4(VAR1, VAR6.VAR9);\nreturn;\n}\nVAR1->VAR10 = VAR6.VAR9;\nVAR1->VAR11 = VAR6.VAR9 | VAR6.VAR12;\nif (VAR1->VAR8 >= 0 && VAR7 >= 0) {\nVAR1->VAR8 = VAR1->VAR10;\nVAR1->VAR13 = VAR1->VAR11;\n} else {\nVAR1->VAR8 = VAR14;\nVAR1->VAR13 = VAR15;\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct bpf_reg_state *VAR1,\nstruct bpf_reg_state *VAR2)\n{\nbool VAR3 = FUN2(VAR2->VAR4);\nbool VAR5 = FUN2(VAR1->VAR4);\nstruct tnum VAR6 = FUN3(VAR1->VAR4);\ns32 VAR7 = VAR2->VAR8;\nif (VAR3 && VAR5)\nreturn;\nVAR1->VAR9 = VAR6.VAR10;\nVAR1->VAR11 = VAR6.VAR10 | VAR6.VAR12;\nif (VAR1->VAR8 >= 0 && VAR7 >= 0) {\nVAR1->VAR8 = VAR1->VAR9;\nVAR1->VAR13 = VAR1->VAR11;\n} else {\nVAR1->VAR8 = VAR14;\nVAR1->VAR13 = VAR15;\n}\n}\n",
      "code_after_change_raw": "static void scalar32_min_max_xor(struct bpf_reg_state *dst_reg,\nstruct bpf_reg_state *src_reg)\n{\nbool src_known = tnum_subreg_is_const(src_reg->var_off);\nbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\nstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\ns32 smin_val = src_reg->s32_min_value;\nif (src_known && dst_known) {\n__mark_reg32_known(dst_reg, var32_off.value);\nreturn;\n}\ndst_reg->u32_min_value = var32_off.value;\ndst_reg->u32_max_value = var32_off.value | var32_off.mask;\nif (dst_reg->s32_min_value >= 0 && smin_val >= 0) {\ndst_reg->s32_min_value = dst_reg->u32_min_value;\ndst_reg->s32_max_value = dst_reg->u32_max_value;\n} else {\ndst_reg->s32_min_value = S32_MIN;\ndst_reg->s32_max_value = S32_MAX;\n}\n}\n",
      "code_before_change_raw": "static void scalar32_min_max_xor(struct bpf_reg_state *dst_reg,\nstruct bpf_reg_state *src_reg)\n{\nbool src_known = tnum_subreg_is_const(src_reg->var_off);\nbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\nstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\ns32 smin_val = src_reg->s32_min_value;\nif (src_known && dst_known)\nreturn;\ndst_reg->u32_min_value = var32_off.value;\ndst_reg->u32_max_value = var32_off.value | var32_off.mask;\nif (dst_reg->s32_min_value >= 0 && smin_val >= 0) {\ndst_reg->s32_min_value = dst_reg->u32_min_value;\ndst_reg->s32_max_value = dst_reg->u32_max_value;\n} else {\ndst_reg->s32_min_value = S32_MIN;\ndst_reg->s32_max_value = S32_MAX;\n}\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper updating of 32-bit bounds during bitwise operations in the Linux kernel.",
        "trigger_condition": "The code fails to accurately track the 32-bit bounds during bitwise OR operations, leading to out-of-bounds reads and writes.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly update the 32-bit bounds of the destination register during bitwise OR operations, potentially allowing for arbitrary code execution due to out-of-bounds accesses."
      },
      "solution": "To mitigate the vulnerability, it is necessary to mark the destination register's 32-bit bounds as known based on the value obtained from the bitwise OR operation. This ensures accurate tracking of the 32-bit values involved in the operation and prevents out-of-bounds accesses. The solution involves adding a line to mark the register as known with the correct value obtained from the operation.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-3490 related to the eBPF ALU32 bounds tracking for bitwise operations in the Linux kernel. The vulnerability arises from the improper updating of 32-bit bounds during bitwise operations (AND, OR, and XOR), which could lead to out-of-bounds reads and writes in the kernel, potentially resulting in arbitrary code execution.\n\nThe modification involves adding the following line:\n```c\n__mark_reg32_known(dst_reg, var32_off.value);\n```\nThis line ensures that the destination register's 32-bit bounds are properly marked as known based on the value obtained from the bitwise OR operation. By marking the register as known, the code can prevent the vulnerability by accurately tracking the bounds of the 32-bit values involved in the operation.\n\nIn summary, the modification is necessary to fix the vulnerability by correctly updating the 32-bit bounds during bitwise OR operations, thus preventing potential out-of-bounds accesses and enhancing the security of the Linux kernel.",
      "GPT_purpose": "Update the minimum and maximum values for a 32-bit register based on bitwise OR operation.",
      "GPT_function": "\n1. Update the minimum and maximum values for a 32-bit register based on bitwise OR operation.\n2. Handle cases where the result of the OR operation involves negative numbers to ensure correct bounds.\n3. Cast the result of the OR operation into a signed 32-bit integer if both operands are positive.",
      "CVE_id": "CVE-2021-3490",
      "code_before_change": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\t/* Assuming scalar64_min_max_or will be called so it is safe\n\t * to skip updating register for known case.\n\t */\n\tif (src_known && dst_known)\n\t\treturn;\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}",
      "code_after_change": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\n\t\t\t\tstruct bpf_reg_state *src_reg)\n{\n\tbool src_known = tnum_subreg_is_const(src_reg->var_off);\n\tbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\n\tstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\n\ts32 smin_val = src_reg->s32_min_value;\n\tu32 umin_val = src_reg->u32_min_value;\n\n\tif (src_known && dst_known) {\n\t\t__mark_reg32_known(dst_reg, var32_off.value);\n\t\treturn;\n\t}\n\n\t/* We get our maximum from the var_off, and our minimum is the\n\t * maximum of the operands' minima\n\t */\n\tdst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\n\tdst_reg->u32_max_value = var32_off.value | var32_off.mask;\n\tif (dst_reg->s32_min_value < 0 || smin_val < 0) {\n\t\t/* Lose signed bounds when ORing negative numbers,\n\t\t * ain't nobody got time for that.\n\t\t */\n\t\tdst_reg->s32_min_value = S32_MIN;\n\t\tdst_reg->s32_max_value = S32_MAX;\n\t} else {\n\t\t/* ORing two positives gives a positive, so safe to\n\t\t * cast result into s64.\n\t\t */\n\t\tdst_reg->s32_min_value = dst_reg->u32_min_value;\n\t\tdst_reg->s32_max_value = dst_reg->u32_max_value;\n\t}\n}",
      "modified_lines": {
        "added": [
          "\tif (src_known && dst_known) {",
          "\t\t__mark_reg32_known(dst_reg, var32_off.value);",
          "\t}"
        ],
        "deleted": [
          "\t/* Assuming scalar64_min_max_or will be called so it is safe",
          "\t * to skip updating register for known case.",
          "\t */",
          "\tif (src_known && dst_known)"
        ]
      },
      "preconditions_for_vulnerability": "Improper updating of 32-bit bounds during bitwise operations in the Linux kernel.",
      "trigger_condition": "The code fails to accurately track the 32-bit bounds during bitwise OR operations, leading to out-of-bounds reads and writes.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly update the 32-bit bounds of the destination register during bitwise OR operations, potentially allowing for arbitrary code execution due to out-of-bounds accesses.",
      "id": 113,
      "code_after_change_normalized": "static void FUN1(struct bpf_reg_state *VAR1,\nstruct bpf_reg_state *VAR2)\n{\nbool VAR3 = FUN2(VAR2->VAR4);\nbool VAR5 = FUN2(VAR1->VAR4);\nstruct tnum VAR6 = FUN3(VAR1->VAR4);\ns32 VAR7 = VAR2->VAR8;\nu32 VAR9 = VAR2->VAR10;\nif (VAR3 && VAR5) {\nFUN4(VAR1, VAR6.VAR11);\nreturn;\n}\nVAR1->VAR10 = FUN5(VAR1->VAR10, VAR9);\nVAR1->VAR12 = VAR6.VAR11 | VAR6.VAR13;\nif (VAR1->VAR8 < 0 || VAR7 < 0) {\nVAR1->VAR8 = VAR14;\nVAR1->VAR15 = VAR16;\n} else {\nVAR1->VAR8 = VAR1->VAR10;\nVAR1->VAR15 = VAR1->VAR12;\n}\n}\n",
      "code_before_change_normalized": "static void FUN1(struct bpf_reg_state *VAR1,\nstruct bpf_reg_state *VAR2)\n{\nbool VAR3 = FUN2(VAR2->VAR4);\nbool VAR5 = FUN2(VAR1->VAR4);\nstruct tnum VAR6 = FUN3(VAR1->VAR4);\ns32 VAR7 = VAR2->VAR8;\nu32 VAR9 = VAR2->VAR10;\nif (VAR3 && VAR5)\nreturn;\nVAR1->VAR10 = FUN4(VAR1->VAR10, VAR9);\nVAR1->VAR11 = VAR6.VAR12 | VAR6.VAR13;\nif (VAR1->VAR8 < 0 || VAR7 < 0) {\nVAR1->VAR8 = VAR14;\nVAR1->VAR15 = VAR16;\n} else {\nVAR1->VAR8 = VAR1->VAR10;\nVAR1->VAR15 = VAR1->VAR11;\n}\n}\n",
      "code_after_change_raw": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\nstruct bpf_reg_state *src_reg)\n{\nbool src_known = tnum_subreg_is_const(src_reg->var_off);\nbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\nstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\ns32 smin_val = src_reg->s32_min_value;\nu32 umin_val = src_reg->u32_min_value;\nif (src_known && dst_known) {\n__mark_reg32_known(dst_reg, var32_off.value);\nreturn;\n}\ndst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\ndst_reg->u32_max_value = var32_off.value | var32_off.mask;\nif (dst_reg->s32_min_value < 0 || smin_val < 0) {\ndst_reg->s32_min_value = S32_MIN;\ndst_reg->s32_max_value = S32_MAX;\n} else {\ndst_reg->s32_min_value = dst_reg->u32_min_value;\ndst_reg->s32_max_value = dst_reg->u32_max_value;\n}\n}\n",
      "code_before_change_raw": "static void scalar32_min_max_or(struct bpf_reg_state *dst_reg,\nstruct bpf_reg_state *src_reg)\n{\nbool src_known = tnum_subreg_is_const(src_reg->var_off);\nbool dst_known = tnum_subreg_is_const(dst_reg->var_off);\nstruct tnum var32_off = tnum_subreg(dst_reg->var_off);\ns32 smin_val = src_reg->s32_min_value;\nu32 umin_val = src_reg->u32_min_value;\nif (src_known && dst_known)\nreturn;\ndst_reg->u32_min_value = max(dst_reg->u32_min_value, umin_val);\ndst_reg->u32_max_value = var32_off.value | var32_off.mask;\nif (dst_reg->s32_min_value < 0 || smin_val < 0) {\ndst_reg->s32_min_value = S32_MIN;\ndst_reg->s32_max_value = S32_MAX;\n} else {\ndst_reg->s32_min_value = dst_reg->u32_min_value;\ndst_reg->s32_max_value = dst_reg->u32_max_value;\n}\n}\n"
    }
  ],
  "CVE-2021-3491": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and enforcement of buffer length limits in the PROVIDE_BUFFERS operation within the io_uring subsystem.",
        "trigger_condition": "The MAX_RW_COUNT limit is bypassed, allowing buffer lengths larger than the limit to be used in mem_rw operations, potentially leading to a heap overflow.",
        "specific_code_behavior_causing_vulnerability": "The code does not enforce a maximum buffer length limit, allowing the possibility of buffer lengths exceeding the MAX_RW_COUNT limit, which can result in a heap overflow and potential arbitrary code execution."
      },
      "solution": "To mitigate the vulnerability, it is necessary to enforce a maximum buffer length limit by restricting the buffer length to the minimum of the provided length and MAX_RW_COUNT using the min_t macro. This ensures that buffer lengths do not exceed the specified limit and reduces the risk of heap overflow and arbitrary code execution. In this case, the solution involves modifying the code to limit the buffer length appropriately.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-3491 in the io_uring subsystem of the Linux kernel. The vulnerability allowed the MAX_RW_COUNT limit to be bypassed in the PROVIDE_BUFFERS operation, leading to negative values being used in mem_rw when reading /proc/<PID>/mem. This could result in a heap overflow and potentially allow an attacker to execute arbitrary code in the kernel.\n\nThe modification introduced in the code snippet limits the buffer length to be the minimum of pbuf->len and MAX_RW_COUNT using the min_t macro. By ensuring that the buffer length does not exceed the MAX_RW_COUNT limit, the vulnerability is mitigated, and the potential for heap overflow is reduced.\n\nTherefore, the modification is necessary to prevent the exploitation of the vulnerability and enhance the security of the io_uring subsystem in the Linux kernel.",
      "GPT_purpose": "Allocate and add buffers to a linked list for input/output operations.",
      "GPT_function": "\n1. Allocate memory for an io_buffer struct.\n2. Initialize the fields of the io_buffer struct with provided values.\n3. Add the io_buffer struct to a linked list.",
      "CVE_id": "CVE-2021-3491",
      "code_before_change": "static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = pbuf->len;\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\n\treturn i ? i : -ENOMEM;\n}",
      "code_after_change": "static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\n\tstruct io_buffer *buf;\n\tu64 addr = pbuf->addr;\n\tint i, bid = pbuf->bid;\n\n\tfor (i = 0; i < pbuf->nbufs; i++) {\n\t\tbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\n\t\tif (!buf)\n\t\t\tbreak;\n\n\t\tbuf->addr = addr;\n\t\tbuf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);\n\t\tbuf->bid = bid;\n\t\taddr += pbuf->len;\n\t\tbid++;\n\t\tif (!*head) {\n\t\t\tINIT_LIST_HEAD(&buf->list);\n\t\t\t*head = buf;\n\t\t} else {\n\t\t\tlist_add_tail(&buf->list, &(*head)->list);\n\t\t}\n\t}\n\n\treturn i ? i : -ENOMEM;\n}",
      "modified_lines": {
        "added": [
          "\t\tbuf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);"
        ],
        "deleted": [
          "\t\tbuf->len = pbuf->len;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and enforcement of buffer length limits in the PROVIDE_BUFFERS operation within the io_uring subsystem.",
      "trigger_condition": "The MAX_RW_COUNT limit is bypassed, allowing buffer lengths larger than the limit to be used in mem_rw operations, potentially leading to a heap overflow.",
      "specific_code_behavior_causing_vulnerability": "The code does not enforce a maximum buffer length limit, allowing the possibility of buffer lengths exceeding the MAX_RW_COUNT limit, which can result in a heap overflow and potential arbitrary code execution.",
      "id": 114,
      "code_after_change_normalized": "static int FUN1(struct io_provide_buf *VAR1, struct io_buffer **VAR2)\n{\nstruct io_buffer *VAR3;\nu64 VAR4 = VAR1->VAR4;\nint VAR5, VAR6 = VAR1->VAR6;\nfor (VAR5 = 0; VAR5 < VAR1->VAR7; VAR5++) {\nVAR3 = FUN2(sizeof(*VAR3), VAR8);\nif (!VAR3)\nbreak;\nVAR3->VAR4 = VAR4;\nVAR3->VAR9 = FUN3(VAR10, VAR1->VAR9, VAR11);\nVAR3->VAR6 = VAR6;\nVAR4 += VAR1->VAR9;\nVAR6++;\nif (!*VAR2) {\nFUN4(&VAR3->VAR12);\n*VAR2 = VAR3;\n} else {\nFUN5(&VAR3->VAR12, &(*VAR2)->VAR12);\n}\n}\nreturn VAR5 ? VAR5 : -VAR13;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct io_provide_buf *VAR1, struct io_buffer **VAR2)\n{\nstruct io_buffer *VAR3;\nu64 VAR4 = VAR1->VAR4;\nint VAR5, VAR6 = VAR1->VAR6;\nfor (VAR5 = 0; VAR5 < VAR1->VAR7; VAR5++) {\nVAR3 = FUN2(sizeof(*VAR3), VAR8);\nif (!VAR3)\nbreak;\nVAR3->VAR4 = VAR4;\nVAR3->VAR9 = VAR1->VAR9;\nVAR3->VAR6 = VAR6;\nVAR4 += VAR1->VAR9;\nVAR6++;\nif (!*VAR2) {\nFUN3(&VAR3->VAR10);\n*VAR2 = VAR3;\n} else {\nFUN4(&VAR3->VAR10, &(*VAR2)->VAR10);\n}\n}\nreturn VAR5 ? VAR5 : -VAR11;\n}\n",
      "code_after_change_raw": "static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\nstruct io_buffer *buf;\nu64 addr = pbuf->addr;\nint i, bid = pbuf->bid;\nfor (i = 0; i < pbuf->nbufs; i++) {\nbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\nif (!buf)\nbreak;\nbuf->addr = addr;\nbuf->len = min_t(__u32, pbuf->len, MAX_RW_COUNT);\nbuf->bid = bid;\naddr += pbuf->len;\nbid++;\nif (!*head) {\nINIT_LIST_HEAD(&buf->list);\n*head = buf;\n} else {\nlist_add_tail(&buf->list, &(*head)->list);\n}\n}\nreturn i ? i : -ENOMEM;\n}\n",
      "code_before_change_raw": "static int io_add_buffers(struct io_provide_buf *pbuf, struct io_buffer **head)\n{\nstruct io_buffer *buf;\nu64 addr = pbuf->addr;\nint i, bid = pbuf->bid;\nfor (i = 0; i < pbuf->nbufs; i++) {\nbuf = kmalloc(sizeof(*buf), GFP_KERNEL);\nif (!buf)\nbreak;\nbuf->addr = addr;\nbuf->len = pbuf->len;\nbuf->bid = bid;\naddr += pbuf->len;\nbid++;\nif (!*head) {\nINIT_LIST_HEAD(&buf->list);\n*head = buf;\n} else {\nlist_add_tail(&buf->list, &(*head)->list);\n}\n}\nreturn i ? i : -ENOMEM;\n}\n"
    }
  ],
  "CVE-2021-3501": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Mapping a user-controlled value to an array index without proper bounds checking.",
        "trigger_condition": "A user process updates the value mapped to the array index at runtime, potentially leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code maps the value of internal.ndata directly to an array index without ensuring it stays within bounds, allowing a user process to manipulate the value and trigger an out-of-bounds write."
      },
      "solution": "To mitigate the vulnerability, it is necessary to introduce a separate variable to properly index the array and ensure it stays within bounds. By using this separate variable to index the array, the code can prevent out-of-bounds writes and enhance the security of the system. In this case, the solution involves adding a new variable to track the index and use it to access the array elements safely.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-3501, which involves an out-of-bounds write due to the mapping of the value of internal.ndata to an array index. By introducing the variable `ndata` and using it to properly index the `internal.data` array, we ensure that the array index stays within bounds and prevent potential out-of-bounds writes that could lead to data integrity and system availability issues. This modification helps to mitigate the security risk posed by the vulnerability and ensures the safe handling of data within the KVM API.",
      "GPT_purpose": "Handle different exit reasons during virtual machine execution in a KVM-based virtualization environment.",
      "GPT_function": "\n1. Handling VM exits in a virtual machine monitor (VMM) for a KVM virtual CPU.\n2. Checking and handling various exit reasons and conditions during VM exits.\n3. Emulating guest state if necessary.\n4. Handling failures during VM entry.\n5. Handling internal errors and unexpected exit reasons.\n6. Invoking specific exit handlers based on the exit reason.\n7. Handling specific exit reasons such as MSR write, preemption timer, interrupt window, external interrupt, HLT, and EPT misconfiguration.",
      "CVE_id": "CVE-2021-3501",
      "code_before_change": "static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunion vmx_exit_reason exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\tu16 exit_handler_index;\n\n\t/*\n\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more\n\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before\n\t * querying dirty_bitmap, we only need to kick all vcpus out of guest\n\t * mode as if vcpus is in root mode, the PML buffer must has been\n\t * flushed already.  Note, PML is never enabled in hardware while\n\t * running L2.\n\t */\n\tif (enable_pml && !is_guest_mode(vcpu))\n\t\tvmx_flush_pml_buffer(vcpu);\n\n\t/*\n\t * We should never reach this point with a pending nested VM-Enter, and\n\t * more specifically emulation of L2 due to invalid guest state (see\n\t * below) should never happen as that means we incorrectly allowed a\n\t * nested VM-Enter with an invalid vmcs12.\n\t */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\t/* If guest state is invalid, start emulating */\n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * PML is never enabled when running L2, bail immediately if a\n\t\t * PML full exit occurs as something is horribly wrong.\n\t\t */\n\t\tif (exit_reason.basic == EXIT_REASON_PML_FULL)\n\t\t\tgoto unexpected_vmexit;\n\n\t\t/*\n\t\t * The host physical addresses of some pages of guest memory\n\t\t * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC\n\t\t * Page). The CPU may write to these pages via their host\n\t\t * physical address while L2 is running, bypassing any\n\t\t * address-translation-based dirty tracking (e.g. EPT write\n\t\t * protection).\n\t\t *\n\t\t * Mark them dirty on every exit from L2 to prevent them from\n\t\t * getting out of sync with dirty tracking.\n\t\t */\n\t\tnested_mark_vmcs12_pages_dirty(vcpu);\n\n\t\tif (nested_vmx_reflect_vmexit(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tif (exit_reason.failed_vmentry) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason.full;\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(vmx->fail)) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Note:\n\t * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by\n\t * delivery event since it indicates guest is accessing MMIO.\n\t * The vm-exit can be triggered again after return to guest that\n\t * will cause infinite loop.\n\t */\n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t    (exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\n\t     exit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\n\t     exit_reason.basic != EXIT_REASON_PML_FULL &&\n\t     exit_reason.basic != EXIT_REASON_APIC_ACCESS &&\n\t     exit_reason.basic != EXIT_REASON_TASK_SWITCH)) {\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.ndata = 3;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason.full;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.ndata++;\n\t\t\tvcpu->run->internal.data[3] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\tvcpu->run->internal.data[vcpu->run->internal.ndata++] =\n\t\t\tvcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (!vmx_interrupt_blocked(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t/*\n\t\t\t * This CPU don't support us in finding the end of an\n\t\t\t * NMI-blocked window if the guest runs with IRQs\n\t\t\t * disabled. So we pull the trigger after 1 s of\n\t\t\t * futile waiting, but inform the user about this.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}\n\n\tif (exit_fastpath != EXIT_FASTPATH_NONE)\n\t\treturn 1;\n\n\tif (exit_reason.basic >= kvm_vmx_max_exit_handlers)\n\t\tgoto unexpected_vmexit;\n#ifdef CONFIG_RETPOLINE\n\tif (exit_reason.basic == EXIT_REASON_MSR_WRITE)\n\t\treturn kvm_emulate_wrmsr(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_PREEMPTION_TIMER)\n\t\treturn handle_preemption_timer(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_INTERRUPT_WINDOW)\n\t\treturn handle_interrupt_window(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)\n\t\treturn handle_external_interrupt(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_HLT)\n\t\treturn kvm_emulate_halt(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG)\n\t\treturn handle_ept_misconfig(vcpu);\n#endif\n\n\texit_handler_index = array_index_nospec((u16)exit_reason.basic,\n\t\t\t\t\t\tkvm_vmx_max_exit_handlers);\n\tif (!kvm_vmx_exit_handlers[exit_handler_index])\n\t\tgoto unexpected_vmexit;\n\n\treturn kvm_vmx_exit_handlers[exit_handler_index](vcpu);\n\nunexpected_vmexit:\n\tvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\n\t\t    exit_reason.full);\n\tdump_vmcs();\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror =\n\t\t\tKVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;\n\tvcpu->run->internal.ndata = 2;\n\tvcpu->run->internal.data[0] = exit_reason.full;\n\tvcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;\n\treturn 0;\n}",
      "code_after_change": "static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\n\tstruct vcpu_vmx *vmx = to_vmx(vcpu);\n\tunion vmx_exit_reason exit_reason = vmx->exit_reason;\n\tu32 vectoring_info = vmx->idt_vectoring_info;\n\tu16 exit_handler_index;\n\n\t/*\n\t * Flush logged GPAs PML buffer, this will make dirty_bitmap more\n\t * updated. Another good is, in kvm_vm_ioctl_get_dirty_log, before\n\t * querying dirty_bitmap, we only need to kick all vcpus out of guest\n\t * mode as if vcpus is in root mode, the PML buffer must has been\n\t * flushed already.  Note, PML is never enabled in hardware while\n\t * running L2.\n\t */\n\tif (enable_pml && !is_guest_mode(vcpu))\n\t\tvmx_flush_pml_buffer(vcpu);\n\n\t/*\n\t * We should never reach this point with a pending nested VM-Enter, and\n\t * more specifically emulation of L2 due to invalid guest state (see\n\t * below) should never happen as that means we incorrectly allowed a\n\t * nested VM-Enter with an invalid vmcs12.\n\t */\n\tWARN_ON_ONCE(vmx->nested.nested_run_pending);\n\n\t/* If guest state is invalid, start emulating */\n\tif (vmx->emulation_required)\n\t\treturn handle_invalid_guest_state(vcpu);\n\n\tif (is_guest_mode(vcpu)) {\n\t\t/*\n\t\t * PML is never enabled when running L2, bail immediately if a\n\t\t * PML full exit occurs as something is horribly wrong.\n\t\t */\n\t\tif (exit_reason.basic == EXIT_REASON_PML_FULL)\n\t\t\tgoto unexpected_vmexit;\n\n\t\t/*\n\t\t * The host physical addresses of some pages of guest memory\n\t\t * are loaded into the vmcs02 (e.g. vmcs12's Virtual APIC\n\t\t * Page). The CPU may write to these pages via their host\n\t\t * physical address while L2 is running, bypassing any\n\t\t * address-translation-based dirty tracking (e.g. EPT write\n\t\t * protection).\n\t\t *\n\t\t * Mark them dirty on every exit from L2 to prevent them from\n\t\t * getting out of sync with dirty tracking.\n\t\t */\n\t\tnested_mark_vmcs12_pages_dirty(vcpu);\n\n\t\tif (nested_vmx_reflect_vmexit(vcpu))\n\t\t\treturn 1;\n\t}\n\n\tif (exit_reason.failed_vmentry) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= exit_reason.full;\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(vmx->fail)) {\n\t\tdump_vmcs();\n\t\tvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\n\t\tvcpu->run->fail_entry.hardware_entry_failure_reason\n\t\t\t= vmcs_read32(VM_INSTRUCTION_ERROR);\n\t\tvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\n\t\treturn 0;\n\t}\n\n\t/*\n\t * Note:\n\t * Do not try to fix EXIT_REASON_EPT_MISCONFIG if it caused by\n\t * delivery event since it indicates guest is accessing MMIO.\n\t * The vm-exit can be triggered again after return to guest that\n\t * will cause infinite loop.\n\t */\n\tif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n\t    (exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\n\t     exit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\n\t     exit_reason.basic != EXIT_REASON_PML_FULL &&\n\t     exit_reason.basic != EXIT_REASON_APIC_ACCESS &&\n\t     exit_reason.basic != EXIT_REASON_TASK_SWITCH)) {\n\t\tint ndata = 3;\n\n\t\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\t\tvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\n\t\tvcpu->run->internal.data[0] = vectoring_info;\n\t\tvcpu->run->internal.data[1] = exit_reason.full;\n\t\tvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\n\t\tif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\n\t\t\tvcpu->run->internal.data[ndata++] =\n\t\t\t\tvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n\t\t}\n\t\tvcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;\n\t\tvcpu->run->internal.ndata = ndata;\n\t\treturn 0;\n\t}\n\n\tif (unlikely(!enable_vnmi &&\n\t\t     vmx->loaded_vmcs->soft_vnmi_blocked)) {\n\t\tif (!vmx_interrupt_blocked(vcpu)) {\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\n\t\t\t   vcpu->arch.nmi_pending) {\n\t\t\t/*\n\t\t\t * This CPU don't support us in finding the end of an\n\t\t\t * NMI-blocked window if the guest runs with IRQs\n\t\t\t * disabled. So we pull the trigger after 1 s of\n\t\t\t * futile waiting, but inform the user about this.\n\t\t\t */\n\t\t\tprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\t\t\t       \"state on VCPU %d after 1 s timeout\\n\",\n\t\t\t       __func__, vcpu->vcpu_id);\n\t\t\tvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n\t\t}\n\t}\n\n\tif (exit_fastpath != EXIT_FASTPATH_NONE)\n\t\treturn 1;\n\n\tif (exit_reason.basic >= kvm_vmx_max_exit_handlers)\n\t\tgoto unexpected_vmexit;\n#ifdef CONFIG_RETPOLINE\n\tif (exit_reason.basic == EXIT_REASON_MSR_WRITE)\n\t\treturn kvm_emulate_wrmsr(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_PREEMPTION_TIMER)\n\t\treturn handle_preemption_timer(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_INTERRUPT_WINDOW)\n\t\treturn handle_interrupt_window(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)\n\t\treturn handle_external_interrupt(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_HLT)\n\t\treturn kvm_emulate_halt(vcpu);\n\telse if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG)\n\t\treturn handle_ept_misconfig(vcpu);\n#endif\n\n\texit_handler_index = array_index_nospec((u16)exit_reason.basic,\n\t\t\t\t\t\tkvm_vmx_max_exit_handlers);\n\tif (!kvm_vmx_exit_handlers[exit_handler_index])\n\t\tgoto unexpected_vmexit;\n\n\treturn kvm_vmx_exit_handlers[exit_handler_index](vcpu);\n\nunexpected_vmexit:\n\tvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\n\t\t    exit_reason.full);\n\tdump_vmcs();\n\tvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\n\tvcpu->run->internal.suberror =\n\t\t\tKVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;\n\tvcpu->run->internal.ndata = 2;\n\tvcpu->run->internal.data[0] = exit_reason.full;\n\tvcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\tint ndata = 3;",
          "",
          "\t\t\tvcpu->run->internal.data[ndata++] =",
          "\t\tvcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;",
          "\t\tvcpu->run->internal.ndata = ndata;"
        ],
        "deleted": [
          "\t\tvcpu->run->internal.ndata = 3;",
          "\t\t\tvcpu->run->internal.ndata++;",
          "\t\t\tvcpu->run->internal.data[3] =",
          "\t\tvcpu->run->internal.data[vcpu->run->internal.ndata++] =",
          "\t\t\tvcpu->arch.last_vmentry_cpu;"
        ]
      },
      "preconditions_for_vulnerability": "Mapping a user-controlled value to an array index without proper bounds checking.",
      "trigger_condition": "A user process updates the value mapped to the array index at runtime, potentially leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code maps the value of internal.ndata directly to an array index without ensuring it stays within bounds, allowing a user process to manipulate the value and trigger an out-of-bounds write.",
      "id": 115,
      "code_after_change_normalized": "static int FUN1(struct kvm_vcpu *VAR1, fastpath_t VAR2)\n{\nstruct vcpu_vmx *VAR3 = FUN2(VAR1);\nunion vmx_exit_reason VAR4 = VAR3->VAR4;\nu32 VAR5 = VAR3->VAR6;\nu16 VAR7;\nif (VAR8 && !FUN3(VAR1))\nFUN4(VAR1);\nFUN5(VAR3->VAR9.VAR10);\nif (VAR3->VAR11)\nreturn FUN6(VAR1);\nif (FUN3(VAR1)) {\nif (VAR4.VAR12 == VAR13)\ngoto VAR14;\nFUN7(VAR1);\nif (FUN8(VAR1))\nreturn 1;\n}\nif (VAR4.VAR15) {\nFUN9();\nVAR1->VAR16->VAR4 = VAR17;\nVAR1->VAR16->VAR18.VAR19\n= VAR4.VAR20;\nVAR1->VAR16->VAR18.VAR21 = VAR1->VAR22.VAR23;\nreturn 0;\n}\nif (FUN10(VAR3->VAR24)) {\nFUN9();\nVAR1->VAR16->VAR4 = VAR17;\nVAR1->VAR16->VAR18.VAR19\n= FUN11(VAR25);\nVAR1->VAR16->VAR18.VAR21 = VAR1->VAR22.VAR23;\nreturn 0;\n}\nif ((VAR5 & VAR26) &&\n(VAR4.VAR12 != VAR27 &&\nVAR4.VAR12 != VAR28 &&\nVAR4.VAR12 != VAR13 &&\nVAR4.VAR12 != VAR29 &&\nVAR4.VAR12 != VAR30)) {\nint VAR31 = 3;\nVAR1->VAR16->VAR4 = VAR32;\nVAR1->VAR16->VAR33.VAR34 = VAR35;\nVAR1->VAR16->VAR33.VAR36[0] = VAR5;\nVAR1->VAR16->VAR33.VAR36[1] = VAR4.VAR20;\nVAR1->VAR16->VAR33.VAR36[2] = VAR1->VAR22.VAR37;\nif (VAR4.VAR12 == VAR38) {\nVAR1->VAR16->VAR33.VAR36[VAR31++] =\nFUN12(VAR39);\n}\nVAR1->VAR16->VAR33.VAR36[VAR31++] = VAR1->VAR22.VAR23;\nVAR1->VAR16->VAR33.VAR31 = VAR31;\nreturn 0;\n}\nif (FUN10(!VAR40 &&\nVAR3->VAR41->VAR42)) {\nif (!FUN13(VAR1)) {\nVAR3->VAR41->VAR42 = 0;\n} else if (VAR3->VAR41->VAR43 > 1000000000LL &&\nVAR1->VAR22.VAR44) {\nFUN14(VAR45 \"STR\"\n\"STR\",\nVAR46, VAR1->VAR47);\nVAR3->VAR41->VAR42 = 0;\n}\n}\nif (VAR2 != VAR48)\nreturn 1;\nif (VAR4.VAR12 >= VAR49)\ngoto VAR14;\n#ifdef VAR50\nif (VAR4.VAR12 == VAR51)\nreturn FUN15(VAR1);\nelse if (VAR4.VAR12 == VAR52)\nreturn FUN16(VAR1);\nelse if (VAR4.VAR12 == VAR53)\nreturn FUN17(VAR1);\nelse if (VAR4.VAR12 == VAR54)\nreturn FUN18(VAR1);\nelse if (VAR4.VAR12 == VAR55)\nreturn FUN19(VAR1);\nelse if (VAR4.VAR12 == VAR38)\nreturn FUN20(VAR1);\n#VAR56\nVAR7 = FUN21((VAR57)VAR4.VAR12,\nVAR49);\nif (!VAR58[VAR7])\ngoto VAR14;\nreturn VAR58[VAR7](VAR1);\nVAR14:\nFUN22(VAR1, \"STR\",\nVAR4.VAR20);\nFUN9();\nVAR1->VAR16->VAR4 = VAR32;\nVAR1->VAR16->VAR33.VAR34 =\nVAR59;\nVAR1->VAR16->VAR33.VAR31 = 2;\nVAR1->VAR16->VAR33.VAR36[0] = VAR4.VAR20;\nVAR1->VAR16->VAR33.VAR36[1] = VAR1->VAR22.VAR23;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct kvm_vcpu *VAR1, fastpath_t VAR2)\n{\nstruct vcpu_vmx *VAR3 = FUN2(VAR1);\nunion vmx_exit_reason VAR4 = VAR3->VAR4;\nu32 VAR5 = VAR3->VAR6;\nu16 VAR7;\nif (VAR8 && !FUN3(VAR1))\nFUN4(VAR1);\nFUN5(VAR3->VAR9.VAR10);\nif (VAR3->VAR11)\nreturn FUN6(VAR1);\nif (FUN3(VAR1)) {\nif (VAR4.VAR12 == VAR13)\ngoto VAR14;\nFUN7(VAR1);\nif (FUN8(VAR1))\nreturn 1;\n}\nif (VAR4.VAR15) {\nFUN9();\nVAR1->VAR16->VAR4 = VAR17;\nVAR1->VAR16->VAR18.VAR19\n= VAR4.VAR20;\nVAR1->VAR16->VAR18.VAR21 = VAR1->VAR22.VAR23;\nreturn 0;\n}\nif (FUN10(VAR3->VAR24)) {\nFUN9();\nVAR1->VAR16->VAR4 = VAR17;\nVAR1->VAR16->VAR18.VAR19\n= FUN11(VAR25);\nVAR1->VAR16->VAR18.VAR21 = VAR1->VAR22.VAR23;\nreturn 0;\n}\nif ((VAR5 & VAR26) &&\n(VAR4.VAR12 != VAR27 &&\nVAR4.VAR12 != VAR28 &&\nVAR4.VAR12 != VAR13 &&\nVAR4.VAR12 != VAR29 &&\nVAR4.VAR12 != VAR30)) {\nVAR1->VAR16->VAR4 = VAR31;\nVAR1->VAR16->VAR32.VAR33 = VAR34;\nVAR1->VAR16->VAR32.VAR35 = 3;\nVAR1->VAR16->VAR32.VAR36[0] = VAR5;\nVAR1->VAR16->VAR32.VAR36[1] = VAR4.VAR20;\nVAR1->VAR16->VAR32.VAR36[2] = VAR1->VAR22.VAR37;\nif (VAR4.VAR12 == VAR38) {\nVAR1->VAR16->VAR32.VAR35++;\nVAR1->VAR16->VAR32.VAR36[3] =\nFUN12(VAR39);\n}\nVAR1->VAR16->VAR32.VAR36[VAR1->VAR16->VAR32.VAR35++] =\nVAR1->VAR22.VAR23;\nreturn 0;\n}\nif (FUN10(!VAR40 &&\nVAR3->VAR41->VAR42)) {\nif (!FUN13(VAR1)) {\nVAR3->VAR41->VAR42 = 0;\n} else if (VAR3->VAR41->VAR43 > 1000000000LL &&\nVAR1->VAR22.VAR44) {\nFUN14(VAR45 \"STR\"\n\"STR\",\nVAR46, VAR1->VAR47);\nVAR3->VAR41->VAR42 = 0;\n}\n}\nif (VAR2 != VAR48)\nreturn 1;\nif (VAR4.VAR12 >= VAR49)\ngoto VAR14;\n#ifdef VAR50\nif (VAR4.VAR12 == VAR51)\nreturn FUN15(VAR1);\nelse if (VAR4.VAR12 == VAR52)\nreturn FUN16(VAR1);\nelse if (VAR4.VAR12 == VAR53)\nreturn FUN17(VAR1);\nelse if (VAR4.VAR12 == VAR54)\nreturn FUN18(VAR1);\nelse if (VAR4.VAR12 == VAR55)\nreturn FUN19(VAR1);\nelse if (VAR4.VAR12 == VAR38)\nreturn FUN20(VAR1);\n#VAR56\nVAR7 = FUN21((VAR57)VAR4.VAR12,\nVAR49);\nif (!VAR58[VAR7])\ngoto VAR14;\nreturn VAR58[VAR7](VAR1);\nVAR14:\nFUN22(VAR1, \"STR\",\nVAR4.VAR20);\nFUN9();\nVAR1->VAR16->VAR4 = VAR31;\nVAR1->VAR16->VAR32.VAR33 =\nVAR59;\nVAR1->VAR16->VAR32.VAR35 = 2;\nVAR1->VAR16->VAR32.VAR36[0] = VAR4.VAR20;\nVAR1->VAR16->VAR32.VAR36[1] = VAR1->VAR22.VAR23;\nreturn 0;\n}\n",
      "code_after_change_raw": "static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\nstruct vcpu_vmx *vmx = to_vmx(vcpu);\nunion vmx_exit_reason exit_reason = vmx->exit_reason;\nu32 vectoring_info = vmx->idt_vectoring_info;\nu16 exit_handler_index;\nif (enable_pml && !is_guest_mode(vcpu))\nvmx_flush_pml_buffer(vcpu);\nWARN_ON_ONCE(vmx->nested.nested_run_pending);\nif (vmx->emulation_required)\nreturn handle_invalid_guest_state(vcpu);\nif (is_guest_mode(vcpu)) {\nif (exit_reason.basic == EXIT_REASON_PML_FULL)\ngoto unexpected_vmexit;\nnested_mark_vmcs12_pages_dirty(vcpu);\nif (nested_vmx_reflect_vmexit(vcpu))\nreturn 1;\n}\nif (exit_reason.failed_vmentry) {\ndump_vmcs();\nvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\nvcpu->run->fail_entry.hardware_entry_failure_reason\n= exit_reason.full;\nvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\nif (unlikely(vmx->fail)) {\ndump_vmcs();\nvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\nvcpu->run->fail_entry.hardware_entry_failure_reason\n= vmcs_read32(VM_INSTRUCTION_ERROR);\nvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\nif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n(exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\nexit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\nexit_reason.basic != EXIT_REASON_PML_FULL &&\nexit_reason.basic != EXIT_REASON_APIC_ACCESS &&\nexit_reason.basic != EXIT_REASON_TASK_SWITCH)) {\nint ndata = 3;\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\nvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\nvcpu->run->internal.data[0] = vectoring_info;\nvcpu->run->internal.data[1] = exit_reason.full;\nvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\nif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\nvcpu->run->internal.data[ndata++] =\nvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n}\nvcpu->run->internal.data[ndata++] = vcpu->arch.last_vmentry_cpu;\nvcpu->run->internal.ndata = ndata;\nreturn 0;\n}\nif (unlikely(!enable_vnmi &&\nvmx->loaded_vmcs->soft_vnmi_blocked)) {\nif (!vmx_interrupt_blocked(vcpu)) {\nvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\nvcpu->arch.nmi_pending) {\nprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\"state on VCPU %d after 1 s timeout\\n\",\n__func__, vcpu->vcpu_id);\nvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n}\n}\nif (exit_fastpath != EXIT_FASTPATH_NONE)\nreturn 1;\nif (exit_reason.basic >= kvm_vmx_max_exit_handlers)\ngoto unexpected_vmexit;\n#ifdef CONFIG_RETPOLINE\nif (exit_reason.basic == EXIT_REASON_MSR_WRITE)\nreturn kvm_emulate_wrmsr(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_PREEMPTION_TIMER)\nreturn handle_preemption_timer(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_INTERRUPT_WINDOW)\nreturn handle_interrupt_window(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)\nreturn handle_external_interrupt(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_HLT)\nreturn kvm_emulate_halt(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG)\nreturn handle_ept_misconfig(vcpu);\n#endif\nexit_handler_index = array_index_nospec((u16)exit_reason.basic,\nkvm_vmx_max_exit_handlers);\nif (!kvm_vmx_exit_handlers[exit_handler_index])\ngoto unexpected_vmexit;\nreturn kvm_vmx_exit_handlers[exit_handler_index](vcpu);\nunexpected_vmexit:\nvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\nexit_reason.full);\ndump_vmcs();\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\nvcpu->run->internal.suberror =\nKVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;\nvcpu->run->internal.ndata = 2;\nvcpu->run->internal.data[0] = exit_reason.full;\nvcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int __vmx_handle_exit(struct kvm_vcpu *vcpu, fastpath_t exit_fastpath)\n{\nstruct vcpu_vmx *vmx = to_vmx(vcpu);\nunion vmx_exit_reason exit_reason = vmx->exit_reason;\nu32 vectoring_info = vmx->idt_vectoring_info;\nu16 exit_handler_index;\nif (enable_pml && !is_guest_mode(vcpu))\nvmx_flush_pml_buffer(vcpu);\nWARN_ON_ONCE(vmx->nested.nested_run_pending);\nif (vmx->emulation_required)\nreturn handle_invalid_guest_state(vcpu);\nif (is_guest_mode(vcpu)) {\nif (exit_reason.basic == EXIT_REASON_PML_FULL)\ngoto unexpected_vmexit;\nnested_mark_vmcs12_pages_dirty(vcpu);\nif (nested_vmx_reflect_vmexit(vcpu))\nreturn 1;\n}\nif (exit_reason.failed_vmentry) {\ndump_vmcs();\nvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\nvcpu->run->fail_entry.hardware_entry_failure_reason\n= exit_reason.full;\nvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\nif (unlikely(vmx->fail)) {\ndump_vmcs();\nvcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;\nvcpu->run->fail_entry.hardware_entry_failure_reason\n= vmcs_read32(VM_INSTRUCTION_ERROR);\nvcpu->run->fail_entry.cpu = vcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\nif ((vectoring_info & VECTORING_INFO_VALID_MASK) &&\n(exit_reason.basic != EXIT_REASON_EXCEPTION_NMI &&\nexit_reason.basic != EXIT_REASON_EPT_VIOLATION &&\nexit_reason.basic != EXIT_REASON_PML_FULL &&\nexit_reason.basic != EXIT_REASON_APIC_ACCESS &&\nexit_reason.basic != EXIT_REASON_TASK_SWITCH)) {\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\nvcpu->run->internal.suberror = KVM_INTERNAL_ERROR_DELIVERY_EV;\nvcpu->run->internal.ndata = 3;\nvcpu->run->internal.data[0] = vectoring_info;\nvcpu->run->internal.data[1] = exit_reason.full;\nvcpu->run->internal.data[2] = vcpu->arch.exit_qualification;\nif (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG) {\nvcpu->run->internal.ndata++;\nvcpu->run->internal.data[3] =\nvmcs_read64(GUEST_PHYSICAL_ADDRESS);\n}\nvcpu->run->internal.data[vcpu->run->internal.ndata++] =\nvcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\nif (unlikely(!enable_vnmi &&\nvmx->loaded_vmcs->soft_vnmi_blocked)) {\nif (!vmx_interrupt_blocked(vcpu)) {\nvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n} else if (vmx->loaded_vmcs->vnmi_blocked_time > 1000000000LL &&\nvcpu->arch.nmi_pending) {\nprintk(KERN_WARNING \"%s: Breaking out of NMI-blocked \"\n\"state on VCPU %d after 1 s timeout\\n\",\n__func__, vcpu->vcpu_id);\nvmx->loaded_vmcs->soft_vnmi_blocked = 0;\n}\n}\nif (exit_fastpath != EXIT_FASTPATH_NONE)\nreturn 1;\nif (exit_reason.basic >= kvm_vmx_max_exit_handlers)\ngoto unexpected_vmexit;\n#ifdef CONFIG_RETPOLINE\nif (exit_reason.basic == EXIT_REASON_MSR_WRITE)\nreturn kvm_emulate_wrmsr(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_PREEMPTION_TIMER)\nreturn handle_preemption_timer(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_INTERRUPT_WINDOW)\nreturn handle_interrupt_window(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_EXTERNAL_INTERRUPT)\nreturn handle_external_interrupt(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_HLT)\nreturn kvm_emulate_halt(vcpu);\nelse if (exit_reason.basic == EXIT_REASON_EPT_MISCONFIG)\nreturn handle_ept_misconfig(vcpu);\n#endif\nexit_handler_index = array_index_nospec((u16)exit_reason.basic,\nkvm_vmx_max_exit_handlers);\nif (!kvm_vmx_exit_handlers[exit_handler_index])\ngoto unexpected_vmexit;\nreturn kvm_vmx_exit_handlers[exit_handler_index](vcpu);\nunexpected_vmexit:\nvcpu_unimpl(vcpu, \"vmx: unexpected exit reason 0x%x\\n\",\nexit_reason.full);\ndump_vmcs();\nvcpu->run->exit_reason = KVM_EXIT_INTERNAL_ERROR;\nvcpu->run->internal.suberror =\nKVM_INTERNAL_ERROR_UNEXPECTED_EXIT_REASON;\nvcpu->run->internal.ndata = 2;\nvcpu->run->internal.data[0] = exit_reason.full;\nvcpu->run->internal.data[1] = vcpu->arch.last_vmentry_cpu;\nreturn 0;\n}\n"
    }
  ],
  "CVE-2021-3612": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper input validation and boundary checks when handling user input data in a specific ioctl function.",
        "trigger_condition": "A local user invokes the vulnerable ioctl function with crafted input data that can lead to out-of-bounds memory writes.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the input data properly, allowing for potential buffer overflows and out-of-bounds memory writes. Additionally, the loop iterating over the input data does not have correct boundary conditions, leading to possible memory corruption."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper input validation and boundary checks when handling user input data in the vulnerable ioctl function. This involves checking the length of the input data to prevent buffer overflows, validating the input values to ensure they fall within expected ranges, and setting correct loop boundaries to avoid out-of-bounds memory writes. In this case, the solution includes adding checks for the input data length, validating key values, and setting appropriate loop conditions to iterate over the input data safely.",
      "GPT_analysis": "The modification to the code snippet is necessary to address the out-of-bounds memory write vulnerability (CVE-2021-3612) present in the original code. Here's why the modification is needed:\n\n1. **Check for Proper Length**: The addition of the check `if (len % sizeof(*keypam)) return -EINVAL;` ensures that the length of the input data is a multiple of the size of the data type being used (`__u16`). This check helps prevent potential buffer overflows that could occur if the length is not properly aligned.\n\n2. **Correct Loop Boundary**: The loop condition `for (i = 0; i < (len / 2) && i < joydev->nkey; i++)` ensures that the loop iterates up to the minimum of half the length of the input data and the number of keys (`joydev->nkey`). This prevents iterating beyond the bounds of the input data and the key array, thus avoiding potential out-of-bounds writes.\n\n3. **Properly Validate Input**: By checking `if (keypam[i] > KEY_MAX || keypam[i] < BTN_MISC)`, the modification ensures that each key value falls within the expected range. This validation helps prevent invalid key values from being used to access memory outside the allowed boundaries.\n\n4. **Prevent Memory Corruption**: The changes made in the modified code snippet help in preventing memory corruption issues that could lead to system crashes or privilege escalation by ensuring that memory accesses are within the bounds of allocated memory regions.\n\nBy incorporating these modifications, the code becomes more robust and secure, mitigating the vulnerability and reducing the risk of exploitation.",
      "GPT_purpose": "Handle the ioctl JSIOCSBTNMAP for a joystick device, validating and copying a button map provided by the user.",
      "GPT_function": "\n1. Handle the JSIOCSBTNMAP ioctl request for a joystick device.\n2. Validate and copy the button map provided by the user.\n3. Update the keymap based on the new button map.",
      "CVE_id": "CVE-2021-3612",
      "code_before_change": "static int joydev_handle_JSIOCSBTNMAP(struct joydev *joydev,\n\t\t\t\t      void __user *argp, size_t len)\n{\n\t__u16 *keypam;\n\tint i;\n\tint retval = 0;\n\n\tlen = min(len, sizeof(joydev->keypam));\n\n\t/* Validate the map. */\n\tkeypam = memdup_user(argp, len);\n\tif (IS_ERR(keypam))\n\t\treturn PTR_ERR(keypam);\n\n\tfor (i = 0; i < joydev->nkey; i++) {\n\t\tif (keypam[i] > KEY_MAX || keypam[i] < BTN_MISC) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmemcpy(joydev->keypam, keypam, len);\n\n\tfor (i = 0; i < joydev->nkey; i++)\n\t\tjoydev->keymap[keypam[i] - BTN_MISC] = i;\n\n out:\n\tkfree(keypam);\n\treturn retval;\n}",
      "code_after_change": "static int joydev_handle_JSIOCSBTNMAP(struct joydev *joydev,\n\t\t\t\t      void __user *argp, size_t len)\n{\n\t__u16 *keypam;\n\tint i;\n\tint retval = 0;\n\n\tif (len % sizeof(*keypam))\n\t\treturn -EINVAL;\n\n\tlen = min(len, sizeof(joydev->keypam));\n\n\t/* Validate the map. */\n\tkeypam = memdup_user(argp, len);\n\tif (IS_ERR(keypam))\n\t\treturn PTR_ERR(keypam);\n\n\tfor (i = 0; i < (len / 2) && i < joydev->nkey; i++) {\n\t\tif (keypam[i] > KEY_MAX || keypam[i] < BTN_MISC) {\n\t\t\tretval = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t}\n\n\tmemcpy(joydev->keypam, keypam, len);\n\n\tfor (i = 0; i < joydev->nkey; i++)\n\t\tjoydev->keymap[keypam[i] - BTN_MISC] = i;\n\n out:\n\tkfree(keypam);\n\treturn retval;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (len % sizeof(*keypam))",
          "\t\treturn -EINVAL;",
          "\tfor (i = 0; i < (len / 2) && i < joydev->nkey; i++) {"
        ],
        "deleted": [
          "\tfor (i = 0; i < joydev->nkey; i++) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper input validation and boundary checks when handling user input data in a specific ioctl function.",
      "trigger_condition": "A local user invokes the vulnerable ioctl function with crafted input data that can lead to out-of-bounds memory writes.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate the length of the input data properly, allowing for potential buffer overflows and out-of-bounds memory writes. Additionally, the loop iterating over the input data does not have correct boundary conditions, leading to possible memory corruption.",
      "id": 116,
      "code_after_change_normalized": "static int FUN1(struct VAR1 *VAR1,\nvoid __user *VAR2, size_t VAR3)\n{\n__u16 *VAR4;\nint VAR5;\nint VAR6 = 0;\nif (VAR3 % sizeof(*VAR4))\nreturn -VAR7;\nVAR3 = FUN2(VAR3, sizeof(VAR1->VAR4));\nVAR4 = FUN3(VAR2, VAR3);\nif (FUN4(VAR4))\nreturn FUN5(VAR4);\nfor (VAR5 = 0; VAR5 < (VAR3 / 2) && VAR5 < VAR1->VAR8; VAR5++) {\nif (VAR4[VAR5] > VAR9 || VAR4[VAR5] < VAR10) {\nVAR6 = -VAR7;\ngoto VAR11;\n}\n}\nFUN6(VAR1->VAR4, VAR4, VAR3);\nfor (VAR5 = 0; VAR5 < VAR1->VAR8; VAR5++)\nVAR1->VAR12[VAR4[VAR5] - VAR10] = VAR5;\nVAR11:\nFUN7(VAR4);\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct VAR1 *VAR1,\nvoid __user *VAR2, size_t VAR3)\n{\n__u16 *VAR4;\nint VAR5;\nint VAR6 = 0;\nVAR3 = FUN2(VAR3, sizeof(VAR1->VAR4));\nVAR4 = FUN3(VAR2, VAR3);\nif (FUN4(VAR4))\nreturn FUN5(VAR4);\nfor (VAR5 = 0; VAR5 < VAR1->VAR7; VAR5++) {\nif (VAR4[VAR5] > VAR8 || VAR4[VAR5] < VAR9) {\nVAR6 = -VAR10;\ngoto VAR11;\n}\n}\nFUN6(VAR1->VAR4, VAR4, VAR3);\nfor (VAR5 = 0; VAR5 < VAR1->VAR7; VAR5++)\nVAR1->VAR12[VAR4[VAR5] - VAR9] = VAR5;\nVAR11:\nFUN7(VAR4);\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static int joydev_handle_JSIOCSBTNMAP(struct joydev *joydev,\nvoid __user *argp, size_t len)\n{\n__u16 *keypam;\nint i;\nint retval = 0;\nif (len % sizeof(*keypam))\nreturn -EINVAL;\nlen = min(len, sizeof(joydev->keypam));\nkeypam = memdup_user(argp, len);\nif (IS_ERR(keypam))\nreturn PTR_ERR(keypam);\nfor (i = 0; i < (len / 2) && i < joydev->nkey; i++) {\nif (keypam[i] > KEY_MAX || keypam[i] < BTN_MISC) {\nretval = -EINVAL;\ngoto out;\n}\n}\nmemcpy(joydev->keypam, keypam, len);\nfor (i = 0; i < joydev->nkey; i++)\njoydev->keymap[keypam[i] - BTN_MISC] = i;\nout:\nkfree(keypam);\nreturn retval;\n}\n",
      "code_before_change_raw": "static int joydev_handle_JSIOCSBTNMAP(struct joydev *joydev,\nvoid __user *argp, size_t len)\n{\n__u16 *keypam;\nint i;\nint retval = 0;\nlen = min(len, sizeof(joydev->keypam));\nkeypam = memdup_user(argp, len);\nif (IS_ERR(keypam))\nreturn PTR_ERR(keypam);\nfor (i = 0; i < joydev->nkey; i++) {\nif (keypam[i] > KEY_MAX || keypam[i] < BTN_MISC) {\nretval = -EINVAL;\ngoto out;\n}\n}\nmemcpy(joydev->keypam, keypam, len);\nfor (i = 0; i < joydev->nkey; i++)\njoydev->keymap[keypam[i] - BTN_MISC] = i;\nout:\nkfree(keypam);\nreturn retval;\n}\n"
    }
  ],
  "CVE-2021-37576": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for the size of the nargs value in the rtas_args structure, allowing it to potentially exceed the size of the args array.",
        "trigger_condition": "When a KVM guest OS user provides a maliciously crafted value for nargs that is larger than the size of the args array, it can lead to memory corruption in the host OS.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the value of nargs is within a valid range before using it to access elements in the args array. This lack of validation can result in an out-of-bounds access and potential memory corruption."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the value of nargs does not exceed the size of the args array before using it to access elements. By adding this validation, the code prevents potential out-of-bounds accesses and memory corruption issues. In this case, the solution involves adding the check 'if (be32_to_cpu(args.nargs) >= ARRAY_SIZE(args.args))' to verify the validity of the nargs value.",
      "GPT_analysis": "The modification in the code snippet is necessary to prevent a vulnerability (CVE-2021-37576) that allows KVM guest OS users to cause host OS memory corruption via rtas_args.nargs. By adding the check `if (be32_to_cpu(args.nargs) >= ARRAY_SIZE(args.args))`, the modified code ensures that the value of `nargs` does not exceed the size of the `args` array. \n\nIf `nargs` exceeds the size of the `args` array, it could lead to an overflow, potentially causing memory corruption and security issues. By checking and handling this condition appropriately, the modified code prevents this vulnerability from being exploited and helps maintain the integrity and security of the system.",
      "GPT_purpose": "Handling RTAS hypercalls in a KVM virtual CPU on the PowerPC platform.",
      "GPT_function": "\n1. Extracts the guest physical address of the RTAS arguments.\n2. Reads the RTAS arguments from guest memory.\n3. Calls the appropriate handler based on the RTAS token.\n4. Writes back the modified RTAS arguments to guest memory if the handler was successfully called.\n5. Handles failure cases where the guest has provided a bogus args pointer.",
      "CVE_id": "CVE-2021-37576",
      "code_before_change": "int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu)\n{\n\tstruct rtas_token_definition *d;\n\tstruct rtas_args args;\n\trtas_arg_t *orig_rets;\n\tgpa_t args_phys;\n\tint rc;\n\n\t/*\n\t * r4 contains the guest physical address of the RTAS args\n\t * Mask off the top 4 bits since this is a guest real address\n\t */\n\targs_phys = kvmppc_get_gpr(vcpu, 4) & KVM_PAM;\n\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\trc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\tif (rc)\n\t\tgoto fail;\n\n\t/*\n\t * args->rets is a pointer into args->args. Now that we've\n\t * copied args we need to fix it up to point into our copy,\n\t * not the guest args. We also need to save the original\n\t * value so we can restore it on the way out.\n\t */\n\torig_rets = args.rets;\n\targs.rets = &args.args[be32_to_cpu(args.nargs)];\n\n\tmutex_lock(&vcpu->kvm->arch.rtas_token_lock);\n\n\trc = -ENOENT;\n\tlist_for_each_entry(d, &vcpu->kvm->arch.rtas_tokens, list) {\n\t\tif (d->token == be32_to_cpu(args.token)) {\n\t\t\td->handler->handler(vcpu, &args);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tmutex_unlock(&vcpu->kvm->arch.rtas_token_lock);\n\n\tif (rc == 0) {\n\t\targs.rets = orig_rets;\n\t\trc = kvm_write_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\treturn rc;\n\nfail:\n\t/*\n\t * We only get here if the guest has called RTAS with a bogus\n\t * args pointer. That means we can't get to the args, and so we\n\t * can't fail the RTAS call. So fail right out to userspace,\n\t * which should kill the guest.\n\t */\n\treturn rc;\n}",
      "code_after_change": "int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu)\n{\n\tstruct rtas_token_definition *d;\n\tstruct rtas_args args;\n\trtas_arg_t *orig_rets;\n\tgpa_t args_phys;\n\tint rc;\n\n\t/*\n\t * r4 contains the guest physical address of the RTAS args\n\t * Mask off the top 4 bits since this is a guest real address\n\t */\n\targs_phys = kvmppc_get_gpr(vcpu, 4) & KVM_PAM;\n\n\tvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\n\trc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\tsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\n\tif (rc)\n\t\tgoto fail;\n\n\t/*\n\t * args->rets is a pointer into args->args. Now that we've\n\t * copied args we need to fix it up to point into our copy,\n\t * not the guest args. We also need to save the original\n\t * value so we can restore it on the way out.\n\t */\n\torig_rets = args.rets;\n\tif (be32_to_cpu(args.nargs) >= ARRAY_SIZE(args.args)) {\n\t\t/*\n\t\t * Don't overflow our args array: ensure there is room for\n\t\t * at least rets[0] (even if the call specifies 0 nret).\n\t\t *\n\t\t * Each handler must then check for the correct nargs and nret\n\t\t * values, but they may always return failure in rets[0].\n\t\t */\n\t\trc = -EINVAL;\n\t\tgoto fail;\n\t}\n\targs.rets = &args.args[be32_to_cpu(args.nargs)];\n\n\tmutex_lock(&vcpu->kvm->arch.rtas_token_lock);\n\n\trc = -ENOENT;\n\tlist_for_each_entry(d, &vcpu->kvm->arch.rtas_tokens, list) {\n\t\tif (d->token == be32_to_cpu(args.token)) {\n\t\t\td->handler->handler(vcpu, &args);\n\t\t\trc = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tmutex_unlock(&vcpu->kvm->arch.rtas_token_lock);\n\n\tif (rc == 0) {\n\t\targs.rets = orig_rets;\n\t\trc = kvm_write_guest(vcpu->kvm, args_phys, &args, sizeof(args));\n\t\tif (rc)\n\t\t\tgoto fail;\n\t}\n\n\treturn rc;\n\nfail:\n\t/*\n\t * We only get here if the guest has called RTAS with a bogus\n\t * args pointer or nargs/nret values that would overflow the\n\t * array. That means we can't get to the args, and so we can't\n\t * fail the RTAS call. So fail right out to userspace, which\n\t * should kill the guest.\n\t *\n\t * SLOF should actually pass the hcall return value from the\n\t * rtas handler call in r3, so enter_rtas could be modified to\n\t * return a failure indication in r3 and we could return such\n\t * errors to the guest rather than failing to host userspace.\n\t * However old guests that don't test for failure could then\n\t * continue silently after errors, so for now we won't do this.\n\t */\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tif (be32_to_cpu(args.nargs) >= ARRAY_SIZE(args.args)) {",
          "\t\t/*",
          "\t\t * Don't overflow our args array: ensure there is room for",
          "\t\t * at least rets[0] (even if the call specifies 0 nret).",
          "\t\t *",
          "\t\t * Each handler must then check for the correct nargs and nret",
          "\t\t * values, but they may always return failure in rets[0].",
          "\t\t */",
          "\t\trc = -EINVAL;",
          "\t\tgoto fail;",
          "\t}",
          "\t * args pointer or nargs/nret values that would overflow the",
          "\t * array. That means we can't get to the args, and so we can't",
          "\t * fail the RTAS call. So fail right out to userspace, which",
          "\t * should kill the guest.",
          "\t *",
          "\t * SLOF should actually pass the hcall return value from the",
          "\t * rtas handler call in r3, so enter_rtas could be modified to",
          "\t * return a failure indication in r3 and we could return such",
          "\t * errors to the guest rather than failing to host userspace.",
          "\t * However old guests that don't test for failure could then",
          "\t * continue silently after errors, so for now we won't do this."
        ],
        "deleted": [
          "\t * args pointer. That means we can't get to the args, and so we",
          "\t * can't fail the RTAS call. So fail right out to userspace,",
          "\t * which should kill the guest."
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for the size of the nargs value in the rtas_args structure, allowing it to potentially exceed the size of the args array.",
      "trigger_condition": "When a KVM guest OS user provides a maliciously crafted value for nargs that is larger than the size of the args array, it can lead to memory corruption in the host OS.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the value of nargs is within a valid range before using it to access elements in the args array. This lack of validation can result in an out-of-bounds access and potential memory corruption.",
      "id": 117,
      "code_after_change_normalized": "int FUN1(struct kvm_vcpu *VAR1)\n{\nstruct rtas_token_definition *VAR2;\nstruct rtas_args VAR3;\nrtas_arg_t *VAR4;\ngpa_t VAR5;\nint VAR6;\nVAR5 = FUN2(VAR1, 4) & VAR7;\nVAR1->VAR8 = FUN3(&VAR1->VAR9->VAR10);\nVAR6 = FUN4(VAR1->VAR9, VAR5, &VAR3, sizeof(VAR3));\nFUN5(&VAR1->VAR9->VAR10, VAR1->VAR8);\nif (VAR6)\ngoto VAR11;\nVAR4 = VAR3.VAR12;\nif (FUN6(VAR3.VAR13) >= FUN7(VAR3.VAR3)) {\nVAR6 = -VAR14;\ngoto VAR11;\n}\nVAR3.VAR12 = &VAR3.VAR3[FUN6(VAR3.VAR13)];\nFUN8(&VAR1->VAR9->VAR15.VAR16);\nVAR6 = -VAR17;\nFUN9(VAR2, &VAR1->VAR9->VAR15.VAR18, VAR19) {\nif (VAR2->VAR20 == FUN6(VAR3.VAR20)) {\nVAR2->VAR21->FUN10(VAR1, &VAR3);\nVAR6 = 0;\nbreak;\n}\n}\nFUN11(&VAR1->VAR9->VAR15.VAR16);\nif (VAR6 == 0) {\nVAR3.VAR12 = VAR4;\nVAR6 = FUN12(VAR1->VAR9, VAR5, &VAR3, sizeof(VAR3));\nif (VAR6)\ngoto VAR11;\n}\nreturn VAR6;\nVAR11:\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "int FUN1(struct kvm_vcpu *VAR1)\n{\nstruct rtas_token_definition *VAR2;\nstruct rtas_args VAR3;\nrtas_arg_t *VAR4;\ngpa_t VAR5;\nint VAR6;\nVAR5 = FUN2(VAR1, 4) & VAR7;\nVAR1->VAR8 = FUN3(&VAR1->VAR9->VAR10);\nVAR6 = FUN4(VAR1->VAR9, VAR5, &VAR3, sizeof(VAR3));\nFUN5(&VAR1->VAR9->VAR10, VAR1->VAR8);\nif (VAR6)\ngoto VAR11;\nVAR4 = VAR3.VAR12;\nVAR3.VAR12 = &VAR3.VAR3[FUN6(VAR3.VAR13)];\nFUN7(&VAR1->VAR9->VAR14.VAR15);\nVAR6 = -VAR16;\nFUN8(VAR2, &VAR1->VAR9->VAR14.VAR17, VAR18) {\nif (VAR2->VAR19 == FUN6(VAR3.VAR19)) {\nVAR2->VAR20->FUN9(VAR1, &VAR3);\nVAR6 = 0;\nbreak;\n}\n}\nFUN10(&VAR1->VAR9->VAR14.VAR15);\nif (VAR6 == 0) {\nVAR3.VAR12 = VAR4;\nVAR6 = FUN11(VAR1->VAR9, VAR5, &VAR3, sizeof(VAR3));\nif (VAR6)\ngoto VAR11;\n}\nreturn VAR6;\nVAR11:\nreturn VAR6;\n}\n",
      "code_after_change_raw": "int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu)\n{\nstruct rtas_token_definition *d;\nstruct rtas_args args;\nrtas_arg_t *orig_rets;\ngpa_t args_phys;\nint rc;\nargs_phys = kvmppc_get_gpr(vcpu, 4) & KVM_PAM;\nvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\nrc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));\nsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\nif (rc)\ngoto fail;\norig_rets = args.rets;\nif (be32_to_cpu(args.nargs) >= ARRAY_SIZE(args.args)) {\nrc = -EINVAL;\ngoto fail;\n}\nargs.rets = &args.args[be32_to_cpu(args.nargs)];\nmutex_lock(&vcpu->kvm->arch.rtas_token_lock);\nrc = -ENOENT;\nlist_for_each_entry(d, &vcpu->kvm->arch.rtas_tokens, list) {\nif (d->token == be32_to_cpu(args.token)) {\nd->handler->handler(vcpu, &args);\nrc = 0;\nbreak;\n}\n}\nmutex_unlock(&vcpu->kvm->arch.rtas_token_lock);\nif (rc == 0) {\nargs.rets = orig_rets;\nrc = kvm_write_guest(vcpu->kvm, args_phys, &args, sizeof(args));\nif (rc)\ngoto fail;\n}\nreturn rc;\nfail:\nreturn rc;\n}\n",
      "code_before_change_raw": "int kvmppc_rtas_hcall(struct kvm_vcpu *vcpu)\n{\nstruct rtas_token_definition *d;\nstruct rtas_args args;\nrtas_arg_t *orig_rets;\ngpa_t args_phys;\nint rc;\nargs_phys = kvmppc_get_gpr(vcpu, 4) & KVM_PAM;\nvcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);\nrc = kvm_read_guest(vcpu->kvm, args_phys, &args, sizeof(args));\nsrcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);\nif (rc)\ngoto fail;\norig_rets = args.rets;\nargs.rets = &args.args[be32_to_cpu(args.nargs)];\nmutex_lock(&vcpu->kvm->arch.rtas_token_lock);\nrc = -ENOENT;\nlist_for_each_entry(d, &vcpu->kvm->arch.rtas_tokens, list) {\nif (d->token == be32_to_cpu(args.token)) {\nd->handler->handler(vcpu, &args);\nrc = 0;\nbreak;\n}\n}\nmutex_unlock(&vcpu->kvm->arch.rtas_token_lock);\nif (rc == 0) {\nargs.rets = orig_rets;\nrc = kvm_write_guest(vcpu->kvm, args_phys, &args, sizeof(args));\nif (rc)\ngoto fail;\n}\nreturn rc;\nfail:\nreturn rc;\n}\n"
    }
  ],
  "CVE-2021-38166": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate memory allocation strategy leading to an integer overflow and out-of-bounds write when many elements are placed in a single bucket.",
        "trigger_condition": "When a large number of elements are inserted into a single bucket, causing the allocated memory size to overflow and write beyond the bounds of the allocated memory.",
        "specific_code_behavior_causing_vulnerability": "The code uses kvmalloc for memory allocation without considering the number of elements to be stored in a bucket, leading to potential integer overflow and out-of-bounds write vulnerabilities."
      },
      "solution": "To mitigate the vulnerability, it is necessary to use kvmalloc_array for memory allocation, ensuring that the allocated memory size is calculated based on the number of elements to be stored in a bucket. This prevents integer overflow and out-of-bounds write issues by accurately allocating memory space for the expected data.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-38166, which involves an integer overflow and out-of-bounds write when many elements are placed in a single bucket. By changing the memory allocation methods from `kvmalloc` to `kvmalloc_array`, we ensure that the allocated memory size is calculated correctly based on the number of elements to be stored in the bucket. This helps prevent the integer overflow and out-of-bounds write issues that could potentially be exploited by an attacker. Making this modification enhances the security and stability of the code, reducing the risk of exploitation of the vulnerability.",
      "GPT_purpose": "This function is responsible for looking up and deleting multiple elements in a hash table batch-wise.",
      "GPT_function": "\n1. Lookup and delete elements in a hash table batch.\n2. Handle allocation of memory for keys and values.\n3. Copy elements from the hash table to user space.\n4. Manage locking and unlocking of buckets.\n5. Handle deletion of elements and potential LRU operations.\n6. Perform necessary clean-up and error handling.",
      "CVE_id": "CVE-2021-38166",
      "code_before_change": "static int\n__htab_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t   const union bpf_attr *attr,\n\t\t\t\t   union bpf_attr __user *uattr,\n\t\t\t\t   bool do_delete, bool is_lru_map,\n\t\t\t\t   bool is_percpu)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;\n\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;\n\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);\n\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);\n\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);\n\tu32 batch, max_count, size, bucket_size;\n\tstruct htab_elem *node_to_free = NULL;\n\tu64 elem_map_flags, map_flags;\n\tstruct hlist_nulls_head *head;\n\tstruct hlist_nulls_node *n;\n\tunsigned long flags = 0;\n\tbool locked = false;\n\tstruct htab_elem *l;\n\tstruct bucket *b;\n\tint ret = 0;\n\n\telem_map_flags = attr->batch.elem_flags;\n\tif ((elem_map_flags & ~BPF_F_LOCK) ||\n\t    ((elem_map_flags & BPF_F_LOCK) && !map_value_has_spin_lock(map)))\n\t\treturn -EINVAL;\n\n\tmap_flags = attr->batch.flags;\n\tif (map_flags)\n\t\treturn -EINVAL;\n\n\tmax_count = attr->batch.count;\n\tif (!max_count)\n\t\treturn 0;\n\n\tif (put_user(0, &uattr->batch.count))\n\t\treturn -EFAULT;\n\n\tbatch = 0;\n\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))\n\t\treturn -EFAULT;\n\n\tif (batch >= htab->n_buckets)\n\t\treturn -ENOENT;\n\n\tkey_size = htab->map.key_size;\n\troundup_key_size = round_up(htab->map.key_size, 8);\n\tvalue_size = htab->map.value_size;\n\tsize = round_up(value_size, 8);\n\tif (is_percpu)\n\t\tvalue_size = size * num_possible_cpus();\n\ttotal = 0;\n\t/* while experimenting with hash tables with sizes ranging from 10 to\n\t * 1000, it was observed that a bucket can have upto 5 entries.\n\t */\n\tbucket_size = 5;\n\nalloc:\n\t/* We cannot do copy_from_user or copy_to_user inside\n\t * the rcu_read_lock. Allocate enough space here.\n\t */\n\tkeys = kvmalloc(key_size * bucket_size, GFP_USER | __GFP_NOWARN);\n\tvalues = kvmalloc(value_size * bucket_size, GFP_USER | __GFP_NOWARN);\n\tif (!keys || !values) {\n\t\tret = -ENOMEM;\n\t\tgoto after_loop;\n\t}\n\nagain:\n\tbpf_disable_instrumentation();\n\trcu_read_lock();\nagain_nocopy:\n\tdst_key = keys;\n\tdst_val = values;\n\tb = &htab->buckets[batch];\n\thead = &b->head;\n\t/* do not grab the lock unless need it (bucket_cnt > 0). */\n\tif (locked) {\n\t\tret = htab_lock_bucket(htab, b, batch, &flags);\n\t\tif (ret)\n\t\t\tgoto next_batch;\n\t}\n\n\tbucket_cnt = 0;\n\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\n\t\tbucket_cnt++;\n\n\tif (bucket_cnt && !locked) {\n\t\tlocked = true;\n\t\tgoto again_nocopy;\n\t}\n\n\tif (bucket_cnt > (max_count - total)) {\n\t\tif (total == 0)\n\t\t\tret = -ENOSPC;\n\t\t/* Note that since bucket_cnt > 0 here, it is implicit\n\t\t * that the locked was grabbed, so release it.\n\t\t */\n\t\thtab_unlock_bucket(htab, b, batch, flags);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tgoto after_loop;\n\t}\n\n\tif (bucket_cnt > bucket_size) {\n\t\tbucket_size = bucket_cnt;\n\t\t/* Note that since bucket_cnt > 0 here, it is implicit\n\t\t * that the locked was grabbed, so release it.\n\t\t */\n\t\thtab_unlock_bucket(htab, b, batch, flags);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tkvfree(keys);\n\t\tkvfree(values);\n\t\tgoto alloc;\n\t}\n\n\t/* Next block is only safe to run if you have grabbed the lock */\n\tif (!locked)\n\t\tgoto next_batch;\n\n\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\n\t\tmemcpy(dst_key, l->key, key_size);\n\n\t\tif (is_percpu) {\n\t\t\tint off = 0, cpu;\n\t\t\tvoid __percpu *pptr;\n\n\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tbpf_long_memcpy(dst_val + off,\n\t\t\t\t\t\tper_cpu_ptr(pptr, cpu), size);\n\t\t\t\toff += size;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue = l->key + roundup_key_size;\n\t\t\tif (elem_map_flags & BPF_F_LOCK)\n\t\t\t\tcopy_map_value_locked(map, dst_val, value,\n\t\t\t\t\t\t      true);\n\t\t\telse\n\t\t\t\tcopy_map_value(map, dst_val, value);\n\t\t\tcheck_and_init_map_lock(map, dst_val);\n\t\t}\n\t\tif (do_delete) {\n\t\t\thlist_nulls_del_rcu(&l->hash_node);\n\n\t\t\t/* bpf_lru_push_free() will acquire lru_lock, which\n\t\t\t * may cause deadlock. See comments in function\n\t\t\t * prealloc_lru_pop(). Let us do bpf_lru_push_free()\n\t\t\t * after releasing the bucket lock.\n\t\t\t */\n\t\t\tif (is_lru_map) {\n\t\t\t\tl->batch_flink = node_to_free;\n\t\t\t\tnode_to_free = l;\n\t\t\t} else {\n\t\t\t\tfree_htab_elem(htab, l);\n\t\t\t}\n\t\t}\n\t\tdst_key += key_size;\n\t\tdst_val += value_size;\n\t}\n\n\thtab_unlock_bucket(htab, b, batch, flags);\n\tlocked = false;\n\n\twhile (node_to_free) {\n\t\tl = node_to_free;\n\t\tnode_to_free = node_to_free->batch_flink;\n\t\tbpf_lru_push_free(&htab->lru, &l->lru_node);\n\t}\n\nnext_batch:\n\t/* If we are not copying data, we can go to next bucket and avoid\n\t * unlocking the rcu.\n\t */\n\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {\n\t\tbatch++;\n\t\tgoto again_nocopy;\n\t}\n\n\trcu_read_unlock();\n\tbpf_enable_instrumentation();\n\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,\n\t    key_size * bucket_cnt) ||\n\t    copy_to_user(uvalues + total * value_size, values,\n\t    value_size * bucket_cnt))) {\n\t\tret = -EFAULT;\n\t\tgoto after_loop;\n\t}\n\n\ttotal += bucket_cnt;\n\tbatch++;\n\tif (batch >= htab->n_buckets) {\n\t\tret = -ENOENT;\n\t\tgoto after_loop;\n\t}\n\tgoto again;\n\nafter_loop:\n\tif (ret == -EFAULT)\n\t\tgoto out;\n\n\t/* copy # of entries and next batch */\n\tubatch = u64_to_user_ptr(attr->batch.out_batch);\n\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||\n\t    put_user(total, &uattr->batch.count))\n\t\tret = -EFAULT;\n\nout:\n\tkvfree(keys);\n\tkvfree(values);\n\treturn ret;\n}",
      "code_after_change": "static int\n__htab_map_lookup_and_delete_batch(struct bpf_map *map,\n\t\t\t\t   const union bpf_attr *attr,\n\t\t\t\t   union bpf_attr __user *uattr,\n\t\t\t\t   bool do_delete, bool is_lru_map,\n\t\t\t\t   bool is_percpu)\n{\n\tstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\n\tu32 bucket_cnt, total, key_size, value_size, roundup_key_size;\n\tvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;\n\tvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);\n\tvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);\n\tvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);\n\tu32 batch, max_count, size, bucket_size;\n\tstruct htab_elem *node_to_free = NULL;\n\tu64 elem_map_flags, map_flags;\n\tstruct hlist_nulls_head *head;\n\tstruct hlist_nulls_node *n;\n\tunsigned long flags = 0;\n\tbool locked = false;\n\tstruct htab_elem *l;\n\tstruct bucket *b;\n\tint ret = 0;\n\n\telem_map_flags = attr->batch.elem_flags;\n\tif ((elem_map_flags & ~BPF_F_LOCK) ||\n\t    ((elem_map_flags & BPF_F_LOCK) && !map_value_has_spin_lock(map)))\n\t\treturn -EINVAL;\n\n\tmap_flags = attr->batch.flags;\n\tif (map_flags)\n\t\treturn -EINVAL;\n\n\tmax_count = attr->batch.count;\n\tif (!max_count)\n\t\treturn 0;\n\n\tif (put_user(0, &uattr->batch.count))\n\t\treturn -EFAULT;\n\n\tbatch = 0;\n\tif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))\n\t\treturn -EFAULT;\n\n\tif (batch >= htab->n_buckets)\n\t\treturn -ENOENT;\n\n\tkey_size = htab->map.key_size;\n\troundup_key_size = round_up(htab->map.key_size, 8);\n\tvalue_size = htab->map.value_size;\n\tsize = round_up(value_size, 8);\n\tif (is_percpu)\n\t\tvalue_size = size * num_possible_cpus();\n\ttotal = 0;\n\t/* while experimenting with hash tables with sizes ranging from 10 to\n\t * 1000, it was observed that a bucket can have upto 5 entries.\n\t */\n\tbucket_size = 5;\n\nalloc:\n\t/* We cannot do copy_from_user or copy_to_user inside\n\t * the rcu_read_lock. Allocate enough space here.\n\t */\n\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);\n\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);\n\tif (!keys || !values) {\n\t\tret = -ENOMEM;\n\t\tgoto after_loop;\n\t}\n\nagain:\n\tbpf_disable_instrumentation();\n\trcu_read_lock();\nagain_nocopy:\n\tdst_key = keys;\n\tdst_val = values;\n\tb = &htab->buckets[batch];\n\thead = &b->head;\n\t/* do not grab the lock unless need it (bucket_cnt > 0). */\n\tif (locked) {\n\t\tret = htab_lock_bucket(htab, b, batch, &flags);\n\t\tif (ret)\n\t\t\tgoto next_batch;\n\t}\n\n\tbucket_cnt = 0;\n\thlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\n\t\tbucket_cnt++;\n\n\tif (bucket_cnt && !locked) {\n\t\tlocked = true;\n\t\tgoto again_nocopy;\n\t}\n\n\tif (bucket_cnt > (max_count - total)) {\n\t\tif (total == 0)\n\t\t\tret = -ENOSPC;\n\t\t/* Note that since bucket_cnt > 0 here, it is implicit\n\t\t * that the locked was grabbed, so release it.\n\t\t */\n\t\thtab_unlock_bucket(htab, b, batch, flags);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tgoto after_loop;\n\t}\n\n\tif (bucket_cnt > bucket_size) {\n\t\tbucket_size = bucket_cnt;\n\t\t/* Note that since bucket_cnt > 0 here, it is implicit\n\t\t * that the locked was grabbed, so release it.\n\t\t */\n\t\thtab_unlock_bucket(htab, b, batch, flags);\n\t\trcu_read_unlock();\n\t\tbpf_enable_instrumentation();\n\t\tkvfree(keys);\n\t\tkvfree(values);\n\t\tgoto alloc;\n\t}\n\n\t/* Next block is only safe to run if you have grabbed the lock */\n\tif (!locked)\n\t\tgoto next_batch;\n\n\thlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\n\t\tmemcpy(dst_key, l->key, key_size);\n\n\t\tif (is_percpu) {\n\t\t\tint off = 0, cpu;\n\t\t\tvoid __percpu *pptr;\n\n\t\t\tpptr = htab_elem_get_ptr(l, map->key_size);\n\t\t\tfor_each_possible_cpu(cpu) {\n\t\t\t\tbpf_long_memcpy(dst_val + off,\n\t\t\t\t\t\tper_cpu_ptr(pptr, cpu), size);\n\t\t\t\toff += size;\n\t\t\t}\n\t\t} else {\n\t\t\tvalue = l->key + roundup_key_size;\n\t\t\tif (elem_map_flags & BPF_F_LOCK)\n\t\t\t\tcopy_map_value_locked(map, dst_val, value,\n\t\t\t\t\t\t      true);\n\t\t\telse\n\t\t\t\tcopy_map_value(map, dst_val, value);\n\t\t\tcheck_and_init_map_lock(map, dst_val);\n\t\t}\n\t\tif (do_delete) {\n\t\t\thlist_nulls_del_rcu(&l->hash_node);\n\n\t\t\t/* bpf_lru_push_free() will acquire lru_lock, which\n\t\t\t * may cause deadlock. See comments in function\n\t\t\t * prealloc_lru_pop(). Let us do bpf_lru_push_free()\n\t\t\t * after releasing the bucket lock.\n\t\t\t */\n\t\t\tif (is_lru_map) {\n\t\t\t\tl->batch_flink = node_to_free;\n\t\t\t\tnode_to_free = l;\n\t\t\t} else {\n\t\t\t\tfree_htab_elem(htab, l);\n\t\t\t}\n\t\t}\n\t\tdst_key += key_size;\n\t\tdst_val += value_size;\n\t}\n\n\thtab_unlock_bucket(htab, b, batch, flags);\n\tlocked = false;\n\n\twhile (node_to_free) {\n\t\tl = node_to_free;\n\t\tnode_to_free = node_to_free->batch_flink;\n\t\tbpf_lru_push_free(&htab->lru, &l->lru_node);\n\t}\n\nnext_batch:\n\t/* If we are not copying data, we can go to next bucket and avoid\n\t * unlocking the rcu.\n\t */\n\tif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {\n\t\tbatch++;\n\t\tgoto again_nocopy;\n\t}\n\n\trcu_read_unlock();\n\tbpf_enable_instrumentation();\n\tif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,\n\t    key_size * bucket_cnt) ||\n\t    copy_to_user(uvalues + total * value_size, values,\n\t    value_size * bucket_cnt))) {\n\t\tret = -EFAULT;\n\t\tgoto after_loop;\n\t}\n\n\ttotal += bucket_cnt;\n\tbatch++;\n\tif (batch >= htab->n_buckets) {\n\t\tret = -ENOENT;\n\t\tgoto after_loop;\n\t}\n\tgoto again;\n\nafter_loop:\n\tif (ret == -EFAULT)\n\t\tgoto out;\n\n\t/* copy # of entries and next batch */\n\tubatch = u64_to_user_ptr(attr->batch.out_batch);\n\tif (copy_to_user(ubatch, &batch, sizeof(batch)) ||\n\t    put_user(total, &uattr->batch.count))\n\t\tret = -EFAULT;\n\nout:\n\tkvfree(keys);\n\tkvfree(values);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);",
          "\tvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);"
        ],
        "deleted": [
          "\tkeys = kvmalloc(key_size * bucket_size, GFP_USER | __GFP_NOWARN);",
          "\tvalues = kvmalloc(value_size * bucket_size, GFP_USER | __GFP_NOWARN);"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate memory allocation strategy leading to an integer overflow and out-of-bounds write when many elements are placed in a single bucket.",
      "trigger_condition": "When a large number of elements are inserted into a single bucket, causing the allocated memory size to overflow and write beyond the bounds of the allocated memory.",
      "specific_code_behavior_causing_vulnerability": "The code uses kvmalloc for memory allocation without considering the number of elements to be stored in a bucket, leading to potential integer overflow and out-of-bounds write vulnerabilities.",
      "id": 118,
      "code_after_change_normalized": "static int\nFUN1(struct bpf_map *VAR1,\nconst union bpf_attr *VAR2,\nunion bpf_attr __user *VAR3,\nbool VAR4, bool VAR5,\nbool VAR6)\n{\nstruct VAR8 *VAR7 = FUN2(VAR1, struct VAR8, VAR1);\nu32 VAR9, VAR10, VAR11, VAR12, VAR13;\nvoid *VAR14 = NULL, *VAR15 = NULL, *VAR16, *VAR17, *VAR18;\nvoid __user *VAR19 = FUN3(VAR2->VAR20.VAR15);\nvoid __user *VAR21 = FUN3(VAR2->VAR20.VAR14);\nvoid __user *VAR22 = FUN3(VAR2->VAR20.VAR23);\nu32 VAR20, VAR24, VAR25, VAR26;\nstruct htab_elem *VAR27 = NULL;\nu64 VAR28, VAR29;\nstruct hlist_nulls_head *VAR30;\nstruct hlist_nulls_node *VAR31;\nunsigned long VAR32 = 0;\nbool VAR33 = false;\nstruct htab_elem *VAR34;\nstruct bucket *VAR35;\nint VAR36 = 0;\nVAR28 = VAR2->VAR20.VAR37;\nif ((VAR28 & ~VAR38) ||\n((VAR28 & VAR38) && !FUN4(VAR1)))\nreturn -VAR39;\nVAR29 = VAR2->VAR20.VAR32;\nif (VAR29)\nreturn -VAR39;\nVAR24 = VAR2->VAR20.VAR40;\nif (!VAR24)\nreturn 0;\nif (FUN5(0, &VAR3->VAR20.VAR40))\nreturn -VAR41;\nVAR20 = 0;\nif (VAR22 && FUN6(&VAR20, VAR22, sizeof(VAR20)))\nreturn -VAR41;\nif (VAR20 >= VAR7->VAR42)\nreturn -VAR43;\nVAR11 = VAR7->VAR1.VAR11;\nVAR13 = FUN7(VAR7->VAR1.VAR11, 8);\nVAR12 = VAR7->VAR1.VAR12;\nVAR25 = FUN7(VAR12, 8);\nif (VAR6)\nVAR12 = VAR25 * FUN8();\nVAR10 = 0;\nVAR26 = 5;\nVAR44:\nVAR14 = FUN9(VAR11, VAR26, VAR45 | VAR46);\nVAR15 = FUN9(VAR12, VAR26, VAR45 | VAR46);\nif (!VAR14 || !VAR15) {\nVAR36 = -VAR47;\ngoto VAR48;\n}\nVAR49:\nFUN10();\nFUN11();\nVAR50:\nVAR17 = VAR14;\nVAR18 = VAR15;\nVAR35 = &VAR7->VAR51[VAR20];\nVAR30 = &VAR35->VAR30;\nif (VAR33) {\nVAR36 = FUN12(VAR7, VAR35, VAR20, &VAR32);\nif (VAR36)\ngoto VAR52;\n}\nVAR9 = 0;\nFUN13(VAR34, VAR31, VAR30, VAR53)\nVAR9++;\nif (VAR9 && !VAR33) {\nVAR33 = true;\ngoto VAR50;\n}\nif (VAR9 > (VAR24 - VAR10)) {\nif (VAR10 == 0)\nVAR36 = -VAR54;\nFUN14(VAR7, VAR35, VAR20, VAR32);\nFUN15();\nFUN16();\ngoto VAR48;\n}\nif (VAR9 > VAR26) {\nVAR26 = VAR9;\nFUN14(VAR7, VAR35, VAR20, VAR32);\nFUN15();\nFUN16();\nFUN17(VAR14);\nFUN17(VAR15);\ngoto VAR44;\n}\nif (!VAR33)\ngoto VAR52;\nFUN18(VAR34, VAR31, VAR30, VAR53) {\nFUN19(VAR17, VAR34->VAR55, VAR11);\nif (VAR6) {\nint VAR56 = 0, VAR57;\nvoid __percpu *VAR58;\nVAR58 = FUN20(VAR34, VAR1->VAR11);\nFUN21(VAR57) {\nFUN22(VAR18 + VAR56,\nFUN23(VAR58, VAR57), VAR25);\nVAR56 += VAR25;\n}\n} else {\nVAR16 = VAR34->VAR55 + VAR13;\nif (VAR28 & VAR38)\nFUN24(VAR1, VAR18, VAR16,\ntrue);\nelse\nFUN25(VAR1, VAR18, VAR16);\nFUN26(VAR1, VAR18);\n}\nif (VAR4) {\nFUN27(&VAR34->VAR53);\nif (VAR5) {\nVAR34->VAR59 = VAR27;\nVAR27 = VAR34;\n} else {\nFUN28(VAR7, VAR34);\n}\n}\nVAR17 += VAR11;\nVAR18 += VAR12;\n}\nFUN14(VAR7, VAR35, VAR20, VAR32);\nVAR33 = false;\nwhile (VAR27) {\nVAR34 = VAR27;\nVAR27 = VAR27->VAR59;\nFUN29(&VAR7->VAR60, &VAR34->VAR61);\n}\nVAR52:\nif (!VAR9 && (VAR20 + 1 < VAR7->VAR42)) {\nVAR20++;\ngoto VAR50;\n}\nFUN15();\nFUN16();\nif (VAR9 && (FUN30(VAR21 + VAR10 * VAR11, VAR14,\nVAR11 * VAR9) ||\nFUN30(VAR19 + VAR10 * VAR12, VAR15,\nVAR12 * VAR9))) {\nVAR36 = -VAR41;\ngoto VAR48;\n}\nVAR10 += VAR9;\nVAR20++;\nif (VAR20 >= VAR7->VAR42) {\nVAR36 = -VAR43;\ngoto VAR48;\n}\ngoto VAR49;\nVAR48:\nif (VAR36 == -VAR41)\ngoto VAR62;\nVAR22 = FUN3(VAR2->VAR20.VAR63);\nif (FUN30(VAR22, &VAR20, sizeof(VAR20)) ||\nFUN5(VAR10, &VAR3->VAR20.VAR40))\nVAR36 = -VAR41;\nVAR62:\nFUN17(VAR14);\nFUN17(VAR15);\nreturn VAR36;\n}\n",
      "code_before_change_normalized": "static int\nFUN1(struct bpf_map *VAR1,\nconst union bpf_attr *VAR2,\nunion bpf_attr __user *VAR3,\nbool VAR4, bool VAR5,\nbool VAR6)\n{\nstruct VAR8 *VAR7 = FUN2(VAR1, struct VAR8, VAR1);\nu32 VAR9, VAR10, VAR11, VAR12, VAR13;\nvoid *VAR14 = NULL, *VAR15 = NULL, *VAR16, *VAR17, *VAR18;\nvoid __user *VAR19 = FUN3(VAR2->VAR20.VAR15);\nvoid __user *VAR21 = FUN3(VAR2->VAR20.VAR14);\nvoid __user *VAR22 = FUN3(VAR2->VAR20.VAR23);\nu32 VAR20, VAR24, VAR25, VAR26;\nstruct htab_elem *VAR27 = NULL;\nu64 VAR28, VAR29;\nstruct hlist_nulls_head *VAR30;\nstruct hlist_nulls_node *VAR31;\nunsigned long VAR32 = 0;\nbool VAR33 = false;\nstruct htab_elem *VAR34;\nstruct bucket *VAR35;\nint VAR36 = 0;\nVAR28 = VAR2->VAR20.VAR37;\nif ((VAR28 & ~VAR38) ||\n((VAR28 & VAR38) && !FUN4(VAR1)))\nreturn -VAR39;\nVAR29 = VAR2->VAR20.VAR32;\nif (VAR29)\nreturn -VAR39;\nVAR24 = VAR2->VAR20.VAR40;\nif (!VAR24)\nreturn 0;\nif (FUN5(0, &VAR3->VAR20.VAR40))\nreturn -VAR41;\nVAR20 = 0;\nif (VAR22 && FUN6(&VAR20, VAR22, sizeof(VAR20)))\nreturn -VAR41;\nif (VAR20 >= VAR7->VAR42)\nreturn -VAR43;\nVAR11 = VAR7->VAR1.VAR11;\nVAR13 = FUN7(VAR7->VAR1.VAR11, 8);\nVAR12 = VAR7->VAR1.VAR12;\nVAR25 = FUN7(VAR12, 8);\nif (VAR6)\nVAR12 = VAR25 * FUN8();\nVAR10 = 0;\nVAR26 = 5;\nVAR44:\nVAR14 = FUN9(VAR11 * VAR26, VAR45 | VAR46);\nVAR15 = FUN9(VAR12 * VAR26, VAR45 | VAR46);\nif (!VAR14 || !VAR15) {\nVAR36 = -VAR47;\ngoto VAR48;\n}\nVAR49:\nFUN10();\nFUN11();\nVAR50:\nVAR17 = VAR14;\nVAR18 = VAR15;\nVAR35 = &VAR7->VAR51[VAR20];\nVAR30 = &VAR35->VAR30;\nif (VAR33) {\nVAR36 = FUN12(VAR7, VAR35, VAR20, &VAR32);\nif (VAR36)\ngoto VAR52;\n}\nVAR9 = 0;\nFUN13(VAR34, VAR31, VAR30, VAR53)\nVAR9++;\nif (VAR9 && !VAR33) {\nVAR33 = true;\ngoto VAR50;\n}\nif (VAR9 > (VAR24 - VAR10)) {\nif (VAR10 == 0)\nVAR36 = -VAR54;\nFUN14(VAR7, VAR35, VAR20, VAR32);\nFUN15();\nFUN16();\ngoto VAR48;\n}\nif (VAR9 > VAR26) {\nVAR26 = VAR9;\nFUN14(VAR7, VAR35, VAR20, VAR32);\nFUN15();\nFUN16();\nFUN17(VAR14);\nFUN17(VAR15);\ngoto VAR44;\n}\nif (!VAR33)\ngoto VAR52;\nFUN18(VAR34, VAR31, VAR30, VAR53) {\nFUN19(VAR17, VAR34->VAR55, VAR11);\nif (VAR6) {\nint VAR56 = 0, VAR57;\nvoid __percpu *VAR58;\nVAR58 = FUN20(VAR34, VAR1->VAR11);\nFUN21(VAR57) {\nFUN22(VAR18 + VAR56,\nFUN23(VAR58, VAR57), VAR25);\nVAR56 += VAR25;\n}\n} else {\nVAR16 = VAR34->VAR55 + VAR13;\nif (VAR28 & VAR38)\nFUN24(VAR1, VAR18, VAR16,\ntrue);\nelse\nFUN25(VAR1, VAR18, VAR16);\nFUN26(VAR1, VAR18);\n}\nif (VAR4) {\nFUN27(&VAR34->VAR53);\nif (VAR5) {\nVAR34->VAR59 = VAR27;\nVAR27 = VAR34;\n} else {\nFUN28(VAR7, VAR34);\n}\n}\nVAR17 += VAR11;\nVAR18 += VAR12;\n}\nFUN14(VAR7, VAR35, VAR20, VAR32);\nVAR33 = false;\nwhile (VAR27) {\nVAR34 = VAR27;\nVAR27 = VAR27->VAR59;\nFUN29(&VAR7->VAR60, &VAR34->VAR61);\n}\nVAR52:\nif (!VAR9 && (VAR20 + 1 < VAR7->VAR42)) {\nVAR20++;\ngoto VAR50;\n}\nFUN15();\nFUN16();\nif (VAR9 && (FUN30(VAR21 + VAR10 * VAR11, VAR14,\nVAR11 * VAR9) ||\nFUN30(VAR19 + VAR10 * VAR12, VAR15,\nVAR12 * VAR9))) {\nVAR36 = -VAR41;\ngoto VAR48;\n}\nVAR10 += VAR9;\nVAR20++;\nif (VAR20 >= VAR7->VAR42) {\nVAR36 = -VAR43;\ngoto VAR48;\n}\ngoto VAR49;\nVAR48:\nif (VAR36 == -VAR41)\ngoto VAR62;\nVAR22 = FUN3(VAR2->VAR20.VAR63);\nif (FUN30(VAR22, &VAR20, sizeof(VAR20)) ||\nFUN5(VAR10, &VAR3->VAR20.VAR40))\nVAR36 = -VAR41;\nVAR62:\nFUN17(VAR14);\nFUN17(VAR15);\nreturn VAR36;\n}\n",
      "code_after_change_raw": "static int\n__htab_map_lookup_and_delete_batch(struct bpf_map *map,\nconst union bpf_attr *attr,\nunion bpf_attr __user *uattr,\nbool do_delete, bool is_lru_map,\nbool is_percpu)\n{\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\nu32 bucket_cnt, total, key_size, value_size, roundup_key_size;\nvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;\nvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);\nvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);\nvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);\nu32 batch, max_count, size, bucket_size;\nstruct htab_elem *node_to_free = NULL;\nu64 elem_map_flags, map_flags;\nstruct hlist_nulls_head *head;\nstruct hlist_nulls_node *n;\nunsigned long flags = 0;\nbool locked = false;\nstruct htab_elem *l;\nstruct bucket *b;\nint ret = 0;\nelem_map_flags = attr->batch.elem_flags;\nif ((elem_map_flags & ~BPF_F_LOCK) ||\n((elem_map_flags & BPF_F_LOCK) && !map_value_has_spin_lock(map)))\nreturn -EINVAL;\nmap_flags = attr->batch.flags;\nif (map_flags)\nreturn -EINVAL;\nmax_count = attr->batch.count;\nif (!max_count)\nreturn 0;\nif (put_user(0, &uattr->batch.count))\nreturn -EFAULT;\nbatch = 0;\nif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))\nreturn -EFAULT;\nif (batch >= htab->n_buckets)\nreturn -ENOENT;\nkey_size = htab->map.key_size;\nroundup_key_size = round_up(htab->map.key_size, 8);\nvalue_size = htab->map.value_size;\nsize = round_up(value_size, 8);\nif (is_percpu)\nvalue_size = size * num_possible_cpus();\ntotal = 0;\nbucket_size = 5;\nalloc:\nkeys = kvmalloc_array(key_size, bucket_size, GFP_USER | __GFP_NOWARN);\nvalues = kvmalloc_array(value_size, bucket_size, GFP_USER | __GFP_NOWARN);\nif (!keys || !values) {\nret = -ENOMEM;\ngoto after_loop;\n}\nagain:\nbpf_disable_instrumentation();\nrcu_read_lock();\nagain_nocopy:\ndst_key = keys;\ndst_val = values;\nb = &htab->buckets[batch];\nhead = &b->head;\nif (locked) {\nret = htab_lock_bucket(htab, b, batch, &flags);\nif (ret)\ngoto next_batch;\n}\nbucket_cnt = 0;\nhlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\nbucket_cnt++;\nif (bucket_cnt && !locked) {\nlocked = true;\ngoto again_nocopy;\n}\nif (bucket_cnt > (max_count - total)) {\nif (total == 0)\nret = -ENOSPC;\nhtab_unlock_bucket(htab, b, batch, flags);\nrcu_read_unlock();\nbpf_enable_instrumentation();\ngoto after_loop;\n}\nif (bucket_cnt > bucket_size) {\nbucket_size = bucket_cnt;\nhtab_unlock_bucket(htab, b, batch, flags);\nrcu_read_unlock();\nbpf_enable_instrumentation();\nkvfree(keys);\nkvfree(values);\ngoto alloc;\n}\nif (!locked)\ngoto next_batch;\nhlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\nmemcpy(dst_key, l->key, key_size);\nif (is_percpu) {\nint off = 0, cpu;\nvoid __percpu *pptr;\npptr = htab_elem_get_ptr(l, map->key_size);\nfor_each_possible_cpu(cpu) {\nbpf_long_memcpy(dst_val + off,\nper_cpu_ptr(pptr, cpu), size);\noff += size;\n}\n} else {\nvalue = l->key + roundup_key_size;\nif (elem_map_flags & BPF_F_LOCK)\ncopy_map_value_locked(map, dst_val, value,\ntrue);\nelse\ncopy_map_value(map, dst_val, value);\ncheck_and_init_map_lock(map, dst_val);\n}\nif (do_delete) {\nhlist_nulls_del_rcu(&l->hash_node);\nif (is_lru_map) {\nl->batch_flink = node_to_free;\nnode_to_free = l;\n} else {\nfree_htab_elem(htab, l);\n}\n}\ndst_key += key_size;\ndst_val += value_size;\n}\nhtab_unlock_bucket(htab, b, batch, flags);\nlocked = false;\nwhile (node_to_free) {\nl = node_to_free;\nnode_to_free = node_to_free->batch_flink;\nbpf_lru_push_free(&htab->lru, &l->lru_node);\n}\nnext_batch:\nif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {\nbatch++;\ngoto again_nocopy;\n}\nrcu_read_unlock();\nbpf_enable_instrumentation();\nif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,\nkey_size * bucket_cnt) ||\ncopy_to_user(uvalues + total * value_size, values,\nvalue_size * bucket_cnt))) {\nret = -EFAULT;\ngoto after_loop;\n}\ntotal += bucket_cnt;\nbatch++;\nif (batch >= htab->n_buckets) {\nret = -ENOENT;\ngoto after_loop;\n}\ngoto again;\nafter_loop:\nif (ret == -EFAULT)\ngoto out;\nubatch = u64_to_user_ptr(attr->batch.out_batch);\nif (copy_to_user(ubatch, &batch, sizeof(batch)) ||\nput_user(total, &uattr->batch.count))\nret = -EFAULT;\nout:\nkvfree(keys);\nkvfree(values);\nreturn ret;\n}\n",
      "code_before_change_raw": "static int\n__htab_map_lookup_and_delete_batch(struct bpf_map *map,\nconst union bpf_attr *attr,\nunion bpf_attr __user *uattr,\nbool do_delete, bool is_lru_map,\nbool is_percpu)\n{\nstruct bpf_htab *htab = container_of(map, struct bpf_htab, map);\nu32 bucket_cnt, total, key_size, value_size, roundup_key_size;\nvoid *keys = NULL, *values = NULL, *value, *dst_key, *dst_val;\nvoid __user *uvalues = u64_to_user_ptr(attr->batch.values);\nvoid __user *ukeys = u64_to_user_ptr(attr->batch.keys);\nvoid __user *ubatch = u64_to_user_ptr(attr->batch.in_batch);\nu32 batch, max_count, size, bucket_size;\nstruct htab_elem *node_to_free = NULL;\nu64 elem_map_flags, map_flags;\nstruct hlist_nulls_head *head;\nstruct hlist_nulls_node *n;\nunsigned long flags = 0;\nbool locked = false;\nstruct htab_elem *l;\nstruct bucket *b;\nint ret = 0;\nelem_map_flags = attr->batch.elem_flags;\nif ((elem_map_flags & ~BPF_F_LOCK) ||\n((elem_map_flags & BPF_F_LOCK) && !map_value_has_spin_lock(map)))\nreturn -EINVAL;\nmap_flags = attr->batch.flags;\nif (map_flags)\nreturn -EINVAL;\nmax_count = attr->batch.count;\nif (!max_count)\nreturn 0;\nif (put_user(0, &uattr->batch.count))\nreturn -EFAULT;\nbatch = 0;\nif (ubatch && copy_from_user(&batch, ubatch, sizeof(batch)))\nreturn -EFAULT;\nif (batch >= htab->n_buckets)\nreturn -ENOENT;\nkey_size = htab->map.key_size;\nroundup_key_size = round_up(htab->map.key_size, 8);\nvalue_size = htab->map.value_size;\nsize = round_up(value_size, 8);\nif (is_percpu)\nvalue_size = size * num_possible_cpus();\ntotal = 0;\nbucket_size = 5;\nalloc:\nkeys = kvmalloc(key_size * bucket_size, GFP_USER | __GFP_NOWARN);\nvalues = kvmalloc(value_size * bucket_size, GFP_USER | __GFP_NOWARN);\nif (!keys || !values) {\nret = -ENOMEM;\ngoto after_loop;\n}\nagain:\nbpf_disable_instrumentation();\nrcu_read_lock();\nagain_nocopy:\ndst_key = keys;\ndst_val = values;\nb = &htab->buckets[batch];\nhead = &b->head;\nif (locked) {\nret = htab_lock_bucket(htab, b, batch, &flags);\nif (ret)\ngoto next_batch;\n}\nbucket_cnt = 0;\nhlist_nulls_for_each_entry_rcu(l, n, head, hash_node)\nbucket_cnt++;\nif (bucket_cnt && !locked) {\nlocked = true;\ngoto again_nocopy;\n}\nif (bucket_cnt > (max_count - total)) {\nif (total == 0)\nret = -ENOSPC;\nhtab_unlock_bucket(htab, b, batch, flags);\nrcu_read_unlock();\nbpf_enable_instrumentation();\ngoto after_loop;\n}\nif (bucket_cnt > bucket_size) {\nbucket_size = bucket_cnt;\nhtab_unlock_bucket(htab, b, batch, flags);\nrcu_read_unlock();\nbpf_enable_instrumentation();\nkvfree(keys);\nkvfree(values);\ngoto alloc;\n}\nif (!locked)\ngoto next_batch;\nhlist_nulls_for_each_entry_safe(l, n, head, hash_node) {\nmemcpy(dst_key, l->key, key_size);\nif (is_percpu) {\nint off = 0, cpu;\nvoid __percpu *pptr;\npptr = htab_elem_get_ptr(l, map->key_size);\nfor_each_possible_cpu(cpu) {\nbpf_long_memcpy(dst_val + off,\nper_cpu_ptr(pptr, cpu), size);\noff += size;\n}\n} else {\nvalue = l->key + roundup_key_size;\nif (elem_map_flags & BPF_F_LOCK)\ncopy_map_value_locked(map, dst_val, value,\ntrue);\nelse\ncopy_map_value(map, dst_val, value);\ncheck_and_init_map_lock(map, dst_val);\n}\nif (do_delete) {\nhlist_nulls_del_rcu(&l->hash_node);\nif (is_lru_map) {\nl->batch_flink = node_to_free;\nnode_to_free = l;\n} else {\nfree_htab_elem(htab, l);\n}\n}\ndst_key += key_size;\ndst_val += value_size;\n}\nhtab_unlock_bucket(htab, b, batch, flags);\nlocked = false;\nwhile (node_to_free) {\nl = node_to_free;\nnode_to_free = node_to_free->batch_flink;\nbpf_lru_push_free(&htab->lru, &l->lru_node);\n}\nnext_batch:\nif (!bucket_cnt && (batch + 1 < htab->n_buckets)) {\nbatch++;\ngoto again_nocopy;\n}\nrcu_read_unlock();\nbpf_enable_instrumentation();\nif (bucket_cnt && (copy_to_user(ukeys + total * key_size, keys,\nkey_size * bucket_cnt) ||\ncopy_to_user(uvalues + total * value_size, values,\nvalue_size * bucket_cnt))) {\nret = -EFAULT;\ngoto after_loop;\n}\ntotal += bucket_cnt;\nbatch++;\nif (batch >= htab->n_buckets) {\nret = -ENOENT;\ngoto after_loop;\n}\ngoto again;\nafter_loop:\nif (ret == -EFAULT)\ngoto out;\nubatch = u64_to_user_ptr(attr->batch.out_batch);\nif (copy_to_user(ubatch, &batch, sizeof(batch)) ||\nput_user(total, &uattr->batch.count))\nret = -EFAULT;\nout:\nkvfree(keys);\nkvfree(values);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2021-39685": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for the length of data in USB control requests, leading to a possible out-of-bounds write.",
        "trigger_condition": "When processing USB control requests, if the length of data in the request exceeds a certain buffer size and the request is not handled appropriately, an out-of-bounds write vulnerability can occur.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the length of data in USB control requests exceeds a specified buffer size. This can lead to an out-of-bounds write if the request is not handled correctly, potentially resulting in a local escalation of privilege.",
        "solution": "To mitigate the vulnerability, it is necessary to validate the length of data in USB control requests and ensure that it does not exceed the specified buffer size. If the length exceeds the buffer size, the code should handle the request appropriately by either skipping the operation (for OUT requests) or adjusting the length to the buffer size before proceeding. This helps prevent the out-of-bounds write and enhances the security of the code."
      },
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability described in CVE-2021-39685. The vulnerability involves a possible out-of-bounds write due to an incorrect flag check in the USB gadget subsystem setup methods. By checking if the `w_length` exceeds a certain buffer size (`USB_COMP_EP0_BUFSIZ`), and adjusting it accordingly, we can prevent the potential out-of-bounds write and mitigate the vulnerability.\n\nThe added code snippet ensures that if `w_length` exceeds the specified buffer size, the code handles the situation appropriately by either skipping the operation (if it's an OUT request) or adjusting the `w_length` to the buffer size and continuing with the operation. This helps prevent the out-of-bounds write and improves the security of the code.\n\nBy making this modification, the code becomes more robust and less susceptible to the vulnerability, thereby enhancing the overall security of the system.",
      "GPT_purpose": "Handle USB control requests for setting up a composite USB gadget device.",
      "GPT_function": "\n1. composite_setup\n2. config_desc\n3. set_config",
      "CVE_id": "CVE-2021-39685",
      "code_before_change": "int\ncomposite_setup(struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\n\tstruct usb_composite_dev\t*cdev = get_gadget_data(gadget);\n\tstruct usb_request\t\t*req = cdev->req;\n\tint\t\t\t\tvalue = -EOPNOTSUPP;\n\tint\t\t\t\tstatus = 0;\n\tu16\t\t\t\tw_index = le16_to_cpu(ctrl->wIndex);\n\tu8\t\t\t\tintf = w_index & 0xFF;\n\tu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\n\tu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\n\tstruct usb_function\t\t*f = NULL;\n\tu8\t\t\t\tendp;\n\n\t/* partial re-init of the response message; the function or the\n\t * gadget might need to intercept e.g. a control-OUT completion\n\t * when we delegate to it.\n\t */\n\treq->zero = 0;\n\treq->context = cdev;\n\treq->complete = composite_setup_complete;\n\treq->length = 0;\n\tgadget->ep0->driver_data = cdev;\n\n\t/*\n\t * Don't let non-standard requests match any of the cases below\n\t * by accident.\n\t */\n\tif ((ctrl->bRequestType & USB_TYPE_MASK) != USB_TYPE_STANDARD)\n\t\tgoto unknown;\n\n\tswitch (ctrl->bRequest) {\n\n\t/* we handle all standard USB descriptors */\n\tcase USB_REQ_GET_DESCRIPTOR:\n\t\tif (ctrl->bRequestType != USB_DIR_IN)\n\t\t\tgoto unknown;\n\t\tswitch (w_value >> 8) {\n\n\t\tcase USB_DT_DEVICE:\n\t\t\tcdev->desc.bNumConfigurations =\n\t\t\t\tcount_configs(cdev, USB_DT_DEVICE);\n\t\t\tcdev->desc.bMaxPacketSize0 =\n\t\t\t\tcdev->gadget->ep0->maxpacket;\n\t\t\tif (gadget_is_superspeed(gadget)) {\n\t\t\t\tif (gadget->speed >= USB_SPEED_SUPER) {\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0320);\n\t\t\t\t\tcdev->desc.bMaxPacketSize0 = 9;\n\t\t\t\t} else {\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0210);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (gadget->lpm_capable)\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0201);\n\t\t\t\telse\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0200);\n\t\t\t}\n\n\t\t\tvalue = min(w_length, (u16) sizeof cdev->desc);\n\t\t\tmemcpy(req->buf, &cdev->desc, value);\n\t\t\tbreak;\n\t\tcase USB_DT_DEVICE_QUALIFIER:\n\t\t\tif (!gadget_is_dualspeed(gadget) ||\n\t\t\t    gadget->speed >= USB_SPEED_SUPER)\n\t\t\t\tbreak;\n\t\t\tdevice_qual(cdev);\n\t\t\tvalue = min_t(int, w_length,\n\t\t\t\tsizeof(struct usb_qualifier_descriptor));\n\t\t\tbreak;\n\t\tcase USB_DT_OTHER_SPEED_CONFIG:\n\t\t\tif (!gadget_is_dualspeed(gadget) ||\n\t\t\t    gadget->speed >= USB_SPEED_SUPER)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase USB_DT_CONFIG:\n\t\t\tvalue = config_desc(cdev, w_value);\n\t\t\tif (value >= 0)\n\t\t\t\tvalue = min(w_length, (u16) value);\n\t\t\tbreak;\n\t\tcase USB_DT_STRING:\n\t\t\tvalue = get_string(cdev, req->buf,\n\t\t\t\t\tw_index, w_value & 0xff);\n\t\t\tif (value >= 0)\n\t\t\t\tvalue = min(w_length, (u16) value);\n\t\t\tbreak;\n\t\tcase USB_DT_BOS:\n\t\t\tif (gadget_is_superspeed(gadget) ||\n\t\t\t    gadget->lpm_capable) {\n\t\t\t\tvalue = bos_desc(cdev);\n\t\t\t\tvalue = min(w_length, (u16) value);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase USB_DT_OTG:\n\t\t\tif (gadget_is_otg(gadget)) {\n\t\t\t\tstruct usb_configuration *config;\n\t\t\t\tint otg_desc_len = 0;\n\n\t\t\t\tif (cdev->config)\n\t\t\t\t\tconfig = cdev->config;\n\t\t\t\telse\n\t\t\t\t\tconfig = list_first_entry(\n\t\t\t\t\t\t\t&cdev->configs,\n\t\t\t\t\t\tstruct usb_configuration, list);\n\t\t\t\tif (!config)\n\t\t\t\t\tgoto done;\n\n\t\t\t\tif (gadget->otg_caps &&\n\t\t\t\t\t(gadget->otg_caps->otg_rev >= 0x0200))\n\t\t\t\t\totg_desc_len += sizeof(\n\t\t\t\t\t\tstruct usb_otg20_descriptor);\n\t\t\t\telse\n\t\t\t\t\totg_desc_len += sizeof(\n\t\t\t\t\t\tstruct usb_otg_descriptor);\n\n\t\t\t\tvalue = min_t(int, w_length, otg_desc_len);\n\t\t\t\tmemcpy(req->buf, config->descriptors[0], value);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t/* any number of configs can work */\n\tcase USB_REQ_SET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != 0)\n\t\t\tgoto unknown;\n\t\tif (gadget_is_otg(gadget)) {\n\t\t\tif (gadget->a_hnp_support)\n\t\t\t\tDBG(cdev, \"HNP available\\n\");\n\t\t\telse if (gadget->a_alt_hnp_support)\n\t\t\t\tDBG(cdev, \"HNP on another port\\n\");\n\t\t\telse\n\t\t\t\tVDBG(cdev, \"HNP inactive\\n\");\n\t\t}\n\t\tspin_lock(&cdev->lock);\n\t\tvalue = set_config(cdev, ctrl, w_value);\n\t\tspin_unlock(&cdev->lock);\n\t\tbreak;\n\tcase USB_REQ_GET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != USB_DIR_IN)\n\t\t\tgoto unknown;\n\t\tif (cdev->config)\n\t\t\t*(u8 *)req->buf = cdev->config->bConfigurationValue;\n\t\telse\n\t\t\t*(u8 *)req->buf = 0;\n\t\tvalue = min(w_length, (u16) 1);\n\t\tbreak;\n\n\t/* function drivers must handle get/set altsetting */\n\tcase USB_REQ_SET_INTERFACE:\n\t\tif (ctrl->bRequestType != USB_RECIP_INTERFACE)\n\t\t\tgoto unknown;\n\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\tbreak;\n\t\tf = cdev->config->interface[intf];\n\t\tif (!f)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If there's no get_alt() method, we know only altsetting zero\n\t\t * works. There is no need to check if set_alt() is not NULL\n\t\t * as we check this in usb_add_function().\n\t\t */\n\t\tif (w_value && !f->get_alt)\n\t\t\tbreak;\n\n\t\tspin_lock(&cdev->lock);\n\t\tvalue = f->set_alt(f, w_index, w_value);\n\t\tif (value == USB_GADGET_DELAYED_STATUS) {\n\t\t\tDBG(cdev,\n\t\t\t \"%s: interface %d (%s) requested delayed status\\n\",\n\t\t\t\t\t__func__, intf, f->name);\n\t\t\tcdev->delayed_status++;\n\t\t\tDBG(cdev, \"delayed_status count %d\\n\",\n\t\t\t\t\tcdev->delayed_status);\n\t\t}\n\t\tspin_unlock(&cdev->lock);\n\t\tbreak;\n\tcase USB_REQ_GET_INTERFACE:\n\t\tif (ctrl->bRequestType != (USB_DIR_IN|USB_RECIP_INTERFACE))\n\t\t\tgoto unknown;\n\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\tbreak;\n\t\tf = cdev->config->interface[intf];\n\t\tif (!f)\n\t\t\tbreak;\n\t\t/* lots of interfaces only need altsetting zero... */\n\t\tvalue = f->get_alt ? f->get_alt(f, w_index) : 0;\n\t\tif (value < 0)\n\t\t\tbreak;\n\t\t*((u8 *)req->buf) = value;\n\t\tvalue = min(w_length, (u16) 1);\n\t\tbreak;\n\tcase USB_REQ_GET_STATUS:\n\t\tif (gadget_is_otg(gadget) && gadget->hnp_polling_support &&\n\t\t\t\t\t\t(w_index == OTG_STS_SELECTOR)) {\n\t\t\tif (ctrl->bRequestType != (USB_DIR_IN |\n\t\t\t\t\t\t\tUSB_RECIP_DEVICE))\n\t\t\t\tgoto unknown;\n\t\t\t*((u8 *)req->buf) = gadget->host_request_flag;\n\t\t\tvalue = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * USB 3.0 additions:\n\t\t * Function driver should handle get_status request. If such cb\n\t\t * wasn't supplied we respond with default value = 0\n\t\t * Note: function driver should supply such cb only for the\n\t\t * first interface of the function\n\t\t */\n\t\tif (!gadget_is_superspeed(gadget))\n\t\t\tgoto unknown;\n\t\tif (ctrl->bRequestType != (USB_DIR_IN | USB_RECIP_INTERFACE))\n\t\t\tgoto unknown;\n\t\tvalue = 2;\t/* This is the length of the get_status reply */\n\t\tput_unaligned_le16(0, req->buf);\n\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\tbreak;\n\t\tf = cdev->config->interface[intf];\n\t\tif (!f)\n\t\t\tbreak;\n\t\tstatus = f->get_status ? f->get_status(f) : 0;\n\t\tif (status < 0)\n\t\t\tbreak;\n\t\tput_unaligned_le16(status & 0x0000ffff, req->buf);\n\t\tbreak;\n\t/*\n\t * Function drivers should handle SetFeature/ClearFeature\n\t * (FUNCTION_SUSPEND) request. function_suspend cb should be supplied\n\t * only for the first interface of the function\n\t */\n\tcase USB_REQ_CLEAR_FEATURE:\n\tcase USB_REQ_SET_FEATURE:\n\t\tif (!gadget_is_superspeed(gadget))\n\t\t\tgoto unknown;\n\t\tif (ctrl->bRequestType != (USB_DIR_OUT | USB_RECIP_INTERFACE))\n\t\t\tgoto unknown;\n\t\tswitch (w_value) {\n\t\tcase USB_INTRF_FUNC_SUSPEND:\n\t\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\t\tbreak;\n\t\t\tf = cdev->config->interface[intf];\n\t\t\tif (!f)\n\t\t\t\tbreak;\n\t\t\tvalue = 0;\n\t\t\tif (f->func_suspend)\n\t\t\t\tvalue = f->func_suspend(f, w_index >> 8);\n\t\t\tif (value < 0) {\n\t\t\t\tERROR(cdev,\n\t\t\t\t      \"func_suspend() returned error %d\\n\",\n\t\t\t\t      value);\n\t\t\t\tvalue = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\nunknown:\n\t\t/*\n\t\t * OS descriptors handling\n\t\t */\n\t\tif (cdev->use_os_string && cdev->os_desc_config &&\n\t\t    (ctrl->bRequestType & USB_TYPE_VENDOR) &&\n\t\t    ctrl->bRequest == cdev->b_vendor_code) {\n\t\t\tstruct usb_configuration\t*os_desc_cfg;\n\t\t\tu8\t\t\t\t*buf;\n\t\t\tint\t\t\t\tinterface;\n\t\t\tint\t\t\t\tcount = 0;\n\n\t\t\treq = cdev->os_desc_req;\n\t\t\treq->context = cdev;\n\t\t\treq->complete = composite_setup_complete;\n\t\t\tbuf = req->buf;\n\t\t\tos_desc_cfg = cdev->os_desc_config;\n\t\t\tw_length = min_t(u16, w_length, USB_COMP_EP0_OS_DESC_BUFSIZ);\n\t\t\tmemset(buf, 0, w_length);\n\t\t\tbuf[5] = 0x01;\n\t\t\tswitch (ctrl->bRequestType & USB_RECIP_MASK) {\n\t\t\tcase USB_RECIP_DEVICE:\n\t\t\t\tif (w_index != 0x4 || (w_value >> 8))\n\t\t\t\t\tbreak;\n\t\t\t\tbuf[6] = w_index;\n\t\t\t\t/* Number of ext compat interfaces */\n\t\t\t\tcount = count_ext_compat(os_desc_cfg);\n\t\t\t\tbuf[8] = count;\n\t\t\t\tcount *= 24; /* 24 B/ext compat desc */\n\t\t\t\tcount += 16; /* header */\n\t\t\t\tput_unaligned_le32(count, buf);\n\t\t\t\tvalue = w_length;\n\t\t\t\tif (w_length > 0x10) {\n\t\t\t\t\tvalue = fill_ext_compat(os_desc_cfg, buf);\n\t\t\t\t\tvalue = min_t(u16, w_length, value);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase USB_RECIP_INTERFACE:\n\t\t\t\tif (w_index != 0x5 || (w_value >> 8))\n\t\t\t\t\tbreak;\n\t\t\t\tinterface = w_value & 0xFF;\n\t\t\t\tbuf[6] = w_index;\n\t\t\t\tcount = count_ext_prop(os_desc_cfg,\n\t\t\t\t\tinterface);\n\t\t\t\tput_unaligned_le16(count, buf + 8);\n\t\t\t\tcount = len_ext_prop(os_desc_cfg,\n\t\t\t\t\tinterface);\n\t\t\t\tput_unaligned_le32(count, buf);\n\t\t\t\tvalue = w_length;\n\t\t\t\tif (w_length > 0x0A) {\n\t\t\t\t\tvalue = fill_ext_prop(os_desc_cfg,\n\t\t\t\t\t\t\t      interface, buf);\n\t\t\t\t\tif (value >= 0)\n\t\t\t\t\t\tvalue = min_t(u16, w_length, value);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgoto check_value;\n\t\t}\n\n\t\tVDBG(cdev,\n\t\t\t\"non-core control req%02x.%02x v%04x i%04x l%d\\n\",\n\t\t\tctrl->bRequestType, ctrl->bRequest,\n\t\t\tw_value, w_index, w_length);\n\n\t\t/* functions always handle their interfaces and endpoints...\n\t\t * punt other recipients (other, WUSB, ...) to the current\n\t\t * configuration code.\n\t\t */\n\t\tif (cdev->config) {\n\t\t\tlist_for_each_entry(f, &cdev->config->functions, list)\n\t\t\t\tif (f->req_match &&\n\t\t\t\t    f->req_match(f, ctrl, false))\n\t\t\t\t\tgoto try_fun_setup;\n\t\t} else {\n\t\t\tstruct usb_configuration *c;\n\t\t\tlist_for_each_entry(c, &cdev->configs, list)\n\t\t\t\tlist_for_each_entry(f, &c->functions, list)\n\t\t\t\t\tif (f->req_match &&\n\t\t\t\t\t    f->req_match(f, ctrl, true))\n\t\t\t\t\t\tgoto try_fun_setup;\n\t\t}\n\t\tf = NULL;\n\n\t\tswitch (ctrl->bRequestType & USB_RECIP_MASK) {\n\t\tcase USB_RECIP_INTERFACE:\n\t\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\t\tbreak;\n\t\t\tf = cdev->config->interface[intf];\n\t\t\tbreak;\n\n\t\tcase USB_RECIP_ENDPOINT:\n\t\t\tif (!cdev->config)\n\t\t\t\tbreak;\n\t\t\tendp = ((w_index & 0x80) >> 3) | (w_index & 0x0f);\n\t\t\tlist_for_each_entry(f, &cdev->config->functions, list) {\n\t\t\t\tif (test_bit(endp, f->endpoints))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (&f->list == &cdev->config->functions)\n\t\t\t\tf = NULL;\n\t\t\tbreak;\n\t\t}\ntry_fun_setup:\n\t\tif (f && f->setup)\n\t\t\tvalue = f->setup(f, ctrl);\n\t\telse {\n\t\t\tstruct usb_configuration\t*c;\n\n\t\t\tc = cdev->config;\n\t\t\tif (!c)\n\t\t\t\tgoto done;\n\n\t\t\t/* try current config's setup */\n\t\t\tif (c->setup) {\n\t\t\t\tvalue = c->setup(c, ctrl);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* try the only function in the current config */\n\t\t\tif (!list_is_singular(&c->functions))\n\t\t\t\tgoto done;\n\t\t\tf = list_first_entry(&c->functions, struct usb_function,\n\t\t\t\t\t     list);\n\t\t\tif (f->setup)\n\t\t\t\tvalue = f->setup(f, ctrl);\n\t\t}\n\n\t\tgoto done;\n\t}\n\ncheck_value:\n\t/* respond with data transfer before status phase? */\n\tif (value >= 0 && value != USB_GADGET_DELAYED_STATUS) {\n\t\treq->length = value;\n\t\treq->context = cdev;\n\t\treq->zero = value < w_length;\n\t\tvalue = composite_ep0_queue(cdev, req, GFP_ATOMIC);\n\t\tif (value < 0) {\n\t\t\tDBG(cdev, \"ep_queue --> %d\\n\", value);\n\t\t\treq->status = 0;\n\t\t\tcomposite_setup_complete(gadget->ep0, req);\n\t\t}\n\t} else if (value == USB_GADGET_DELAYED_STATUS && w_length != 0) {\n\t\tWARN(cdev,\n\t\t\t\"%s: Delayed status not supported for w_length != 0\",\n\t\t\t__func__);\n\t}\n\ndone:\n\t/* device either stalls (value < 0) or reports success */\n\treturn value;\n}",
      "code_after_change": "int\ncomposite_setup(struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\n\tstruct usb_composite_dev\t*cdev = get_gadget_data(gadget);\n\tstruct usb_request\t\t*req = cdev->req;\n\tint\t\t\t\tvalue = -EOPNOTSUPP;\n\tint\t\t\t\tstatus = 0;\n\tu16\t\t\t\tw_index = le16_to_cpu(ctrl->wIndex);\n\tu8\t\t\t\tintf = w_index & 0xFF;\n\tu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\n\tu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\n\tstruct usb_function\t\t*f = NULL;\n\tu8\t\t\t\tendp;\n\n\tif (w_length > USB_COMP_EP0_BUFSIZ) {\n\t\tif (ctrl->bRequestType == USB_DIR_OUT) {\n\t\t\tgoto done;\n\t\t} else {\n\t\t\t/* Cast away the const, we are going to overwrite on purpose. */\n\t\t\t__le16 *temp = (__le16 *)&ctrl->wLength;\n\n\t\t\t*temp = cpu_to_le16(USB_COMP_EP0_BUFSIZ);\n\t\t\tw_length = USB_COMP_EP0_BUFSIZ;\n\t\t}\n\t}\n\n\t/* partial re-init of the response message; the function or the\n\t * gadget might need to intercept e.g. a control-OUT completion\n\t * when we delegate to it.\n\t */\n\treq->zero = 0;\n\treq->context = cdev;\n\treq->complete = composite_setup_complete;\n\treq->length = 0;\n\tgadget->ep0->driver_data = cdev;\n\n\t/*\n\t * Don't let non-standard requests match any of the cases below\n\t * by accident.\n\t */\n\tif ((ctrl->bRequestType & USB_TYPE_MASK) != USB_TYPE_STANDARD)\n\t\tgoto unknown;\n\n\tswitch (ctrl->bRequest) {\n\n\t/* we handle all standard USB descriptors */\n\tcase USB_REQ_GET_DESCRIPTOR:\n\t\tif (ctrl->bRequestType != USB_DIR_IN)\n\t\t\tgoto unknown;\n\t\tswitch (w_value >> 8) {\n\n\t\tcase USB_DT_DEVICE:\n\t\t\tcdev->desc.bNumConfigurations =\n\t\t\t\tcount_configs(cdev, USB_DT_DEVICE);\n\t\t\tcdev->desc.bMaxPacketSize0 =\n\t\t\t\tcdev->gadget->ep0->maxpacket;\n\t\t\tif (gadget_is_superspeed(gadget)) {\n\t\t\t\tif (gadget->speed >= USB_SPEED_SUPER) {\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0320);\n\t\t\t\t\tcdev->desc.bMaxPacketSize0 = 9;\n\t\t\t\t} else {\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0210);\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (gadget->lpm_capable)\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0201);\n\t\t\t\telse\n\t\t\t\t\tcdev->desc.bcdUSB = cpu_to_le16(0x0200);\n\t\t\t}\n\n\t\t\tvalue = min(w_length, (u16) sizeof cdev->desc);\n\t\t\tmemcpy(req->buf, &cdev->desc, value);\n\t\t\tbreak;\n\t\tcase USB_DT_DEVICE_QUALIFIER:\n\t\t\tif (!gadget_is_dualspeed(gadget) ||\n\t\t\t    gadget->speed >= USB_SPEED_SUPER)\n\t\t\t\tbreak;\n\t\t\tdevice_qual(cdev);\n\t\t\tvalue = min_t(int, w_length,\n\t\t\t\tsizeof(struct usb_qualifier_descriptor));\n\t\t\tbreak;\n\t\tcase USB_DT_OTHER_SPEED_CONFIG:\n\t\t\tif (!gadget_is_dualspeed(gadget) ||\n\t\t\t    gadget->speed >= USB_SPEED_SUPER)\n\t\t\t\tbreak;\n\t\t\tfallthrough;\n\t\tcase USB_DT_CONFIG:\n\t\t\tvalue = config_desc(cdev, w_value);\n\t\t\tif (value >= 0)\n\t\t\t\tvalue = min(w_length, (u16) value);\n\t\t\tbreak;\n\t\tcase USB_DT_STRING:\n\t\t\tvalue = get_string(cdev, req->buf,\n\t\t\t\t\tw_index, w_value & 0xff);\n\t\t\tif (value >= 0)\n\t\t\t\tvalue = min(w_length, (u16) value);\n\t\t\tbreak;\n\t\tcase USB_DT_BOS:\n\t\t\tif (gadget_is_superspeed(gadget) ||\n\t\t\t    gadget->lpm_capable) {\n\t\t\t\tvalue = bos_desc(cdev);\n\t\t\t\tvalue = min(w_length, (u16) value);\n\t\t\t}\n\t\t\tbreak;\n\t\tcase USB_DT_OTG:\n\t\t\tif (gadget_is_otg(gadget)) {\n\t\t\t\tstruct usb_configuration *config;\n\t\t\t\tint otg_desc_len = 0;\n\n\t\t\t\tif (cdev->config)\n\t\t\t\t\tconfig = cdev->config;\n\t\t\t\telse\n\t\t\t\t\tconfig = list_first_entry(\n\t\t\t\t\t\t\t&cdev->configs,\n\t\t\t\t\t\tstruct usb_configuration, list);\n\t\t\t\tif (!config)\n\t\t\t\t\tgoto done;\n\n\t\t\t\tif (gadget->otg_caps &&\n\t\t\t\t\t(gadget->otg_caps->otg_rev >= 0x0200))\n\t\t\t\t\totg_desc_len += sizeof(\n\t\t\t\t\t\tstruct usb_otg20_descriptor);\n\t\t\t\telse\n\t\t\t\t\totg_desc_len += sizeof(\n\t\t\t\t\t\tstruct usb_otg_descriptor);\n\n\t\t\t\tvalue = min_t(int, w_length, otg_desc_len);\n\t\t\t\tmemcpy(req->buf, config->descriptors[0], value);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t/* any number of configs can work */\n\tcase USB_REQ_SET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != 0)\n\t\t\tgoto unknown;\n\t\tif (gadget_is_otg(gadget)) {\n\t\t\tif (gadget->a_hnp_support)\n\t\t\t\tDBG(cdev, \"HNP available\\n\");\n\t\t\telse if (gadget->a_alt_hnp_support)\n\t\t\t\tDBG(cdev, \"HNP on another port\\n\");\n\t\t\telse\n\t\t\t\tVDBG(cdev, \"HNP inactive\\n\");\n\t\t}\n\t\tspin_lock(&cdev->lock);\n\t\tvalue = set_config(cdev, ctrl, w_value);\n\t\tspin_unlock(&cdev->lock);\n\t\tbreak;\n\tcase USB_REQ_GET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != USB_DIR_IN)\n\t\t\tgoto unknown;\n\t\tif (cdev->config)\n\t\t\t*(u8 *)req->buf = cdev->config->bConfigurationValue;\n\t\telse\n\t\t\t*(u8 *)req->buf = 0;\n\t\tvalue = min(w_length, (u16) 1);\n\t\tbreak;\n\n\t/* function drivers must handle get/set altsetting */\n\tcase USB_REQ_SET_INTERFACE:\n\t\tif (ctrl->bRequestType != USB_RECIP_INTERFACE)\n\t\t\tgoto unknown;\n\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\tbreak;\n\t\tf = cdev->config->interface[intf];\n\t\tif (!f)\n\t\t\tbreak;\n\n\t\t/*\n\t\t * If there's no get_alt() method, we know only altsetting zero\n\t\t * works. There is no need to check if set_alt() is not NULL\n\t\t * as we check this in usb_add_function().\n\t\t */\n\t\tif (w_value && !f->get_alt)\n\t\t\tbreak;\n\n\t\tspin_lock(&cdev->lock);\n\t\tvalue = f->set_alt(f, w_index, w_value);\n\t\tif (value == USB_GADGET_DELAYED_STATUS) {\n\t\t\tDBG(cdev,\n\t\t\t \"%s: interface %d (%s) requested delayed status\\n\",\n\t\t\t\t\t__func__, intf, f->name);\n\t\t\tcdev->delayed_status++;\n\t\t\tDBG(cdev, \"delayed_status count %d\\n\",\n\t\t\t\t\tcdev->delayed_status);\n\t\t}\n\t\tspin_unlock(&cdev->lock);\n\t\tbreak;\n\tcase USB_REQ_GET_INTERFACE:\n\t\tif (ctrl->bRequestType != (USB_DIR_IN|USB_RECIP_INTERFACE))\n\t\t\tgoto unknown;\n\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\tbreak;\n\t\tf = cdev->config->interface[intf];\n\t\tif (!f)\n\t\t\tbreak;\n\t\t/* lots of interfaces only need altsetting zero... */\n\t\tvalue = f->get_alt ? f->get_alt(f, w_index) : 0;\n\t\tif (value < 0)\n\t\t\tbreak;\n\t\t*((u8 *)req->buf) = value;\n\t\tvalue = min(w_length, (u16) 1);\n\t\tbreak;\n\tcase USB_REQ_GET_STATUS:\n\t\tif (gadget_is_otg(gadget) && gadget->hnp_polling_support &&\n\t\t\t\t\t\t(w_index == OTG_STS_SELECTOR)) {\n\t\t\tif (ctrl->bRequestType != (USB_DIR_IN |\n\t\t\t\t\t\t\tUSB_RECIP_DEVICE))\n\t\t\t\tgoto unknown;\n\t\t\t*((u8 *)req->buf) = gadget->host_request_flag;\n\t\t\tvalue = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\t/*\n\t\t * USB 3.0 additions:\n\t\t * Function driver should handle get_status request. If such cb\n\t\t * wasn't supplied we respond with default value = 0\n\t\t * Note: function driver should supply such cb only for the\n\t\t * first interface of the function\n\t\t */\n\t\tif (!gadget_is_superspeed(gadget))\n\t\t\tgoto unknown;\n\t\tif (ctrl->bRequestType != (USB_DIR_IN | USB_RECIP_INTERFACE))\n\t\t\tgoto unknown;\n\t\tvalue = 2;\t/* This is the length of the get_status reply */\n\t\tput_unaligned_le16(0, req->buf);\n\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\tbreak;\n\t\tf = cdev->config->interface[intf];\n\t\tif (!f)\n\t\t\tbreak;\n\t\tstatus = f->get_status ? f->get_status(f) : 0;\n\t\tif (status < 0)\n\t\t\tbreak;\n\t\tput_unaligned_le16(status & 0x0000ffff, req->buf);\n\t\tbreak;\n\t/*\n\t * Function drivers should handle SetFeature/ClearFeature\n\t * (FUNCTION_SUSPEND) request. function_suspend cb should be supplied\n\t * only for the first interface of the function\n\t */\n\tcase USB_REQ_CLEAR_FEATURE:\n\tcase USB_REQ_SET_FEATURE:\n\t\tif (!gadget_is_superspeed(gadget))\n\t\t\tgoto unknown;\n\t\tif (ctrl->bRequestType != (USB_DIR_OUT | USB_RECIP_INTERFACE))\n\t\t\tgoto unknown;\n\t\tswitch (w_value) {\n\t\tcase USB_INTRF_FUNC_SUSPEND:\n\t\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\t\tbreak;\n\t\t\tf = cdev->config->interface[intf];\n\t\t\tif (!f)\n\t\t\t\tbreak;\n\t\t\tvalue = 0;\n\t\t\tif (f->func_suspend)\n\t\t\t\tvalue = f->func_suspend(f, w_index >> 8);\n\t\t\tif (value < 0) {\n\t\t\t\tERROR(cdev,\n\t\t\t\t      \"func_suspend() returned error %d\\n\",\n\t\t\t\t      value);\n\t\t\t\tvalue = 0;\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tdefault:\nunknown:\n\t\t/*\n\t\t * OS descriptors handling\n\t\t */\n\t\tif (cdev->use_os_string && cdev->os_desc_config &&\n\t\t    (ctrl->bRequestType & USB_TYPE_VENDOR) &&\n\t\t    ctrl->bRequest == cdev->b_vendor_code) {\n\t\t\tstruct usb_configuration\t*os_desc_cfg;\n\t\t\tu8\t\t\t\t*buf;\n\t\t\tint\t\t\t\tinterface;\n\t\t\tint\t\t\t\tcount = 0;\n\n\t\t\treq = cdev->os_desc_req;\n\t\t\treq->context = cdev;\n\t\t\treq->complete = composite_setup_complete;\n\t\t\tbuf = req->buf;\n\t\t\tos_desc_cfg = cdev->os_desc_config;\n\t\t\tw_length = min_t(u16, w_length, USB_COMP_EP0_OS_DESC_BUFSIZ);\n\t\t\tmemset(buf, 0, w_length);\n\t\t\tbuf[5] = 0x01;\n\t\t\tswitch (ctrl->bRequestType & USB_RECIP_MASK) {\n\t\t\tcase USB_RECIP_DEVICE:\n\t\t\t\tif (w_index != 0x4 || (w_value >> 8))\n\t\t\t\t\tbreak;\n\t\t\t\tbuf[6] = w_index;\n\t\t\t\t/* Number of ext compat interfaces */\n\t\t\t\tcount = count_ext_compat(os_desc_cfg);\n\t\t\t\tbuf[8] = count;\n\t\t\t\tcount *= 24; /* 24 B/ext compat desc */\n\t\t\t\tcount += 16; /* header */\n\t\t\t\tput_unaligned_le32(count, buf);\n\t\t\t\tvalue = w_length;\n\t\t\t\tif (w_length > 0x10) {\n\t\t\t\t\tvalue = fill_ext_compat(os_desc_cfg, buf);\n\t\t\t\t\tvalue = min_t(u16, w_length, value);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase USB_RECIP_INTERFACE:\n\t\t\t\tif (w_index != 0x5 || (w_value >> 8))\n\t\t\t\t\tbreak;\n\t\t\t\tinterface = w_value & 0xFF;\n\t\t\t\tbuf[6] = w_index;\n\t\t\t\tcount = count_ext_prop(os_desc_cfg,\n\t\t\t\t\tinterface);\n\t\t\t\tput_unaligned_le16(count, buf + 8);\n\t\t\t\tcount = len_ext_prop(os_desc_cfg,\n\t\t\t\t\tinterface);\n\t\t\t\tput_unaligned_le32(count, buf);\n\t\t\t\tvalue = w_length;\n\t\t\t\tif (w_length > 0x0A) {\n\t\t\t\t\tvalue = fill_ext_prop(os_desc_cfg,\n\t\t\t\t\t\t\t      interface, buf);\n\t\t\t\t\tif (value >= 0)\n\t\t\t\t\t\tvalue = min_t(u16, w_length, value);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tgoto check_value;\n\t\t}\n\n\t\tVDBG(cdev,\n\t\t\t\"non-core control req%02x.%02x v%04x i%04x l%d\\n\",\n\t\t\tctrl->bRequestType, ctrl->bRequest,\n\t\t\tw_value, w_index, w_length);\n\n\t\t/* functions always handle their interfaces and endpoints...\n\t\t * punt other recipients (other, WUSB, ...) to the current\n\t\t * configuration code.\n\t\t */\n\t\tif (cdev->config) {\n\t\t\tlist_for_each_entry(f, &cdev->config->functions, list)\n\t\t\t\tif (f->req_match &&\n\t\t\t\t    f->req_match(f, ctrl, false))\n\t\t\t\t\tgoto try_fun_setup;\n\t\t} else {\n\t\t\tstruct usb_configuration *c;\n\t\t\tlist_for_each_entry(c, &cdev->configs, list)\n\t\t\t\tlist_for_each_entry(f, &c->functions, list)\n\t\t\t\t\tif (f->req_match &&\n\t\t\t\t\t    f->req_match(f, ctrl, true))\n\t\t\t\t\t\tgoto try_fun_setup;\n\t\t}\n\t\tf = NULL;\n\n\t\tswitch (ctrl->bRequestType & USB_RECIP_MASK) {\n\t\tcase USB_RECIP_INTERFACE:\n\t\t\tif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\n\t\t\t\tbreak;\n\t\t\tf = cdev->config->interface[intf];\n\t\t\tbreak;\n\n\t\tcase USB_RECIP_ENDPOINT:\n\t\t\tif (!cdev->config)\n\t\t\t\tbreak;\n\t\t\tendp = ((w_index & 0x80) >> 3) | (w_index & 0x0f);\n\t\t\tlist_for_each_entry(f, &cdev->config->functions, list) {\n\t\t\t\tif (test_bit(endp, f->endpoints))\n\t\t\t\t\tbreak;\n\t\t\t}\n\t\t\tif (&f->list == &cdev->config->functions)\n\t\t\t\tf = NULL;\n\t\t\tbreak;\n\t\t}\ntry_fun_setup:\n\t\tif (f && f->setup)\n\t\t\tvalue = f->setup(f, ctrl);\n\t\telse {\n\t\t\tstruct usb_configuration\t*c;\n\n\t\t\tc = cdev->config;\n\t\t\tif (!c)\n\t\t\t\tgoto done;\n\n\t\t\t/* try current config's setup */\n\t\t\tif (c->setup) {\n\t\t\t\tvalue = c->setup(c, ctrl);\n\t\t\t\tgoto done;\n\t\t\t}\n\n\t\t\t/* try the only function in the current config */\n\t\t\tif (!list_is_singular(&c->functions))\n\t\t\t\tgoto done;\n\t\t\tf = list_first_entry(&c->functions, struct usb_function,\n\t\t\t\t\t     list);\n\t\t\tif (f->setup)\n\t\t\t\tvalue = f->setup(f, ctrl);\n\t\t}\n\n\t\tgoto done;\n\t}\n\ncheck_value:\n\t/* respond with data transfer before status phase? */\n\tif (value >= 0 && value != USB_GADGET_DELAYED_STATUS) {\n\t\treq->length = value;\n\t\treq->context = cdev;\n\t\treq->zero = value < w_length;\n\t\tvalue = composite_ep0_queue(cdev, req, GFP_ATOMIC);\n\t\tif (value < 0) {\n\t\t\tDBG(cdev, \"ep_queue --> %d\\n\", value);\n\t\t\treq->status = 0;\n\t\t\tcomposite_setup_complete(gadget->ep0, req);\n\t\t}\n\t} else if (value == USB_GADGET_DELAYED_STATUS && w_length != 0) {\n\t\tWARN(cdev,\n\t\t\t\"%s: Delayed status not supported for w_length != 0\",\n\t\t\t__func__);\n\t}\n\ndone:\n\t/* device either stalls (value < 0) or reports success */\n\treturn value;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (w_length > USB_COMP_EP0_BUFSIZ) {",
          "\t\tif (ctrl->bRequestType == USB_DIR_OUT) {",
          "\t\t\tgoto done;",
          "\t\t} else {",
          "\t\t\t/* Cast away the const, we are going to overwrite on purpose. */",
          "\t\t\t__le16 *temp = (__le16 *)&ctrl->wLength;",
          "",
          "\t\t\t*temp = cpu_to_le16(USB_COMP_EP0_BUFSIZ);",
          "\t\t\tw_length = USB_COMP_EP0_BUFSIZ;",
          "\t\t}",
          "\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation for the length of data in USB control requests, leading to a possible out-of-bounds write.",
      "trigger_condition": "When processing USB control requests, if the length of data in the request exceeds a certain buffer size and the request is not handled appropriately, an out-of-bounds write vulnerability can occur.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the length of data in USB control requests exceeds a specified buffer size. This can lead to an out-of-bounds write if the request is not handled correctly, potentially resulting in a local escalation of privilege.",
      "solution": "To mitigate the vulnerability, it is necessary to validate the length of data in USB control requests and ensure that it does not exceed the specified buffer size. If the length exceeds the buffer size, the code should handle the request appropriately by either skipping the operation (for OUT requests) or adjusting the length to the buffer size before proceeding. This helps prevent the out-of-bounds write and enhances the security of the code.",
      "id": 119,
      "code_after_change_normalized": "int\nFUN1(struct usb_gadget *VAR1, const struct usb_ctrlrequest *VAR2)\n{\nstruct usb_composite_dev\t*VAR3 = FUN2(VAR1);\nstruct usb_request\t\t*VAR4 = VAR3->VAR4;\nint\t\t\t\tVAR5 = -VAR6;\nint\t\t\t\tVAR7 = 0;\nu16\t\t\t\tVAR8 = FUN3(VAR2->VAR9);\nu8\t\t\t\tVAR10 = VAR8 & VAR11;\nu16\t\t\t\tVAR12 = FUN3(VAR2->VAR13);\nu16\t\t\t\tVAR14 = FUN3(VAR2->VAR15);\nstruct usb_function\t\t*VAR16 = NULL;\nu8\t\t\t\tVAR17;\nif (VAR14 > VAR18) {\nif (VAR2->VAR19 == VAR20) {\ngoto VAR21;\n} else {\nVAR23 *VAR22 = (VAR23 *)&VAR2->VAR15;\n*VAR22 = FUN4(VAR18);\nVAR14 = VAR18;\n}\n}\nVAR4->VAR24 = 0;\nVAR4->VAR25 = VAR3;\nVAR4->VAR26 = VAR27;\nVAR4->VAR28 = 0;\nVAR1->VAR29->VAR30 = VAR3;\nif ((VAR2->VAR19 & VAR31) != VAR32)\ngoto VAR33;\nswitch (VAR2->VAR34) {\ncase VAR35:\nif (VAR2->VAR19 != VAR36)\ngoto VAR33;\nswitch (VAR12 >> 8) {\ncase VAR37:\nVAR3->VAR38.VAR39 =\nFUN5(VAR3, VAR37);\nVAR3->VAR38.VAR40 =\nVAR3->VAR1->VAR29->VAR41;\nif (FUN6(VAR1)) {\nif (VAR1->VAR42 >= VAR43) {\nVAR3->VAR38.VAR44 = FUN4(VAR11);\nVAR3->VAR38.VAR40 = 9;\n} else {\nVAR3->VAR38.VAR44 = FUN4(VAR11);\n}\n} else {\nif (VAR1->VAR45)\nVAR3->VAR38.VAR44 = FUN4(VAR11);\nelse\nVAR3->VAR38.VAR44 = FUN4(VAR11);\n}\nVAR5 = FUN7(VAR14, (VAR46) sizeof VAR3->VAR38);\nFUN8(VAR4->VAR47, &VAR3->VAR38, VAR5);\nbreak;\ncase VAR48:\nif (!FUN9(VAR1) ||\nVAR1->VAR42 >= VAR43)\nbreak;\nFUN10(VAR3);\nVAR5 = FUN11(int, VAR14,\nsizeof(struct VAR49));\nbreak;\ncase VAR50:\nif (!FUN9(VAR1) ||\nVAR1->VAR42 >= VAR43)\nbreak;\nVAR51;\ncase VAR52:\nVAR5 = FUN12(VAR3, VAR12);\nif (VAR5 >= 0)\nVAR5 = FUN7(VAR14, (VAR46) VAR5);\nbreak;\ncase VAR53:\nVAR5 = FUN13(VAR3, VAR4->VAR47,\nVAR8, VAR12 & VAR11);\nif (VAR5 >= 0)\nVAR5 = FUN7(VAR14, (VAR46) VAR5);\nbreak;\ncase VAR54:\nif (FUN6(VAR1) ||\nVAR1->VAR45) {\nVAR5 = FUN14(VAR3);\nVAR5 = FUN7(VAR14, (VAR46) VAR5);\n}\nbreak;\ncase VAR55:\nif (FUN15(VAR1)) {\nstruct usb_configuration *VAR56;\nint VAR57 = 0;\nif (VAR3->VAR56)\nVAR56 = VAR3->VAR56;\nelse\nVAR56 = FUN16(\n&VAR3->VAR58,\nstruct VAR59, VAR60);\nif (!VAR56)\ngoto VAR21;\nif (VAR1->VAR61 &&\n(VAR1->VAR61->VAR62 >= VAR11))\nVAR57 += sizeof(\nstruct VAR63);\nelse\nVAR57 += sizeof(\nstruct VAR64);\nVAR5 = FUN11(int, VAR14, VAR57);\nFUN8(VAR4->VAR47, VAR56->VAR65[0], VAR5);\n}\nbreak;\n}\nbreak;\ncase VAR66:\nif (VAR2->VAR19 != 0)\ngoto VAR33;\nif (FUN15(VAR1)) {\nif (VAR1->VAR67)\nFUN17(VAR3, \"STR\");\nelse if (VAR1->VAR68)\nFUN17(VAR3, \"STR\");\nelse\nFUN18(VAR3, \"STR\");\n}\nFUN19(&VAR3->VAR69);\nVAR5 = FUN20(VAR3, VAR2, VAR12);\nFUN21(&VAR3->VAR69);\nbreak;\ncase VAR70:\nif (VAR2->VAR19 != VAR36)\ngoto VAR33;\nif (VAR3->VAR56)\n*(VAR71 *)VAR4->VAR47 = VAR3->VAR56->VAR72;\nelse\n*(VAR71 *)VAR4->VAR47 = 0;\nVAR5 = FUN7(VAR14, (VAR46) 1);\nbreak;\ncase VAR73:\nif (VAR2->VAR19 != VAR74)\ngoto VAR33;\nif (!VAR3->VAR56 || VAR10 >= VAR75)\nbreak;\nVAR16 = VAR3->VAR56->VAR76[VAR10];\nif (!VAR16)\nbreak;\nif (VAR12 && !VAR16->VAR77)\nbreak;\nFUN19(&VAR3->VAR69);\nVAR5 = VAR16->FUN22(VAR16, VAR8, VAR12);\nif (VAR5 == VAR78) {\nFUN17(VAR3,\n\"STR\",\nVAR79, VAR10, VAR16->VAR80);\nVAR3->VAR81++;\nFUN17(VAR3, \"STR\",\nVAR3->VAR81);\n}\nFUN21(&VAR3->VAR69);\nbreak;\ncase VAR82:\nif (VAR2->VAR19 != (VAR36|VAR74))\ngoto VAR33;\nif (!VAR3->VAR56 || VAR10 >= VAR75)\nbreak;\nVAR16 = VAR3->VAR56->VAR76[VAR10];\nif (!VAR16)\nbreak;\nVAR5 = VAR16->VAR77 ? VAR16->FUN23(VAR16, VAR8) : 0;\nif (VAR5 < 0)\nbreak;\n*((VAR71 *)VAR4->VAR47) = VAR5;\nVAR5 = FUN7(VAR14, (VAR46) 1);\nbreak;\ncase VAR83:\nif (FUN15(VAR1) && VAR1->VAR84 &&\n(VAR8 == VAR85)) {\nif (VAR2->VAR19 != (VAR36 |\nVAR86))\ngoto VAR33;\n*((VAR71 *)VAR4->VAR47) = VAR1->VAR87;\nVAR5 = 1;\nbreak;\n}\nif (!FUN6(VAR1))\ngoto VAR33;\nif (VAR2->VAR19 != (VAR36 | VAR74))\ngoto VAR33;\nVAR5 = 2;\t\nFUN24(0, VAR4->VAR47);\nif (!VAR3->VAR56 || VAR10 >= VAR75)\nbreak;\nVAR16 = VAR3->VAR56->VAR76[VAR10];\nif (!VAR16)\nbreak;\nVAR7 = VAR16->VAR88 ? VAR16->FUN25(VAR16) : 0;\nif (VAR7 < 0)\nbreak;\nFUN24(VAR7 & VAR11, VAR4->VAR47);\nbreak;\ncase VAR89:\ncase VAR90:\nif (!FUN6(VAR1))\ngoto VAR33;\nif (VAR2->VAR19 != (VAR20 | VAR74))\ngoto VAR33;\nswitch (VAR12) {\ncase VAR91:\nif (!VAR3->VAR56 || VAR10 >= VAR75)\nbreak;\nVAR16 = VAR3->VAR56->VAR76[VAR10];\nif (!VAR16)\nbreak;\nVAR5 = 0;\nif (VAR16->VAR92)\nVAR5 = VAR16->FUN26(VAR16, VAR8 >> 8);\nif (VAR5 < 0) {\nFUN27(VAR3,\n\"STR\",\nVAR5);\nVAR5 = 0;\n}\nbreak;\n}\nbreak;\ndefault:\nVAR33:\nif (VAR3->VAR93 && VAR3->VAR94 &&\n(VAR2->VAR19 & VAR95) &&\nVAR2->VAR34 == VAR3->VAR96) {\nstruct usb_configuration\t*VAR97;\nu8\t\t\t\t*VAR47;\nint\t\t\t\tVAR76;\nint\t\t\t\tVAR98 = 0;\nVAR4 = VAR3->VAR99;\nVAR4->VAR25 = VAR3;\nVAR4->VAR26 = VAR27;\nVAR47 = VAR4->VAR47;\nVAR97 = VAR3->VAR94;\nVAR14 = FUN11(VAR46, VAR14, VAR100);\nFUN28(VAR47, 0, VAR14);\nVAR47[5] = VAR11;\nswitch (VAR2->VAR19 & VAR101) {\ncase VAR86:\nif (VAR8 != VAR11 || (VAR12 >> 8))\nbreak;\nVAR47[6] = VAR8;\nVAR98 = FUN29(VAR97);\nVAR47[8] = VAR98;\nVAR98 *= 24; \nVAR98 += 16; \nFUN30(VAR98, VAR47);\nVAR5 = VAR14;\nif (VAR14 > VAR11) {\nVAR5 = FUN31(VAR97, VAR47);\nVAR5 = FUN11(VAR46, VAR14, VAR5);\n}\nbreak;\ncase VAR74:\nif (VAR8 != VAR11 || (VAR12 >> 8))\nbreak;\nVAR76 = VAR12 & VAR11;\nVAR47[6] = VAR8;\nVAR98 = FUN32(VAR97,\nVAR76);\nFUN24(VAR98, VAR47 + 8);\nVAR98 = FUN33(VAR97,\nVAR76);\nFUN30(VAR98, VAR47);\nVAR5 = VAR14;\nif (VAR14 > VAR11) {\nVAR5 = FUN34(VAR97,\nVAR76, VAR47);\nif (VAR5 >= 0)\nVAR5 = FUN11(VAR46, VAR14, VAR5);\n}\nbreak;\n}\ngoto VAR102;\n}\nFUN18(VAR3,\n\"STR\",\nVAR2->VAR19, VAR2->VAR34,\nVAR12, VAR8, VAR14);\nif (VAR3->VAR56) {\nFUN35(VAR16, &VAR3->VAR56->VAR103, VAR60)\nif (VAR16->VAR104 &&\nVAR16->FUN36(VAR16, VAR2, false))\ngoto VAR105;\n} else {\nstruct usb_configuration *VAR106;\nFUN35(VAR106, &VAR3->VAR58, VAR60)\nFUN35(VAR16, &VAR106->VAR103, VAR60)\nif (VAR16->VAR104 &&\nVAR16->FUN36(VAR16, VAR2, true))\ngoto VAR105;\n}\nVAR16 = NULL;\nswitch (VAR2->VAR19 & VAR101) {\ncase VAR74:\nif (!VAR3->VAR56 || VAR10 >= VAR75)\nbreak;\nVAR16 = VAR3->VAR56->VAR76[VAR10];\nbreak;\ncase VAR107:\nif (!VAR3->VAR56)\nbreak;\nVAR17 = ((VAR8 & VAR11) >> 3) | (VAR8 & VAR11);\nFUN35(VAR16, &VAR3->VAR56->VAR103, VAR60) {\nif (FUN37(VAR17, VAR16->VAR108))\nbreak;\n}\nif (&VAR16->VAR60 == &VAR3->VAR56->VAR103)\nVAR16 = NULL;\nbreak;\n}\nVAR105:\nif (VAR16 && VAR16->VAR109)\nVAR5 = VAR16->FUN38(VAR16, VAR2);\nelse {\nstruct usb_configuration\t*VAR106;\nVAR106 = VAR3->VAR56;\nif (!VAR106)\ngoto VAR21;\nif (VAR106->VAR109) {\nVAR5 = VAR106->FUN38(VAR106, VAR2);\ngoto VAR21;\n}\nif (!FUN39(&VAR106->VAR103))\ngoto VAR21;\nVAR16 = FUN16(&VAR106->VAR103, struct VAR110,\nVAR60);\nif (VAR16->VAR109)\nVAR5 = VAR16->FUN38(VAR16, VAR2);\n}\ngoto VAR21;\n}\nVAR102:\nif (VAR5 >= 0 && VAR5 != VAR78) {\nVAR4->VAR28 = VAR5;\nVAR4->VAR25 = VAR3;\nVAR4->VAR24 = VAR5 < VAR14;\nVAR5 = FUN40(VAR3, VAR4, VAR111);\nif (VAR5 < 0) {\nFUN17(VAR3, \"STR\", VAR5);\nVAR4->VAR7 = 0;\nFUN41(VAR1->VAR29, VAR4);\n}\n} else if (VAR5 == VAR78 && VAR14 != 0) {\nFUN42(VAR3,\n\"STR\",\nVAR79);\n}\nVAR21:\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "int\nFUN1(struct usb_gadget *VAR1, const struct usb_ctrlrequest *VAR2)\n{\nstruct usb_composite_dev\t*VAR3 = FUN2(VAR1);\nstruct usb_request\t\t*VAR4 = VAR3->VAR4;\nint\t\t\t\tVAR5 = -VAR6;\nint\t\t\t\tVAR7 = 0;\nu16\t\t\t\tVAR8 = FUN3(VAR2->VAR9);\nu8\t\t\t\tVAR10 = VAR8 & VAR11;\nu16\t\t\t\tVAR12 = FUN3(VAR2->VAR13);\nu16\t\t\t\tVAR14 = FUN3(VAR2->VAR15);\nstruct usb_function\t\t*VAR16 = NULL;\nu8\t\t\t\tVAR17;\nVAR4->VAR18 = 0;\nVAR4->VAR19 = VAR3;\nVAR4->VAR20 = VAR21;\nVAR4->VAR22 = 0;\nVAR1->VAR23->VAR24 = VAR3;\nif ((VAR2->VAR25 & VAR26) != VAR27)\ngoto VAR28;\nswitch (VAR2->VAR29) {\ncase VAR30:\nif (VAR2->VAR25 != VAR31)\ngoto VAR28;\nswitch (VAR12 >> 8) {\ncase VAR32:\nVAR3->VAR33.VAR34 =\nFUN4(VAR3, VAR32);\nVAR3->VAR33.VAR35 =\nVAR3->VAR1->VAR23->VAR36;\nif (FUN5(VAR1)) {\nif (VAR1->VAR37 >= VAR38) {\nVAR3->VAR33.VAR39 = FUN6(VAR11);\nVAR3->VAR33.VAR35 = 9;\n} else {\nVAR3->VAR33.VAR39 = FUN6(VAR11);\n}\n} else {\nif (VAR1->VAR40)\nVAR3->VAR33.VAR39 = FUN6(VAR11);\nelse\nVAR3->VAR33.VAR39 = FUN6(VAR11);\n}\nVAR5 = FUN7(VAR14, (VAR41) sizeof VAR3->VAR33);\nFUN8(VAR4->VAR42, &VAR3->VAR33, VAR5);\nbreak;\ncase VAR43:\nif (!FUN9(VAR1) ||\nVAR1->VAR37 >= VAR38)\nbreak;\nFUN10(VAR3);\nVAR5 = FUN11(int, VAR14,\nsizeof(struct VAR44));\nbreak;\ncase VAR45:\nif (!FUN9(VAR1) ||\nVAR1->VAR37 >= VAR38)\nbreak;\nVAR46;\ncase VAR47:\nVAR5 = FUN12(VAR3, VAR12);\nif (VAR5 >= 0)\nVAR5 = FUN7(VAR14, (VAR41) VAR5);\nbreak;\ncase VAR48:\nVAR5 = FUN13(VAR3, VAR4->VAR42,\nVAR8, VAR12 & VAR11);\nif (VAR5 >= 0)\nVAR5 = FUN7(VAR14, (VAR41) VAR5);\nbreak;\ncase VAR49:\nif (FUN5(VAR1) ||\nVAR1->VAR40) {\nVAR5 = FUN14(VAR3);\nVAR5 = FUN7(VAR14, (VAR41) VAR5);\n}\nbreak;\ncase VAR50:\nif (FUN15(VAR1)) {\nstruct usb_configuration *VAR51;\nint VAR52 = 0;\nif (VAR3->VAR51)\nVAR51 = VAR3->VAR51;\nelse\nVAR51 = FUN16(\n&VAR3->VAR53,\nstruct VAR54, VAR55);\nif (!VAR51)\ngoto VAR56;\nif (VAR1->VAR57 &&\n(VAR1->VAR57->VAR58 >= VAR11))\nVAR52 += sizeof(\nstruct VAR59);\nelse\nVAR52 += sizeof(\nstruct VAR60);\nVAR5 = FUN11(int, VAR14, VAR52);\nFUN8(VAR4->VAR42, VAR51->VAR61[0], VAR5);\n}\nbreak;\n}\nbreak;\ncase VAR62:\nif (VAR2->VAR25 != 0)\ngoto VAR28;\nif (FUN15(VAR1)) {\nif (VAR1->VAR63)\nFUN17(VAR3, \"STR\");\nelse if (VAR1->VAR64)\nFUN17(VAR3, \"STR\");\nelse\nFUN18(VAR3, \"STR\");\n}\nFUN19(&VAR3->VAR65);\nVAR5 = FUN20(VAR3, VAR2, VAR12);\nFUN21(&VAR3->VAR65);\nbreak;\ncase VAR66:\nif (VAR2->VAR25 != VAR31)\ngoto VAR28;\nif (VAR3->VAR51)\n*(VAR67 *)VAR4->VAR42 = VAR3->VAR51->VAR68;\nelse\n*(VAR67 *)VAR4->VAR42 = 0;\nVAR5 = FUN7(VAR14, (VAR41) 1);\nbreak;\ncase VAR69:\nif (VAR2->VAR25 != VAR70)\ngoto VAR28;\nif (!VAR3->VAR51 || VAR10 >= VAR71)\nbreak;\nVAR16 = VAR3->VAR51->VAR72[VAR10];\nif (!VAR16)\nbreak;\nif (VAR12 && !VAR16->VAR73)\nbreak;\nFUN19(&VAR3->VAR65);\nVAR5 = VAR16->FUN22(VAR16, VAR8, VAR12);\nif (VAR5 == VAR74) {\nFUN17(VAR3,\n\"STR\",\nVAR75, VAR10, VAR16->VAR76);\nVAR3->VAR77++;\nFUN17(VAR3, \"STR\",\nVAR3->VAR77);\n}\nFUN21(&VAR3->VAR65);\nbreak;\ncase VAR78:\nif (VAR2->VAR25 != (VAR31|VAR70))\ngoto VAR28;\nif (!VAR3->VAR51 || VAR10 >= VAR71)\nbreak;\nVAR16 = VAR3->VAR51->VAR72[VAR10];\nif (!VAR16)\nbreak;\nVAR5 = VAR16->VAR73 ? VAR16->FUN23(VAR16, VAR8) : 0;\nif (VAR5 < 0)\nbreak;\n*((VAR67 *)VAR4->VAR42) = VAR5;\nVAR5 = FUN7(VAR14, (VAR41) 1);\nbreak;\ncase VAR79:\nif (FUN15(VAR1) && VAR1->VAR80 &&\n(VAR8 == VAR81)) {\nif (VAR2->VAR25 != (VAR31 |\nVAR82))\ngoto VAR28;\n*((VAR67 *)VAR4->VAR42) = VAR1->VAR83;\nVAR5 = 1;\nbreak;\n}\nif (!FUN5(VAR1))\ngoto VAR28;\nif (VAR2->VAR25 != (VAR31 | VAR70))\ngoto VAR28;\nVAR5 = 2;\t\nFUN24(0, VAR4->VAR42);\nif (!VAR3->VAR51 || VAR10 >= VAR71)\nbreak;\nVAR16 = VAR3->VAR51->VAR72[VAR10];\nif (!VAR16)\nbreak;\nVAR7 = VAR16->VAR84 ? VAR16->FUN25(VAR16) : 0;\nif (VAR7 < 0)\nbreak;\nFUN24(VAR7 & VAR11, VAR4->VAR42);\nbreak;\ncase VAR85:\ncase VAR86:\nif (!FUN5(VAR1))\ngoto VAR28;\nif (VAR2->VAR25 != (VAR87 | VAR70))\ngoto VAR28;\nswitch (VAR12) {\ncase VAR88:\nif (!VAR3->VAR51 || VAR10 >= VAR71)\nbreak;\nVAR16 = VAR3->VAR51->VAR72[VAR10];\nif (!VAR16)\nbreak;\nVAR5 = 0;\nif (VAR16->VAR89)\nVAR5 = VAR16->FUN26(VAR16, VAR8 >> 8);\nif (VAR5 < 0) {\nFUN27(VAR3,\n\"STR\",\nVAR5);\nVAR5 = 0;\n}\nbreak;\n}\nbreak;\ndefault:\nVAR28:\nif (VAR3->VAR90 && VAR3->VAR91 &&\n(VAR2->VAR25 & VAR92) &&\nVAR2->VAR29 == VAR3->VAR93) {\nstruct usb_configuration\t*VAR94;\nu8\t\t\t\t*VAR42;\nint\t\t\t\tVAR72;\nint\t\t\t\tVAR95 = 0;\nVAR4 = VAR3->VAR96;\nVAR4->VAR19 = VAR3;\nVAR4->VAR20 = VAR21;\nVAR42 = VAR4->VAR42;\nVAR94 = VAR3->VAR91;\nVAR14 = FUN11(VAR41, VAR14, VAR97);\nFUN28(VAR42, 0, VAR14);\nVAR42[5] = VAR11;\nswitch (VAR2->VAR25 & VAR98) {\ncase VAR82:\nif (VAR8 != VAR11 || (VAR12 >> 8))\nbreak;\nVAR42[6] = VAR8;\nVAR95 = FUN29(VAR94);\nVAR42[8] = VAR95;\nVAR95 *= 24; \nVAR95 += 16; \nFUN30(VAR95, VAR42);\nVAR5 = VAR14;\nif (VAR14 > VAR11) {\nVAR5 = FUN31(VAR94, VAR42);\nVAR5 = FUN11(VAR41, VAR14, VAR5);\n}\nbreak;\ncase VAR70:\nif (VAR8 != VAR11 || (VAR12 >> 8))\nbreak;\nVAR72 = VAR12 & VAR11;\nVAR42[6] = VAR8;\nVAR95 = FUN32(VAR94,\nVAR72);\nFUN24(VAR95, VAR42 + 8);\nVAR95 = FUN33(VAR94,\nVAR72);\nFUN30(VAR95, VAR42);\nVAR5 = VAR14;\nif (VAR14 > VAR11) {\nVAR5 = FUN34(VAR94,\nVAR72, VAR42);\nif (VAR5 >= 0)\nVAR5 = FUN11(VAR41, VAR14, VAR5);\n}\nbreak;\n}\ngoto VAR99;\n}\nFUN18(VAR3,\n\"STR\",\nVAR2->VAR25, VAR2->VAR29,\nVAR12, VAR8, VAR14);\nif (VAR3->VAR51) {\nFUN35(VAR16, &VAR3->VAR51->VAR100, VAR55)\nif (VAR16->VAR101 &&\nVAR16->FUN36(VAR16, VAR2, false))\ngoto VAR102;\n} else {\nstruct usb_configuration *VAR103;\nFUN35(VAR103, &VAR3->VAR53, VAR55)\nFUN35(VAR16, &VAR103->VAR100, VAR55)\nif (VAR16->VAR101 &&\nVAR16->FUN36(VAR16, VAR2, true))\ngoto VAR102;\n}\nVAR16 = NULL;\nswitch (VAR2->VAR25 & VAR98) {\ncase VAR70:\nif (!VAR3->VAR51 || VAR10 >= VAR71)\nbreak;\nVAR16 = VAR3->VAR51->VAR72[VAR10];\nbreak;\ncase VAR104:\nif (!VAR3->VAR51)\nbreak;\nVAR17 = ((VAR8 & VAR11) >> 3) | (VAR8 & VAR11);\nFUN35(VAR16, &VAR3->VAR51->VAR100, VAR55) {\nif (FUN37(VAR17, VAR16->VAR105))\nbreak;\n}\nif (&VAR16->VAR55 == &VAR3->VAR51->VAR100)\nVAR16 = NULL;\nbreak;\n}\nVAR102:\nif (VAR16 && VAR16->VAR106)\nVAR5 = VAR16->FUN38(VAR16, VAR2);\nelse {\nstruct usb_configuration\t*VAR103;\nVAR103 = VAR3->VAR51;\nif (!VAR103)\ngoto VAR56;\nif (VAR103->VAR106) {\nVAR5 = VAR103->FUN38(VAR103, VAR2);\ngoto VAR56;\n}\nif (!FUN39(&VAR103->VAR100))\ngoto VAR56;\nVAR16 = FUN16(&VAR103->VAR100, struct VAR107,\nVAR55);\nif (VAR16->VAR106)\nVAR5 = VAR16->FUN38(VAR16, VAR2);\n}\ngoto VAR56;\n}\nVAR99:\nif (VAR5 >= 0 && VAR5 != VAR74) {\nVAR4->VAR22 = VAR5;\nVAR4->VAR19 = VAR3;\nVAR4->VAR18 = VAR5 < VAR14;\nVAR5 = FUN40(VAR3, VAR4, VAR108);\nif (VAR5 < 0) {\nFUN17(VAR3, \"STR\", VAR5);\nVAR4->VAR7 = 0;\nFUN41(VAR1->VAR23, VAR4);\n}\n} else if (VAR5 == VAR74 && VAR14 != 0) {\nFUN42(VAR3,\n\"STR\",\nVAR75);\n}\nVAR56:\nreturn VAR5;\n}\n",
      "code_after_change_raw": "int\ncomposite_setup(struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\nstruct usb_composite_dev\t*cdev = get_gadget_data(gadget);\nstruct usb_request\t\t*req = cdev->req;\nint\t\t\t\tvalue = -EOPNOTSUPP;\nint\t\t\t\tstatus = 0;\nu16\t\t\t\tw_index = le16_to_cpu(ctrl->wIndex);\nu8\t\t\t\tintf = w_index & 0xFF;\nu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\nu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\nstruct usb_function\t\t*f = NULL;\nu8\t\t\t\tendp;\nif (w_length > USB_COMP_EP0_BUFSIZ) {\nif (ctrl->bRequestType == USB_DIR_OUT) {\ngoto done;\n} else {\n__le16 *temp = (__le16 *)&ctrl->wLength;\n*temp = cpu_to_le16(USB_COMP_EP0_BUFSIZ);\nw_length = USB_COMP_EP0_BUFSIZ;\n}\n}\nreq->zero = 0;\nreq->context = cdev;\nreq->complete = composite_setup_complete;\nreq->length = 0;\ngadget->ep0->driver_data = cdev;\nif ((ctrl->bRequestType & USB_TYPE_MASK) != USB_TYPE_STANDARD)\ngoto unknown;\nswitch (ctrl->bRequest) {\ncase USB_REQ_GET_DESCRIPTOR:\nif (ctrl->bRequestType != USB_DIR_IN)\ngoto unknown;\nswitch (w_value >> 8) {\ncase USB_DT_DEVICE:\ncdev->desc.bNumConfigurations =\ncount_configs(cdev, USB_DT_DEVICE);\ncdev->desc.bMaxPacketSize0 =\ncdev->gadget->ep0->maxpacket;\nif (gadget_is_superspeed(gadget)) {\nif (gadget->speed >= USB_SPEED_SUPER) {\ncdev->desc.bcdUSB = cpu_to_le16(0x0320);\ncdev->desc.bMaxPacketSize0 = 9;\n} else {\ncdev->desc.bcdUSB = cpu_to_le16(0x0210);\n}\n} else {\nif (gadget->lpm_capable)\ncdev->desc.bcdUSB = cpu_to_le16(0x0201);\nelse\ncdev->desc.bcdUSB = cpu_to_le16(0x0200);\n}\nvalue = min(w_length, (u16) sizeof cdev->desc);\nmemcpy(req->buf, &cdev->desc, value);\nbreak;\ncase USB_DT_DEVICE_QUALIFIER:\nif (!gadget_is_dualspeed(gadget) ||\ngadget->speed >= USB_SPEED_SUPER)\nbreak;\ndevice_qual(cdev);\nvalue = min_t(int, w_length,\nsizeof(struct usb_qualifier_descriptor));\nbreak;\ncase USB_DT_OTHER_SPEED_CONFIG:\nif (!gadget_is_dualspeed(gadget) ||\ngadget->speed >= USB_SPEED_SUPER)\nbreak;\nfallthrough;\ncase USB_DT_CONFIG:\nvalue = config_desc(cdev, w_value);\nif (value >= 0)\nvalue = min(w_length, (u16) value);\nbreak;\ncase USB_DT_STRING:\nvalue = get_string(cdev, req->buf,\nw_index, w_value & 0xff);\nif (value >= 0)\nvalue = min(w_length, (u16) value);\nbreak;\ncase USB_DT_BOS:\nif (gadget_is_superspeed(gadget) ||\ngadget->lpm_capable) {\nvalue = bos_desc(cdev);\nvalue = min(w_length, (u16) value);\n}\nbreak;\ncase USB_DT_OTG:\nif (gadget_is_otg(gadget)) {\nstruct usb_configuration *config;\nint otg_desc_len = 0;\nif (cdev->config)\nconfig = cdev->config;\nelse\nconfig = list_first_entry(\n&cdev->configs,\nstruct usb_configuration, list);\nif (!config)\ngoto done;\nif (gadget->otg_caps &&\n(gadget->otg_caps->otg_rev >= 0x0200))\notg_desc_len += sizeof(\nstruct usb_otg20_descriptor);\nelse\notg_desc_len += sizeof(\nstruct usb_otg_descriptor);\nvalue = min_t(int, w_length, otg_desc_len);\nmemcpy(req->buf, config->descriptors[0], value);\n}\nbreak;\n}\nbreak;\ncase USB_REQ_SET_CONFIGURATION:\nif (ctrl->bRequestType != 0)\ngoto unknown;\nif (gadget_is_otg(gadget)) {\nif (gadget->a_hnp_support)\nDBG(cdev, \"HNP available\\n\");\nelse if (gadget->a_alt_hnp_support)\nDBG(cdev, \"HNP on another port\\n\");\nelse\nVDBG(cdev, \"HNP inactive\\n\");\n}\nspin_lock(&cdev->lock);\nvalue = set_config(cdev, ctrl, w_value);\nspin_unlock(&cdev->lock);\nbreak;\ncase USB_REQ_GET_CONFIGURATION:\nif (ctrl->bRequestType != USB_DIR_IN)\ngoto unknown;\nif (cdev->config)\n*(u8 *)req->buf = cdev->config->bConfigurationValue;\nelse\n*(u8 *)req->buf = 0;\nvalue = min(w_length, (u16) 1);\nbreak;\ncase USB_REQ_SET_INTERFACE:\nif (ctrl->bRequestType != USB_RECIP_INTERFACE)\ngoto unknown;\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nif (w_value && !f->get_alt)\nbreak;\nspin_lock(&cdev->lock);\nvalue = f->set_alt(f, w_index, w_value);\nif (value == USB_GADGET_DELAYED_STATUS) {\nDBG(cdev,\n\"%s: interface %d (%s) requested delayed status\\n\",\n__func__, intf, f->name);\ncdev->delayed_status++;\nDBG(cdev, \"delayed_status count %d\\n\",\ncdev->delayed_status);\n}\nspin_unlock(&cdev->lock);\nbreak;\ncase USB_REQ_GET_INTERFACE:\nif (ctrl->bRequestType != (USB_DIR_IN|USB_RECIP_INTERFACE))\ngoto unknown;\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nvalue = f->get_alt ? f->get_alt(f, w_index) : 0;\nif (value < 0)\nbreak;\n*((u8 *)req->buf) = value;\nvalue = min(w_length, (u16) 1);\nbreak;\ncase USB_REQ_GET_STATUS:\nif (gadget_is_otg(gadget) && gadget->hnp_polling_support &&\n(w_index == OTG_STS_SELECTOR)) {\nif (ctrl->bRequestType != (USB_DIR_IN |\nUSB_RECIP_DEVICE))\ngoto unknown;\n*((u8 *)req->buf) = gadget->host_request_flag;\nvalue = 1;\nbreak;\n}\nif (!gadget_is_superspeed(gadget))\ngoto unknown;\nif (ctrl->bRequestType != (USB_DIR_IN | USB_RECIP_INTERFACE))\ngoto unknown;\nvalue = 2;\t\nput_unaligned_le16(0, req->buf);\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nstatus = f->get_status ? f->get_status(f) : 0;\nif (status < 0)\nbreak;\nput_unaligned_le16(status & 0x0000ffff, req->buf);\nbreak;\ncase USB_REQ_CLEAR_FEATURE:\ncase USB_REQ_SET_FEATURE:\nif (!gadget_is_superspeed(gadget))\ngoto unknown;\nif (ctrl->bRequestType != (USB_DIR_OUT | USB_RECIP_INTERFACE))\ngoto unknown;\nswitch (w_value) {\ncase USB_INTRF_FUNC_SUSPEND:\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nvalue = 0;\nif (f->func_suspend)\nvalue = f->func_suspend(f, w_index >> 8);\nif (value < 0) {\nERROR(cdev,\n\"func_suspend() returned error %d\\n\",\nvalue);\nvalue = 0;\n}\nbreak;\n}\nbreak;\ndefault:\nunknown:\nif (cdev->use_os_string && cdev->os_desc_config &&\n(ctrl->bRequestType & USB_TYPE_VENDOR) &&\nctrl->bRequest == cdev->b_vendor_code) {\nstruct usb_configuration\t*os_desc_cfg;\nu8\t\t\t\t*buf;\nint\t\t\t\tinterface;\nint\t\t\t\tcount = 0;\nreq = cdev->os_desc_req;\nreq->context = cdev;\nreq->complete = composite_setup_complete;\nbuf = req->buf;\nos_desc_cfg = cdev->os_desc_config;\nw_length = min_t(u16, w_length, USB_COMP_EP0_OS_DESC_BUFSIZ);\nmemset(buf, 0, w_length);\nbuf[5] = 0x01;\nswitch (ctrl->bRequestType & USB_RECIP_MASK) {\ncase USB_RECIP_DEVICE:\nif (w_index != 0x4 || (w_value >> 8))\nbreak;\nbuf[6] = w_index;\ncount = count_ext_compat(os_desc_cfg);\nbuf[8] = count;\ncount *= 24; \ncount += 16; \nput_unaligned_le32(count, buf);\nvalue = w_length;\nif (w_length > 0x10) {\nvalue = fill_ext_compat(os_desc_cfg, buf);\nvalue = min_t(u16, w_length, value);\n}\nbreak;\ncase USB_RECIP_INTERFACE:\nif (w_index != 0x5 || (w_value >> 8))\nbreak;\ninterface = w_value & 0xFF;\nbuf[6] = w_index;\ncount = count_ext_prop(os_desc_cfg,\ninterface);\nput_unaligned_le16(count, buf + 8);\ncount = len_ext_prop(os_desc_cfg,\ninterface);\nput_unaligned_le32(count, buf);\nvalue = w_length;\nif (w_length > 0x0A) {\nvalue = fill_ext_prop(os_desc_cfg,\ninterface, buf);\nif (value >= 0)\nvalue = min_t(u16, w_length, value);\n}\nbreak;\n}\ngoto check_value;\n}\nVDBG(cdev,\n\"non-core control req%02x.%02x v%04x i%04x l%d\\n\",\nctrl->bRequestType, ctrl->bRequest,\nw_value, w_index, w_length);\nif (cdev->config) {\nlist_for_each_entry(f, &cdev->config->functions, list)\nif (f->req_match &&\nf->req_match(f, ctrl, false))\ngoto try_fun_setup;\n} else {\nstruct usb_configuration *c;\nlist_for_each_entry(c, &cdev->configs, list)\nlist_for_each_entry(f, &c->functions, list)\nif (f->req_match &&\nf->req_match(f, ctrl, true))\ngoto try_fun_setup;\n}\nf = NULL;\nswitch (ctrl->bRequestType & USB_RECIP_MASK) {\ncase USB_RECIP_INTERFACE:\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nbreak;\ncase USB_RECIP_ENDPOINT:\nif (!cdev->config)\nbreak;\nendp = ((w_index & 0x80) >> 3) | (w_index & 0x0f);\nlist_for_each_entry(f, &cdev->config->functions, list) {\nif (test_bit(endp, f->endpoints))\nbreak;\n}\nif (&f->list == &cdev->config->functions)\nf = NULL;\nbreak;\n}\ntry_fun_setup:\nif (f && f->setup)\nvalue = f->setup(f, ctrl);\nelse {\nstruct usb_configuration\t*c;\nc = cdev->config;\nif (!c)\ngoto done;\nif (c->setup) {\nvalue = c->setup(c, ctrl);\ngoto done;\n}\nif (!list_is_singular(&c->functions))\ngoto done;\nf = list_first_entry(&c->functions, struct usb_function,\nlist);\nif (f->setup)\nvalue = f->setup(f, ctrl);\n}\ngoto done;\n}\ncheck_value:\nif (value >= 0 && value != USB_GADGET_DELAYED_STATUS) {\nreq->length = value;\nreq->context = cdev;\nreq->zero = value < w_length;\nvalue = composite_ep0_queue(cdev, req, GFP_ATOMIC);\nif (value < 0) {\nDBG(cdev, \"ep_queue --> %d\\n\", value);\nreq->status = 0;\ncomposite_setup_complete(gadget->ep0, req);\n}\n} else if (value == USB_GADGET_DELAYED_STATUS && w_length != 0) {\nWARN(cdev,\n\"%s: Delayed status not supported for w_length != 0\",\n__func__);\n}\ndone:\nreturn value;\n}\n",
      "code_before_change_raw": "int\ncomposite_setup(struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\nstruct usb_composite_dev\t*cdev = get_gadget_data(gadget);\nstruct usb_request\t\t*req = cdev->req;\nint\t\t\t\tvalue = -EOPNOTSUPP;\nint\t\t\t\tstatus = 0;\nu16\t\t\t\tw_index = le16_to_cpu(ctrl->wIndex);\nu8\t\t\t\tintf = w_index & 0xFF;\nu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\nu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\nstruct usb_function\t\t*f = NULL;\nu8\t\t\t\tendp;\nreq->zero = 0;\nreq->context = cdev;\nreq->complete = composite_setup_complete;\nreq->length = 0;\ngadget->ep0->driver_data = cdev;\nif ((ctrl->bRequestType & USB_TYPE_MASK) != USB_TYPE_STANDARD)\ngoto unknown;\nswitch (ctrl->bRequest) {\ncase USB_REQ_GET_DESCRIPTOR:\nif (ctrl->bRequestType != USB_DIR_IN)\ngoto unknown;\nswitch (w_value >> 8) {\ncase USB_DT_DEVICE:\ncdev->desc.bNumConfigurations =\ncount_configs(cdev, USB_DT_DEVICE);\ncdev->desc.bMaxPacketSize0 =\ncdev->gadget->ep0->maxpacket;\nif (gadget_is_superspeed(gadget)) {\nif (gadget->speed >= USB_SPEED_SUPER) {\ncdev->desc.bcdUSB = cpu_to_le16(0x0320);\ncdev->desc.bMaxPacketSize0 = 9;\n} else {\ncdev->desc.bcdUSB = cpu_to_le16(0x0210);\n}\n} else {\nif (gadget->lpm_capable)\ncdev->desc.bcdUSB = cpu_to_le16(0x0201);\nelse\ncdev->desc.bcdUSB = cpu_to_le16(0x0200);\n}\nvalue = min(w_length, (u16) sizeof cdev->desc);\nmemcpy(req->buf, &cdev->desc, value);\nbreak;\ncase USB_DT_DEVICE_QUALIFIER:\nif (!gadget_is_dualspeed(gadget) ||\ngadget->speed >= USB_SPEED_SUPER)\nbreak;\ndevice_qual(cdev);\nvalue = min_t(int, w_length,\nsizeof(struct usb_qualifier_descriptor));\nbreak;\ncase USB_DT_OTHER_SPEED_CONFIG:\nif (!gadget_is_dualspeed(gadget) ||\ngadget->speed >= USB_SPEED_SUPER)\nbreak;\nfallthrough;\ncase USB_DT_CONFIG:\nvalue = config_desc(cdev, w_value);\nif (value >= 0)\nvalue = min(w_length, (u16) value);\nbreak;\ncase USB_DT_STRING:\nvalue = get_string(cdev, req->buf,\nw_index, w_value & 0xff);\nif (value >= 0)\nvalue = min(w_length, (u16) value);\nbreak;\ncase USB_DT_BOS:\nif (gadget_is_superspeed(gadget) ||\ngadget->lpm_capable) {\nvalue = bos_desc(cdev);\nvalue = min(w_length, (u16) value);\n}\nbreak;\ncase USB_DT_OTG:\nif (gadget_is_otg(gadget)) {\nstruct usb_configuration *config;\nint otg_desc_len = 0;\nif (cdev->config)\nconfig = cdev->config;\nelse\nconfig = list_first_entry(\n&cdev->configs,\nstruct usb_configuration, list);\nif (!config)\ngoto done;\nif (gadget->otg_caps &&\n(gadget->otg_caps->otg_rev >= 0x0200))\notg_desc_len += sizeof(\nstruct usb_otg20_descriptor);\nelse\notg_desc_len += sizeof(\nstruct usb_otg_descriptor);\nvalue = min_t(int, w_length, otg_desc_len);\nmemcpy(req->buf, config->descriptors[0], value);\n}\nbreak;\n}\nbreak;\ncase USB_REQ_SET_CONFIGURATION:\nif (ctrl->bRequestType != 0)\ngoto unknown;\nif (gadget_is_otg(gadget)) {\nif (gadget->a_hnp_support)\nDBG(cdev, \"HNP available\\n\");\nelse if (gadget->a_alt_hnp_support)\nDBG(cdev, \"HNP on another port\\n\");\nelse\nVDBG(cdev, \"HNP inactive\\n\");\n}\nspin_lock(&cdev->lock);\nvalue = set_config(cdev, ctrl, w_value);\nspin_unlock(&cdev->lock);\nbreak;\ncase USB_REQ_GET_CONFIGURATION:\nif (ctrl->bRequestType != USB_DIR_IN)\ngoto unknown;\nif (cdev->config)\n*(u8 *)req->buf = cdev->config->bConfigurationValue;\nelse\n*(u8 *)req->buf = 0;\nvalue = min(w_length, (u16) 1);\nbreak;\ncase USB_REQ_SET_INTERFACE:\nif (ctrl->bRequestType != USB_RECIP_INTERFACE)\ngoto unknown;\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nif (w_value && !f->get_alt)\nbreak;\nspin_lock(&cdev->lock);\nvalue = f->set_alt(f, w_index, w_value);\nif (value == USB_GADGET_DELAYED_STATUS) {\nDBG(cdev,\n\"%s: interface %d (%s) requested delayed status\\n\",\n__func__, intf, f->name);\ncdev->delayed_status++;\nDBG(cdev, \"delayed_status count %d\\n\",\ncdev->delayed_status);\n}\nspin_unlock(&cdev->lock);\nbreak;\ncase USB_REQ_GET_INTERFACE:\nif (ctrl->bRequestType != (USB_DIR_IN|USB_RECIP_INTERFACE))\ngoto unknown;\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nvalue = f->get_alt ? f->get_alt(f, w_index) : 0;\nif (value < 0)\nbreak;\n*((u8 *)req->buf) = value;\nvalue = min(w_length, (u16) 1);\nbreak;\ncase USB_REQ_GET_STATUS:\nif (gadget_is_otg(gadget) && gadget->hnp_polling_support &&\n(w_index == OTG_STS_SELECTOR)) {\nif (ctrl->bRequestType != (USB_DIR_IN |\nUSB_RECIP_DEVICE))\ngoto unknown;\n*((u8 *)req->buf) = gadget->host_request_flag;\nvalue = 1;\nbreak;\n}\nif (!gadget_is_superspeed(gadget))\ngoto unknown;\nif (ctrl->bRequestType != (USB_DIR_IN | USB_RECIP_INTERFACE))\ngoto unknown;\nvalue = 2;\t\nput_unaligned_le16(0, req->buf);\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nstatus = f->get_status ? f->get_status(f) : 0;\nif (status < 0)\nbreak;\nput_unaligned_le16(status & 0x0000ffff, req->buf);\nbreak;\ncase USB_REQ_CLEAR_FEATURE:\ncase USB_REQ_SET_FEATURE:\nif (!gadget_is_superspeed(gadget))\ngoto unknown;\nif (ctrl->bRequestType != (USB_DIR_OUT | USB_RECIP_INTERFACE))\ngoto unknown;\nswitch (w_value) {\ncase USB_INTRF_FUNC_SUSPEND:\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nif (!f)\nbreak;\nvalue = 0;\nif (f->func_suspend)\nvalue = f->func_suspend(f, w_index >> 8);\nif (value < 0) {\nERROR(cdev,\n\"func_suspend() returned error %d\\n\",\nvalue);\nvalue = 0;\n}\nbreak;\n}\nbreak;\ndefault:\nunknown:\nif (cdev->use_os_string && cdev->os_desc_config &&\n(ctrl->bRequestType & USB_TYPE_VENDOR) &&\nctrl->bRequest == cdev->b_vendor_code) {\nstruct usb_configuration\t*os_desc_cfg;\nu8\t\t\t\t*buf;\nint\t\t\t\tinterface;\nint\t\t\t\tcount = 0;\nreq = cdev->os_desc_req;\nreq->context = cdev;\nreq->complete = composite_setup_complete;\nbuf = req->buf;\nos_desc_cfg = cdev->os_desc_config;\nw_length = min_t(u16, w_length, USB_COMP_EP0_OS_DESC_BUFSIZ);\nmemset(buf, 0, w_length);\nbuf[5] = 0x01;\nswitch (ctrl->bRequestType & USB_RECIP_MASK) {\ncase USB_RECIP_DEVICE:\nif (w_index != 0x4 || (w_value >> 8))\nbreak;\nbuf[6] = w_index;\ncount = count_ext_compat(os_desc_cfg);\nbuf[8] = count;\ncount *= 24; \ncount += 16; \nput_unaligned_le32(count, buf);\nvalue = w_length;\nif (w_length > 0x10) {\nvalue = fill_ext_compat(os_desc_cfg, buf);\nvalue = min_t(u16, w_length, value);\n}\nbreak;\ncase USB_RECIP_INTERFACE:\nif (w_index != 0x5 || (w_value >> 8))\nbreak;\ninterface = w_value & 0xFF;\nbuf[6] = w_index;\ncount = count_ext_prop(os_desc_cfg,\ninterface);\nput_unaligned_le16(count, buf + 8);\ncount = len_ext_prop(os_desc_cfg,\ninterface);\nput_unaligned_le32(count, buf);\nvalue = w_length;\nif (w_length > 0x0A) {\nvalue = fill_ext_prop(os_desc_cfg,\ninterface, buf);\nif (value >= 0)\nvalue = min_t(u16, w_length, value);\n}\nbreak;\n}\ngoto check_value;\n}\nVDBG(cdev,\n\"non-core control req%02x.%02x v%04x i%04x l%d\\n\",\nctrl->bRequestType, ctrl->bRequest,\nw_value, w_index, w_length);\nif (cdev->config) {\nlist_for_each_entry(f, &cdev->config->functions, list)\nif (f->req_match &&\nf->req_match(f, ctrl, false))\ngoto try_fun_setup;\n} else {\nstruct usb_configuration *c;\nlist_for_each_entry(c, &cdev->configs, list)\nlist_for_each_entry(f, &c->functions, list)\nif (f->req_match &&\nf->req_match(f, ctrl, true))\ngoto try_fun_setup;\n}\nf = NULL;\nswitch (ctrl->bRequestType & USB_RECIP_MASK) {\ncase USB_RECIP_INTERFACE:\nif (!cdev->config || intf >= MAX_CONFIG_INTERFACES)\nbreak;\nf = cdev->config->interface[intf];\nbreak;\ncase USB_RECIP_ENDPOINT:\nif (!cdev->config)\nbreak;\nendp = ((w_index & 0x80) >> 3) | (w_index & 0x0f);\nlist_for_each_entry(f, &cdev->config->functions, list) {\nif (test_bit(endp, f->endpoints))\nbreak;\n}\nif (&f->list == &cdev->config->functions)\nf = NULL;\nbreak;\n}\ntry_fun_setup:\nif (f && f->setup)\nvalue = f->setup(f, ctrl);\nelse {\nstruct usb_configuration\t*c;\nc = cdev->config;\nif (!c)\ngoto done;\nif (c->setup) {\nvalue = c->setup(c, ctrl);\ngoto done;\n}\nif (!list_is_singular(&c->functions))\ngoto done;\nf = list_first_entry(&c->functions, struct usb_function,\nlist);\nif (f->setup)\nvalue = f->setup(f, ctrl);\n}\ngoto done;\n}\ncheck_value:\nif (value >= 0 && value != USB_GADGET_DELAYED_STATUS) {\nreq->length = value;\nreq->context = cdev;\nreq->zero = value < w_length;\nvalue = composite_ep0_queue(cdev, req, GFP_ATOMIC);\nif (value < 0) {\nDBG(cdev, \"ep_queue --> %d\\n\", value);\nreq->status = 0;\ncomposite_setup_complete(gadget->ep0, req);\n}\n} else if (value == USB_GADGET_DELAYED_STATUS && w_length != 0) {\nWARN(cdev,\n\"%s: Delayed status not supported for w_length != 0\",\n__func__);\n}\ndone:\nreturn value;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for the length of data being processed, allowing potential out-of-bounds write operations.",
        "trigger_condition": "When processing control requests, the code does not adequately check and handle cases where the length of the data exceeds a predefined buffer size, leading to a possible out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not include a validation check to ensure that the length of the data being processed does not exceed a predefined buffer size. This oversight can result in potential out-of-bounds write operations, which may lead to security vulnerabilities such as local privilege escalation.",
        "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure that the length of the data being processed does not exceed a predefined buffer size. If the length exceeds the buffer size, appropriate actions should be taken, such as returning an error value or adjusting the length to fit within the buffer size. By implementing this validation check, the code can prevent potential out-of-bounds write vulnerabilities and enhance its security posture."
      },
      "GPT_analysis": "The modification is necessary to address a vulnerability (CVE-2021-39685) in the code snippet provided. The vulnerability involves a possible out-of-bounds write due to an incorrect flag check in various setup methods of the USB gadget subsystem. This vulnerability could potentially lead to local escalation of privilege without requiring additional execution privileges and without the need for user interaction.\n\nTo fix this vulnerability, the modification adds a check to ensure that the length of the data being processed (w_length) does not exceed a predefined buffer size (RBUF_SIZE). If the length exceeds the buffer size, the modification handles the situation appropriately by either returning an error value or adjusting the length to fit within the buffer size.\n\nBy implementing this check and handling the data length appropriately, the code is made more secure and less susceptible to potential out-of-bounds write vulnerabilities, thereby addressing the reported security issue.",
      "GPT_purpose": "Handle setup requests for a USB gadget device, including processing control requests such as getting descriptors, setting configurations, and managing data transfer phases.",
      "GPT_function": "\n1. `gadgetfs_setup`: Handles setup requests for the USB gadget subsystem, including processing control requests such as GET_DESCRIPTOR and SET_CONFIGURATION.\n2. `make_qualifier`: Generates a USB qualifier descriptor.\n3. `config_buf`: Handles configuration buffer operations based on the request type and value.",
      "CVE_id": "CVE-2021-39685",
      "code_before_change": "static int\ngadgetfs_setup (struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\n\tstruct dev_data\t\t\t*dev = get_gadget_data (gadget);\n\tstruct usb_request\t\t*req = dev->req;\n\tint\t\t\t\tvalue = -EOPNOTSUPP;\n\tstruct usb_gadgetfs_event\t*event;\n\tu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\n\tu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\n\n\tspin_lock (&dev->lock);\n\tdev->setup_abort = 0;\n\tif (dev->state == STATE_DEV_UNCONNECTED) {\n\t\tif (gadget_is_dualspeed(gadget)\n\t\t\t\t&& gadget->speed == USB_SPEED_HIGH\n\t\t\t\t&& dev->hs_config == NULL) {\n\t\t\tspin_unlock(&dev->lock);\n\t\t\tERROR (dev, \"no high speed config??\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tdev->state = STATE_DEV_CONNECTED;\n\n\t\tINFO (dev, \"connected\\n\");\n\t\tevent = next_event (dev, GADGETFS_CONNECT);\n\t\tevent->u.speed = gadget->speed;\n\t\tep0_readable (dev);\n\n\t/* host may have given up waiting for response.  we can miss control\n\t * requests handled lower down (device/endpoint status and features);\n\t * then ep0_{read,write} will report the wrong status. controller\n\t * driver will have aborted pending i/o.\n\t */\n\t} else if (dev->state == STATE_DEV_SETUP)\n\t\tdev->setup_abort = 1;\n\n\treq->buf = dev->rbuf;\n\treq->context = NULL;\n\tswitch (ctrl->bRequest) {\n\n\tcase USB_REQ_GET_DESCRIPTOR:\n\t\tif (ctrl->bRequestType != USB_DIR_IN)\n\t\t\tgoto unrecognized;\n\t\tswitch (w_value >> 8) {\n\n\t\tcase USB_DT_DEVICE:\n\t\t\tvalue = min (w_length, (u16) sizeof *dev->dev);\n\t\t\tdev->dev->bMaxPacketSize0 = dev->gadget->ep0->maxpacket;\n\t\t\treq->buf = dev->dev;\n\t\t\tbreak;\n\t\tcase USB_DT_DEVICE_QUALIFIER:\n\t\t\tif (!dev->hs_config)\n\t\t\t\tbreak;\n\t\t\tvalue = min (w_length, (u16)\n\t\t\t\tsizeof (struct usb_qualifier_descriptor));\n\t\t\tmake_qualifier (dev);\n\t\t\tbreak;\n\t\tcase USB_DT_OTHER_SPEED_CONFIG:\n\t\tcase USB_DT_CONFIG:\n\t\t\tvalue = config_buf (dev,\n\t\t\t\t\tw_value >> 8,\n\t\t\t\t\tw_value & 0xff);\n\t\t\tif (value >= 0)\n\t\t\t\tvalue = min (w_length, (u16) value);\n\t\t\tbreak;\n\t\tcase USB_DT_STRING:\n\t\t\tgoto unrecognized;\n\n\t\tdefault:\t\t// all others are errors\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t/* currently one config, two speeds */\n\tcase USB_REQ_SET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != 0)\n\t\t\tgoto unrecognized;\n\t\tif (0 == (u8) w_value) {\n\t\t\tvalue = 0;\n\t\t\tdev->current_config = 0;\n\t\t\tusb_gadget_vbus_draw(gadget, 8 /* mA */ );\n\t\t\t// user mode expected to disable endpoints\n\t\t} else {\n\t\t\tu8\tconfig, power;\n\n\t\t\tif (gadget_is_dualspeed(gadget)\n\t\t\t\t\t&& gadget->speed == USB_SPEED_HIGH) {\n\t\t\t\tconfig = dev->hs_config->bConfigurationValue;\n\t\t\t\tpower = dev->hs_config->bMaxPower;\n\t\t\t} else {\n\t\t\t\tconfig = dev->config->bConfigurationValue;\n\t\t\t\tpower = dev->config->bMaxPower;\n\t\t\t}\n\n\t\t\tif (config == (u8) w_value) {\n\t\t\t\tvalue = 0;\n\t\t\t\tdev->current_config = config;\n\t\t\t\tusb_gadget_vbus_draw(gadget, 2 * power);\n\t\t\t}\n\t\t}\n\n\t\t/* report SET_CONFIGURATION like any other control request,\n\t\t * except that usermode may not stall this.  the next\n\t\t * request mustn't be allowed start until this finishes:\n\t\t * endpoints and threads set up, etc.\n\t\t *\n\t\t * NOTE:  older PXA hardware (before PXA 255: without UDCCFR)\n\t\t * has bad/racey automagic that prevents synchronizing here.\n\t\t * even kernel mode drivers often miss them.\n\t\t */\n\t\tif (value == 0) {\n\t\t\tINFO (dev, \"configuration #%d\\n\", dev->current_config);\n\t\t\tusb_gadget_set_state(gadget, USB_STATE_CONFIGURED);\n\t\t\tif (dev->usermode_setup) {\n\t\t\t\tdev->setup_can_stall = 0;\n\t\t\t\tgoto delegate;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#ifndef\tCONFIG_USB_PXA25X\n\t/* PXA automagically handles this request too */\n\tcase USB_REQ_GET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != 0x80)\n\t\t\tgoto unrecognized;\n\t\t*(u8 *)req->buf = dev->current_config;\n\t\tvalue = min (w_length, (u16) 1);\n\t\tbreak;\n#endif\n\n\tdefault:\nunrecognized:\n\t\tVDEBUG (dev, \"%s req%02x.%02x v%04x i%04x l%d\\n\",\n\t\t\tdev->usermode_setup ? \"delegate\" : \"fail\",\n\t\t\tctrl->bRequestType, ctrl->bRequest,\n\t\t\tw_value, le16_to_cpu(ctrl->wIndex), w_length);\n\n\t\t/* if there's an ep0 reader, don't stall */\n\t\tif (dev->usermode_setup) {\n\t\t\tdev->setup_can_stall = 1;\ndelegate:\n\t\t\tdev->setup_in = (ctrl->bRequestType & USB_DIR_IN)\n\t\t\t\t\t\t? 1 : 0;\n\t\t\tdev->setup_wLength = w_length;\n\t\t\tdev->setup_out_ready = 0;\n\t\t\tdev->setup_out_error = 0;\n\n\t\t\t/* read DATA stage for OUT right away */\n\t\t\tif (unlikely (!dev->setup_in && w_length)) {\n\t\t\t\tvalue = setup_req (gadget->ep0, dev->req,\n\t\t\t\t\t\t\tw_length);\n\t\t\t\tif (value < 0)\n\t\t\t\t\tbreak;\n\n\t\t\t\t++dev->udc_usage;\n\t\t\t\tspin_unlock (&dev->lock);\n\t\t\t\tvalue = usb_ep_queue (gadget->ep0, dev->req,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\tspin_lock (&dev->lock);\n\t\t\t\t--dev->udc_usage;\n\t\t\t\tif (value < 0) {\n\t\t\t\t\tclean_req (gadget->ep0, dev->req);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t/* we can't currently stall these */\n\t\t\t\tdev->setup_can_stall = 0;\n\t\t\t}\n\n\t\t\t/* state changes when reader collects event */\n\t\t\tevent = next_event (dev, GADGETFS_SETUP);\n\t\t\tevent->u.setup = *ctrl;\n\t\t\tep0_readable (dev);\n\t\t\tspin_unlock (&dev->lock);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* proceed with data transfer and status phases? */\n\tif (value >= 0 && dev->state != STATE_DEV_SETUP) {\n\t\treq->length = value;\n\t\treq->zero = value < w_length;\n\n\t\t++dev->udc_usage;\n\t\tspin_unlock (&dev->lock);\n\t\tvalue = usb_ep_queue (gadget->ep0, req, GFP_KERNEL);\n\t\tspin_lock(&dev->lock);\n\t\t--dev->udc_usage;\n\t\tspin_unlock(&dev->lock);\n\t\tif (value < 0) {\n\t\t\tDBG (dev, \"ep_queue --> %d\\n\", value);\n\t\t\treq->status = 0;\n\t\t}\n\t\treturn value;\n\t}\n\n\t/* device stalls when value < 0 */\n\tspin_unlock (&dev->lock);\n\treturn value;\n}",
      "code_after_change": "static int\ngadgetfs_setup (struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\n\tstruct dev_data\t\t\t*dev = get_gadget_data (gadget);\n\tstruct usb_request\t\t*req = dev->req;\n\tint\t\t\t\tvalue = -EOPNOTSUPP;\n\tstruct usb_gadgetfs_event\t*event;\n\tu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\n\tu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\n\n\tif (w_length > RBUF_SIZE) {\n\t\tif (ctrl->bRequestType == USB_DIR_OUT) {\n\t\t\treturn value;\n\t\t} else {\n\t\t\t/* Cast away the const, we are going to overwrite on purpose. */\n\t\t\t__le16 *temp = (__le16 *)&ctrl->wLength;\n\n\t\t\t*temp = cpu_to_le16(RBUF_SIZE);\n\t\t\tw_length = RBUF_SIZE;\n\t\t}\n\t}\n\n\tspin_lock (&dev->lock);\n\tdev->setup_abort = 0;\n\tif (dev->state == STATE_DEV_UNCONNECTED) {\n\t\tif (gadget_is_dualspeed(gadget)\n\t\t\t\t&& gadget->speed == USB_SPEED_HIGH\n\t\t\t\t&& dev->hs_config == NULL) {\n\t\t\tspin_unlock(&dev->lock);\n\t\t\tERROR (dev, \"no high speed config??\\n\");\n\t\t\treturn -EINVAL;\n\t\t}\n\n\t\tdev->state = STATE_DEV_CONNECTED;\n\n\t\tINFO (dev, \"connected\\n\");\n\t\tevent = next_event (dev, GADGETFS_CONNECT);\n\t\tevent->u.speed = gadget->speed;\n\t\tep0_readable (dev);\n\n\t/* host may have given up waiting for response.  we can miss control\n\t * requests handled lower down (device/endpoint status and features);\n\t * then ep0_{read,write} will report the wrong status. controller\n\t * driver will have aborted pending i/o.\n\t */\n\t} else if (dev->state == STATE_DEV_SETUP)\n\t\tdev->setup_abort = 1;\n\n\treq->buf = dev->rbuf;\n\treq->context = NULL;\n\tswitch (ctrl->bRequest) {\n\n\tcase USB_REQ_GET_DESCRIPTOR:\n\t\tif (ctrl->bRequestType != USB_DIR_IN)\n\t\t\tgoto unrecognized;\n\t\tswitch (w_value >> 8) {\n\n\t\tcase USB_DT_DEVICE:\n\t\t\tvalue = min (w_length, (u16) sizeof *dev->dev);\n\t\t\tdev->dev->bMaxPacketSize0 = dev->gadget->ep0->maxpacket;\n\t\t\treq->buf = dev->dev;\n\t\t\tbreak;\n\t\tcase USB_DT_DEVICE_QUALIFIER:\n\t\t\tif (!dev->hs_config)\n\t\t\t\tbreak;\n\t\t\tvalue = min (w_length, (u16)\n\t\t\t\tsizeof (struct usb_qualifier_descriptor));\n\t\t\tmake_qualifier (dev);\n\t\t\tbreak;\n\t\tcase USB_DT_OTHER_SPEED_CONFIG:\n\t\tcase USB_DT_CONFIG:\n\t\t\tvalue = config_buf (dev,\n\t\t\t\t\tw_value >> 8,\n\t\t\t\t\tw_value & 0xff);\n\t\t\tif (value >= 0)\n\t\t\t\tvalue = min (w_length, (u16) value);\n\t\t\tbreak;\n\t\tcase USB_DT_STRING:\n\t\t\tgoto unrecognized;\n\n\t\tdefault:\t\t// all others are errors\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\n\t/* currently one config, two speeds */\n\tcase USB_REQ_SET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != 0)\n\t\t\tgoto unrecognized;\n\t\tif (0 == (u8) w_value) {\n\t\t\tvalue = 0;\n\t\t\tdev->current_config = 0;\n\t\t\tusb_gadget_vbus_draw(gadget, 8 /* mA */ );\n\t\t\t// user mode expected to disable endpoints\n\t\t} else {\n\t\t\tu8\tconfig, power;\n\n\t\t\tif (gadget_is_dualspeed(gadget)\n\t\t\t\t\t&& gadget->speed == USB_SPEED_HIGH) {\n\t\t\t\tconfig = dev->hs_config->bConfigurationValue;\n\t\t\t\tpower = dev->hs_config->bMaxPower;\n\t\t\t} else {\n\t\t\t\tconfig = dev->config->bConfigurationValue;\n\t\t\t\tpower = dev->config->bMaxPower;\n\t\t\t}\n\n\t\t\tif (config == (u8) w_value) {\n\t\t\t\tvalue = 0;\n\t\t\t\tdev->current_config = config;\n\t\t\t\tusb_gadget_vbus_draw(gadget, 2 * power);\n\t\t\t}\n\t\t}\n\n\t\t/* report SET_CONFIGURATION like any other control request,\n\t\t * except that usermode may not stall this.  the next\n\t\t * request mustn't be allowed start until this finishes:\n\t\t * endpoints and threads set up, etc.\n\t\t *\n\t\t * NOTE:  older PXA hardware (before PXA 255: without UDCCFR)\n\t\t * has bad/racey automagic that prevents synchronizing here.\n\t\t * even kernel mode drivers often miss them.\n\t\t */\n\t\tif (value == 0) {\n\t\t\tINFO (dev, \"configuration #%d\\n\", dev->current_config);\n\t\t\tusb_gadget_set_state(gadget, USB_STATE_CONFIGURED);\n\t\t\tif (dev->usermode_setup) {\n\t\t\t\tdev->setup_can_stall = 0;\n\t\t\t\tgoto delegate;\n\t\t\t}\n\t\t}\n\t\tbreak;\n\n#ifndef\tCONFIG_USB_PXA25X\n\t/* PXA automagically handles this request too */\n\tcase USB_REQ_GET_CONFIGURATION:\n\t\tif (ctrl->bRequestType != 0x80)\n\t\t\tgoto unrecognized;\n\t\t*(u8 *)req->buf = dev->current_config;\n\t\tvalue = min (w_length, (u16) 1);\n\t\tbreak;\n#endif\n\n\tdefault:\nunrecognized:\n\t\tVDEBUG (dev, \"%s req%02x.%02x v%04x i%04x l%d\\n\",\n\t\t\tdev->usermode_setup ? \"delegate\" : \"fail\",\n\t\t\tctrl->bRequestType, ctrl->bRequest,\n\t\t\tw_value, le16_to_cpu(ctrl->wIndex), w_length);\n\n\t\t/* if there's an ep0 reader, don't stall */\n\t\tif (dev->usermode_setup) {\n\t\t\tdev->setup_can_stall = 1;\ndelegate:\n\t\t\tdev->setup_in = (ctrl->bRequestType & USB_DIR_IN)\n\t\t\t\t\t\t? 1 : 0;\n\t\t\tdev->setup_wLength = w_length;\n\t\t\tdev->setup_out_ready = 0;\n\t\t\tdev->setup_out_error = 0;\n\n\t\t\t/* read DATA stage for OUT right away */\n\t\t\tif (unlikely (!dev->setup_in && w_length)) {\n\t\t\t\tvalue = setup_req (gadget->ep0, dev->req,\n\t\t\t\t\t\t\tw_length);\n\t\t\t\tif (value < 0)\n\t\t\t\t\tbreak;\n\n\t\t\t\t++dev->udc_usage;\n\t\t\t\tspin_unlock (&dev->lock);\n\t\t\t\tvalue = usb_ep_queue (gadget->ep0, dev->req,\n\t\t\t\t\t\t\tGFP_KERNEL);\n\t\t\t\tspin_lock (&dev->lock);\n\t\t\t\t--dev->udc_usage;\n\t\t\t\tif (value < 0) {\n\t\t\t\t\tclean_req (gadget->ep0, dev->req);\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\t/* we can't currently stall these */\n\t\t\t\tdev->setup_can_stall = 0;\n\t\t\t}\n\n\t\t\t/* state changes when reader collects event */\n\t\t\tevent = next_event (dev, GADGETFS_SETUP);\n\t\t\tevent->u.setup = *ctrl;\n\t\t\tep0_readable (dev);\n\t\t\tspin_unlock (&dev->lock);\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\t/* proceed with data transfer and status phases? */\n\tif (value >= 0 && dev->state != STATE_DEV_SETUP) {\n\t\treq->length = value;\n\t\treq->zero = value < w_length;\n\n\t\t++dev->udc_usage;\n\t\tspin_unlock (&dev->lock);\n\t\tvalue = usb_ep_queue (gadget->ep0, req, GFP_KERNEL);\n\t\tspin_lock(&dev->lock);\n\t\t--dev->udc_usage;\n\t\tspin_unlock(&dev->lock);\n\t\tif (value < 0) {\n\t\t\tDBG (dev, \"ep_queue --> %d\\n\", value);\n\t\t\treq->status = 0;\n\t\t}\n\t\treturn value;\n\t}\n\n\t/* device stalls when value < 0 */\n\tspin_unlock (&dev->lock);\n\treturn value;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (w_length > RBUF_SIZE) {",
          "\t\tif (ctrl->bRequestType == USB_DIR_OUT) {",
          "\t\t\treturn value;",
          "\t\t} else {",
          "\t\t\t/* Cast away the const, we are going to overwrite on purpose. */",
          "\t\t\t__le16 *temp = (__le16 *)&ctrl->wLength;",
          "",
          "\t\t\t*temp = cpu_to_le16(RBUF_SIZE);",
          "\t\t\tw_length = RBUF_SIZE;",
          "\t\t}",
          "\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation for the length of data being processed, allowing potential out-of-bounds write operations.",
      "trigger_condition": "When processing control requests, the code does not adequately check and handle cases where the length of the data exceeds a predefined buffer size, leading to a possible out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not include a validation check to ensure that the length of the data being processed does not exceed a predefined buffer size. This oversight can result in potential out-of-bounds write operations, which may lead to security vulnerabilities such as local privilege escalation.",
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure that the length of the data being processed does not exceed a predefined buffer size. If the length exceeds the buffer size, appropriate actions should be taken, such as returning an error value or adjusting the length to fit within the buffer size. By implementing this validation check, the code can prevent potential out-of-bounds write vulnerabilities and enhance its security posture.",
      "id": 120,
      "code_after_change_normalized": "static int\nFUN1 (struct usb_gadget *VAR1, const struct usb_ctrlrequest *VAR2)\n{\nstruct dev_data\t\t\t*VAR3 = FUN2 (VAR1);\nstruct usb_request\t\t*VAR4 = VAR3->VAR4;\nint\t\t\t\tVAR5 = -VAR6;\nstruct usb_gadgetfs_event\t*VAR7;\nu16\t\t\t\tVAR8 = FUN3(VAR2->VAR9);\nu16\t\t\t\tVAR10 = FUN3(VAR2->VAR11);\nif (VAR10 > VAR12) {\nif (VAR2->VAR13 == VAR14) {\nreturn VAR5;\n} else {\nVAR16 *VAR15 = (VAR16 *)&VAR2->VAR11;\n*VAR15 = FUN4(VAR12);\nVAR10 = VAR12;\n}\n}\nFUN5 (&VAR3->VAR17);\nVAR3->VAR18 = 0;\nif (VAR3->VAR19 == VAR20) {\nif (FUN6(VAR1)\n&& VAR1->VAR21 == VAR22\n&& VAR3->VAR23 == NULL) {\nFUN7(&VAR3->VAR17);\nFUN8 (VAR3, \"STR\");\nreturn -VAR24;\n}\nVAR3->VAR19 = VAR25;\nFUN9 (VAR3, \"STR\");\nVAR7 = FUN10 (VAR3, VAR26);\nVAR7->VAR27.VAR21 = VAR1->VAR21;\nFUN11 (VAR3);\n} else if (VAR3->VAR19 == VAR28)\nVAR3->VAR18 = 1;\nVAR4->VAR29 = VAR3->VAR30;\nVAR4->VAR31 = NULL;\nswitch (VAR2->VAR32) {\ncase VAR33:\nif (VAR2->VAR13 != VAR34)\ngoto VAR35;\nswitch (VAR8 >> 8) {\ncase VAR36:\nVAR5 = FUN12 (VAR10, (VAR37) sizeof *VAR3->VAR3);\nVAR3->VAR3->VAR38 = VAR3->VAR1->VAR39->VAR40;\nVAR4->VAR29 = VAR3->VAR3;\nbreak;\ncase VAR41:\nif (!VAR3->VAR23)\nbreak;\nVAR5 = FUN12 (VAR10, (VAR37)\nsizeof (struct VAR42));\nFUN13 (VAR3);\nbreak;\ncase VAR43:\ncase VAR44:\nVAR5 = FUN14 (VAR3,\nVAR8 >> 8,\nVAR8 & VAR45);\nif (VAR5 >= 0)\nVAR5 = FUN12 (VAR10, (VAR37) VAR5);\nbreak;\ncase VAR46:\ngoto VAR35;\ndefault:\t\t\nbreak;\n}\nbreak;\ncase VAR47:\nif (VAR2->VAR13 != 0)\ngoto VAR35;\nif (0 == (VAR48) VAR8) {\nVAR5 = 0;\nVAR3->VAR49 = 0;\nFUN15(VAR1, 8  );\n} else {\nu8\tVAR50, VAR51;\nif (FUN6(VAR1)\n&& VAR1->VAR21 == VAR22) {\nVAR50 = VAR3->VAR23->VAR52;\nVAR51 = VAR3->VAR23->VAR53;\n} else {\nVAR50 = VAR3->VAR50->VAR52;\nVAR51 = VAR3->VAR50->VAR53;\n}\nif (VAR50 == (VAR48) VAR8) {\nVAR5 = 0;\nVAR3->VAR49 = VAR50;\nFUN15(VAR1, 2 * VAR51);\n}\n}\nif (VAR5 == 0) {\nFUN9 (VAR3, \"STR\", VAR3->VAR49);\nFUN16(VAR1, VAR54);\nif (VAR3->VAR55) {\nVAR3->VAR56 = 0;\ngoto VAR57;\n}\n}\nbreak;\n#ifndef\tVAR58\ncase VAR59:\nif (VAR2->VAR13 != VAR45)\ngoto VAR35;\n*(VAR48 *)VAR4->VAR29 = VAR3->VAR49;\nVAR5 = FUN12 (VAR10, (VAR37) 1);\nbreak;\n#VAR60\ndefault:\nVAR35:\nFUN17 (VAR3, \"STR\",\nVAR3->VAR55 ? \"STR\" : \"STR\",\nVAR2->VAR13, VAR2->VAR32,\nVAR8, FUN3(VAR2->VAR61), VAR10);\nif (VAR3->VAR55) {\nVAR3->VAR56 = 1;\nVAR57:\nVAR3->VAR62 = (VAR2->VAR13 & VAR34)\n? 1 : 0;\nVAR3->VAR63 = VAR10;\nVAR3->VAR64 = 0;\nVAR3->VAR65 = 0;\nif (FUN18 (!VAR3->VAR62 && VAR10)) {\nVAR5 = FUN19 (VAR1->VAR39, VAR3->VAR4,\nVAR10);\nif (VAR5 < 0)\nbreak;\n++VAR3->VAR66;\nFUN7 (&VAR3->VAR17);\nVAR5 = FUN20 (VAR1->VAR39, VAR3->VAR4,\nVAR67);\nFUN5 (&VAR3->VAR17);\n--VAR3->VAR66;\nif (VAR5 < 0) {\nFUN21 (VAR1->VAR39, VAR3->VAR4);\nbreak;\n}\nVAR3->VAR56 = 0;\n}\nVAR7 = FUN10 (VAR3, VAR68);\nVAR7->VAR27.VAR69 = *VAR2;\nFUN11 (VAR3);\nFUN7 (&VAR3->VAR17);\nreturn 0;\n}\n}\nif (VAR5 >= 0 && VAR3->VAR19 != VAR28) {\nVAR4->VAR70 = VAR5;\nVAR4->VAR71 = VAR5 < VAR10;\n++VAR3->VAR66;\nFUN7 (&VAR3->VAR17);\nVAR5 = FUN20 (VAR1->VAR39, VAR4, VAR67);\nFUN5(&VAR3->VAR17);\n--VAR3->VAR66;\nFUN7(&VAR3->VAR17);\nif (VAR5 < 0) {\nFUN22 (VAR3, \"STR\", VAR5);\nVAR4->VAR72 = 0;\n}\nreturn VAR5;\n}\nFUN7 (&VAR3->VAR17);\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "static int\nFUN1 (struct usb_gadget *VAR1, const struct usb_ctrlrequest *VAR2)\n{\nstruct dev_data\t\t\t*VAR3 = FUN2 (VAR1);\nstruct usb_request\t\t*VAR4 = VAR3->VAR4;\nint\t\t\t\tVAR5 = -VAR6;\nstruct usb_gadgetfs_event\t*VAR7;\nu16\t\t\t\tVAR8 = FUN3(VAR2->VAR9);\nu16\t\t\t\tVAR10 = FUN3(VAR2->VAR11);\nFUN4 (&VAR3->VAR12);\nVAR3->VAR13 = 0;\nif (VAR3->VAR14 == VAR15) {\nif (FUN5(VAR1)\n&& VAR1->VAR16 == VAR17\n&& VAR3->VAR18 == NULL) {\nFUN6(&VAR3->VAR12);\nFUN7 (VAR3, \"STR\");\nreturn -VAR19;\n}\nVAR3->VAR14 = VAR20;\nFUN8 (VAR3, \"STR\");\nVAR7 = FUN9 (VAR3, VAR21);\nVAR7->VAR22.VAR16 = VAR1->VAR16;\nFUN10 (VAR3);\n} else if (VAR3->VAR14 == VAR23)\nVAR3->VAR13 = 1;\nVAR4->VAR24 = VAR3->VAR25;\nVAR4->VAR26 = NULL;\nswitch (VAR2->VAR27) {\ncase VAR28:\nif (VAR2->VAR29 != VAR30)\ngoto VAR31;\nswitch (VAR8 >> 8) {\ncase VAR32:\nVAR5 = FUN11 (VAR10, (VAR33) sizeof *VAR3->VAR3);\nVAR3->VAR3->VAR34 = VAR3->VAR1->VAR35->VAR36;\nVAR4->VAR24 = VAR3->VAR3;\nbreak;\ncase VAR37:\nif (!VAR3->VAR18)\nbreak;\nVAR5 = FUN11 (VAR10, (VAR33)\nsizeof (struct VAR38));\nFUN12 (VAR3);\nbreak;\ncase VAR39:\ncase VAR40:\nVAR5 = FUN13 (VAR3,\nVAR8 >> 8,\nVAR8 & VAR41);\nif (VAR5 >= 0)\nVAR5 = FUN11 (VAR10, (VAR33) VAR5);\nbreak;\ncase VAR42:\ngoto VAR31;\ndefault:\t\t\nbreak;\n}\nbreak;\ncase VAR43:\nif (VAR2->VAR29 != 0)\ngoto VAR31;\nif (0 == (VAR44) VAR8) {\nVAR5 = 0;\nVAR3->VAR45 = 0;\nFUN14(VAR1, 8  );\n} else {\nu8\tVAR46, VAR47;\nif (FUN5(VAR1)\n&& VAR1->VAR16 == VAR17) {\nVAR46 = VAR3->VAR18->VAR48;\nVAR47 = VAR3->VAR18->VAR49;\n} else {\nVAR46 = VAR3->VAR46->VAR48;\nVAR47 = VAR3->VAR46->VAR49;\n}\nif (VAR46 == (VAR44) VAR8) {\nVAR5 = 0;\nVAR3->VAR45 = VAR46;\nFUN14(VAR1, 2 * VAR47);\n}\n}\nif (VAR5 == 0) {\nFUN8 (VAR3, \"STR\", VAR3->VAR45);\nFUN15(VAR1, VAR50);\nif (VAR3->VAR51) {\nVAR3->VAR52 = 0;\ngoto VAR53;\n}\n}\nbreak;\n#ifndef\tVAR54\ncase VAR55:\nif (VAR2->VAR29 != VAR41)\ngoto VAR31;\n*(VAR44 *)VAR4->VAR24 = VAR3->VAR45;\nVAR5 = FUN11 (VAR10, (VAR33) 1);\nbreak;\n#VAR56\ndefault:\nVAR31:\nFUN16 (VAR3, \"STR\",\nVAR3->VAR51 ? \"STR\" : \"STR\",\nVAR2->VAR29, VAR2->VAR27,\nVAR8, FUN3(VAR2->VAR57), VAR10);\nif (VAR3->VAR51) {\nVAR3->VAR52 = 1;\nVAR53:\nVAR3->VAR58 = (VAR2->VAR29 & VAR30)\n? 1 : 0;\nVAR3->VAR59 = VAR10;\nVAR3->VAR60 = 0;\nVAR3->VAR61 = 0;\nif (FUN17 (!VAR3->VAR58 && VAR10)) {\nVAR5 = FUN18 (VAR1->VAR35, VAR3->VAR4,\nVAR10);\nif (VAR5 < 0)\nbreak;\n++VAR3->VAR62;\nFUN6 (&VAR3->VAR12);\nVAR5 = FUN19 (VAR1->VAR35, VAR3->VAR4,\nVAR63);\nFUN4 (&VAR3->VAR12);\n--VAR3->VAR62;\nif (VAR5 < 0) {\nFUN20 (VAR1->VAR35, VAR3->VAR4);\nbreak;\n}\nVAR3->VAR52 = 0;\n}\nVAR7 = FUN9 (VAR3, VAR64);\nVAR7->VAR22.VAR65 = *VAR2;\nFUN10 (VAR3);\nFUN6 (&VAR3->VAR12);\nreturn 0;\n}\n}\nif (VAR5 >= 0 && VAR3->VAR14 != VAR23) {\nVAR4->VAR66 = VAR5;\nVAR4->VAR67 = VAR5 < VAR10;\n++VAR3->VAR62;\nFUN6 (&VAR3->VAR12);\nVAR5 = FUN19 (VAR1->VAR35, VAR4, VAR63);\nFUN4(&VAR3->VAR12);\n--VAR3->VAR62;\nFUN6(&VAR3->VAR12);\nif (VAR5 < 0) {\nFUN21 (VAR3, \"STR\", VAR5);\nVAR4->VAR68 = 0;\n}\nreturn VAR5;\n}\nFUN6 (&VAR3->VAR12);\nreturn VAR5;\n}\n",
      "code_after_change_raw": "static int\ngadgetfs_setup (struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\nstruct dev_data\t\t\t*dev = get_gadget_data (gadget);\nstruct usb_request\t\t*req = dev->req;\nint\t\t\t\tvalue = -EOPNOTSUPP;\nstruct usb_gadgetfs_event\t*event;\nu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\nu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\nif (w_length > RBUF_SIZE) {\nif (ctrl->bRequestType == USB_DIR_OUT) {\nreturn value;\n} else {\n__le16 *temp = (__le16 *)&ctrl->wLength;\n*temp = cpu_to_le16(RBUF_SIZE);\nw_length = RBUF_SIZE;\n}\n}\nspin_lock (&dev->lock);\ndev->setup_abort = 0;\nif (dev->state == STATE_DEV_UNCONNECTED) {\nif (gadget_is_dualspeed(gadget)\n&& gadget->speed == USB_SPEED_HIGH\n&& dev->hs_config == NULL) {\nspin_unlock(&dev->lock);\nERROR (dev, \"no high speed config??\\n\");\nreturn -EINVAL;\n}\ndev->state = STATE_DEV_CONNECTED;\nINFO (dev, \"connected\\n\");\nevent = next_event (dev, GADGETFS_CONNECT);\nevent->u.speed = gadget->speed;\nep0_readable (dev);\n} else if (dev->state == STATE_DEV_SETUP)\ndev->setup_abort = 1;\nreq->buf = dev->rbuf;\nreq->context = NULL;\nswitch (ctrl->bRequest) {\ncase USB_REQ_GET_DESCRIPTOR:\nif (ctrl->bRequestType != USB_DIR_IN)\ngoto unrecognized;\nswitch (w_value >> 8) {\ncase USB_DT_DEVICE:\nvalue = min (w_length, (u16) sizeof *dev->dev);\ndev->dev->bMaxPacketSize0 = dev->gadget->ep0->maxpacket;\nreq->buf = dev->dev;\nbreak;\ncase USB_DT_DEVICE_QUALIFIER:\nif (!dev->hs_config)\nbreak;\nvalue = min (w_length, (u16)\nsizeof (struct usb_qualifier_descriptor));\nmake_qualifier (dev);\nbreak;\ncase USB_DT_OTHER_SPEED_CONFIG:\ncase USB_DT_CONFIG:\nvalue = config_buf (dev,\nw_value >> 8,\nw_value & 0xff);\nif (value >= 0)\nvalue = min (w_length, (u16) value);\nbreak;\ncase USB_DT_STRING:\ngoto unrecognized;\ndefault:\t\t\nbreak;\n}\nbreak;\ncase USB_REQ_SET_CONFIGURATION:\nif (ctrl->bRequestType != 0)\ngoto unrecognized;\nif (0 == (u8) w_value) {\nvalue = 0;\ndev->current_config = 0;\nusb_gadget_vbus_draw(gadget, 8  );\n} else {\nu8\tconfig, power;\nif (gadget_is_dualspeed(gadget)\n&& gadget->speed == USB_SPEED_HIGH) {\nconfig = dev->hs_config->bConfigurationValue;\npower = dev->hs_config->bMaxPower;\n} else {\nconfig = dev->config->bConfigurationValue;\npower = dev->config->bMaxPower;\n}\nif (config == (u8) w_value) {\nvalue = 0;\ndev->current_config = config;\nusb_gadget_vbus_draw(gadget, 2 * power);\n}\n}\nif (value == 0) {\nINFO (dev, \"configuration #%d\\n\", dev->current_config);\nusb_gadget_set_state(gadget, USB_STATE_CONFIGURED);\nif (dev->usermode_setup) {\ndev->setup_can_stall = 0;\ngoto delegate;\n}\n}\nbreak;\n#ifndef\tCONFIG_USB_PXA25X\ncase USB_REQ_GET_CONFIGURATION:\nif (ctrl->bRequestType != 0x80)\ngoto unrecognized;\n*(u8 *)req->buf = dev->current_config;\nvalue = min (w_length, (u16) 1);\nbreak;\n#endif\ndefault:\nunrecognized:\nVDEBUG (dev, \"%s req%02x.%02x v%04x i%04x l%d\\n\",\ndev->usermode_setup ? \"delegate\" : \"fail\",\nctrl->bRequestType, ctrl->bRequest,\nw_value, le16_to_cpu(ctrl->wIndex), w_length);\nif (dev->usermode_setup) {\ndev->setup_can_stall = 1;\ndelegate:\ndev->setup_in = (ctrl->bRequestType & USB_DIR_IN)\n? 1 : 0;\ndev->setup_wLength = w_length;\ndev->setup_out_ready = 0;\ndev->setup_out_error = 0;\nif (unlikely (!dev->setup_in && w_length)) {\nvalue = setup_req (gadget->ep0, dev->req,\nw_length);\nif (value < 0)\nbreak;\n++dev->udc_usage;\nspin_unlock (&dev->lock);\nvalue = usb_ep_queue (gadget->ep0, dev->req,\nGFP_KERNEL);\nspin_lock (&dev->lock);\n--dev->udc_usage;\nif (value < 0) {\nclean_req (gadget->ep0, dev->req);\nbreak;\n}\ndev->setup_can_stall = 0;\n}\nevent = next_event (dev, GADGETFS_SETUP);\nevent->u.setup = *ctrl;\nep0_readable (dev);\nspin_unlock (&dev->lock);\nreturn 0;\n}\n}\nif (value >= 0 && dev->state != STATE_DEV_SETUP) {\nreq->length = value;\nreq->zero = value < w_length;\n++dev->udc_usage;\nspin_unlock (&dev->lock);\nvalue = usb_ep_queue (gadget->ep0, req, GFP_KERNEL);\nspin_lock(&dev->lock);\n--dev->udc_usage;\nspin_unlock(&dev->lock);\nif (value < 0) {\nDBG (dev, \"ep_queue --> %d\\n\", value);\nreq->status = 0;\n}\nreturn value;\n}\nspin_unlock (&dev->lock);\nreturn value;\n}\n",
      "code_before_change_raw": "static int\ngadgetfs_setup (struct usb_gadget *gadget, const struct usb_ctrlrequest *ctrl)\n{\nstruct dev_data\t\t\t*dev = get_gadget_data (gadget);\nstruct usb_request\t\t*req = dev->req;\nint\t\t\t\tvalue = -EOPNOTSUPP;\nstruct usb_gadgetfs_event\t*event;\nu16\t\t\t\tw_value = le16_to_cpu(ctrl->wValue);\nu16\t\t\t\tw_length = le16_to_cpu(ctrl->wLength);\nspin_lock (&dev->lock);\ndev->setup_abort = 0;\nif (dev->state == STATE_DEV_UNCONNECTED) {\nif (gadget_is_dualspeed(gadget)\n&& gadget->speed == USB_SPEED_HIGH\n&& dev->hs_config == NULL) {\nspin_unlock(&dev->lock);\nERROR (dev, \"no high speed config??\\n\");\nreturn -EINVAL;\n}\ndev->state = STATE_DEV_CONNECTED;\nINFO (dev, \"connected\\n\");\nevent = next_event (dev, GADGETFS_CONNECT);\nevent->u.speed = gadget->speed;\nep0_readable (dev);\n} else if (dev->state == STATE_DEV_SETUP)\ndev->setup_abort = 1;\nreq->buf = dev->rbuf;\nreq->context = NULL;\nswitch (ctrl->bRequest) {\ncase USB_REQ_GET_DESCRIPTOR:\nif (ctrl->bRequestType != USB_DIR_IN)\ngoto unrecognized;\nswitch (w_value >> 8) {\ncase USB_DT_DEVICE:\nvalue = min (w_length, (u16) sizeof *dev->dev);\ndev->dev->bMaxPacketSize0 = dev->gadget->ep0->maxpacket;\nreq->buf = dev->dev;\nbreak;\ncase USB_DT_DEVICE_QUALIFIER:\nif (!dev->hs_config)\nbreak;\nvalue = min (w_length, (u16)\nsizeof (struct usb_qualifier_descriptor));\nmake_qualifier (dev);\nbreak;\ncase USB_DT_OTHER_SPEED_CONFIG:\ncase USB_DT_CONFIG:\nvalue = config_buf (dev,\nw_value >> 8,\nw_value & 0xff);\nif (value >= 0)\nvalue = min (w_length, (u16) value);\nbreak;\ncase USB_DT_STRING:\ngoto unrecognized;\ndefault:\t\t\nbreak;\n}\nbreak;\ncase USB_REQ_SET_CONFIGURATION:\nif (ctrl->bRequestType != 0)\ngoto unrecognized;\nif (0 == (u8) w_value) {\nvalue = 0;\ndev->current_config = 0;\nusb_gadget_vbus_draw(gadget, 8  );\n} else {\nu8\tconfig, power;\nif (gadget_is_dualspeed(gadget)\n&& gadget->speed == USB_SPEED_HIGH) {\nconfig = dev->hs_config->bConfigurationValue;\npower = dev->hs_config->bMaxPower;\n} else {\nconfig = dev->config->bConfigurationValue;\npower = dev->config->bMaxPower;\n}\nif (config == (u8) w_value) {\nvalue = 0;\ndev->current_config = config;\nusb_gadget_vbus_draw(gadget, 2 * power);\n}\n}\nif (value == 0) {\nINFO (dev, \"configuration #%d\\n\", dev->current_config);\nusb_gadget_set_state(gadget, USB_STATE_CONFIGURED);\nif (dev->usermode_setup) {\ndev->setup_can_stall = 0;\ngoto delegate;\n}\n}\nbreak;\n#ifndef\tCONFIG_USB_PXA25X\ncase USB_REQ_GET_CONFIGURATION:\nif (ctrl->bRequestType != 0x80)\ngoto unrecognized;\n*(u8 *)req->buf = dev->current_config;\nvalue = min (w_length, (u16) 1);\nbreak;\n#endif\ndefault:\nunrecognized:\nVDEBUG (dev, \"%s req%02x.%02x v%04x i%04x l%d\\n\",\ndev->usermode_setup ? \"delegate\" : \"fail\",\nctrl->bRequestType, ctrl->bRequest,\nw_value, le16_to_cpu(ctrl->wIndex), w_length);\nif (dev->usermode_setup) {\ndev->setup_can_stall = 1;\ndelegate:\ndev->setup_in = (ctrl->bRequestType & USB_DIR_IN)\n? 1 : 0;\ndev->setup_wLength = w_length;\ndev->setup_out_ready = 0;\ndev->setup_out_error = 0;\nif (unlikely (!dev->setup_in && w_length)) {\nvalue = setup_req (gadget->ep0, dev->req,\nw_length);\nif (value < 0)\nbreak;\n++dev->udc_usage;\nspin_unlock (&dev->lock);\nvalue = usb_ep_queue (gadget->ep0, dev->req,\nGFP_KERNEL);\nspin_lock (&dev->lock);\n--dev->udc_usage;\nif (value < 0) {\nclean_req (gadget->ep0, dev->req);\nbreak;\n}\ndev->setup_can_stall = 0;\n}\nevent = next_event (dev, GADGETFS_SETUP);\nevent->u.setup = *ctrl;\nep0_readable (dev);\nspin_unlock (&dev->lock);\nreturn 0;\n}\n}\nif (value >= 0 && dev->state != STATE_DEV_SETUP) {\nreq->length = value;\nreq->zero = value < w_length;\n++dev->udc_usage;\nspin_unlock (&dev->lock);\nvalue = usb_ep_queue (gadget->ep0, req, GFP_KERNEL);\nspin_lock(&dev->lock);\n--dev->udc_usage;\nspin_unlock(&dev->lock);\nif (value < 0) {\nDBG (dev, \"ep_queue --> %d\\n\", value);\nreq->status = 0;\n}\nreturn value;\n}\nspin_unlock (&dev->lock);\nreturn value;\n}\n"
    }
  ],
  "CVE-2021-4090": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of boundary checking for memory writes in a loop iterating over an array.",
        "trigger_condition": "The loop writes values to the array without verifying the boundary, potentially causing an out-of-bounds memory write.",
        "specific_code_behavior_causing_vulnerability": "The code iterates over an array without checking the boundary, leading to a risk of writing beyond the allocated memory for the array."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that memory writes within loops are bounded by the allocated size of the array. In this case, modifying the loop to use a for loop that checks the boundary of the array before writing values helps prevent out-of-bounds memory writes and enhances system security.",
      "GPT_analysis": "The modification is necessary to prevent the out-of-bounds memory write vulnerability in the nfsd4_decode_bitmap4 function. By changing the while loop to a for loop that iterates up to the bmlen limit, we ensure that the bmval array is not written beyond its allocated size.\n\nIn the original code, the while loop writes values to the bmval array without checking the boundary of the array. This can lead to a situation where the loop writes beyond the allocated memory for bmval, potentially causing a buffer overflow and leading to a security risk.\n\nBy modifying the code to use a for loop that checks the boundary of the bmval array before writing values, we ensure that only valid memory locations within the array are accessed and written to. This helps prevent the possibility of an out-of-bounds memory write and enhances the security and integrity of the system.",
      "GPT_purpose": "Decode a bitmap from an NFSv4 XDR stream and store it in an array, ensuring the count is within a certain limit.",
      "GPT_function": "\n1. Decode a bitmap value from the NFSv4 compound arguments.\n2. Check the count value for sanity to prevent potential issues.\n3. Copy the decoded bitmap values into the bmval array, ensuring not to exceed the specified length.",
      "CVE_id": "CVE-2021-4090",
      "code_before_change": "static __be32\nnfsd4_decode_bitmap4(struct nfsd4_compoundargs *argp, u32 *bmval, u32 bmlen)\n{\n\tu32 i, count;\n\t__be32 *p;\n\n\tif (xdr_stream_decode_u32(argp->xdr, &count) < 0)\n\t\treturn nfserr_bad_xdr;\n\t/* request sanity */\n\tif (count > 1000)\n\t\treturn nfserr_bad_xdr;\n\tp = xdr_inline_decode(argp->xdr, count << 2);\n\tif (!p)\n\t\treturn nfserr_bad_xdr;\n\ti = 0;\n\twhile (i < count)\n\t\tbmval[i++] = be32_to_cpup(p++);\n\twhile (i < bmlen)\n\t\tbmval[i++] = 0;\n\n\treturn nfs_ok;\n}",
      "code_after_change": "static __be32\nnfsd4_decode_bitmap4(struct nfsd4_compoundargs *argp, u32 *bmval, u32 bmlen)\n{\n\tu32 i, count;\n\t__be32 *p;\n\n\tif (xdr_stream_decode_u32(argp->xdr, &count) < 0)\n\t\treturn nfserr_bad_xdr;\n\t/* request sanity */\n\tif (count > 1000)\n\t\treturn nfserr_bad_xdr;\n\tp = xdr_inline_decode(argp->xdr, count << 2);\n\tif (!p)\n\t\treturn nfserr_bad_xdr;\n\tfor (i = 0; i < bmlen; i++)\n\t\tbmval[i] = (i < count) ? be32_to_cpup(p++) : 0;\n\n\treturn nfs_ok;\n}",
      "modified_lines": {
        "added": [
          "\tfor (i = 0; i < bmlen; i++)",
          "\t\tbmval[i] = (i < count) ? be32_to_cpup(p++) : 0;"
        ],
        "deleted": [
          "\ti = 0;",
          "\twhile (i < count)",
          "\t\tbmval[i++] = be32_to_cpup(p++);",
          "\twhile (i < bmlen)",
          "\t\tbmval[i++] = 0;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of boundary checking for memory writes in a loop iterating over an array.",
      "trigger_condition": "The loop writes values to the array without verifying the boundary, potentially causing an out-of-bounds memory write.",
      "specific_code_behavior_causing_vulnerability": "The code iterates over an array without checking the boundary, leading to a risk of writing beyond the allocated memory for the array.",
      "id": 121,
      "code_after_change_normalized": "static VAR1\nFUN1(struct nfsd4_compoundargs *VAR2, u32 *VAR3, u32 VAR4)\n{\nu32 VAR5, VAR6;\n__be32 *VAR7;\nif (FUN2(VAR2->VAR8, &VAR6) < 0)\nreturn VAR9;\nif (VAR6 > 1000)\nreturn VAR9;\nVAR7 = FUN3(VAR2->VAR8, VAR6 << 2);\nif (!VAR7)\nreturn VAR9;\nfor (VAR5 = 0; VAR5 < VAR4; VAR5++)\nVAR3[VAR5] = (VAR5 < VAR6) ? FUN4(VAR7++) : 0;\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "static VAR1\nFUN1(struct nfsd4_compoundargs *VAR2, u32 *VAR3, u32 VAR4)\n{\nu32 VAR5, VAR6;\n__be32 *VAR7;\nif (FUN2(VAR2->VAR8, &VAR6) < 0)\nreturn VAR9;\nif (VAR6 > 1000)\nreturn VAR9;\nVAR7 = FUN3(VAR2->VAR8, VAR6 << 2);\nif (!VAR7)\nreturn VAR9;\nVAR5 = 0;\nwhile (VAR5 < VAR6)\nVAR3[VAR5++] = FUN4(VAR7++);\nwhile (VAR5 < VAR4)\nVAR3[VAR5++] = 0;\nreturn VAR10;\n}\n",
      "code_after_change_raw": "static __be32\nnfsd4_decode_bitmap4(struct nfsd4_compoundargs *argp, u32 *bmval, u32 bmlen)\n{\nu32 i, count;\n__be32 *p;\nif (xdr_stream_decode_u32(argp->xdr, &count) < 0)\nreturn nfserr_bad_xdr;\nif (count > 1000)\nreturn nfserr_bad_xdr;\np = xdr_inline_decode(argp->xdr, count << 2);\nif (!p)\nreturn nfserr_bad_xdr;\nfor (i = 0; i < bmlen; i++)\nbmval[i] = (i < count) ? be32_to_cpup(p++) : 0;\nreturn nfs_ok;\n}\n",
      "code_before_change_raw": "static __be32\nnfsd4_decode_bitmap4(struct nfsd4_compoundargs *argp, u32 *bmval, u32 bmlen)\n{\nu32 i, count;\n__be32 *p;\nif (xdr_stream_decode_u32(argp->xdr, &count) < 0)\nreturn nfserr_bad_xdr;\nif (count > 1000)\nreturn nfserr_bad_xdr;\np = xdr_inline_decode(argp->xdr, count << 2);\nif (!p)\nreturn nfserr_bad_xdr;\ni = 0;\nwhile (i < count)\nbmval[i++] = be32_to_cpup(p++);\nwhile (i < bmlen)\nbmval[i++] = 0;\nreturn nfs_ok;\n}\n"
    }
  ],
  "CVE-2021-4093": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking and handling for SEV-ES related operations in the code.",
        "trigger_condition": "A KVM guest using SEV-ES triggers out-of-bounds reads and writes in the host kernel via a malicious VMGEXIT for a string I/O instruction, leading to a potential system crash or guest-to-host escape scenario.",
        "specific_code_behavior_causing_vulnerability": "The original code directly performs a memory copy operation without proper bounds checking, allowing potential out-of-bounds accesses when copying data between specific structures."
      },
      "solution": "To mitigate the vulnerability, it is necessary to handle SEV-ES related operations securely by advancing the emulated instruction and checking for pending SEV-ES instructions before processing them. This ensures that the code properly handles SEV-ES instructions and prevents out-of-bounds accesses. In this case, the solution involves modifying the code to advance the emulated instruction and conditionally call a function to handle SEV-ES instructions based on the presence of pending instructions.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-4093 by ensuring that the code properly handles SEV-ES related operations and prevents out-of-bounds reads and writes in the host kernel. \n\nIn the original code snippet, the function `complete_sev_es_emulated_ins` directly performs a memory copy operation using `memcpy` without proper bounds checking. This can lead to potential out-of-bounds accesses when copying data from `vcpu->arch.pio_data` to `vcpu->arch.sev_pio_data`, which can be triggered by a malicious VMGEXIT for a string I/O instruction.\n\nThe modified code snippet addresses this vulnerability by first advancing the SEV-ES emulated instruction using `advance_sev_es_emulated_ins` and then checking if there are pending SEV-ES instructions to be processed (`vcpu->arch.sev_pio_count`). If there are pending SEV-ES instructions, it calls `kvm_sev_es_ins` with the appropriate size and port information to handle the SEV-ES instruction. This approach ensures that SEV-ES related operations are handled correctly and securely, reducing the risk of out-of-bounds accesses and potential system crashes or guest-to-host escape scenarios.",
      "GPT_purpose": "Copy data from one memory location to another in the context of emulating SEV-ES instructions for a KVM VCPU.",
      "GPT_function": "\n1. Copies data from vcpu->arch.pio_data to vcpu->arch.sev_pio_data.\n2. Resets vcpu->arch.pio.count to 0.\n3. Returns 1.",
      "CVE_id": "CVE-2021-4093",
      "code_before_change": "static int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)\n{\n\tmemcpy(vcpu->arch.sev_pio_data, vcpu->arch.pio_data,\n\t       vcpu->arch.pio.count * vcpu->arch.pio.size);\n\tvcpu->arch.pio.count = 0;\n\n\treturn 1;\n}",
      "code_after_change": "static int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)\n{\n\tint size = vcpu->arch.pio.size;\n\tint port = vcpu->arch.pio.port;\n\n\tadvance_sev_es_emulated_ins(vcpu);\n\tif (vcpu->arch.sev_pio_count)\n\t\treturn kvm_sev_es_ins(vcpu, size, port);\n\treturn 1;\n}",
      "modified_lines": {
        "added": [
          "\tint size = vcpu->arch.pio.size;",
          "\tint port = vcpu->arch.pio.port;",
          "\tadvance_sev_es_emulated_ins(vcpu);",
          "\tif (vcpu->arch.sev_pio_count)",
          "\t\treturn kvm_sev_es_ins(vcpu, size, port);"
        ],
        "deleted": [
          "\tmemcpy(vcpu->arch.sev_pio_data, vcpu->arch.pio_data,",
          "\t       vcpu->arch.pio.count * vcpu->arch.pio.size);",
          "\tvcpu->arch.pio.count = 0;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking and handling for SEV-ES related operations in the code.",
      "trigger_condition": "A KVM guest using SEV-ES triggers out-of-bounds reads and writes in the host kernel via a malicious VMGEXIT for a string I/O instruction, leading to a potential system crash or guest-to-host escape scenario.",
      "specific_code_behavior_causing_vulnerability": "The original code directly performs a memory copy operation without proper bounds checking, allowing potential out-of-bounds accesses when copying data between specific structures.",
      "id": 122,
      "code_after_change_normalized": "static int FUN1(struct kvm_vcpu *VAR1)\n{\nint VAR2 = VAR1->VAR3.VAR4.VAR2;\nint VAR5 = VAR1->VAR3.VAR4.VAR5;\nFUN2(VAR1);\nif (VAR1->VAR3.VAR6)\nreturn FUN3(VAR1, VAR2, VAR5);\nreturn 1;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct kvm_vcpu *VAR1)\n{\nFUN2(VAR1->VAR2.VAR3, VAR1->VAR2.VAR4,\nVAR1->VAR2.VAR5.VAR6 * VAR1->VAR2.VAR5.VAR7);\nVAR1->VAR2.VAR5.VAR6 = 0;\nreturn 1;\n}\n",
      "code_after_change_raw": "static int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)\n{\nint size = vcpu->arch.pio.size;\nint port = vcpu->arch.pio.port;\nadvance_sev_es_emulated_ins(vcpu);\nif (vcpu->arch.sev_pio_count)\nreturn kvm_sev_es_ins(vcpu, size, port);\nreturn 1;\n}\n",
      "code_before_change_raw": "static int complete_sev_es_emulated_ins(struct kvm_vcpu *vcpu)\n{\nmemcpy(vcpu->arch.sev_pio_data, vcpu->arch.pio_data,\nvcpu->arch.pio.count * vcpu->arch.pio.size);\nvcpu->arch.pio.count = 0;\nreturn 1;\n}\n"
    }
  ],
  "CVE-2021-42008": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of boundary check for buffer write operations in the code.",
        "trigger_condition": "Input data from a process with CAP_NET_ADMIN capability triggers the vulnerability by allowing an out-of-bounds write operation.",
        "specific_code_behavior_causing_vulnerability": "The code writes data to the 'cooked_buf' buffer without checking if there is enough space available, leading to a potential buffer overflow and privilege escalation."
      },
      "solution": "To mitigate the vulnerability, a boundary check is added to ensure that there is enough space in the 'cooked_buf' buffer before writing data to it. If the available space is insufficient, an error message is printed, the buffer is reset, and the function returns early to prevent the buffer overrun. This solution prevents the out-of-bounds write and potential privilege escalation by ensuring data is written within the bounds of the buffer.",
      "GPT_analysis": "The modification made to the code snippet is necessary to prevent a slab out-of-bounds write vulnerability (CVE-2021-42008) in the Linux kernel. \n\nIn the original code snippet, the function `decode_data` writes data to the `cooked_buf` buffer without checking if there is enough space available in the buffer. This can lead to a buffer overflow if the `cooked_buf` buffer is not large enough to accommodate the data being written to it. An attacker with the CAP_NET_ADMIN capability could exploit this vulnerability to write data beyond the bounds of the `cooked_buf` buffer, potentially leading to a privilege escalation and root access.\n\nThe modification adds a check to ensure that there is enough space in the `cooked_buf` buffer before writing data to it. If the available space is not sufficient, an error message is printed, the `rx_count` is reset to 0, and the function returns early to prevent the buffer overrun. This modification helps to mitigate the vulnerability by preventing the out-of-bounds write and potential privilege escalation.",
      "GPT_purpose": "Decode data received in a specific format and store it in a cooked buffer.",
      "GPT_function": "\n1. Decodes data received by the sixpack structure.\n2. Stores the received byte in the raw buffer until 3 bytes are received.\n3. Converts the raw data into cooked data and stores it in the cooked buffer.",
      "CVE_id": "CVE-2021-42008",
      "code_before_change": "static void decode_data(struct sixpack *sp, unsigned char inbyte)\n{\n\tunsigned char *buf;\n\n\tif (sp->rx_count != 3) {\n\t\tsp->raw_buf[sp->rx_count++] = inbyte;\n\n\t\treturn;\n\t}\n\n\tbuf = sp->raw_buf;\n\tsp->cooked_buf[sp->rx_count_cooked++] =\n\t\tbuf[0] | ((buf[1] << 2) & 0xc0);\n\tsp->cooked_buf[sp->rx_count_cooked++] =\n\t\t(buf[1] & 0x0f) | ((buf[2] << 2) & 0xf0);\n\tsp->cooked_buf[sp->rx_count_cooked++] =\n\t\t(buf[2] & 0x03) | (inbyte << 2);\n\tsp->rx_count = 0;\n}",
      "code_after_change": "static void decode_data(struct sixpack *sp, unsigned char inbyte)\n{\n\tunsigned char *buf;\n\n\tif (sp->rx_count != 3) {\n\t\tsp->raw_buf[sp->rx_count++] = inbyte;\n\n\t\treturn;\n\t}\n\n\tif (sp->rx_count_cooked + 2 >= sizeof(sp->cooked_buf)) {\n\t\tpr_err(\"6pack: cooked buffer overrun, data loss\\n\");\n\t\tsp->rx_count = 0;\n\t\treturn;\n\t}\n\n\tbuf = sp->raw_buf;\n\tsp->cooked_buf[sp->rx_count_cooked++] =\n\t\tbuf[0] | ((buf[1] << 2) & 0xc0);\n\tsp->cooked_buf[sp->rx_count_cooked++] =\n\t\t(buf[1] & 0x0f) | ((buf[2] << 2) & 0xf0);\n\tsp->cooked_buf[sp->rx_count_cooked++] =\n\t\t(buf[2] & 0x03) | (inbyte << 2);\n\tsp->rx_count = 0;\n}",
      "modified_lines": {
        "added": [
          "\t\treturn;",
          "\t}",
          "",
          "\tif (sp->rx_count_cooked + 2 >= sizeof(sp->cooked_buf)) {",
          "\t\tpr_err(\"6pack: cooked buffer overrun, data loss\\n\");",
          "\t\tsp->rx_count = 0;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of boundary check for buffer write operations in the code.",
      "trigger_condition": "Input data from a process with CAP_NET_ADMIN capability triggers the vulnerability by allowing an out-of-bounds write operation.",
      "specific_code_behavior_causing_vulnerability": "The code writes data to the 'cooked_buf' buffer without checking if there is enough space available, leading to a potential buffer overflow and privilege escalation.",
      "id": 123,
      "code_after_change_normalized": "static void FUN1(struct sixpack *VAR1, unsigned char VAR2)\n{\nunsigned char *VAR3;\nif (VAR1->VAR4 != 3) {\nVAR1->VAR5[VAR1->VAR4++] = VAR2;\nreturn;\n}\nif (VAR1->VAR6 + 2 >= sizeof(VAR1->VAR7)) {\nFUN2(\"STR\");\nVAR1->VAR4 = 0;\nreturn;\n}\nVAR3 = VAR1->VAR5;\nVAR1->VAR7[VAR1->VAR6++] =\nVAR3[0] | ((VAR3[1] << 2) & VAR8);\nVAR1->VAR7[VAR1->VAR6++] =\n(VAR3[1] & VAR8) | ((VAR3[2] << 2) & VAR8);\nVAR1->VAR7[VAR1->VAR6++] =\n(VAR3[2] & VAR8) | (VAR2 << 2);\nVAR1->VAR4 = 0;\n}\n",
      "code_before_change_normalized": "static void FUN1(struct sixpack *VAR1, unsigned char VAR2)\n{\nunsigned char *VAR3;\nif (VAR1->VAR4 != 3) {\nVAR1->VAR5[VAR1->VAR4++] = VAR2;\nreturn;\n}\nVAR3 = VAR1->VAR5;\nVAR1->VAR6[VAR1->VAR7++] =\nVAR3[0] | ((VAR3[1] << 2) & VAR8);\nVAR1->VAR6[VAR1->VAR7++] =\n(VAR3[1] & VAR8) | ((VAR3[2] << 2) & VAR8);\nVAR1->VAR6[VAR1->VAR7++] =\n(VAR3[2] & VAR8) | (VAR2 << 2);\nVAR1->VAR4 = 0;\n}\n",
      "code_after_change_raw": "static void decode_data(struct sixpack *sp, unsigned char inbyte)\n{\nunsigned char *buf;\nif (sp->rx_count != 3) {\nsp->raw_buf[sp->rx_count++] = inbyte;\nreturn;\n}\nif (sp->rx_count_cooked + 2 >= sizeof(sp->cooked_buf)) {\npr_err(\"6pack: cooked buffer overrun, data loss\\n\");\nsp->rx_count = 0;\nreturn;\n}\nbuf = sp->raw_buf;\nsp->cooked_buf[sp->rx_count_cooked++] =\nbuf[0] | ((buf[1] << 2) & 0xc0);\nsp->cooked_buf[sp->rx_count_cooked++] =\n(buf[1] & 0x0f) | ((buf[2] << 2) & 0xf0);\nsp->cooked_buf[sp->rx_count_cooked++] =\n(buf[2] & 0x03) | (inbyte << 2);\nsp->rx_count = 0;\n}\n",
      "code_before_change_raw": "static void decode_data(struct sixpack *sp, unsigned char inbyte)\n{\nunsigned char *buf;\nif (sp->rx_count != 3) {\nsp->raw_buf[sp->rx_count++] = inbyte;\nreturn;\n}\nbuf = sp->raw_buf;\nsp->cooked_buf[sp->rx_count_cooked++] =\nbuf[0] | ((buf[1] << 2) & 0xc0);\nsp->cooked_buf[sp->rx_count_cooked++] =\n(buf[1] & 0x0f) | ((buf[2] << 2) & 0xf0);\nsp->cooked_buf[sp->rx_count_cooked++] =\n(buf[2] & 0x03) | (inbyte << 2);\nsp->rx_count = 0;\n}\n"
    }
  ],
  "CVE-2021-42327": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation of user-supplied data size before copying it into a fixed-size heap buffer.",
        "trigger_condition": "An attacker writes a string to the debug filesystem, causing the parse_write_buffer_into_params function to copy user-supplied data into a 40-byte heap buffer without proper size checks.",
        "specific_code_behavior_causing_vulnerability": "The code does not verify the size of user-supplied data before copying it into a fixed-size heap buffer, potentially leading to a heap-based buffer overflow if the data exceeds the buffer size."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the size of user-supplied data is properly validated before copying it into the fixed-size heap buffer. In this case, the solution involves passing the correct buffer size to the parse_write_buffer_into_params function to prevent it from writing data beyond the allocated buffer size. By implementing this validation, the code can prevent heap-based buffer overflow vulnerabilities caused by unchecked data copying.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2021-42327, which is a heap-based buffer overflow in the dp_link_settings_write function. The vulnerability arises from the fact that the size parameter is not properly checked in the parse_write_buffer_into_params function, which copies user-supplied data into a fixed-size heap buffer of 40 bytes.\n\nBy modifying the code to pass the correct buffer size (wr_buf_size) to the parse_write_buffer_into_params function instead of the size parameter received from the user, we ensure that the function only writes data up to the allocated buffer size (40 bytes) and prevents the buffer overflow vulnerability.\n\nIn summary, the modification ensures that the parse_write_buffer_into_params function does not exceed the allocated buffer size, thereby mitigating the heap-based buffer overflow vulnerability in the dp_link_settings_write function.",
      "GPT_purpose": "This function is responsible for writing user-provided display link settings to the AMD GPU display drivers debug filesystem.",
      "GPT_function": "\n1. Write user input to a buffer.\n2. Parse the buffer into parameters.\n3. Check and validate the input parameters.\n4. Save user-defined lane count and link rate to preferred settings.\n5. Retrain the DisplayPort link based on the preferred settings.",
      "CVE_id": "CVE-2021-42327",
      "code_before_change": "static ssize_t dp_link_settings_write(struct file *f, const char __user *buf,\n\t\t\t\t size_t size, loff_t *pos)\n{\n\tstruct amdgpu_dm_connector *connector = file_inode(f)->i_private;\n\tstruct dc_link *link = connector->dc_link;\n\tstruct dc_link_settings prefer_link_settings;\n\tchar *wr_buf = NULL;\n\tconst uint32_t wr_buf_size = 40;\n\t/* 0: lane_count; 1: link_rate */\n\tint max_param_num = 2;\n\tuint8_t param_nums = 0;\n\tlong param[2];\n\tbool valid_input = true;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\twr_buf = kcalloc(wr_buf_size, sizeof(char), GFP_KERNEL);\n\tif (!wr_buf)\n\t\treturn -ENOSPC;\n\n\tif (parse_write_buffer_into_params(wr_buf, size,\n\t\t\t\t\t   (long *)param, buf,\n\t\t\t\t\t   max_param_num,\n\t\t\t\t\t   &param_nums)) {\n\t\tkfree(wr_buf);\n\t\treturn -EINVAL;\n\t}\n\n\tif (param_nums <= 0) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"user data not be read\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (param[0]) {\n\tcase LANE_COUNT_ONE:\n\tcase LANE_COUNT_TWO:\n\tcase LANE_COUNT_FOUR:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\n\tswitch (param[1]) {\n\tcase LINK_RATE_LOW:\n\tcase LINK_RATE_HIGH:\n\tcase LINK_RATE_RBR2:\n\tcase LINK_RATE_HIGH2:\n\tcase LINK_RATE_HIGH3:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\n\tif (!valid_input) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"Invalid Input value No HW will be programmed\\n\");\n\t\treturn size;\n\t}\n\n\t/* save user force lane_count, link_rate to preferred settings\n\t * spread spectrum will not be changed\n\t */\n\tprefer_link_settings.link_spread = link->cur_link_settings.link_spread;\n\tprefer_link_settings.use_link_rate_set = false;\n\tprefer_link_settings.lane_count = param[0];\n\tprefer_link_settings.link_rate = param[1];\n\n\tdp_retrain_link_dp_test(link, &prefer_link_settings, false);\n\n\tkfree(wr_buf);\n\treturn size;\n}",
      "code_after_change": "static ssize_t dp_link_settings_write(struct file *f, const char __user *buf,\n\t\t\t\t size_t size, loff_t *pos)\n{\n\tstruct amdgpu_dm_connector *connector = file_inode(f)->i_private;\n\tstruct dc_link *link = connector->dc_link;\n\tstruct dc_link_settings prefer_link_settings;\n\tchar *wr_buf = NULL;\n\tconst uint32_t wr_buf_size = 40;\n\t/* 0: lane_count; 1: link_rate */\n\tint max_param_num = 2;\n\tuint8_t param_nums = 0;\n\tlong param[2];\n\tbool valid_input = true;\n\n\tif (size == 0)\n\t\treturn -EINVAL;\n\n\twr_buf = kcalloc(wr_buf_size, sizeof(char), GFP_KERNEL);\n\tif (!wr_buf)\n\t\treturn -ENOSPC;\n\n\tif (parse_write_buffer_into_params(wr_buf, wr_buf_size,\n\t\t\t\t\t   (long *)param, buf,\n\t\t\t\t\t   max_param_num,\n\t\t\t\t\t   &param_nums)) {\n\t\tkfree(wr_buf);\n\t\treturn -EINVAL;\n\t}\n\n\tif (param_nums <= 0) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"user data not be read\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\tswitch (param[0]) {\n\tcase LANE_COUNT_ONE:\n\tcase LANE_COUNT_TWO:\n\tcase LANE_COUNT_FOUR:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\n\tswitch (param[1]) {\n\tcase LINK_RATE_LOW:\n\tcase LINK_RATE_HIGH:\n\tcase LINK_RATE_RBR2:\n\tcase LINK_RATE_HIGH2:\n\tcase LINK_RATE_HIGH3:\n\t\tbreak;\n\tdefault:\n\t\tvalid_input = false;\n\t\tbreak;\n\t}\n\n\tif (!valid_input) {\n\t\tkfree(wr_buf);\n\t\tDRM_DEBUG_DRIVER(\"Invalid Input value No HW will be programmed\\n\");\n\t\treturn size;\n\t}\n\n\t/* save user force lane_count, link_rate to preferred settings\n\t * spread spectrum will not be changed\n\t */\n\tprefer_link_settings.link_spread = link->cur_link_settings.link_spread;\n\tprefer_link_settings.use_link_rate_set = false;\n\tprefer_link_settings.lane_count = param[0];\n\tprefer_link_settings.link_rate = param[1];\n\n\tdp_retrain_link_dp_test(link, &prefer_link_settings, false);\n\n\tkfree(wr_buf);\n\treturn size;\n}",
      "modified_lines": {
        "added": [
          "\tif (parse_write_buffer_into_params(wr_buf, wr_buf_size,"
        ],
        "deleted": [
          "\tif (parse_write_buffer_into_params(wr_buf, size,"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation of user-supplied data size before copying it into a fixed-size heap buffer.",
      "trigger_condition": "An attacker writes a string to the debug filesystem, causing the parse_write_buffer_into_params function to copy user-supplied data into a 40-byte heap buffer without proper size checks.",
      "specific_code_behavior_causing_vulnerability": "The code does not verify the size of user-supplied data before copying it into a fixed-size heap buffer, potentially leading to a heap-based buffer overflow if the data exceeds the buffer size.",
      "id": 124,
      "code_after_change_normalized": "static ssize_t FUN1(struct file *VAR1, const char __user *VAR2,\nsize_t VAR3, loff_t *VAR4)\n{\nstruct amdgpu_dm_connector *VAR5 = FUN2(VAR1)->VAR6;\nstruct VAR8 *VAR7 = VAR5->VAR8;\nstruct dc_link_settings VAR9;\nchar *VAR10 = NULL;\nconst uint32_t VAR11 = 40;\nint VAR12 = 2;\nuint8_t VAR13 = 0;\nlong VAR14[2];\nbool VAR15 = true;\nif (VAR3 == 0)\nreturn -VAR16;\nVAR10 = FUN3(VAR11, sizeof(char), VAR17);\nif (!VAR10)\nreturn -VAR18;\nif (FUN4(VAR10, VAR11,\n(long *)VAR14, VAR2,\nVAR12,\n&VAR13)) {\nFUN5(VAR10);\nreturn -VAR16;\n}\nif (VAR13 <= 0) {\nFUN5(VAR10);\nFUN6(\"STR\");\nreturn -VAR16;\n}\nswitch (VAR14[0]) {\ncase VAR19:\ncase VAR20:\ncase VAR21:\nbreak;\ndefault:\nVAR15 = false;\nbreak;\n}\nswitch (VAR14[1]) {\ncase VAR22:\ncase VAR23:\ncase VAR24:\ncase VAR25:\ncase VAR26:\nbreak;\ndefault:\nVAR15 = false;\nbreak;\n}\nif (!VAR15) {\nFUN5(VAR10);\nFUN6(\"STR\");\nreturn VAR3;\n}\nVAR9.VAR27 = VAR7->VAR28.VAR27;\nVAR9.VAR29 = false;\nVAR9.VAR30 = VAR14[0];\nVAR9.VAR31 = VAR14[1];\nFUN7(VAR7, &VAR9, false);\nFUN5(VAR10);\nreturn VAR3;\n}\n",
      "code_before_change_normalized": "static ssize_t FUN1(struct file *VAR1, const char __user *VAR2,\nsize_t VAR3, loff_t *VAR4)\n{\nstruct amdgpu_dm_connector *VAR5 = FUN2(VAR1)->VAR6;\nstruct VAR8 *VAR7 = VAR5->VAR8;\nstruct dc_link_settings VAR9;\nchar *VAR10 = NULL;\nconst uint32_t VAR11 = 40;\nint VAR12 = 2;\nuint8_t VAR13 = 0;\nlong VAR14[2];\nbool VAR15 = true;\nif (VAR3 == 0)\nreturn -VAR16;\nVAR10 = FUN3(VAR11, sizeof(char), VAR17);\nif (!VAR10)\nreturn -VAR18;\nif (FUN4(VAR10, VAR3,\n(long *)VAR14, VAR2,\nVAR12,\n&VAR13)) {\nFUN5(VAR10);\nreturn -VAR16;\n}\nif (VAR13 <= 0) {\nFUN5(VAR10);\nFUN6(\"STR\");\nreturn -VAR16;\n}\nswitch (VAR14[0]) {\ncase VAR19:\ncase VAR20:\ncase VAR21:\nbreak;\ndefault:\nVAR15 = false;\nbreak;\n}\nswitch (VAR14[1]) {\ncase VAR22:\ncase VAR23:\ncase VAR24:\ncase VAR25:\ncase VAR26:\nbreak;\ndefault:\nVAR15 = false;\nbreak;\n}\nif (!VAR15) {\nFUN5(VAR10);\nFUN6(\"STR\");\nreturn VAR3;\n}\nVAR9.VAR27 = VAR7->VAR28.VAR27;\nVAR9.VAR29 = false;\nVAR9.VAR30 = VAR14[0];\nVAR9.VAR31 = VAR14[1];\nFUN7(VAR7, &VAR9, false);\nFUN5(VAR10);\nreturn VAR3;\n}\n",
      "code_after_change_raw": "static ssize_t dp_link_settings_write(struct file *f, const char __user *buf,\nsize_t size, loff_t *pos)\n{\nstruct amdgpu_dm_connector *connector = file_inode(f)->i_private;\nstruct dc_link *link = connector->dc_link;\nstruct dc_link_settings prefer_link_settings;\nchar *wr_buf = NULL;\nconst uint32_t wr_buf_size = 40;\nint max_param_num = 2;\nuint8_t param_nums = 0;\nlong param[2];\nbool valid_input = true;\nif (size == 0)\nreturn -EINVAL;\nwr_buf = kcalloc(wr_buf_size, sizeof(char), GFP_KERNEL);\nif (!wr_buf)\nreturn -ENOSPC;\nif (parse_write_buffer_into_params(wr_buf, wr_buf_size,\n(long *)param, buf,\nmax_param_num,\n&param_nums)) {\nkfree(wr_buf);\nreturn -EINVAL;\n}\nif (param_nums <= 0) {\nkfree(wr_buf);\nDRM_DEBUG_DRIVER(\"user data not be read\\n\");\nreturn -EINVAL;\n}\nswitch (param[0]) {\ncase LANE_COUNT_ONE:\ncase LANE_COUNT_TWO:\ncase LANE_COUNT_FOUR:\nbreak;\ndefault:\nvalid_input = false;\nbreak;\n}\nswitch (param[1]) {\ncase LINK_RATE_LOW:\ncase LINK_RATE_HIGH:\ncase LINK_RATE_RBR2:\ncase LINK_RATE_HIGH2:\ncase LINK_RATE_HIGH3:\nbreak;\ndefault:\nvalid_input = false;\nbreak;\n}\nif (!valid_input) {\nkfree(wr_buf);\nDRM_DEBUG_DRIVER(\"Invalid Input value No HW will be programmed\\n\");\nreturn size;\n}\nprefer_link_settings.link_spread = link->cur_link_settings.link_spread;\nprefer_link_settings.use_link_rate_set = false;\nprefer_link_settings.lane_count = param[0];\nprefer_link_settings.link_rate = param[1];\ndp_retrain_link_dp_test(link, &prefer_link_settings, false);\nkfree(wr_buf);\nreturn size;\n}\n",
      "code_before_change_raw": "static ssize_t dp_link_settings_write(struct file *f, const char __user *buf,\nsize_t size, loff_t *pos)\n{\nstruct amdgpu_dm_connector *connector = file_inode(f)->i_private;\nstruct dc_link *link = connector->dc_link;\nstruct dc_link_settings prefer_link_settings;\nchar *wr_buf = NULL;\nconst uint32_t wr_buf_size = 40;\nint max_param_num = 2;\nuint8_t param_nums = 0;\nlong param[2];\nbool valid_input = true;\nif (size == 0)\nreturn -EINVAL;\nwr_buf = kcalloc(wr_buf_size, sizeof(char), GFP_KERNEL);\nif (!wr_buf)\nreturn -ENOSPC;\nif (parse_write_buffer_into_params(wr_buf, size,\n(long *)param, buf,\nmax_param_num,\n&param_nums)) {\nkfree(wr_buf);\nreturn -EINVAL;\n}\nif (param_nums <= 0) {\nkfree(wr_buf);\nDRM_DEBUG_DRIVER(\"user data not be read\\n\");\nreturn -EINVAL;\n}\nswitch (param[0]) {\ncase LANE_COUNT_ONE:\ncase LANE_COUNT_TWO:\ncase LANE_COUNT_FOUR:\nbreak;\ndefault:\nvalid_input = false;\nbreak;\n}\nswitch (param[1]) {\ncase LINK_RATE_LOW:\ncase LINK_RATE_HIGH:\ncase LINK_RATE_RBR2:\ncase LINK_RATE_HIGH2:\ncase LINK_RATE_HIGH3:\nbreak;\ndefault:\nvalid_input = false;\nbreak;\n}\nif (!valid_input) {\nkfree(wr_buf);\nDRM_DEBUG_DRIVER(\"Invalid Input value No HW will be programmed\\n\");\nreturn size;\n}\nprefer_link_settings.link_spread = link->cur_link_settings.link_spread;\nprefer_link_settings.use_link_rate_set = false;\nprefer_link_settings.lane_count = param[0];\nprefer_link_settings.link_rate = param[1];\ndp_retrain_link_dp_test(link, &prefer_link_settings, false);\nkfree(wr_buf);\nreturn size;\n}\n"
    }
  ],
  "CVE-2021-42739": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper boundary checks when writing data into an array, leading to a heap-based buffer overflow vulnerability.",
        "trigger_condition": "The code writes data into the array without verifying the boundaries, allowing an attacker to provide input that exceeds the array size and overwrites adjacent memory locations.",
        "specific_code_behavior_causing_vulnerability": "The code writes data into the 'c->operand' array without sufficient checks on the write positions and lengths, potentially causing buffer overflow issues. This behavior can be exploited by an attacker to overwrite critical data structures or execute arbitrary code."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add proper boundary checks and validation of write operations to ensure that data is written within the bounds of the array. Additionally, checks should be implemented to prevent reading beyond the length of the input buffer. By enforcing these safeguards, the code can prevent buffer overflow vulnerabilities and enhance security.",
      "GPT_analysis": "The modification is necessary to address the heap-based buffer overflow vulnerability present in the original code snippet. In the vulnerable code, there are multiple instances where the code writes data into the `c->operand` array without proper boundary checks, potentially leading to buffer overflow issues.\n\nBy modifying the code as suggested, the following changes are made to mitigate the vulnerability:\n1. Added additional boundary checks to ensure that the write operations do not exceed the bounds of the `c->operand` array.\n2. Added checks to prevent reading beyond the `length` of the `msg` buffer to avoid accessing invalid memory locations.\n3. Ensured that the length of the data being copied into `c->operand` does not exceed the available space in the array.\n\nThese modifications help prevent buffer overflow vulnerabilities that could be exploited by a local user to crash the system or potentially escalate privileges on the system. By implementing these changes, the code becomes more secure and less susceptible to exploitation.",
      "GPT_purpose": "This function is responsible for handling Conditional Access Program Map Table (CA PMT) messages in the FireDTV media card driver.",
      "GPT_function": "\n1. Constructing an AVC command frame for CA PMT (Conditional Access Program Map Table).\n2. Handling CA descriptors at the program level and stream level.\n3. Calculating CRC32 checksum and padding operands.\n4. Sending the constructed AVC command frame using avc_write.\n5. Checking the response status and handling errors.",
      "CVE_id": "CVE-2021-42739",
      "code_before_change": "int avc_ca_pmt(struct firedtv *fdtv, char *msg, int length)\n{\n\tstruct avc_command_frame *c = (void *)fdtv->avc_data;\n\tstruct avc_response_frame *r = (void *)fdtv->avc_data;\n\tint list_management;\n\tint program_info_length;\n\tint pmt_cmd_id;\n\tint read_pos;\n\tint write_pos;\n\tint es_info_length;\n\tint crc32_csum;\n\tint ret;\n\n\tif (unlikely(avc_debug & AVC_DEBUG_APPLICATION_PMT))\n\t\tdebug_pmt(msg, length);\n\n\tmutex_lock(&fdtv->avc_mutex);\n\n\tc->ctype   = AVC_CTYPE_CONTROL;\n\tc->subunit = AVC_SUBUNIT_TYPE_TUNER | fdtv->subunit;\n\tc->opcode  = AVC_OPCODE_VENDOR;\n\n\tif (msg[0] != EN50221_LIST_MANAGEMENT_ONLY) {\n\t\tdev_info(fdtv->device, \"forcing list_management to ONLY\\n\");\n\t\tmsg[0] = EN50221_LIST_MANAGEMENT_ONLY;\n\t}\n\t/* We take the cmd_id from the programme level only! */\n\tlist_management = msg[0];\n\tprogram_info_length = ((msg[4] & 0x0f) << 8) + msg[5];\n\tif (program_info_length > 0)\n\t\tprogram_info_length--; /* Remove pmt_cmd_id */\n\tpmt_cmd_id = msg[6];\n\n\tc->operand[0] = SFE_VENDOR_DE_COMPANYID_0;\n\tc->operand[1] = SFE_VENDOR_DE_COMPANYID_1;\n\tc->operand[2] = SFE_VENDOR_DE_COMPANYID_2;\n\tc->operand[3] = SFE_VENDOR_OPCODE_HOST2CA;\n\tc->operand[4] = 0; /* slot */\n\tc->operand[5] = SFE_VENDOR_TAG_CA_PMT; /* ca tag */\n\tc->operand[6] = 0; /* more/last */\n\t/* Use three bytes for length field in case length > 127 */\n\tc->operand[10] = list_management;\n\tc->operand[11] = 0x01; /* pmt_cmd=OK_descramble */\n\n\t/* TS program map table */\n\n\tc->operand[12] = 0x02; /* Table id=2 */\n\tc->operand[13] = 0x80; /* Section syntax + length */\n\n\tc->operand[15] = msg[1]; /* Program number */\n\tc->operand[16] = msg[2];\n\tc->operand[17] = msg[3]; /* Version number and current/next */\n\tc->operand[18] = 0x00; /* Section number=0 */\n\tc->operand[19] = 0x00; /* Last section number=0 */\n\tc->operand[20] = 0x1f; /* PCR_PID=1FFF */\n\tc->operand[21] = 0xff;\n\tc->operand[22] = (program_info_length >> 8); /* Program info length */\n\tc->operand[23] = (program_info_length & 0xff);\n\n\t/* CA descriptors at programme level */\n\tread_pos = 6;\n\twrite_pos = 24;\n\tif (program_info_length > 0) {\n\t\tpmt_cmd_id = msg[read_pos++];\n\t\tif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\n\t\t\tdev_err(fdtv->device,\n\t\t\t\t\"invalid pmt_cmd_id %d\\n\", pmt_cmd_id);\n\t\tif (program_info_length > sizeof(c->operand) - 4 - write_pos) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmemcpy(&c->operand[write_pos], &msg[read_pos],\n\t\t       program_info_length);\n\t\tread_pos += program_info_length;\n\t\twrite_pos += program_info_length;\n\t}\n\twhile (read_pos < length) {\n\t\tc->operand[write_pos++] = msg[read_pos++];\n\t\tc->operand[write_pos++] = msg[read_pos++];\n\t\tc->operand[write_pos++] = msg[read_pos++];\n\t\tes_info_length =\n\t\t\t((msg[read_pos] & 0x0f) << 8) + msg[read_pos + 1];\n\t\tread_pos += 2;\n\t\tif (es_info_length > 0)\n\t\t\tes_info_length--; /* Remove pmt_cmd_id */\n\t\tc->operand[write_pos++] = es_info_length >> 8;\n\t\tc->operand[write_pos++] = es_info_length & 0xff;\n\t\tif (es_info_length > 0) {\n\t\t\tpmt_cmd_id = msg[read_pos++];\n\t\t\tif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\n\t\t\t\tdev_err(fdtv->device, \"invalid pmt_cmd_id %d at stream level\\n\",\n\t\t\t\t\tpmt_cmd_id);\n\n\t\t\tif (es_info_length > sizeof(c->operand) - 4 -\n\t\t\t\t\t     write_pos) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmemcpy(&c->operand[write_pos], &msg[read_pos],\n\t\t\t       es_info_length);\n\t\t\tread_pos += es_info_length;\n\t\t\twrite_pos += es_info_length;\n\t\t}\n\t}\n\twrite_pos += 4; /* CRC */\n\n\tc->operand[7] = 0x82;\n\tc->operand[8] = (write_pos - 10) >> 8;\n\tc->operand[9] = (write_pos - 10) & 0xff;\n\tc->operand[14] = write_pos - 15;\n\n\tcrc32_csum = crc32_be(0, &c->operand[10], c->operand[12] - 1);\n\tc->operand[write_pos - 4] = (crc32_csum >> 24) & 0xff;\n\tc->operand[write_pos - 3] = (crc32_csum >> 16) & 0xff;\n\tc->operand[write_pos - 2] = (crc32_csum >>  8) & 0xff;\n\tc->operand[write_pos - 1] = (crc32_csum >>  0) & 0xff;\n\tpad_operands(c, write_pos);\n\n\tfdtv->avc_data_length = ALIGN(3 + write_pos, 4);\n\tret = avc_write(fdtv);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (r->response != AVC_RESPONSE_ACCEPTED) {\n\t\tdev_err(fdtv->device,\n\t\t\t\"CA PMT failed with response 0x%x\\n\", r->response);\n\t\tret = -EACCES;\n\t}\nout:\n\tmutex_unlock(&fdtv->avc_mutex);\n\n\treturn ret;\n}",
      "code_after_change": "int avc_ca_pmt(struct firedtv *fdtv, char *msg, int length)\n{\n\tstruct avc_command_frame *c = (void *)fdtv->avc_data;\n\tstruct avc_response_frame *r = (void *)fdtv->avc_data;\n\tint list_management;\n\tint program_info_length;\n\tint pmt_cmd_id;\n\tint read_pos;\n\tint write_pos;\n\tint es_info_length;\n\tint crc32_csum;\n\tint ret;\n\n\tif (unlikely(avc_debug & AVC_DEBUG_APPLICATION_PMT))\n\t\tdebug_pmt(msg, length);\n\n\tmutex_lock(&fdtv->avc_mutex);\n\n\tc->ctype   = AVC_CTYPE_CONTROL;\n\tc->subunit = AVC_SUBUNIT_TYPE_TUNER | fdtv->subunit;\n\tc->opcode  = AVC_OPCODE_VENDOR;\n\n\tif (msg[0] != EN50221_LIST_MANAGEMENT_ONLY) {\n\t\tdev_info(fdtv->device, \"forcing list_management to ONLY\\n\");\n\t\tmsg[0] = EN50221_LIST_MANAGEMENT_ONLY;\n\t}\n\t/* We take the cmd_id from the programme level only! */\n\tlist_management = msg[0];\n\tprogram_info_length = ((msg[4] & 0x0f) << 8) + msg[5];\n\tif (program_info_length > 0)\n\t\tprogram_info_length--; /* Remove pmt_cmd_id */\n\tpmt_cmd_id = msg[6];\n\n\tc->operand[0] = SFE_VENDOR_DE_COMPANYID_0;\n\tc->operand[1] = SFE_VENDOR_DE_COMPANYID_1;\n\tc->operand[2] = SFE_VENDOR_DE_COMPANYID_2;\n\tc->operand[3] = SFE_VENDOR_OPCODE_HOST2CA;\n\tc->operand[4] = 0; /* slot */\n\tc->operand[5] = SFE_VENDOR_TAG_CA_PMT; /* ca tag */\n\tc->operand[6] = 0; /* more/last */\n\t/* Use three bytes for length field in case length > 127 */\n\tc->operand[10] = list_management;\n\tc->operand[11] = 0x01; /* pmt_cmd=OK_descramble */\n\n\t/* TS program map table */\n\n\tc->operand[12] = 0x02; /* Table id=2 */\n\tc->operand[13] = 0x80; /* Section syntax + length */\n\n\tc->operand[15] = msg[1]; /* Program number */\n\tc->operand[16] = msg[2];\n\tc->operand[17] = msg[3]; /* Version number and current/next */\n\tc->operand[18] = 0x00; /* Section number=0 */\n\tc->operand[19] = 0x00; /* Last section number=0 */\n\tc->operand[20] = 0x1f; /* PCR_PID=1FFF */\n\tc->operand[21] = 0xff;\n\tc->operand[22] = (program_info_length >> 8); /* Program info length */\n\tc->operand[23] = (program_info_length & 0xff);\n\n\t/* CA descriptors at programme level */\n\tread_pos = 6;\n\twrite_pos = 24;\n\tif (program_info_length > 0) {\n\t\tpmt_cmd_id = msg[read_pos++];\n\t\tif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\n\t\t\tdev_err(fdtv->device,\n\t\t\t\t\"invalid pmt_cmd_id %d\\n\", pmt_cmd_id);\n\t\tif (program_info_length > sizeof(c->operand) - 4 - write_pos) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\n\t\tmemcpy(&c->operand[write_pos], &msg[read_pos],\n\t\t       program_info_length);\n\t\tread_pos += program_info_length;\n\t\twrite_pos += program_info_length;\n\t}\n\twhile (read_pos + 4 < length) {\n\t\tif (write_pos + 4 >= sizeof(c->operand) - 4) {\n\t\t\tret = -EINVAL;\n\t\t\tgoto out;\n\t\t}\n\t\tc->operand[write_pos++] = msg[read_pos++];\n\t\tc->operand[write_pos++] = msg[read_pos++];\n\t\tc->operand[write_pos++] = msg[read_pos++];\n\t\tes_info_length =\n\t\t\t((msg[read_pos] & 0x0f) << 8) + msg[read_pos + 1];\n\t\tread_pos += 2;\n\t\tif (es_info_length > 0)\n\t\t\tes_info_length--; /* Remove pmt_cmd_id */\n\t\tc->operand[write_pos++] = es_info_length >> 8;\n\t\tc->operand[write_pos++] = es_info_length & 0xff;\n\t\tif (es_info_length > 0) {\n\t\t\tif (read_pos >= length) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\t\t\tpmt_cmd_id = msg[read_pos++];\n\t\t\tif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\n\t\t\t\tdev_err(fdtv->device, \"invalid pmt_cmd_id %d at stream level\\n\",\n\t\t\t\t\tpmt_cmd_id);\n\n\t\t\tif (es_info_length > sizeof(c->operand) - 4 - write_pos ||\n\t\t\t    es_info_length > length - read_pos) {\n\t\t\t\tret = -EINVAL;\n\t\t\t\tgoto out;\n\t\t\t}\n\n\t\t\tmemcpy(&c->operand[write_pos], &msg[read_pos],\n\t\t\t       es_info_length);\n\t\t\tread_pos += es_info_length;\n\t\t\twrite_pos += es_info_length;\n\t\t}\n\t}\n\twrite_pos += 4; /* CRC */\n\n\tc->operand[7] = 0x82;\n\tc->operand[8] = (write_pos - 10) >> 8;\n\tc->operand[9] = (write_pos - 10) & 0xff;\n\tc->operand[14] = write_pos - 15;\n\n\tcrc32_csum = crc32_be(0, &c->operand[10], c->operand[12] - 1);\n\tc->operand[write_pos - 4] = (crc32_csum >> 24) & 0xff;\n\tc->operand[write_pos - 3] = (crc32_csum >> 16) & 0xff;\n\tc->operand[write_pos - 2] = (crc32_csum >>  8) & 0xff;\n\tc->operand[write_pos - 1] = (crc32_csum >>  0) & 0xff;\n\tpad_operands(c, write_pos);\n\n\tfdtv->avc_data_length = ALIGN(3 + write_pos, 4);\n\tret = avc_write(fdtv);\n\tif (ret < 0)\n\t\tgoto out;\n\n\tif (r->response != AVC_RESPONSE_ACCEPTED) {\n\t\tdev_err(fdtv->device,\n\t\t\t\"CA PMT failed with response 0x%x\\n\", r->response);\n\t\tret = -EACCES;\n\t}\nout:\n\tmutex_unlock(&fdtv->avc_mutex);\n\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\twhile (read_pos + 4 < length) {",
          "\t\tif (write_pos + 4 >= sizeof(c->operand) - 4) {",
          "\t\t\tret = -EINVAL;",
          "\t\t\tgoto out;",
          "\t\t}",
          "\t\t\tif (read_pos >= length) {",
          "\t\t\t\tret = -EINVAL;",
          "\t\t\t\tgoto out;",
          "\t\t\t}",
          "\t\t\tif (es_info_length > sizeof(c->operand) - 4 - write_pos ||",
          "\t\t\t    es_info_length > length - read_pos) {"
        ],
        "deleted": [
          "\twhile (read_pos < length) {",
          "\t\t\tif (es_info_length > sizeof(c->operand) - 4 -",
          "\t\t\t\t\t     write_pos) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper boundary checks when writing data into an array, leading to a heap-based buffer overflow vulnerability.",
      "trigger_condition": "The code writes data into the array without verifying the boundaries, allowing an attacker to provide input that exceeds the array size and overwrites adjacent memory locations.",
      "specific_code_behavior_causing_vulnerability": "The code writes data into the 'c->operand' array without sufficient checks on the write positions and lengths, potentially causing buffer overflow issues. This behavior can be exploited by an attacker to overwrite critical data structures or execute arbitrary code.",
      "id": 125,
      "code_after_change_normalized": "int FUN1(struct firedtv *VAR1, char *VAR2, int VAR3)\n{\nstruct avc_command_frame *VAR4 = (void *)VAR1->VAR5;\nstruct avc_response_frame *VAR6 = (void *)VAR1->VAR5;\nint VAR7;\nint VAR8;\nint VAR9;\nint VAR10;\nint VAR11;\nint VAR12;\nint VAR13;\nint VAR14;\nif (FUN2(VAR15 & VAR16))\nFUN3(VAR2, VAR3);\nFUN4(&VAR1->VAR17);\nVAR4->VAR18   = VAR19;\nVAR4->VAR20 = VAR21 | VAR1->VAR20;\nVAR4->VAR22  = VAR23;\nif (VAR2[0] != VAR24) {\nFUN5(VAR1->VAR25, \"STR\");\nVAR2[0] = VAR24;\n}\nVAR7 = VAR2[0];\nVAR8 = ((VAR2[4] & VAR26) << 8) + VAR2[5];\nif (VAR8 > 0)\nVAR8--; \nVAR9 = VAR2[6];\nVAR4->VAR27[0] = VAR28;\nVAR4->VAR27[1] = VAR29;\nVAR4->VAR27[2] = VAR30;\nVAR4->VAR27[3] = VAR31;\nVAR4->VAR27[4] = 0; \nVAR4->VAR27[5] = VAR32; \nVAR4->VAR27[6] = 0; \nVAR4->VAR27[10] = VAR7;\nVAR4->VAR27[11] = VAR26; \nVAR4->VAR27[12] = VAR26; \nVAR4->VAR27[13] = VAR26; \nVAR4->VAR27[15] = VAR2[1]; \nVAR4->VAR27[16] = VAR2[2];\nVAR4->VAR27[17] = VAR2[3]; \nVAR4->VAR27[18] = VAR26; \nVAR4->VAR27[19] = VAR26; \nVAR4->VAR27[20] = VAR26; \nVAR4->VAR27[21] = VAR26;\nVAR4->VAR27[22] = (VAR8 >> 8); \nVAR4->VAR27[23] = (VAR8 & VAR26);\nVAR10 = 6;\nVAR11 = 24;\nif (VAR8 > 0) {\nVAR9 = VAR2[VAR10++];\nif (VAR9 != 1 && VAR9 != 4)\nFUN6(VAR1->VAR25,\n\"STR\", VAR9);\nif (VAR8 > sizeof(VAR4->VAR27) - 4 - VAR11) {\nVAR14 = -VAR33;\ngoto VAR34;\n}\nFUN7(&VAR4->VAR27[VAR11], &VAR2[VAR10],\nVAR8);\nVAR10 += VAR8;\nVAR11 += VAR8;\n}\nwhile (VAR10 + 4 < VAR3) {\nif (VAR11 + 4 >= sizeof(VAR4->VAR27) - 4) {\nVAR14 = -VAR33;\ngoto VAR34;\n}\nVAR4->VAR27[VAR11++] = VAR2[VAR10++];\nVAR4->VAR27[VAR11++] = VAR2[VAR10++];\nVAR4->VAR27[VAR11++] = VAR2[VAR10++];\nVAR12 =\n((VAR2[VAR10] & VAR26) << 8) + VAR2[VAR10 + 1];\nVAR10 += 2;\nif (VAR12 > 0)\nVAR12--; \nVAR4->VAR27[VAR11++] = VAR12 >> 8;\nVAR4->VAR27[VAR11++] = VAR12 & VAR26;\nif (VAR12 > 0) {\nif (VAR10 >= VAR3) {\nVAR14 = -VAR33;\ngoto VAR34;\n}\nVAR9 = VAR2[VAR10++];\nif (VAR9 != 1 && VAR9 != 4)\nFUN6(VAR1->VAR25, \"STR\",\nVAR9);\nif (VAR12 > sizeof(VAR4->VAR27) - 4 - VAR11 ||\nVAR12 > VAR3 - VAR10) {\nVAR14 = -VAR33;\ngoto VAR34;\n}\nFUN7(&VAR4->VAR27[VAR11], &VAR2[VAR10],\nVAR12);\nVAR10 += VAR12;\nVAR11 += VAR12;\n}\n}\nVAR11 += 4; \nVAR4->VAR27[7] = VAR26;\nVAR4->VAR27[8] = (VAR11 - 10) >> 8;\nVAR4->VAR27[9] = (VAR11 - 10) & VAR26;\nVAR4->VAR27[14] = VAR11 - 15;\nVAR13 = FUN8(0, &VAR4->VAR27[10], VAR4->VAR27[12] - 1);\nVAR4->VAR27[VAR11 - 4] = (VAR13 >> 24) & VAR26;\nVAR4->VAR27[VAR11 - 3] = (VAR13 >> 16) & VAR26;\nVAR4->VAR27[VAR11 - 2] = (VAR13 >>  8) & VAR26;\nVAR4->VAR27[VAR11 - 1] = (VAR13 >>  0) & VAR26;\nFUN9(VAR4, VAR11);\nVAR1->VAR35 = FUN10(3 + VAR11, 4);\nVAR14 = FUN11(VAR1);\nif (VAR14 < 0)\ngoto VAR34;\nif (VAR6->VAR36 != VAR37) {\nFUN6(VAR1->VAR25,\n\"STR\", VAR6->VAR36);\nVAR14 = -VAR38;\n}\nVAR34:\nFUN12(&VAR1->VAR17);\nreturn VAR14;\n}\n",
      "code_before_change_normalized": "int FUN1(struct firedtv *VAR1, char *VAR2, int VAR3)\n{\nstruct avc_command_frame *VAR4 = (void *)VAR1->VAR5;\nstruct avc_response_frame *VAR6 = (void *)VAR1->VAR5;\nint VAR7;\nint VAR8;\nint VAR9;\nint VAR10;\nint VAR11;\nint VAR12;\nint VAR13;\nint VAR14;\nif (FUN2(VAR15 & VAR16))\nFUN3(VAR2, VAR3);\nFUN4(&VAR1->VAR17);\nVAR4->VAR18   = VAR19;\nVAR4->VAR20 = VAR21 | VAR1->VAR20;\nVAR4->VAR22  = VAR23;\nif (VAR2[0] != VAR24) {\nFUN5(VAR1->VAR25, \"STR\");\nVAR2[0] = VAR24;\n}\nVAR7 = VAR2[0];\nVAR8 = ((VAR2[4] & VAR26) << 8) + VAR2[5];\nif (VAR8 > 0)\nVAR8--; \nVAR9 = VAR2[6];\nVAR4->VAR27[0] = VAR28;\nVAR4->VAR27[1] = VAR29;\nVAR4->VAR27[2] = VAR30;\nVAR4->VAR27[3] = VAR31;\nVAR4->VAR27[4] = 0; \nVAR4->VAR27[5] = VAR32; \nVAR4->VAR27[6] = 0; \nVAR4->VAR27[10] = VAR7;\nVAR4->VAR27[11] = VAR26; \nVAR4->VAR27[12] = VAR26; \nVAR4->VAR27[13] = VAR26; \nVAR4->VAR27[15] = VAR2[1]; \nVAR4->VAR27[16] = VAR2[2];\nVAR4->VAR27[17] = VAR2[3]; \nVAR4->VAR27[18] = VAR26; \nVAR4->VAR27[19] = VAR26; \nVAR4->VAR27[20] = VAR26; \nVAR4->VAR27[21] = VAR26;\nVAR4->VAR27[22] = (VAR8 >> 8); \nVAR4->VAR27[23] = (VAR8 & VAR26);\nVAR10 = 6;\nVAR11 = 24;\nif (VAR8 > 0) {\nVAR9 = VAR2[VAR10++];\nif (VAR9 != 1 && VAR9 != 4)\nFUN6(VAR1->VAR25,\n\"STR\", VAR9);\nif (VAR8 > sizeof(VAR4->VAR27) - 4 - VAR11) {\nVAR14 = -VAR33;\ngoto VAR34;\n}\nFUN7(&VAR4->VAR27[VAR11], &VAR2[VAR10],\nVAR8);\nVAR10 += VAR8;\nVAR11 += VAR8;\n}\nwhile (VAR10 < VAR3) {\nVAR4->VAR27[VAR11++] = VAR2[VAR10++];\nVAR4->VAR27[VAR11++] = VAR2[VAR10++];\nVAR4->VAR27[VAR11++] = VAR2[VAR10++];\nVAR12 =\n((VAR2[VAR10] & VAR26) << 8) + VAR2[VAR10 + 1];\nVAR10 += 2;\nif (VAR12 > 0)\nVAR12--; \nVAR4->VAR27[VAR11++] = VAR12 >> 8;\nVAR4->VAR27[VAR11++] = VAR12 & VAR26;\nif (VAR12 > 0) {\nVAR9 = VAR2[VAR10++];\nif (VAR9 != 1 && VAR9 != 4)\nFUN6(VAR1->VAR25, \"STR\",\nVAR9);\nif (VAR12 > sizeof(VAR4->VAR27) - 4 -\nVAR11) {\nVAR14 = -VAR33;\ngoto VAR34;\n}\nFUN7(&VAR4->VAR27[VAR11], &VAR2[VAR10],\nVAR12);\nVAR10 += VAR12;\nVAR11 += VAR12;\n}\n}\nVAR11 += 4; \nVAR4->VAR27[7] = VAR26;\nVAR4->VAR27[8] = (VAR11 - 10) >> 8;\nVAR4->VAR27[9] = (VAR11 - 10) & VAR26;\nVAR4->VAR27[14] = VAR11 - 15;\nVAR13 = FUN8(0, &VAR4->VAR27[10], VAR4->VAR27[12] - 1);\nVAR4->VAR27[VAR11 - 4] = (VAR13 >> 24) & VAR26;\nVAR4->VAR27[VAR11 - 3] = (VAR13 >> 16) & VAR26;\nVAR4->VAR27[VAR11 - 2] = (VAR13 >>  8) & VAR26;\nVAR4->VAR27[VAR11 - 1] = (VAR13 >>  0) & VAR26;\nFUN9(VAR4, VAR11);\nVAR1->VAR35 = FUN10(3 + VAR11, 4);\nVAR14 = FUN11(VAR1);\nif (VAR14 < 0)\ngoto VAR34;\nif (VAR6->VAR36 != VAR37) {\nFUN6(VAR1->VAR25,\n\"STR\", VAR6->VAR36);\nVAR14 = -VAR38;\n}\nVAR34:\nFUN12(&VAR1->VAR17);\nreturn VAR14;\n}\n",
      "code_after_change_raw": "int avc_ca_pmt(struct firedtv *fdtv, char *msg, int length)\n{\nstruct avc_command_frame *c = (void *)fdtv->avc_data;\nstruct avc_response_frame *r = (void *)fdtv->avc_data;\nint list_management;\nint program_info_length;\nint pmt_cmd_id;\nint read_pos;\nint write_pos;\nint es_info_length;\nint crc32_csum;\nint ret;\nif (unlikely(avc_debug & AVC_DEBUG_APPLICATION_PMT))\ndebug_pmt(msg, length);\nmutex_lock(&fdtv->avc_mutex);\nc->ctype   = AVC_CTYPE_CONTROL;\nc->subunit = AVC_SUBUNIT_TYPE_TUNER | fdtv->subunit;\nc->opcode  = AVC_OPCODE_VENDOR;\nif (msg[0] != EN50221_LIST_MANAGEMENT_ONLY) {\ndev_info(fdtv->device, \"forcing list_management to ONLY\\n\");\nmsg[0] = EN50221_LIST_MANAGEMENT_ONLY;\n}\nlist_management = msg[0];\nprogram_info_length = ((msg[4] & 0x0f) << 8) + msg[5];\nif (program_info_length > 0)\nprogram_info_length--; \npmt_cmd_id = msg[6];\nc->operand[0] = SFE_VENDOR_DE_COMPANYID_0;\nc->operand[1] = SFE_VENDOR_DE_COMPANYID_1;\nc->operand[2] = SFE_VENDOR_DE_COMPANYID_2;\nc->operand[3] = SFE_VENDOR_OPCODE_HOST2CA;\nc->operand[4] = 0; \nc->operand[5] = SFE_VENDOR_TAG_CA_PMT; \nc->operand[6] = 0; \nc->operand[10] = list_management;\nc->operand[11] = 0x01; \nc->operand[12] = 0x02; \nc->operand[13] = 0x80; \nc->operand[15] = msg[1]; \nc->operand[16] = msg[2];\nc->operand[17] = msg[3]; \nc->operand[18] = 0x00; \nc->operand[19] = 0x00; \nc->operand[20] = 0x1f; \nc->operand[21] = 0xff;\nc->operand[22] = (program_info_length >> 8); \nc->operand[23] = (program_info_length & 0xff);\nread_pos = 6;\nwrite_pos = 24;\nif (program_info_length > 0) {\npmt_cmd_id = msg[read_pos++];\nif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\ndev_err(fdtv->device,\n\"invalid pmt_cmd_id %d\\n\", pmt_cmd_id);\nif (program_info_length > sizeof(c->operand) - 4 - write_pos) {\nret = -EINVAL;\ngoto out;\n}\nmemcpy(&c->operand[write_pos], &msg[read_pos],\nprogram_info_length);\nread_pos += program_info_length;\nwrite_pos += program_info_length;\n}\nwhile (read_pos + 4 < length) {\nif (write_pos + 4 >= sizeof(c->operand) - 4) {\nret = -EINVAL;\ngoto out;\n}\nc->operand[write_pos++] = msg[read_pos++];\nc->operand[write_pos++] = msg[read_pos++];\nc->operand[write_pos++] = msg[read_pos++];\nes_info_length =\n((msg[read_pos] & 0x0f) << 8) + msg[read_pos + 1];\nread_pos += 2;\nif (es_info_length > 0)\nes_info_length--; \nc->operand[write_pos++] = es_info_length >> 8;\nc->operand[write_pos++] = es_info_length & 0xff;\nif (es_info_length > 0) {\nif (read_pos >= length) {\nret = -EINVAL;\ngoto out;\n}\npmt_cmd_id = msg[read_pos++];\nif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\ndev_err(fdtv->device, \"invalid pmt_cmd_id %d at stream level\\n\",\npmt_cmd_id);\nif (es_info_length > sizeof(c->operand) - 4 - write_pos ||\nes_info_length > length - read_pos) {\nret = -EINVAL;\ngoto out;\n}\nmemcpy(&c->operand[write_pos], &msg[read_pos],\nes_info_length);\nread_pos += es_info_length;\nwrite_pos += es_info_length;\n}\n}\nwrite_pos += 4; \nc->operand[7] = 0x82;\nc->operand[8] = (write_pos - 10) >> 8;\nc->operand[9] = (write_pos - 10) & 0xff;\nc->operand[14] = write_pos - 15;\ncrc32_csum = crc32_be(0, &c->operand[10], c->operand[12] - 1);\nc->operand[write_pos - 4] = (crc32_csum >> 24) & 0xff;\nc->operand[write_pos - 3] = (crc32_csum >> 16) & 0xff;\nc->operand[write_pos - 2] = (crc32_csum >>  8) & 0xff;\nc->operand[write_pos - 1] = (crc32_csum >>  0) & 0xff;\npad_operands(c, write_pos);\nfdtv->avc_data_length = ALIGN(3 + write_pos, 4);\nret = avc_write(fdtv);\nif (ret < 0)\ngoto out;\nif (r->response != AVC_RESPONSE_ACCEPTED) {\ndev_err(fdtv->device,\n\"CA PMT failed with response 0x%x\\n\", r->response);\nret = -EACCES;\n}\nout:\nmutex_unlock(&fdtv->avc_mutex);\nreturn ret;\n}\n",
      "code_before_change_raw": "int avc_ca_pmt(struct firedtv *fdtv, char *msg, int length)\n{\nstruct avc_command_frame *c = (void *)fdtv->avc_data;\nstruct avc_response_frame *r = (void *)fdtv->avc_data;\nint list_management;\nint program_info_length;\nint pmt_cmd_id;\nint read_pos;\nint write_pos;\nint es_info_length;\nint crc32_csum;\nint ret;\nif (unlikely(avc_debug & AVC_DEBUG_APPLICATION_PMT))\ndebug_pmt(msg, length);\nmutex_lock(&fdtv->avc_mutex);\nc->ctype   = AVC_CTYPE_CONTROL;\nc->subunit = AVC_SUBUNIT_TYPE_TUNER | fdtv->subunit;\nc->opcode  = AVC_OPCODE_VENDOR;\nif (msg[0] != EN50221_LIST_MANAGEMENT_ONLY) {\ndev_info(fdtv->device, \"forcing list_management to ONLY\\n\");\nmsg[0] = EN50221_LIST_MANAGEMENT_ONLY;\n}\nlist_management = msg[0];\nprogram_info_length = ((msg[4] & 0x0f) << 8) + msg[5];\nif (program_info_length > 0)\nprogram_info_length--; \npmt_cmd_id = msg[6];\nc->operand[0] = SFE_VENDOR_DE_COMPANYID_0;\nc->operand[1] = SFE_VENDOR_DE_COMPANYID_1;\nc->operand[2] = SFE_VENDOR_DE_COMPANYID_2;\nc->operand[3] = SFE_VENDOR_OPCODE_HOST2CA;\nc->operand[4] = 0; \nc->operand[5] = SFE_VENDOR_TAG_CA_PMT; \nc->operand[6] = 0; \nc->operand[10] = list_management;\nc->operand[11] = 0x01; \nc->operand[12] = 0x02; \nc->operand[13] = 0x80; \nc->operand[15] = msg[1]; \nc->operand[16] = msg[2];\nc->operand[17] = msg[3]; \nc->operand[18] = 0x00; \nc->operand[19] = 0x00; \nc->operand[20] = 0x1f; \nc->operand[21] = 0xff;\nc->operand[22] = (program_info_length >> 8); \nc->operand[23] = (program_info_length & 0xff);\nread_pos = 6;\nwrite_pos = 24;\nif (program_info_length > 0) {\npmt_cmd_id = msg[read_pos++];\nif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\ndev_err(fdtv->device,\n\"invalid pmt_cmd_id %d\\n\", pmt_cmd_id);\nif (program_info_length > sizeof(c->operand) - 4 - write_pos) {\nret = -EINVAL;\ngoto out;\n}\nmemcpy(&c->operand[write_pos], &msg[read_pos],\nprogram_info_length);\nread_pos += program_info_length;\nwrite_pos += program_info_length;\n}\nwhile (read_pos < length) {\nc->operand[write_pos++] = msg[read_pos++];\nc->operand[write_pos++] = msg[read_pos++];\nc->operand[write_pos++] = msg[read_pos++];\nes_info_length =\n((msg[read_pos] & 0x0f) << 8) + msg[read_pos + 1];\nread_pos += 2;\nif (es_info_length > 0)\nes_info_length--; \nc->operand[write_pos++] = es_info_length >> 8;\nc->operand[write_pos++] = es_info_length & 0xff;\nif (es_info_length > 0) {\npmt_cmd_id = msg[read_pos++];\nif (pmt_cmd_id != 1 && pmt_cmd_id != 4)\ndev_err(fdtv->device, \"invalid pmt_cmd_id %d at stream level\\n\",\npmt_cmd_id);\nif (es_info_length > sizeof(c->operand) - 4 -\nwrite_pos) {\nret = -EINVAL;\ngoto out;\n}\nmemcpy(&c->operand[write_pos], &msg[read_pos],\nes_info_length);\nread_pos += es_info_length;\nwrite_pos += es_info_length;\n}\n}\nwrite_pos += 4; \nc->operand[7] = 0x82;\nc->operand[8] = (write_pos - 10) >> 8;\nc->operand[9] = (write_pos - 10) & 0xff;\nc->operand[14] = write_pos - 15;\ncrc32_csum = crc32_be(0, &c->operand[10], c->operand[12] - 1);\nc->operand[write_pos - 4] = (crc32_csum >> 24) & 0xff;\nc->operand[write_pos - 3] = (crc32_csum >> 16) & 0xff;\nc->operand[write_pos - 2] = (crc32_csum >>  8) & 0xff;\nc->operand[write_pos - 1] = (crc32_csum >>  0) & 0xff;\npad_operands(c, write_pos);\nfdtv->avc_data_length = ALIGN(3 + write_pos, 4);\nret = avc_write(fdtv);\nif (ret < 0)\ngoto out;\nif (r->response != AVC_RESPONSE_ACCEPTED) {\ndev_err(fdtv->device,\n\"CA PMT failed with response 0x%x\\n\", r->response);\nret = -EACCES;\n}\nout:\nmutex_unlock(&fdtv->avc_mutex);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2021-43975": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for length values before performing memory operations.",
        "trigger_condition": "An attacker introduces a crafted device with a manipulated length value, triggering an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the length values of 'sw' and 'fw' before writing to memory, allowing an attacker to exploit the vulnerability by providing a crafted length value that exceeds the allocated memory size for 'self->rpc'."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add checks to validate the length values of 'sw' and 'fw' before performing memory operations that involve writing to 'self->rpc'. By ensuring that the length values are within the bounds of the allocated memory size, the code prevents the out-of-bounds write vulnerability from being exploited. In this case, the solution involves adding conditional statements to verify that 'sw.len' and 'fw.len' are not larger than the allocated memory size for 'self->rpc' before proceeding with the memory operations.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2021-43975, which allows an attacker to trigger an out-of-bounds write via a crafted length value. By adding the check for the length of `sw` and `fw` before performing operations that involve writing to memory, the code ensures that the length values are within the bounds of the memory allocated for `self->rpc`.\n\nSpecifically, the added checks `if (sw.len > sizeof(self->rpc))` and `if (fw.len > sizeof(self->rpc))` verify that the length values are not larger than the allocated memory size for `self->rpc`. If the length values are found to be invalid, the code sets an error and exits the function, preventing the out-of-bounds write vulnerability from being exploited.\n\nBy adding these checks, the modified code snippet enhances the security of the function `hw_atl_utils_fw_rpc_wait` by validating the length values before proceeding with potentially unsafe memory operations, thus mitigating the vulnerability.",
      "GPT_purpose": "This function waits for a firmware RPC response and handles the response data accordingly.",
      "GPT_function": "\n1. Wait for firmware RPC completion.\n2. Check for errors and handle them.\n3. Perform a firmware RPC call if the length is 0xFFFFU.\n4. Download data if the length is not zero and update the rpc pointer.\n5. Handle errors and exit if necessary.",
      "CVE_id": "CVE-2021-43975",
      "code_before_change": "int hw_atl_utils_fw_rpc_wait(struct aq_hw_s *self,\n\t\t\t     struct hw_atl_utils_fw_rpc **rpc)\n{\n\tstruct aq_hw_atl_utils_fw_rpc_tid_s sw;\n\tstruct aq_hw_atl_utils_fw_rpc_tid_s fw;\n\tint err = 0;\n\n\tdo {\n\t\tsw.val = aq_hw_read_reg(self, HW_ATL_RPC_CONTROL_ADR);\n\n\t\tself->rpc_tid = sw.tid;\n\n\t\terr = readx_poll_timeout_atomic(hw_atl_utils_rpc_state_get,\n\t\t\t\t\t\tself, fw.val,\n\t\t\t\t\t\tsw.tid == fw.tid,\n\t\t\t\t\t\t1000U, 100000U);\n\t\tif (err < 0)\n\t\t\tgoto err_exit;\n\n\t\terr = aq_hw_err_from_flags(self);\n\t\tif (err < 0)\n\t\t\tgoto err_exit;\n\n\t\tif (fw.len == 0xFFFFU) {\n\t\t\terr = hw_atl_utils_fw_rpc_call(self, sw.len);\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_exit;\n\t\t}\n\t} while (sw.tid != fw.tid || 0xFFFFU == fw.len);\n\n\tif (rpc) {\n\t\tif (fw.len) {\n\t\t\terr =\n\t\t\thw_atl_utils_fw_downld_dwords(self,\n\t\t\t\t\t\t      self->rpc_addr,\n\t\t\t\t\t\t      (u32 *)(void *)\n\t\t\t\t\t\t      &self->rpc,\n\t\t\t\t\t\t      (fw.len + sizeof(u32) -\n\t\t\t\t\t\t       sizeof(u8)) /\n\t\t\t\t\t\t      sizeof(u32));\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_exit;\n\t\t}\n\n\t\t*rpc = &self->rpc;\n\t}\n\nerr_exit:\n\treturn err;\n}",
      "code_after_change": "int hw_atl_utils_fw_rpc_wait(struct aq_hw_s *self,\n\t\t\t     struct hw_atl_utils_fw_rpc **rpc)\n{\n\tstruct aq_hw_atl_utils_fw_rpc_tid_s sw;\n\tstruct aq_hw_atl_utils_fw_rpc_tid_s fw;\n\tint err = 0;\n\n\tdo {\n\t\tsw.val = aq_hw_read_reg(self, HW_ATL_RPC_CONTROL_ADR);\n\n\t\tself->rpc_tid = sw.tid;\n\n\t\terr = readx_poll_timeout_atomic(hw_atl_utils_rpc_state_get,\n\t\t\t\t\t\tself, fw.val,\n\t\t\t\t\t\tsw.tid == fw.tid,\n\t\t\t\t\t\t1000U, 100000U);\n\t\tif (err < 0)\n\t\t\tgoto err_exit;\n\n\t\terr = aq_hw_err_from_flags(self);\n\t\tif (err < 0)\n\t\t\tgoto err_exit;\n\n\t\tif (fw.len == 0xFFFFU) {\n\t\t\tif (sw.len > sizeof(self->rpc)) {\n\t\t\t\tprintk(KERN_INFO \"Invalid sw len: %x\\n\", sw.len);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_exit;\n\t\t\t}\n\t\t\terr = hw_atl_utils_fw_rpc_call(self, sw.len);\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_exit;\n\t\t}\n\t} while (sw.tid != fw.tid || 0xFFFFU == fw.len);\n\n\tif (rpc) {\n\t\tif (fw.len) {\n\t\t\tif (fw.len > sizeof(self->rpc)) {\n\t\t\t\tprintk(KERN_INFO \"Invalid fw len: %x\\n\", fw.len);\n\t\t\t\terr = -EINVAL;\n\t\t\t\tgoto err_exit;\n\t\t\t}\n\t\t\terr =\n\t\t\thw_atl_utils_fw_downld_dwords(self,\n\t\t\t\t\t\t      self->rpc_addr,\n\t\t\t\t\t\t      (u32 *)(void *)\n\t\t\t\t\t\t      &self->rpc,\n\t\t\t\t\t\t      (fw.len + sizeof(u32) -\n\t\t\t\t\t\t       sizeof(u8)) /\n\t\t\t\t\t\t      sizeof(u32));\n\t\t\tif (err < 0)\n\t\t\t\tgoto err_exit;\n\t\t}\n\n\t\t*rpc = &self->rpc;\n\t}\n\nerr_exit:\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tif (sw.len > sizeof(self->rpc)) {",
          "\t\t\t\tprintk(KERN_INFO \"Invalid sw len: %x\\n\", sw.len);",
          "\t\t\t\terr = -EINVAL;",
          "\t\t\t\tgoto err_exit;",
          "\t\t\t}",
          "\t\t\tif (fw.len > sizeof(self->rpc)) {",
          "\t\t\t\tprintk(KERN_INFO \"Invalid fw len: %x\\n\", fw.len);",
          "\t\t\t\terr = -EINVAL;",
          "\t\t\t\tgoto err_exit;",
          "\t\t\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of validation for length values before performing memory operations.",
      "trigger_condition": "An attacker introduces a crafted device with a manipulated length value, triggering an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the length values of 'sw' and 'fw' before writing to memory, allowing an attacker to exploit the vulnerability by providing a crafted length value that exceeds the allocated memory size for 'self->rpc'.",
      "id": 126,
      "code_after_change_normalized": "int FUN1(struct aq_hw_s *VAR1,\nstruct hw_atl_utils_fw_rpc **VAR2)\n{\nstruct aq_hw_atl_utils_fw_rpc_tid_s VAR3;\nstruct aq_hw_atl_utils_fw_rpc_tid_s VAR4;\nint VAR5 = 0;\ndo {\nVAR3.VAR6 = FUN2(VAR1, VAR7);\nVAR1->VAR8 = VAR3.VAR9;\nVAR5 = FUN3(VAR10,\nVAR1, VAR4.VAR6,\nVAR3.VAR9 == VAR4.VAR9,\n1000U, 100000U);\nif (VAR5 < 0)\ngoto VAR11;\nVAR5 = FUN4(VAR1);\nif (VAR5 < 0)\ngoto VAR11;\nif (VAR4.VAR12 == VAR13) {\nif (VAR3.VAR12 > sizeof(VAR1->VAR2)) {\nFUN5(VAR14 \"STR\", VAR3.VAR12);\nVAR5 = -VAR15;\ngoto VAR11;\n}\nVAR5 = FUN6(VAR1, VAR3.VAR12);\nif (VAR5 < 0)\ngoto VAR11;\n}\n} while (VAR3.VAR9 != VAR4.VAR9 || VAR13 == VAR4.VAR12);\nif (VAR2) {\nif (VAR4.VAR12) {\nif (VAR4.VAR12 > sizeof(VAR1->VAR2)) {\nFUN5(VAR14 \"STR\", VAR4.VAR12);\nVAR5 = -VAR15;\ngoto VAR11;\n}\nVAR5 =\nFUN7(VAR1,\nVAR1->VAR16,\n(VAR17 *)(void *)\n&VAR1->VAR2,\n(VAR4.VAR12 + sizeof(VAR17) -\nsizeof(VAR18)) /\nsizeof(VAR17));\nif (VAR5 < 0)\ngoto VAR11;\n}\n*VAR2 = &VAR1->VAR2;\n}\nVAR11:\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "int FUN1(struct aq_hw_s *VAR1,\nstruct hw_atl_utils_fw_rpc **VAR2)\n{\nstruct aq_hw_atl_utils_fw_rpc_tid_s VAR3;\nstruct aq_hw_atl_utils_fw_rpc_tid_s VAR4;\nint VAR5 = 0;\ndo {\nVAR3.VAR6 = FUN2(VAR1, VAR7);\nVAR1->VAR8 = VAR3.VAR9;\nVAR5 = FUN3(VAR10,\nVAR1, VAR4.VAR6,\nVAR3.VAR9 == VAR4.VAR9,\n1000U, 100000U);\nif (VAR5 < 0)\ngoto VAR11;\nVAR5 = FUN4(VAR1);\nif (VAR5 < 0)\ngoto VAR11;\nif (VAR4.VAR12 == VAR13) {\nVAR5 = FUN5(VAR1, VAR3.VAR12);\nif (VAR5 < 0)\ngoto VAR11;\n}\n} while (VAR3.VAR9 != VAR4.VAR9 || VAR13 == VAR4.VAR12);\nif (VAR2) {\nif (VAR4.VAR12) {\nVAR5 =\nFUN6(VAR1,\nVAR1->VAR14,\n(VAR15 *)(void *)\n&VAR1->VAR2,\n(VAR4.VAR12 + sizeof(VAR15) -\nsizeof(VAR16)) /\nsizeof(VAR15));\nif (VAR5 < 0)\ngoto VAR11;\n}\n*VAR2 = &VAR1->VAR2;\n}\nVAR11:\nreturn VAR5;\n}\n",
      "code_after_change_raw": "int hw_atl_utils_fw_rpc_wait(struct aq_hw_s *self,\nstruct hw_atl_utils_fw_rpc **rpc)\n{\nstruct aq_hw_atl_utils_fw_rpc_tid_s sw;\nstruct aq_hw_atl_utils_fw_rpc_tid_s fw;\nint err = 0;\ndo {\nsw.val = aq_hw_read_reg(self, HW_ATL_RPC_CONTROL_ADR);\nself->rpc_tid = sw.tid;\nerr = readx_poll_timeout_atomic(hw_atl_utils_rpc_state_get,\nself, fw.val,\nsw.tid == fw.tid,\n1000U, 100000U);\nif (err < 0)\ngoto err_exit;\nerr = aq_hw_err_from_flags(self);\nif (err < 0)\ngoto err_exit;\nif (fw.len == 0xFFFFU) {\nif (sw.len > sizeof(self->rpc)) {\nprintk(KERN_INFO \"Invalid sw len: %x\\n\", sw.len);\nerr = -EINVAL;\ngoto err_exit;\n}\nerr = hw_atl_utils_fw_rpc_call(self, sw.len);\nif (err < 0)\ngoto err_exit;\n}\n} while (sw.tid != fw.tid || 0xFFFFU == fw.len);\nif (rpc) {\nif (fw.len) {\nif (fw.len > sizeof(self->rpc)) {\nprintk(KERN_INFO \"Invalid fw len: %x\\n\", fw.len);\nerr = -EINVAL;\ngoto err_exit;\n}\nerr =\nhw_atl_utils_fw_downld_dwords(self,\nself->rpc_addr,\n(u32 *)(void *)\n&self->rpc,\n(fw.len + sizeof(u32) -\nsizeof(u8)) /\nsizeof(u32));\nif (err < 0)\ngoto err_exit;\n}\n*rpc = &self->rpc;\n}\nerr_exit:\nreturn err;\n}\n",
      "code_before_change_raw": "int hw_atl_utils_fw_rpc_wait(struct aq_hw_s *self,\nstruct hw_atl_utils_fw_rpc **rpc)\n{\nstruct aq_hw_atl_utils_fw_rpc_tid_s sw;\nstruct aq_hw_atl_utils_fw_rpc_tid_s fw;\nint err = 0;\ndo {\nsw.val = aq_hw_read_reg(self, HW_ATL_RPC_CONTROL_ADR);\nself->rpc_tid = sw.tid;\nerr = readx_poll_timeout_atomic(hw_atl_utils_rpc_state_get,\nself, fw.val,\nsw.tid == fw.tid,\n1000U, 100000U);\nif (err < 0)\ngoto err_exit;\nerr = aq_hw_err_from_flags(self);\nif (err < 0)\ngoto err_exit;\nif (fw.len == 0xFFFFU) {\nerr = hw_atl_utils_fw_rpc_call(self, sw.len);\nif (err < 0)\ngoto err_exit;\n}\n} while (sw.tid != fw.tid || 0xFFFFU == fw.len);\nif (rpc) {\nif (fw.len) {\nerr =\nhw_atl_utils_fw_downld_dwords(self,\nself->rpc_addr,\n(u32 *)(void *)\n&self->rpc,\n(fw.len + sizeof(u32) -\nsizeof(u8)) /\nsizeof(u32));\nif (err < 0)\ngoto err_exit;\n}\n*rpc = &self->rpc;\n}\nerr_exit:\nreturn err;\n}\n"
    }
  ],
  "CVE-2022-0435": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for data length input, allowing a potential stack overflow when processing packets with malicious content.",
        "trigger_condition": "Receiving a packet with a data length exceeding the maximum value that can be stored in a 16-bit unsigned integer, leading to a stack overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the data length input against the maximum value that can be stored in a 16-bit unsigned integer, leading to a potential stack overflow if the data length exceeds this limit."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to validate the data length input and ensure it does not exceed the maximum value that can be stored in a 16-bit unsigned integer. This check prevents the stack overflow condition by limiting the data length to a safe value. In this case, the solution involves adding a conditional statement to verify that the data length is within the acceptable range before proceeding with further processing.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2022-0435, which involves a stack overflow flaw in the Linux kernel's TIPC protocol functionality. The vulnerability occurs when a user sends a packet with malicious content that causes the number of domain member nodes to exceed the allowed limit of 64.\n\nTo mitigate this vulnerability, the code modification introduces a check to ensure that the data length (`dlen`) does not exceed the maximum value that can be stored in a 16-bit unsigned integer (`U16_MAX`). If the data length exceeds this limit, the function will exit early, preventing a potential stack overflow.\n\nBy adding this check, the code prevents the exploitation of the vulnerability by limiting the data length to a safe value and avoiding the stack overflow condition that could lead to system crashes or privilege escalation for remote attackers with access to the TIPC network.",
      "GPT_purpose": "Receive and process TIPC protocol messages on a TIPC link, handling various message types and updating link parameters accordingly.",
      "GPT_function": "\n1. Receiving TIPC protocol messages on a TIPC link.\n2. Processing different types of TIPC messages such as RESET_MSG, ACTIVATE_MSG, and STATE_MSG.\n3. Updating link parameters based on received message content.\n4. Handling gap acknowledgment blocks and sending NACK messages.\n5. Managing link state transitions and updating statistics.\n6. Freeing the received skb buffer after processing.",
      "CVE_id": "CVE-2022-0435",
      "code_before_change": "static int tipc_link_proto_rcv(struct tipc_link *l, struct sk_buff *skb,\n\t\t\t       struct sk_buff_head *xmitq)\n{\n\tstruct tipc_msg *hdr = buf_msg(skb);\n\tstruct tipc_gap_ack_blks *ga = NULL;\n\tbool reply = msg_probe(hdr), retransmitted = false;\n\tu16 dlen = msg_data_sz(hdr), glen = 0;\n\tu16 peers_snd_nxt =  msg_next_sent(hdr);\n\tu16 peers_tol = msg_link_tolerance(hdr);\n\tu16 peers_prio = msg_linkprio(hdr);\n\tu16 gap = msg_seq_gap(hdr);\n\tu16 ack = msg_ack(hdr);\n\tu16 rcv_nxt = l->rcv_nxt;\n\tu16 rcvgap = 0;\n\tint mtyp = msg_type(hdr);\n\tint rc = 0, released;\n\tchar *if_name;\n\tvoid *data;\n\n\ttrace_tipc_proto_rcv(skb, false, l->name);\n\tif (tipc_link_is_blocked(l) || !xmitq)\n\t\tgoto exit;\n\n\tif (tipc_own_addr(l->net) > msg_prevnode(hdr))\n\t\tl->net_plane = msg_net_plane(hdr);\n\n\tskb_linearize(skb);\n\thdr = buf_msg(skb);\n\tdata = msg_data(hdr);\n\n\tif (!tipc_link_validate_msg(l, hdr)) {\n\t\ttrace_tipc_skb_dump(skb, false, \"PROTO invalid (1)!\");\n\t\ttrace_tipc_link_dump(l, TIPC_DUMP_NONE, \"PROTO invalid (1)!\");\n\t\tgoto exit;\n\t}\n\n\tswitch (mtyp) {\n\tcase RESET_MSG:\n\tcase ACTIVATE_MSG:\n\t\t/* Complete own link name with peer's interface name */\n\t\tif_name =  strrchr(l->name, ':') + 1;\n\t\tif (sizeof(l->name) - (if_name - l->name) <= TIPC_MAX_IF_NAME)\n\t\t\tbreak;\n\t\tif (msg_data_sz(hdr) < TIPC_MAX_IF_NAME)\n\t\t\tbreak;\n\t\tstrncpy(if_name, data, TIPC_MAX_IF_NAME);\n\n\t\t/* Update own tolerance if peer indicates a non-zero value */\n\t\tif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\n\t\t\tl->tolerance = peers_tol;\n\t\t\tl->bc_rcvlink->tolerance = peers_tol;\n\t\t}\n\t\t/* Update own priority if peer's priority is higher */\n\t\tif (in_range(peers_prio, l->priority + 1, TIPC_MAX_LINK_PRI))\n\t\t\tl->priority = peers_prio;\n\n\t\t/* If peer is going down we want full re-establish cycle */\n\t\tif (msg_peer_stopping(hdr)) {\n\t\t\trc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* If this endpoint was re-created while peer was ESTABLISHING\n\t\t * it doesn't know current session number. Force re-synch.\n\t\t */\n\t\tif (mtyp == ACTIVATE_MSG && msg_dest_session_valid(hdr) &&\n\t\t    l->session != msg_dest_session(hdr)) {\n\t\t\tif (less(l->session, msg_dest_session(hdr)))\n\t\t\t\tl->session = msg_dest_session(hdr) + 1;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* ACTIVATE_MSG serves as PEER_RESET if link is already down */\n\t\tif (mtyp == RESET_MSG || !link_is_up(l))\n\t\t\trc = tipc_link_fsm_evt(l, LINK_PEER_RESET_EVT);\n\n\t\t/* ACTIVATE_MSG takes up link if it was already locally reset */\n\t\tif (mtyp == ACTIVATE_MSG && l->state == LINK_ESTABLISHING)\n\t\t\trc = TIPC_LINK_UP_EVT;\n\n\t\tl->peer_session = msg_session(hdr);\n\t\tl->in_session = true;\n\t\tl->peer_bearer_id = msg_bearer_id(hdr);\n\t\tif (l->mtu > msg_max_pkt(hdr))\n\t\t\tl->mtu = msg_max_pkt(hdr);\n\t\tbreak;\n\n\tcase STATE_MSG:\n\t\tl->rcv_nxt_state = msg_seqno(hdr) + 1;\n\n\t\t/* Update own tolerance if peer indicates a non-zero value */\n\t\tif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\n\t\t\tl->tolerance = peers_tol;\n\t\t\tl->bc_rcvlink->tolerance = peers_tol;\n\t\t}\n\t\t/* Update own prio if peer indicates a different value */\n\t\tif ((peers_prio != l->priority) &&\n\t\t    in_range(peers_prio, 1, TIPC_MAX_LINK_PRI)) {\n\t\t\tl->priority = peers_prio;\n\t\t\trc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\n\t\t}\n\n\t\tl->silent_intv_cnt = 0;\n\t\tl->stats.recv_states++;\n\t\tif (msg_probe(hdr))\n\t\t\tl->stats.recv_probes++;\n\n\t\tif (!link_is_up(l)) {\n\t\t\tif (l->state == LINK_ESTABLISHING)\n\t\t\t\trc = TIPC_LINK_UP_EVT;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Receive Gap ACK blocks from peer if any */\n\t\tglen = tipc_get_gap_ack_blks(&ga, l, hdr, true);\n\n\t\ttipc_mon_rcv(l->net, data + glen, dlen - glen, l->addr,\n\t\t\t     &l->mon_state, l->bearer_id);\n\n\t\t/* Send NACK if peer has sent pkts we haven't received yet */\n\t\tif ((reply || msg_is_keepalive(hdr)) &&\n\t\t    more(peers_snd_nxt, rcv_nxt) &&\n\t\t    !tipc_link_is_synching(l) &&\n\t\t    skb_queue_empty(&l->deferdq))\n\t\t\trcvgap = peers_snd_nxt - l->rcv_nxt;\n\t\tif (rcvgap || reply)\n\t\t\ttipc_link_build_proto_msg(l, STATE_MSG, 0, reply,\n\t\t\t\t\t\t  rcvgap, 0, 0, xmitq);\n\n\t\treleased = tipc_link_advance_transmq(l, l, ack, gap, ga, xmitq,\n\t\t\t\t\t\t     &retransmitted, &rc);\n\t\tif (gap)\n\t\t\tl->stats.recv_nacks++;\n\t\tif (released || retransmitted)\n\t\t\ttipc_link_update_cwin(l, released, retransmitted);\n\t\tif (released)\n\t\t\ttipc_link_advance_backlog(l, xmitq);\n\t\tif (unlikely(!skb_queue_empty(&l->wakeupq)))\n\t\t\tlink_prepare_wakeup(l);\n\t}\nexit:\n\tkfree_skb(skb);\n\treturn rc;\n}",
      "code_after_change": "static int tipc_link_proto_rcv(struct tipc_link *l, struct sk_buff *skb,\n\t\t\t       struct sk_buff_head *xmitq)\n{\n\tstruct tipc_msg *hdr = buf_msg(skb);\n\tstruct tipc_gap_ack_blks *ga = NULL;\n\tbool reply = msg_probe(hdr), retransmitted = false;\n\tu32 dlen = msg_data_sz(hdr), glen = 0;\n\tu16 peers_snd_nxt =  msg_next_sent(hdr);\n\tu16 peers_tol = msg_link_tolerance(hdr);\n\tu16 peers_prio = msg_linkprio(hdr);\n\tu16 gap = msg_seq_gap(hdr);\n\tu16 ack = msg_ack(hdr);\n\tu16 rcv_nxt = l->rcv_nxt;\n\tu16 rcvgap = 0;\n\tint mtyp = msg_type(hdr);\n\tint rc = 0, released;\n\tchar *if_name;\n\tvoid *data;\n\n\ttrace_tipc_proto_rcv(skb, false, l->name);\n\n\tif (dlen > U16_MAX)\n\t\tgoto exit;\n\n\tif (tipc_link_is_blocked(l) || !xmitq)\n\t\tgoto exit;\n\n\tif (tipc_own_addr(l->net) > msg_prevnode(hdr))\n\t\tl->net_plane = msg_net_plane(hdr);\n\n\tskb_linearize(skb);\n\thdr = buf_msg(skb);\n\tdata = msg_data(hdr);\n\n\tif (!tipc_link_validate_msg(l, hdr)) {\n\t\ttrace_tipc_skb_dump(skb, false, \"PROTO invalid (1)!\");\n\t\ttrace_tipc_link_dump(l, TIPC_DUMP_NONE, \"PROTO invalid (1)!\");\n\t\tgoto exit;\n\t}\n\n\tswitch (mtyp) {\n\tcase RESET_MSG:\n\tcase ACTIVATE_MSG:\n\t\t/* Complete own link name with peer's interface name */\n\t\tif_name =  strrchr(l->name, ':') + 1;\n\t\tif (sizeof(l->name) - (if_name - l->name) <= TIPC_MAX_IF_NAME)\n\t\t\tbreak;\n\t\tif (msg_data_sz(hdr) < TIPC_MAX_IF_NAME)\n\t\t\tbreak;\n\t\tstrncpy(if_name, data, TIPC_MAX_IF_NAME);\n\n\t\t/* Update own tolerance if peer indicates a non-zero value */\n\t\tif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\n\t\t\tl->tolerance = peers_tol;\n\t\t\tl->bc_rcvlink->tolerance = peers_tol;\n\t\t}\n\t\t/* Update own priority if peer's priority is higher */\n\t\tif (in_range(peers_prio, l->priority + 1, TIPC_MAX_LINK_PRI))\n\t\t\tl->priority = peers_prio;\n\n\t\t/* If peer is going down we want full re-establish cycle */\n\t\tif (msg_peer_stopping(hdr)) {\n\t\t\trc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\n\t\t\tbreak;\n\t\t}\n\n\t\t/* If this endpoint was re-created while peer was ESTABLISHING\n\t\t * it doesn't know current session number. Force re-synch.\n\t\t */\n\t\tif (mtyp == ACTIVATE_MSG && msg_dest_session_valid(hdr) &&\n\t\t    l->session != msg_dest_session(hdr)) {\n\t\t\tif (less(l->session, msg_dest_session(hdr)))\n\t\t\t\tl->session = msg_dest_session(hdr) + 1;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* ACTIVATE_MSG serves as PEER_RESET if link is already down */\n\t\tif (mtyp == RESET_MSG || !link_is_up(l))\n\t\t\trc = tipc_link_fsm_evt(l, LINK_PEER_RESET_EVT);\n\n\t\t/* ACTIVATE_MSG takes up link if it was already locally reset */\n\t\tif (mtyp == ACTIVATE_MSG && l->state == LINK_ESTABLISHING)\n\t\t\trc = TIPC_LINK_UP_EVT;\n\n\t\tl->peer_session = msg_session(hdr);\n\t\tl->in_session = true;\n\t\tl->peer_bearer_id = msg_bearer_id(hdr);\n\t\tif (l->mtu > msg_max_pkt(hdr))\n\t\t\tl->mtu = msg_max_pkt(hdr);\n\t\tbreak;\n\n\tcase STATE_MSG:\n\t\tl->rcv_nxt_state = msg_seqno(hdr) + 1;\n\n\t\t/* Update own tolerance if peer indicates a non-zero value */\n\t\tif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\n\t\t\tl->tolerance = peers_tol;\n\t\t\tl->bc_rcvlink->tolerance = peers_tol;\n\t\t}\n\t\t/* Update own prio if peer indicates a different value */\n\t\tif ((peers_prio != l->priority) &&\n\t\t    in_range(peers_prio, 1, TIPC_MAX_LINK_PRI)) {\n\t\t\tl->priority = peers_prio;\n\t\t\trc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\n\t\t}\n\n\t\tl->silent_intv_cnt = 0;\n\t\tl->stats.recv_states++;\n\t\tif (msg_probe(hdr))\n\t\t\tl->stats.recv_probes++;\n\n\t\tif (!link_is_up(l)) {\n\t\t\tif (l->state == LINK_ESTABLISHING)\n\t\t\t\trc = TIPC_LINK_UP_EVT;\n\t\t\tbreak;\n\t\t}\n\n\t\t/* Receive Gap ACK blocks from peer if any */\n\t\tglen = tipc_get_gap_ack_blks(&ga, l, hdr, true);\n\t\tif(glen > dlen)\n\t\t\tbreak;\n\t\ttipc_mon_rcv(l->net, data + glen, dlen - glen, l->addr,\n\t\t\t     &l->mon_state, l->bearer_id);\n\n\t\t/* Send NACK if peer has sent pkts we haven't received yet */\n\t\tif ((reply || msg_is_keepalive(hdr)) &&\n\t\t    more(peers_snd_nxt, rcv_nxt) &&\n\t\t    !tipc_link_is_synching(l) &&\n\t\t    skb_queue_empty(&l->deferdq))\n\t\t\trcvgap = peers_snd_nxt - l->rcv_nxt;\n\t\tif (rcvgap || reply)\n\t\t\ttipc_link_build_proto_msg(l, STATE_MSG, 0, reply,\n\t\t\t\t\t\t  rcvgap, 0, 0, xmitq);\n\n\t\treleased = tipc_link_advance_transmq(l, l, ack, gap, ga, xmitq,\n\t\t\t\t\t\t     &retransmitted, &rc);\n\t\tif (gap)\n\t\t\tl->stats.recv_nacks++;\n\t\tif (released || retransmitted)\n\t\t\ttipc_link_update_cwin(l, released, retransmitted);\n\t\tif (released)\n\t\t\ttipc_link_advance_backlog(l, xmitq);\n\t\tif (unlikely(!skb_queue_empty(&l->wakeupq)))\n\t\t\tlink_prepare_wakeup(l);\n\t}\nexit:\n\tkfree_skb(skb);\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tu32 dlen = msg_data_sz(hdr), glen = 0;",
          "",
          "\tif (dlen > U16_MAX)",
          "\t\tgoto exit;",
          "",
          "\t\tif(glen > dlen)",
          "\t\t\tbreak;"
        ],
        "deleted": [
          "\tu16 dlen = msg_data_sz(hdr), glen = 0;",
          ""
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for data length input, allowing a potential stack overflow when processing packets with malicious content.",
      "trigger_condition": "Receiving a packet with a data length exceeding the maximum value that can be stored in a 16-bit unsigned integer, leading to a stack overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the data length input against the maximum value that can be stored in a 16-bit unsigned integer, leading to a potential stack overflow if the data length exceeds this limit.",
      "id": 127,
      "code_after_change_normalized": "static int FUN1(struct tipc_link *VAR1, struct sk_buff *VAR2,\nstruct sk_buff_head *VAR3)\n{\nstruct tipc_msg *VAR4 = FUN2(VAR2);\nstruct tipc_gap_ack_blks *VAR5 = NULL;\nbool VAR6 = FUN3(VAR4), VAR7 = false;\nu32 VAR8 = FUN4(VAR4), VAR9 = 0;\nu16 VAR10 =  FUN5(VAR4);\nu16 VAR11 = FUN6(VAR4);\nu16 VAR12 = FUN7(VAR4);\nu16 VAR13 = FUN8(VAR4);\nu16 VAR14 = FUN9(VAR4);\nu16 VAR15 = VAR1->VAR15;\nu16 VAR16 = 0;\nint VAR17 = FUN10(VAR4);\nint VAR18 = 0, VAR19;\nchar *VAR20;\nvoid *VAR21;\nFUN11(VAR2, false, VAR1->VAR22);\nif (VAR8 > VAR23)\ngoto VAR24;\nif (FUN12(VAR1) || !VAR3)\ngoto VAR24;\nif (FUN13(VAR1->VAR25) > FUN14(VAR4))\nVAR1->VAR26 = FUN15(VAR4);\nFUN16(VAR2);\nVAR4 = FUN2(VAR2);\nVAR21 = FUN17(VAR4);\nif (!FUN18(VAR1, VAR4)) {\nFUN19(VAR2, false, \"STR\");\nFUN20(VAR1, VAR27, \"STR\");\ngoto VAR24;\n}\nswitch (VAR17) {\ncase VAR28:\ncase VAR29:\nVAR20 =  FUN21(VAR1->VAR22, ) + 1;\nif (sizeof(VAR1->VAR22) - (VAR20 - VAR1->VAR22) <= VAR30)\nbreak;\nif (FUN4(VAR4) < VAR30)\nbreak;\nFUN22(VAR20, VAR21, VAR30);\nif (FUN23(VAR11, VAR31, VAR32)) {\nVAR1->VAR33 = VAR11;\nVAR1->VAR34->VAR33 = VAR11;\n}\nif (FUN23(VAR12, VAR1->VAR35 + 1, VAR36))\nVAR1->VAR35 = VAR12;\nif (FUN24(VAR4)) {\nVAR18 = FUN25(VAR1, VAR37);\nbreak;\n}\nif (VAR17 == VAR29 && FUN26(VAR4) &&\nVAR1->VAR38 != FUN27(VAR4)) {\nif (FUN28(VAR1->VAR38, FUN27(VAR4)))\nVAR1->VAR38 = FUN27(VAR4) + 1;\nbreak;\n}\nif (VAR17 == VAR28 || !FUN29(VAR1))\nVAR18 = FUN25(VAR1, VAR39);\nif (VAR17 == VAR29 && VAR1->VAR40 == VAR41)\nVAR18 = VAR42;\nVAR1->VAR43 = FUN30(VAR4);\nVAR1->VAR44 = true;\nVAR1->VAR45 = FUN31(VAR4);\nif (VAR1->VAR46 > FUN32(VAR4))\nVAR1->VAR46 = FUN32(VAR4);\nbreak;\ncase VAR47:\nVAR1->VAR48 = FUN33(VAR4) + 1;\nif (FUN23(VAR11, VAR31, VAR32)) {\nVAR1->VAR33 = VAR11;\nVAR1->VAR34->VAR33 = VAR11;\n}\nif ((VAR12 != VAR1->VAR35) &&\nFUN23(VAR12, 1, VAR36)) {\nVAR1->VAR35 = VAR12;\nVAR18 = FUN25(VAR1, VAR37);\n}\nVAR1->VAR49 = 0;\nVAR1->VAR50.VAR51++;\nif (FUN3(VAR4))\nVAR1->VAR50.VAR52++;\nif (!FUN29(VAR1)) {\nif (VAR1->VAR40 == VAR41)\nVAR18 = VAR42;\nbreak;\n}\nVAR9 = FUN34(&VAR5, VAR1, VAR4, true);\nif(VAR9 > VAR8)\nbreak;\nFUN35(VAR1->VAR25, VAR21 + VAR9, VAR8 - VAR9, VAR1->VAR53,\n&VAR1->VAR54, VAR1->VAR55);\nif ((VAR6 || FUN36(VAR4)) &&\nFUN37(VAR10, VAR15) &&\n!FUN38(VAR1) &&\nFUN39(&VAR1->VAR56))\nVAR16 = VAR10 - VAR1->VAR15;\nif (VAR16 || VAR6)\nFUN40(VAR1, VAR47, 0, VAR6,\nVAR16, 0, 0, VAR3);\nVAR19 = FUN41(VAR1, VAR1, VAR14, VAR13, VAR5, VAR3,\n&VAR7, &VAR18);\nif (VAR13)\nVAR1->VAR50.VAR57++;\nif (VAR19 || VAR7)\nFUN42(VAR1, VAR19, VAR7);\nif (VAR19)\nFUN43(VAR1, VAR3);\nif (FUN44(!FUN39(&VAR1->VAR58)))\nFUN45(VAR1);\n}\nVAR24:\nFUN46(VAR2);\nreturn VAR18;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct tipc_link *VAR1, struct sk_buff *VAR2,\nstruct sk_buff_head *VAR3)\n{\nstruct tipc_msg *VAR4 = FUN2(VAR2);\nstruct tipc_gap_ack_blks *VAR5 = NULL;\nbool VAR6 = FUN3(VAR4), VAR7 = false;\nu16 VAR8 = FUN4(VAR4), VAR9 = 0;\nu16 VAR10 =  FUN5(VAR4);\nu16 VAR11 = FUN6(VAR4);\nu16 VAR12 = FUN7(VAR4);\nu16 VAR13 = FUN8(VAR4);\nu16 VAR14 = FUN9(VAR4);\nu16 VAR15 = VAR1->VAR15;\nu16 VAR16 = 0;\nint VAR17 = FUN10(VAR4);\nint VAR18 = 0, VAR19;\nchar *VAR20;\nvoid *VAR21;\nFUN11(VAR2, false, VAR1->VAR22);\nif (FUN12(VAR1) || !VAR3)\ngoto VAR23;\nif (FUN13(VAR1->VAR24) > FUN14(VAR4))\nVAR1->VAR25 = FUN15(VAR4);\nFUN16(VAR2);\nVAR4 = FUN2(VAR2);\nVAR21 = FUN17(VAR4);\nif (!FUN18(VAR1, VAR4)) {\nFUN19(VAR2, false, \"STR\");\nFUN20(VAR1, VAR26, \"STR\");\ngoto VAR23;\n}\nswitch (VAR17) {\ncase VAR27:\ncase VAR28:\nVAR20 =  FUN21(VAR1->VAR22, ) + 1;\nif (sizeof(VAR1->VAR22) - (VAR20 - VAR1->VAR22) <= VAR29)\nbreak;\nif (FUN4(VAR4) < VAR29)\nbreak;\nFUN22(VAR20, VAR21, VAR29);\nif (FUN23(VAR11, VAR30, VAR31)) {\nVAR1->VAR32 = VAR11;\nVAR1->VAR33->VAR32 = VAR11;\n}\nif (FUN23(VAR12, VAR1->VAR34 + 1, VAR35))\nVAR1->VAR34 = VAR12;\nif (FUN24(VAR4)) {\nVAR18 = FUN25(VAR1, VAR36);\nbreak;\n}\nif (VAR17 == VAR28 && FUN26(VAR4) &&\nVAR1->VAR37 != FUN27(VAR4)) {\nif (FUN28(VAR1->VAR37, FUN27(VAR4)))\nVAR1->VAR37 = FUN27(VAR4) + 1;\nbreak;\n}\nif (VAR17 == VAR27 || !FUN29(VAR1))\nVAR18 = FUN25(VAR1, VAR38);\nif (VAR17 == VAR28 && VAR1->VAR39 == VAR40)\nVAR18 = VAR41;\nVAR1->VAR42 = FUN30(VAR4);\nVAR1->VAR43 = true;\nVAR1->VAR44 = FUN31(VAR4);\nif (VAR1->VAR45 > FUN32(VAR4))\nVAR1->VAR45 = FUN32(VAR4);\nbreak;\ncase VAR46:\nVAR1->VAR47 = FUN33(VAR4) + 1;\nif (FUN23(VAR11, VAR30, VAR31)) {\nVAR1->VAR32 = VAR11;\nVAR1->VAR33->VAR32 = VAR11;\n}\nif ((VAR12 != VAR1->VAR34) &&\nFUN23(VAR12, 1, VAR35)) {\nVAR1->VAR34 = VAR12;\nVAR18 = FUN25(VAR1, VAR36);\n}\nVAR1->VAR48 = 0;\nVAR1->VAR49.VAR50++;\nif (FUN3(VAR4))\nVAR1->VAR49.VAR51++;\nif (!FUN29(VAR1)) {\nif (VAR1->VAR39 == VAR40)\nVAR18 = VAR41;\nbreak;\n}\nVAR9 = FUN34(&VAR5, VAR1, VAR4, true);\nFUN35(VAR1->VAR24, VAR21 + VAR9, VAR8 - VAR9, VAR1->VAR52,\n&VAR1->VAR53, VAR1->VAR54);\nif ((VAR6 || FUN36(VAR4)) &&\nFUN37(VAR10, VAR15) &&\n!FUN38(VAR1) &&\nFUN39(&VAR1->VAR55))\nVAR16 = VAR10 - VAR1->VAR15;\nif (VAR16 || VAR6)\nFUN40(VAR1, VAR46, 0, VAR6,\nVAR16, 0, 0, VAR3);\nVAR19 = FUN41(VAR1, VAR1, VAR14, VAR13, VAR5, VAR3,\n&VAR7, &VAR18);\nif (VAR13)\nVAR1->VAR49.VAR56++;\nif (VAR19 || VAR7)\nFUN42(VAR1, VAR19, VAR7);\nif (VAR19)\nFUN43(VAR1, VAR3);\nif (FUN44(!FUN39(&VAR1->VAR57)))\nFUN45(VAR1);\n}\nVAR23:\nFUN46(VAR2);\nreturn VAR18;\n}\n",
      "code_after_change_raw": "static int tipc_link_proto_rcv(struct tipc_link *l, struct sk_buff *skb,\nstruct sk_buff_head *xmitq)\n{\nstruct tipc_msg *hdr = buf_msg(skb);\nstruct tipc_gap_ack_blks *ga = NULL;\nbool reply = msg_probe(hdr), retransmitted = false;\nu32 dlen = msg_data_sz(hdr), glen = 0;\nu16 peers_snd_nxt =  msg_next_sent(hdr);\nu16 peers_tol = msg_link_tolerance(hdr);\nu16 peers_prio = msg_linkprio(hdr);\nu16 gap = msg_seq_gap(hdr);\nu16 ack = msg_ack(hdr);\nu16 rcv_nxt = l->rcv_nxt;\nu16 rcvgap = 0;\nint mtyp = msg_type(hdr);\nint rc = 0, released;\nchar *if_name;\nvoid *data;\ntrace_tipc_proto_rcv(skb, false, l->name);\nif (dlen > U16_MAX)\ngoto exit;\nif (tipc_link_is_blocked(l) || !xmitq)\ngoto exit;\nif (tipc_own_addr(l->net) > msg_prevnode(hdr))\nl->net_plane = msg_net_plane(hdr);\nskb_linearize(skb);\nhdr = buf_msg(skb);\ndata = msg_data(hdr);\nif (!tipc_link_validate_msg(l, hdr)) {\ntrace_tipc_skb_dump(skb, false, \"PROTO invalid (1)!\");\ntrace_tipc_link_dump(l, TIPC_DUMP_NONE, \"PROTO invalid (1)!\");\ngoto exit;\n}\nswitch (mtyp) {\ncase RESET_MSG:\ncase ACTIVATE_MSG:\nif_name =  strrchr(l->name, ':') + 1;\nif (sizeof(l->name) - (if_name - l->name) <= TIPC_MAX_IF_NAME)\nbreak;\nif (msg_data_sz(hdr) < TIPC_MAX_IF_NAME)\nbreak;\nstrncpy(if_name, data, TIPC_MAX_IF_NAME);\nif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\nl->tolerance = peers_tol;\nl->bc_rcvlink->tolerance = peers_tol;\n}\nif (in_range(peers_prio, l->priority + 1, TIPC_MAX_LINK_PRI))\nl->priority = peers_prio;\nif (msg_peer_stopping(hdr)) {\nrc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\nbreak;\n}\nif (mtyp == ACTIVATE_MSG && msg_dest_session_valid(hdr) &&\nl->session != msg_dest_session(hdr)) {\nif (less(l->session, msg_dest_session(hdr)))\nl->session = msg_dest_session(hdr) + 1;\nbreak;\n}\nif (mtyp == RESET_MSG || !link_is_up(l))\nrc = tipc_link_fsm_evt(l, LINK_PEER_RESET_EVT);\nif (mtyp == ACTIVATE_MSG && l->state == LINK_ESTABLISHING)\nrc = TIPC_LINK_UP_EVT;\nl->peer_session = msg_session(hdr);\nl->in_session = true;\nl->peer_bearer_id = msg_bearer_id(hdr);\nif (l->mtu > msg_max_pkt(hdr))\nl->mtu = msg_max_pkt(hdr);\nbreak;\ncase STATE_MSG:\nl->rcv_nxt_state = msg_seqno(hdr) + 1;\nif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\nl->tolerance = peers_tol;\nl->bc_rcvlink->tolerance = peers_tol;\n}\nif ((peers_prio != l->priority) &&\nin_range(peers_prio, 1, TIPC_MAX_LINK_PRI)) {\nl->priority = peers_prio;\nrc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\n}\nl->silent_intv_cnt = 0;\nl->stats.recv_states++;\nif (msg_probe(hdr))\nl->stats.recv_probes++;\nif (!link_is_up(l)) {\nif (l->state == LINK_ESTABLISHING)\nrc = TIPC_LINK_UP_EVT;\nbreak;\n}\nglen = tipc_get_gap_ack_blks(&ga, l, hdr, true);\nif(glen > dlen)\nbreak;\ntipc_mon_rcv(l->net, data + glen, dlen - glen, l->addr,\n&l->mon_state, l->bearer_id);\nif ((reply || msg_is_keepalive(hdr)) &&\nmore(peers_snd_nxt, rcv_nxt) &&\n!tipc_link_is_synching(l) &&\nskb_queue_empty(&l->deferdq))\nrcvgap = peers_snd_nxt - l->rcv_nxt;\nif (rcvgap || reply)\ntipc_link_build_proto_msg(l, STATE_MSG, 0, reply,\nrcvgap, 0, 0, xmitq);\nreleased = tipc_link_advance_transmq(l, l, ack, gap, ga, xmitq,\n&retransmitted, &rc);\nif (gap)\nl->stats.recv_nacks++;\nif (released || retransmitted)\ntipc_link_update_cwin(l, released, retransmitted);\nif (released)\ntipc_link_advance_backlog(l, xmitq);\nif (unlikely(!skb_queue_empty(&l->wakeupq)))\nlink_prepare_wakeup(l);\n}\nexit:\nkfree_skb(skb);\nreturn rc;\n}\n",
      "code_before_change_raw": "static int tipc_link_proto_rcv(struct tipc_link *l, struct sk_buff *skb,\nstruct sk_buff_head *xmitq)\n{\nstruct tipc_msg *hdr = buf_msg(skb);\nstruct tipc_gap_ack_blks *ga = NULL;\nbool reply = msg_probe(hdr), retransmitted = false;\nu16 dlen = msg_data_sz(hdr), glen = 0;\nu16 peers_snd_nxt =  msg_next_sent(hdr);\nu16 peers_tol = msg_link_tolerance(hdr);\nu16 peers_prio = msg_linkprio(hdr);\nu16 gap = msg_seq_gap(hdr);\nu16 ack = msg_ack(hdr);\nu16 rcv_nxt = l->rcv_nxt;\nu16 rcvgap = 0;\nint mtyp = msg_type(hdr);\nint rc = 0, released;\nchar *if_name;\nvoid *data;\ntrace_tipc_proto_rcv(skb, false, l->name);\nif (tipc_link_is_blocked(l) || !xmitq)\ngoto exit;\nif (tipc_own_addr(l->net) > msg_prevnode(hdr))\nl->net_plane = msg_net_plane(hdr);\nskb_linearize(skb);\nhdr = buf_msg(skb);\ndata = msg_data(hdr);\nif (!tipc_link_validate_msg(l, hdr)) {\ntrace_tipc_skb_dump(skb, false, \"PROTO invalid (1)!\");\ntrace_tipc_link_dump(l, TIPC_DUMP_NONE, \"PROTO invalid (1)!\");\ngoto exit;\n}\nswitch (mtyp) {\ncase RESET_MSG:\ncase ACTIVATE_MSG:\nif_name =  strrchr(l->name, ':') + 1;\nif (sizeof(l->name) - (if_name - l->name) <= TIPC_MAX_IF_NAME)\nbreak;\nif (msg_data_sz(hdr) < TIPC_MAX_IF_NAME)\nbreak;\nstrncpy(if_name, data, TIPC_MAX_IF_NAME);\nif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\nl->tolerance = peers_tol;\nl->bc_rcvlink->tolerance = peers_tol;\n}\nif (in_range(peers_prio, l->priority + 1, TIPC_MAX_LINK_PRI))\nl->priority = peers_prio;\nif (msg_peer_stopping(hdr)) {\nrc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\nbreak;\n}\nif (mtyp == ACTIVATE_MSG && msg_dest_session_valid(hdr) &&\nl->session != msg_dest_session(hdr)) {\nif (less(l->session, msg_dest_session(hdr)))\nl->session = msg_dest_session(hdr) + 1;\nbreak;\n}\nif (mtyp == RESET_MSG || !link_is_up(l))\nrc = tipc_link_fsm_evt(l, LINK_PEER_RESET_EVT);\nif (mtyp == ACTIVATE_MSG && l->state == LINK_ESTABLISHING)\nrc = TIPC_LINK_UP_EVT;\nl->peer_session = msg_session(hdr);\nl->in_session = true;\nl->peer_bearer_id = msg_bearer_id(hdr);\nif (l->mtu > msg_max_pkt(hdr))\nl->mtu = msg_max_pkt(hdr);\nbreak;\ncase STATE_MSG:\nl->rcv_nxt_state = msg_seqno(hdr) + 1;\nif (in_range(peers_tol, TIPC_MIN_LINK_TOL, TIPC_MAX_LINK_TOL)) {\nl->tolerance = peers_tol;\nl->bc_rcvlink->tolerance = peers_tol;\n}\nif ((peers_prio != l->priority) &&\nin_range(peers_prio, 1, TIPC_MAX_LINK_PRI)) {\nl->priority = peers_prio;\nrc = tipc_link_fsm_evt(l, LINK_FAILURE_EVT);\n}\nl->silent_intv_cnt = 0;\nl->stats.recv_states++;\nif (msg_probe(hdr))\nl->stats.recv_probes++;\nif (!link_is_up(l)) {\nif (l->state == LINK_ESTABLISHING)\nrc = TIPC_LINK_UP_EVT;\nbreak;\n}\nglen = tipc_get_gap_ack_blks(&ga, l, hdr, true);\ntipc_mon_rcv(l->net, data + glen, dlen - glen, l->addr,\n&l->mon_state, l->bearer_id);\nif ((reply || msg_is_keepalive(hdr)) &&\nmore(peers_snd_nxt, rcv_nxt) &&\n!tipc_link_is_synching(l) &&\nskb_queue_empty(&l->deferdq))\nrcvgap = peers_snd_nxt - l->rcv_nxt;\nif (rcvgap || reply)\ntipc_link_build_proto_msg(l, STATE_MSG, 0, reply,\nrcvgap, 0, 0, xmitq);\nreleased = tipc_link_advance_transmq(l, l, ack, gap, ga, xmitq,\n&retransmitted, &rc);\nif (gap)\nl->stats.recv_nacks++;\nif (released || retransmitted)\ntipc_link_update_cwin(l, released, retransmitted);\nif (released)\ntipc_link_advance_backlog(l, xmitq);\nif (unlikely(!skb_queue_empty(&l->wakeupq)))\nlink_prepare_wakeup(l);\n}\nexit:\nkfree_skb(skb);\nreturn rc;\n}\n"
    }
  ],
  "CVE-2022-0995": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of boundary checking for array access based on user input in the watch_queue_set_filter function.",
        "trigger_condition": "User-supplied data is used to access an array without proper validation, leading to potential out-of-bounds memory write.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the bounds of the user-supplied 'type' value before accessing the 'type_filter' array, potentially allowing an out-of-bounds memory write."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the 'type' value provided by the user is within the valid range before accessing the 'type_filter' array. By validating the 'type' value against a predefined constant and skipping processing if it exceeds the valid range, the code can prevent out-of-bounds memory writes and enhance security.",
      "GPT_analysis": "The modification is necessary to address the out-of-bounds (OOB) memory write vulnerability in the `watch_queue_set_filter` function. Specifically, the vulnerability arises from the unchecked access to the `wfilter->type_filter` array based on the `tf[i].type` value, which can lead to potential memory corruption and exploitation by a local user to gain privileged access or cause a denial of service on the system.\n\nBy adding the check `if (tf[i].type >= WATCH_TYPE__NR)` before accessing `wfilter->type_filter` and setting the correct value for `WATCH_TYPE__NR`, the modified code ensures that the access to the `wfilter->type_filter` array stays within bounds and prevents potential memory corruption.\n\nTherefore, the modification helps mitigate the vulnerability by ensuring that the array access is within the valid range, thereby enhancing the security and stability of the system.",
      "GPT_purpose": "This function is responsible for setting a filter for watch notifications in the Linux kernel's watch_queue event notification subsystem.",
      "GPT_function": "\n1. `watch_queue_set_filter`: Function to set a filter for watch notifications on a pipe.\n2. `memdup_user`: Function to duplicate a user space memory area.\n3. `kzalloc`: Function to allocate memory and initialize it to zero.\n4. `kfree`: Function to free previously allocated memory.\n5. `rcu_replace_pointer`: Function to atomically replace a pointer with another pointer under RCU protection.\n6. `kfree_rcu`: Function to free memory under RCU protection.",
      "CVE_id": "CVE-2022-0995",
      "code_before_change": "long watch_queue_set_filter(struct pipe_inode_info *pipe,\n\t\t\t    struct watch_notification_filter __user *_filter)\n{\n\tstruct watch_notification_type_filter *tf;\n\tstruct watch_notification_filter filter;\n\tstruct watch_type_filter *q;\n\tstruct watch_filter *wfilter;\n\tstruct watch_queue *wqueue = pipe->watch_queue;\n\tint ret, nr_filter = 0, i;\n\n\tif (!wqueue)\n\t\treturn -ENODEV;\n\n\tif (!_filter) {\n\t\t/* Remove the old filter */\n\t\twfilter = NULL;\n\t\tgoto set;\n\t}\n\n\t/* Grab the user's filter specification */\n\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)\n\t\treturn -EFAULT;\n\tif (filter.nr_filters == 0 ||\n\t    filter.nr_filters > 16 ||\n\t    filter.__reserved != 0)\n\t\treturn -EINVAL;\n\n\ttf = memdup_user(_filter->filters, filter.nr_filters * sizeof(*tf));\n\tif (IS_ERR(tf))\n\t\treturn PTR_ERR(tf);\n\n\tret = -EINVAL;\n\tfor (i = 0; i < filter.nr_filters; i++) {\n\t\tif ((tf[i].info_filter & ~tf[i].info_mask) ||\n\t\t    tf[i].info_mask & WATCH_INFO_LENGTH)\n\t\t\tgoto err_filter;\n\t\t/* Ignore any unknown types */\n\t\tif (tf[i].type >= sizeof(wfilter->type_filter) * 8)\n\t\t\tcontinue;\n\t\tnr_filter++;\n\t}\n\n\t/* Now we need to build the internal filter from only the relevant\n\t * user-specified filters.\n\t */\n\tret = -ENOMEM;\n\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);\n\tif (!wfilter)\n\t\tgoto err_filter;\n\twfilter->nr_filters = nr_filter;\n\n\tq = wfilter->filters;\n\tfor (i = 0; i < filter.nr_filters; i++) {\n\t\tif (tf[i].type >= sizeof(wfilter->type_filter) * BITS_PER_LONG)\n\t\t\tcontinue;\n\n\t\tq->type\t\t\t= tf[i].type;\n\t\tq->info_filter\t\t= tf[i].info_filter;\n\t\tq->info_mask\t\t= tf[i].info_mask;\n\t\tq->subtype_filter[0]\t= tf[i].subtype_filter[0];\n\t\t__set_bit(q->type, wfilter->type_filter);\n\t\tq++;\n\t}\n\n\tkfree(tf);\nset:\n\tpipe_lock(pipe);\n\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,\n\t\t\t\t      lockdep_is_held(&pipe->mutex));\n\tpipe_unlock(pipe);\n\tif (wfilter)\n\t\tkfree_rcu(wfilter, rcu);\n\treturn 0;\n\nerr_filter:\n\tkfree(tf);\n\treturn ret;\n}",
      "code_after_change": "long watch_queue_set_filter(struct pipe_inode_info *pipe,\n\t\t\t    struct watch_notification_filter __user *_filter)\n{\n\tstruct watch_notification_type_filter *tf;\n\tstruct watch_notification_filter filter;\n\tstruct watch_type_filter *q;\n\tstruct watch_filter *wfilter;\n\tstruct watch_queue *wqueue = pipe->watch_queue;\n\tint ret, nr_filter = 0, i;\n\n\tif (!wqueue)\n\t\treturn -ENODEV;\n\n\tif (!_filter) {\n\t\t/* Remove the old filter */\n\t\twfilter = NULL;\n\t\tgoto set;\n\t}\n\n\t/* Grab the user's filter specification */\n\tif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)\n\t\treturn -EFAULT;\n\tif (filter.nr_filters == 0 ||\n\t    filter.nr_filters > 16 ||\n\t    filter.__reserved != 0)\n\t\treturn -EINVAL;\n\n\ttf = memdup_user(_filter->filters, filter.nr_filters * sizeof(*tf));\n\tif (IS_ERR(tf))\n\t\treturn PTR_ERR(tf);\n\n\tret = -EINVAL;\n\tfor (i = 0; i < filter.nr_filters; i++) {\n\t\tif ((tf[i].info_filter & ~tf[i].info_mask) ||\n\t\t    tf[i].info_mask & WATCH_INFO_LENGTH)\n\t\t\tgoto err_filter;\n\t\t/* Ignore any unknown types */\n\t\tif (tf[i].type >= WATCH_TYPE__NR)\n\t\t\tcontinue;\n\t\tnr_filter++;\n\t}\n\n\t/* Now we need to build the internal filter from only the relevant\n\t * user-specified filters.\n\t */\n\tret = -ENOMEM;\n\twfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);\n\tif (!wfilter)\n\t\tgoto err_filter;\n\twfilter->nr_filters = nr_filter;\n\n\tq = wfilter->filters;\n\tfor (i = 0; i < filter.nr_filters; i++) {\n\t\tif (tf[i].type >= WATCH_TYPE__NR)\n\t\t\tcontinue;\n\n\t\tq->type\t\t\t= tf[i].type;\n\t\tq->info_filter\t\t= tf[i].info_filter;\n\t\tq->info_mask\t\t= tf[i].info_mask;\n\t\tq->subtype_filter[0]\t= tf[i].subtype_filter[0];\n\t\t__set_bit(q->type, wfilter->type_filter);\n\t\tq++;\n\t}\n\n\tkfree(tf);\nset:\n\tpipe_lock(pipe);\n\twfilter = rcu_replace_pointer(wqueue->filter, wfilter,\n\t\t\t\t      lockdep_is_held(&pipe->mutex));\n\tpipe_unlock(pipe);\n\tif (wfilter)\n\t\tkfree_rcu(wfilter, rcu);\n\treturn 0;\n\nerr_filter:\n\tkfree(tf);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\t\tif (tf[i].type >= WATCH_TYPE__NR)",
          "\t\tif (tf[i].type >= WATCH_TYPE__NR)"
        ],
        "deleted": [
          "\t\tif (tf[i].type >= sizeof(wfilter->type_filter) * 8)",
          "\t\tif (tf[i].type >= sizeof(wfilter->type_filter) * BITS_PER_LONG)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of boundary checking for array access based on user input in the watch_queue_set_filter function.",
      "trigger_condition": "User-supplied data is used to access an array without proper validation, leading to potential out-of-bounds memory write.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the bounds of the user-supplied 'type' value before accessing the 'type_filter' array, potentially allowing an out-of-bounds memory write.",
      "id": 128,
      "code_after_change_normalized": "long FUN1(struct pipe_inode_info *VAR1,\nstruct watch_notification_filter __user *VAR2)\n{\nstruct watch_notification_type_filter *VAR3;\nstruct watch_notification_filter VAR4;\nstruct watch_type_filter *VAR5;\nstruct watch_filter *VAR6;\nstruct VAR8 *VAR7 = VAR1->VAR8;\nint VAR9, VAR10 = 0, VAR11;\nif (!VAR7)\nreturn -VAR12;\nif (!VAR2) {\nVAR6 = NULL;\ngoto VAR13;\n}\nif (FUN2(&VAR4, VAR2, sizeof(VAR4)) != 0)\nreturn -VAR14;\nif (VAR4.VAR15 == 0 ||\nVAR4.VAR15 > 16 ||\nVAR4.VAR16 != 0)\nreturn -VAR17;\nVAR3 = FUN3(VAR2->VAR18, VAR4.VAR15 * sizeof(*VAR3));\nif (FUN4(VAR3))\nreturn FUN5(VAR3);\nVAR9 = -VAR17;\nfor (VAR11 = 0; VAR11 < VAR4.VAR15; VAR11++) {\nif ((VAR3[VAR11].VAR19 & ~VAR3[VAR11].VAR20) ||\nVAR3[VAR11].VAR20 & VAR21)\ngoto VAR22;\nif (VAR3[VAR11].VAR23 >= VAR24)\ncontinue;\nVAR10++;\n}\nVAR9 = -VAR25;\nVAR6 = FUN6(FUN7(VAR6, VAR18, VAR10), VAR26);\nif (!VAR6)\ngoto VAR22;\nVAR6->VAR15 = VAR10;\nVAR5 = VAR6->VAR18;\nfor (VAR11 = 0; VAR11 < VAR4.VAR15; VAR11++) {\nif (VAR3[VAR11].VAR23 >= VAR24)\ncontinue;\nVAR5->VAR23\t\t\t= VAR3[VAR11].VAR23;\nVAR5->VAR19\t\t= VAR3[VAR11].VAR19;\nVAR5->VAR20\t\t= VAR3[VAR11].VAR20;\nVAR5->VAR27[0]\t= VAR3[VAR11].VAR27[0];\nFUN8(VAR5->VAR23, VAR6->VAR28);\nVAR5++;\n}\nFUN9(VAR3);\nVAR13:\nFUN10(VAR1);\nVAR6 = FUN11(VAR7->VAR4, VAR6,\nFUN12(&VAR1->VAR29));\nFUN13(VAR1);\nif (VAR6)\nFUN14(VAR6, VAR30);\nreturn 0;\nVAR22:\nFUN9(VAR3);\nreturn VAR9;\n}\n",
      "code_before_change_normalized": "long FUN1(struct pipe_inode_info *VAR1,\nstruct watch_notification_filter __user *VAR2)\n{\nstruct watch_notification_type_filter *VAR3;\nstruct watch_notification_filter VAR4;\nstruct watch_type_filter *VAR5;\nstruct watch_filter *VAR6;\nstruct VAR8 *VAR7 = VAR1->VAR8;\nint VAR9, VAR10 = 0, VAR11;\nif (!VAR7)\nreturn -VAR12;\nif (!VAR2) {\nVAR6 = NULL;\ngoto VAR13;\n}\nif (FUN2(&VAR4, VAR2, sizeof(VAR4)) != 0)\nreturn -VAR14;\nif (VAR4.VAR15 == 0 ||\nVAR4.VAR15 > 16 ||\nVAR4.VAR16 != 0)\nreturn -VAR17;\nVAR3 = FUN3(VAR2->VAR18, VAR4.VAR15 * sizeof(*VAR3));\nif (FUN4(VAR3))\nreturn FUN5(VAR3);\nVAR9 = -VAR17;\nfor (VAR11 = 0; VAR11 < VAR4.VAR15; VAR11++) {\nif ((VAR3[VAR11].VAR19 & ~VAR3[VAR11].VAR20) ||\nVAR3[VAR11].VAR20 & VAR21)\ngoto VAR22;\nif (VAR3[VAR11].VAR23 >= sizeof(VAR6->VAR24) * 8)\ncontinue;\nVAR10++;\n}\nVAR9 = -VAR25;\nVAR6 = FUN6(FUN7(VAR6, VAR18, VAR10), VAR26);\nif (!VAR6)\ngoto VAR22;\nVAR6->VAR15 = VAR10;\nVAR5 = VAR6->VAR18;\nfor (VAR11 = 0; VAR11 < VAR4.VAR15; VAR11++) {\nif (VAR3[VAR11].VAR23 >= sizeof(VAR6->VAR24) * VAR27)\ncontinue;\nVAR5->VAR23\t\t\t= VAR3[VAR11].VAR23;\nVAR5->VAR19\t\t= VAR3[VAR11].VAR19;\nVAR5->VAR20\t\t= VAR3[VAR11].VAR20;\nVAR5->VAR28[0]\t= VAR3[VAR11].VAR28[0];\nFUN8(VAR5->VAR23, VAR6->VAR24);\nVAR5++;\n}\nFUN9(VAR3);\nVAR13:\nFUN10(VAR1);\nVAR6 = FUN11(VAR7->VAR4, VAR6,\nFUN12(&VAR1->VAR29));\nFUN13(VAR1);\nif (VAR6)\nFUN14(VAR6, VAR30);\nreturn 0;\nVAR22:\nFUN9(VAR3);\nreturn VAR9;\n}\n",
      "code_after_change_raw": "long watch_queue_set_filter(struct pipe_inode_info *pipe,\nstruct watch_notification_filter __user *_filter)\n{\nstruct watch_notification_type_filter *tf;\nstruct watch_notification_filter filter;\nstruct watch_type_filter *q;\nstruct watch_filter *wfilter;\nstruct watch_queue *wqueue = pipe->watch_queue;\nint ret, nr_filter = 0, i;\nif (!wqueue)\nreturn -ENODEV;\nif (!_filter) {\nwfilter = NULL;\ngoto set;\n}\nif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)\nreturn -EFAULT;\nif (filter.nr_filters == 0 ||\nfilter.nr_filters > 16 ||\nfilter.__reserved != 0)\nreturn -EINVAL;\ntf = memdup_user(_filter->filters, filter.nr_filters * sizeof(*tf));\nif (IS_ERR(tf))\nreturn PTR_ERR(tf);\nret = -EINVAL;\nfor (i = 0; i < filter.nr_filters; i++) {\nif ((tf[i].info_filter & ~tf[i].info_mask) ||\ntf[i].info_mask & WATCH_INFO_LENGTH)\ngoto err_filter;\nif (tf[i].type >= WATCH_TYPE__NR)\ncontinue;\nnr_filter++;\n}\nret = -ENOMEM;\nwfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);\nif (!wfilter)\ngoto err_filter;\nwfilter->nr_filters = nr_filter;\nq = wfilter->filters;\nfor (i = 0; i < filter.nr_filters; i++) {\nif (tf[i].type >= WATCH_TYPE__NR)\ncontinue;\nq->type\t\t\t= tf[i].type;\nq->info_filter\t\t= tf[i].info_filter;\nq->info_mask\t\t= tf[i].info_mask;\nq->subtype_filter[0]\t= tf[i].subtype_filter[0];\n__set_bit(q->type, wfilter->type_filter);\nq++;\n}\nkfree(tf);\nset:\npipe_lock(pipe);\nwfilter = rcu_replace_pointer(wqueue->filter, wfilter,\nlockdep_is_held(&pipe->mutex));\npipe_unlock(pipe);\nif (wfilter)\nkfree_rcu(wfilter, rcu);\nreturn 0;\nerr_filter:\nkfree(tf);\nreturn ret;\n}\n",
      "code_before_change_raw": "long watch_queue_set_filter(struct pipe_inode_info *pipe,\nstruct watch_notification_filter __user *_filter)\n{\nstruct watch_notification_type_filter *tf;\nstruct watch_notification_filter filter;\nstruct watch_type_filter *q;\nstruct watch_filter *wfilter;\nstruct watch_queue *wqueue = pipe->watch_queue;\nint ret, nr_filter = 0, i;\nif (!wqueue)\nreturn -ENODEV;\nif (!_filter) {\nwfilter = NULL;\ngoto set;\n}\nif (copy_from_user(&filter, _filter, sizeof(filter)) != 0)\nreturn -EFAULT;\nif (filter.nr_filters == 0 ||\nfilter.nr_filters > 16 ||\nfilter.__reserved != 0)\nreturn -EINVAL;\ntf = memdup_user(_filter->filters, filter.nr_filters * sizeof(*tf));\nif (IS_ERR(tf))\nreturn PTR_ERR(tf);\nret = -EINVAL;\nfor (i = 0; i < filter.nr_filters; i++) {\nif ((tf[i].info_filter & ~tf[i].info_mask) ||\ntf[i].info_mask & WATCH_INFO_LENGTH)\ngoto err_filter;\nif (tf[i].type >= sizeof(wfilter->type_filter) * 8)\ncontinue;\nnr_filter++;\n}\nret = -ENOMEM;\nwfilter = kzalloc(struct_size(wfilter, filters, nr_filter), GFP_KERNEL);\nif (!wfilter)\ngoto err_filter;\nwfilter->nr_filters = nr_filter;\nq = wfilter->filters;\nfor (i = 0; i < filter.nr_filters; i++) {\nif (tf[i].type >= sizeof(wfilter->type_filter) * BITS_PER_LONG)\ncontinue;\nq->type\t\t\t= tf[i].type;\nq->info_filter\t\t= tf[i].info_filter;\nq->info_mask\t\t= tf[i].info_mask;\nq->subtype_filter[0]\t= tf[i].subtype_filter[0];\n__set_bit(q->type, wfilter->type_filter);\nq++;\n}\nkfree(tf);\nset:\npipe_lock(pipe);\nwfilter = rcu_replace_pointer(wqueue->filter, wfilter,\nlockdep_is_held(&pipe->mutex));\npipe_unlock(pipe);\nif (wfilter)\nkfree_rcu(wfilter, rcu);\nreturn 0;\nerr_filter:\nkfree(tf);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2022-1015": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of passing a reference to a variable in a function call, leading to potential out-of-bounds write issues.",
        "trigger_condition": "The function is called without passing a reference to a variable, causing unintended memory writes and potential vulnerabilities.",
        "specific_code_behavior_causing_vulnerability": "The original code snippet calls a function without passing a reference to a variable, which can result in out-of-bounds write issues due to improper handling of memory access.",
        "solution": "To mitigate the vulnerability, it is necessary to modify the code to pass a reference to the variable in the function call. This ensures that the function can correctly store the parsed value in the variable and prevents potential out-of-bounds write vulnerabilities. In this case, the solution involves modifying the code to pass a reference to the variable in the function call to address the vulnerability."
      },
      "GPT_analysis": "The modification is necessary to fix the vulnerability CVE-2022-1015, which involves an out-of-bounds write issue in the original code snippet. The vulnerability arises from the fact that the function `nft_parse_register` is called without passing a reference to the `reg` variable, which can lead to unintended memory writes.\n\nBy modifying the code to pass the reference to the `reg` variable in the `nft_parse_register` function call, we ensure that the function can correctly store the parsed register value in the `reg` variable. This modification helps prevent potential out-of-bounds writes and improves the overall security of the code.",
      "GPT_purpose": "Parse and validate a register load operation in the nf_tables API.",
      "GPT_function": "\n1. Parse a register from a netlink attribute.\n2. Validate the register load operation.\n3. Assign the parsed register value to a specified memory location.",
      "CVE_id": "CVE-2022-1015",
      "code_before_change": "int nft_parse_register_load(const struct nlattr *attr, u8 *sreg, u32 len)\n{\n\tu32 reg;\n\tint err;\n\n\treg = nft_parse_register(attr);\n\terr = nft_validate_register_load(reg, len);\n\tif (err < 0)\n\t\treturn err;\n\n\t*sreg = reg;\n\treturn 0;\n}",
      "code_after_change": "int nft_parse_register_load(const struct nlattr *attr, u8 *sreg, u32 len)\n{\n\tu32 reg;\n\tint err;\n\n\terr = nft_parse_register(attr, &reg);\n\tif (err < 0)\n\t\treturn err;\n\n\terr = nft_validate_register_load(reg, len);\n\tif (err < 0)\n\t\treturn err;\n\n\t*sreg = reg;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\terr = nft_parse_register(attr, &reg);",
          "\tif (err < 0)",
          "\t\treturn err;",
          ""
        ],
        "deleted": [
          "\treg = nft_parse_register(attr);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of passing a reference to a variable in a function call, leading to potential out-of-bounds write issues.",
      "trigger_condition": "The function is called without passing a reference to a variable, causing unintended memory writes and potential vulnerabilities.",
      "specific_code_behavior_causing_vulnerability": "The original code snippet calls a function without passing a reference to a variable, which can result in out-of-bounds write issues due to improper handling of memory access.",
      "solution": "To mitigate the vulnerability, it is necessary to modify the code to pass a reference to the variable in the function call. This ensures that the function can correctly store the parsed value in the variable and prevents potential out-of-bounds write vulnerabilities. In this case, the solution involves modifying the code to pass a reference to the variable in the function call to address the vulnerability.",
      "id": 129,
      "code_after_change_normalized": "int FUN1(const struct nlattr *VAR1, u8 *VAR2, u32 VAR3)\n{\nu32 VAR4;\nint VAR5;\nVAR5 = FUN2(VAR1, &VAR4);\nif (VAR5 < 0)\nreturn VAR5;\nVAR5 = FUN3(VAR4, VAR3);\nif (VAR5 < 0)\nreturn VAR5;\n*VAR2 = VAR4;\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(const struct nlattr *VAR1, u8 *VAR2, u32 VAR3)\n{\nu32 VAR4;\nint VAR5;\nVAR4 = FUN2(VAR1);\nVAR5 = FUN3(VAR4, VAR3);\nif (VAR5 < 0)\nreturn VAR5;\n*VAR2 = VAR4;\nreturn 0;\n}\n",
      "code_after_change_raw": "int nft_parse_register_load(const struct nlattr *attr, u8 *sreg, u32 len)\n{\nu32 reg;\nint err;\nerr = nft_parse_register(attr, &reg);\nif (err < 0)\nreturn err;\nerr = nft_validate_register_load(reg, len);\nif (err < 0)\nreturn err;\n*sreg = reg;\nreturn 0;\n}\n",
      "code_before_change_raw": "int nft_parse_register_load(const struct nlattr *attr, u8 *sreg, u32 len)\n{\nu32 reg;\nint err;\nreg = nft_parse_register(attr);\nerr = nft_validate_register_load(reg, len);\nif (err < 0)\nreturn err;\n*sreg = reg;\nreturn 0;\n}\n"
    }
  ],
  "CVE-2022-1943": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper memory access due to incorrect offset calculations and buffer handling in the code.",
        "trigger_condition": "When the offset calculation results in a negative value, memory is accessed beyond the allocated buffer boundaries, leading to a buffer overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code performs memory copy operations based on the offset value without proper boundary checks, potentially causing out-of-bounds memory writes and buffer overflows."
      },
      "solution": "To mitigate the vulnerability, it is necessary to adjust the offset calculations and ensure that memory copy operations are performed within the allocated buffer boundaries. By using the correct buffer pointers and adjusting the offset values, the code can prevent out-of-bounds memory writes and buffer overflows. The solution involves modifying the code to handle offset calculations correctly and perform boundary checks to ensure memory accesses stay within the allocated buffer space.",
      "GPT_analysis": "The above modification is necessary to fix the out-of-bounds memory write vulnerability in the `udf_write_fi` function. In the original code snippet, there are several instances where memory is accessed beyond its allocated boundaries, which can lead to a buffer overflow vulnerability.\n\nThe vulnerability occurs when copying data into the `sfi->impUse` and `fibh->ebh->b_data` buffers based on the `offset` value. If the `offset` calculation results in a negative value, it can lead to accessing memory outside the allocated buffer space, causing a buffer overflow.\n\nThe modification addresses this issue by ensuring that the memory copy operations are performed within the allocated buffer boundaries. By adjusting the offset calculations and using the correct buffer pointers (`sfi->impUse` and `fibh->ebh->b_data`), the code is made safer and less prone to buffer overflow vulnerabilities.\n\nOverall, the modification helps prevent potential crashes or security risks that could be exploited by a local user to compromise system integrity.",
      "GPT_purpose": "Write file identification information to a UDF filesystem structure.",
      "GPT_function": "\n1. Write file identification information to a UDF file system.\n2. Calculate CRC values for data integrity.\n3. Handle padding and copying of data within the file system.",
      "CVE_id": "CVE-2022-1943",
      "code_before_change": "int udf_write_fi(struct inode *inode, struct fileIdentDesc *cfi,\n\t\t struct fileIdentDesc *sfi, struct udf_fileident_bh *fibh,\n\t\t uint8_t *impuse, uint8_t *fileident)\n{\n\tuint16_t crclen = fibh->eoffset - fibh->soffset - sizeof(struct tag);\n\tuint16_t crc;\n\tint offset;\n\tuint16_t liu = le16_to_cpu(cfi->lengthOfImpUse);\n\tuint8_t lfi = cfi->lengthFileIdent;\n\tint padlen = fibh->eoffset - fibh->soffset - liu - lfi -\n\t\tsizeof(struct fileIdentDesc);\n\tint adinicb = 0;\n\n\tif (UDF_I(inode)->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)\n\t\tadinicb = 1;\n\n\toffset = fibh->soffset + sizeof(struct fileIdentDesc);\n\n\tif (impuse) {\n\t\tif (adinicb || (offset + liu < 0)) {\n\t\t\tmemcpy((uint8_t *)sfi->impUse, impuse, liu);\n\t\t} else if (offset >= 0) {\n\t\t\tmemcpy(fibh->ebh->b_data + offset, impuse, liu);\n\t\t} else {\n\t\t\tmemcpy((uint8_t *)sfi->impUse, impuse, -offset);\n\t\t\tmemcpy(fibh->ebh->b_data, impuse - offset,\n\t\t\t\tliu + offset);\n\t\t}\n\t}\n\n\toffset += liu;\n\n\tif (fileident) {\n\t\tif (adinicb || (offset + lfi < 0)) {\n\t\t\tmemcpy(udf_get_fi_ident(sfi), fileident, lfi);\n\t\t} else if (offset >= 0) {\n\t\t\tmemcpy(fibh->ebh->b_data + offset, fileident, lfi);\n\t\t} else {\n\t\t\tmemcpy(udf_get_fi_ident(sfi), fileident, -offset);\n\t\t\tmemcpy(fibh->ebh->b_data, fileident - offset,\n\t\t\t\tlfi + offset);\n\t\t}\n\t}\n\n\toffset += lfi;\n\n\tif (adinicb || (offset + padlen < 0)) {\n\t\tmemset(udf_get_fi_ident(sfi) + lfi, 0x00, padlen);\n\t} else if (offset >= 0) {\n\t\tmemset(fibh->ebh->b_data + offset, 0x00, padlen);\n\t} else {\n\t\tmemset(udf_get_fi_ident(sfi) + lfi, 0x00, -offset);\n\t\tmemset(fibh->ebh->b_data, 0x00, padlen + offset);\n\t}\n\n\tcrc = crc_itu_t(0, (uint8_t *)cfi + sizeof(struct tag),\n\t\t      sizeof(struct fileIdentDesc) - sizeof(struct tag));\n\n\tif (fibh->sbh == fibh->ebh) {\n\t\tcrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\n\t\t\t      crclen + sizeof(struct tag) -\n\t\t\t      sizeof(struct fileIdentDesc));\n\t} else if (sizeof(struct fileIdentDesc) >= -fibh->soffset) {\n\t\tcrc = crc_itu_t(crc, fibh->ebh->b_data +\n\t\t\t\t\tsizeof(struct fileIdentDesc) +\n\t\t\t\t\tfibh->soffset,\n\t\t\t      crclen + sizeof(struct tag) -\n\t\t\t\t\tsizeof(struct fileIdentDesc));\n\t} else {\n\t\tcrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\n\t\t\t      -fibh->soffset - sizeof(struct fileIdentDesc));\n\t\tcrc = crc_itu_t(crc, fibh->ebh->b_data, fibh->eoffset);\n\t}\n\n\tcfi->descTag.descCRC = cpu_to_le16(crc);\n\tcfi->descTag.descCRCLength = cpu_to_le16(crclen);\n\tcfi->descTag.tagChecksum = udf_tag_checksum(&cfi->descTag);\n\n\tif (adinicb || (sizeof(struct fileIdentDesc) <= -fibh->soffset)) {\n\t\tmemcpy((uint8_t *)sfi, (uint8_t *)cfi,\n\t\t\tsizeof(struct fileIdentDesc));\n\t} else {\n\t\tmemcpy((uint8_t *)sfi, (uint8_t *)cfi, -fibh->soffset);\n\t\tmemcpy(fibh->ebh->b_data, (uint8_t *)cfi - fibh->soffset,\n\t\t       sizeof(struct fileIdentDesc) + fibh->soffset);\n\t}\n\n\tif (adinicb) {\n\t\tmark_inode_dirty(inode);\n\t} else {\n\t\tif (fibh->sbh != fibh->ebh)\n\t\t\tmark_buffer_dirty_inode(fibh->ebh, inode);\n\t\tmark_buffer_dirty_inode(fibh->sbh, inode);\n\t}\n\tinode_inc_iversion(inode);\n\n\treturn 0;\n}",
      "code_after_change": "int udf_write_fi(struct inode *inode, struct fileIdentDesc *cfi,\n\t\t struct fileIdentDesc *sfi, struct udf_fileident_bh *fibh,\n\t\t uint8_t *impuse, uint8_t *fileident)\n{\n\tuint16_t crclen = fibh->eoffset - fibh->soffset - sizeof(struct tag);\n\tuint16_t crc;\n\tint offset;\n\tuint16_t liu = le16_to_cpu(cfi->lengthOfImpUse);\n\tuint8_t lfi = cfi->lengthFileIdent;\n\tint padlen = fibh->eoffset - fibh->soffset - liu - lfi -\n\t\tsizeof(struct fileIdentDesc);\n\tint adinicb = 0;\n\n\tif (UDF_I(inode)->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)\n\t\tadinicb = 1;\n\n\toffset = fibh->soffset + sizeof(struct fileIdentDesc);\n\n\tif (impuse) {\n\t\tif (adinicb || (offset + liu < 0)) {\n\t\t\tmemcpy((uint8_t *)sfi->impUse, impuse, liu);\n\t\t} else if (offset >= 0) {\n\t\t\tmemcpy(fibh->ebh->b_data + offset, impuse, liu);\n\t\t} else {\n\t\t\tmemcpy((uint8_t *)sfi->impUse, impuse, -offset);\n\t\t\tmemcpy(fibh->ebh->b_data, impuse - offset,\n\t\t\t\tliu + offset);\n\t\t}\n\t}\n\n\toffset += liu;\n\n\tif (fileident) {\n\t\tif (adinicb || (offset + lfi < 0)) {\n\t\t\tmemcpy(sfi->impUse + liu, fileident, lfi);\n\t\t} else if (offset >= 0) {\n\t\t\tmemcpy(fibh->ebh->b_data + offset, fileident, lfi);\n\t\t} else {\n\t\t\tmemcpy(sfi->impUse + liu, fileident, -offset);\n\t\t\tmemcpy(fibh->ebh->b_data, fileident - offset,\n\t\t\t\tlfi + offset);\n\t\t}\n\t}\n\n\toffset += lfi;\n\n\tif (adinicb || (offset + padlen < 0)) {\n\t\tmemset(sfi->impUse + liu + lfi, 0x00, padlen);\n\t} else if (offset >= 0) {\n\t\tmemset(fibh->ebh->b_data + offset, 0x00, padlen);\n\t} else {\n\t\tmemset(sfi->impUse + liu + lfi, 0x00, -offset);\n\t\tmemset(fibh->ebh->b_data, 0x00, padlen + offset);\n\t}\n\n\tcrc = crc_itu_t(0, (uint8_t *)cfi + sizeof(struct tag),\n\t\t      sizeof(struct fileIdentDesc) - sizeof(struct tag));\n\n\tif (fibh->sbh == fibh->ebh) {\n\t\tcrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\n\t\t\t      crclen + sizeof(struct tag) -\n\t\t\t      sizeof(struct fileIdentDesc));\n\t} else if (sizeof(struct fileIdentDesc) >= -fibh->soffset) {\n\t\tcrc = crc_itu_t(crc, fibh->ebh->b_data +\n\t\t\t\t\tsizeof(struct fileIdentDesc) +\n\t\t\t\t\tfibh->soffset,\n\t\t\t      crclen + sizeof(struct tag) -\n\t\t\t\t\tsizeof(struct fileIdentDesc));\n\t} else {\n\t\tcrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\n\t\t\t      -fibh->soffset - sizeof(struct fileIdentDesc));\n\t\tcrc = crc_itu_t(crc, fibh->ebh->b_data, fibh->eoffset);\n\t}\n\n\tcfi->descTag.descCRC = cpu_to_le16(crc);\n\tcfi->descTag.descCRCLength = cpu_to_le16(crclen);\n\tcfi->descTag.tagChecksum = udf_tag_checksum(&cfi->descTag);\n\n\tif (adinicb || (sizeof(struct fileIdentDesc) <= -fibh->soffset)) {\n\t\tmemcpy((uint8_t *)sfi, (uint8_t *)cfi,\n\t\t\tsizeof(struct fileIdentDesc));\n\t} else {\n\t\tmemcpy((uint8_t *)sfi, (uint8_t *)cfi, -fibh->soffset);\n\t\tmemcpy(fibh->ebh->b_data, (uint8_t *)cfi - fibh->soffset,\n\t\t       sizeof(struct fileIdentDesc) + fibh->soffset);\n\t}\n\n\tif (adinicb) {\n\t\tmark_inode_dirty(inode);\n\t} else {\n\t\tif (fibh->sbh != fibh->ebh)\n\t\t\tmark_buffer_dirty_inode(fibh->ebh, inode);\n\t\tmark_buffer_dirty_inode(fibh->sbh, inode);\n\t}\n\tinode_inc_iversion(inode);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tmemcpy(sfi->impUse + liu, fileident, lfi);",
          "\t\t\tmemcpy(sfi->impUse + liu, fileident, -offset);",
          "\t\tmemset(sfi->impUse + liu + lfi, 0x00, padlen);",
          "\t\tmemset(sfi->impUse + liu + lfi, 0x00, -offset);"
        ],
        "deleted": [
          "\t\t\tmemcpy(udf_get_fi_ident(sfi), fileident, lfi);",
          "\t\t\tmemcpy(udf_get_fi_ident(sfi), fileident, -offset);",
          "\t\tmemset(udf_get_fi_ident(sfi) + lfi, 0x00, padlen);",
          "\t\tmemset(udf_get_fi_ident(sfi) + lfi, 0x00, -offset);"
        ]
      },
      "preconditions_for_vulnerability": "Improper memory access due to incorrect offset calculations and buffer handling in the code.",
      "trigger_condition": "When the offset calculation results in a negative value, memory is accessed beyond the allocated buffer boundaries, leading to a buffer overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code performs memory copy operations based on the offset value without proper boundary checks, potentially causing out-of-bounds memory writes and buffer overflows.",
      "id": 130,
      "code_after_change_normalized": "int FUN1(struct VAR1 *VAR1, struct fileIdentDesc *VAR2,\nstruct fileIdentDesc *VAR3, struct udf_fileident_bh *VAR4,\nuint8_t *VAR5, uint8_t *VAR6)\n{\nuint16_t VAR7 = VAR4->VAR8 - VAR4->VAR9 - sizeof(struct VAR10);\nuint16_t VAR11;\nint VAR12;\nuint16_t VAR13 = FUN2(VAR2->VAR14);\nuint8_t VAR15 = VAR2->VAR16;\nint VAR17 = VAR4->VAR8 - VAR4->VAR9 - VAR13 - VAR15 -\nsizeof(struct VAR18);\nint VAR19 = 0;\nif (FUN3(VAR1)->VAR20 == VAR21)\nVAR19 = 1;\nVAR12 = VAR4->VAR9 + sizeof(struct VAR18);\nif (VAR5) {\nif (VAR19 || (VAR12 + VAR13 < 0)) {\nFUN4((VAR22 *)VAR3->VAR23, VAR5, VAR13);\n} else if (VAR12 >= 0) {\nFUN4(VAR4->VAR24->VAR25 + VAR12, VAR5, VAR13);\n} else {\nFUN4((VAR22 *)VAR3->VAR23, VAR5, -VAR12);\nFUN4(VAR4->VAR24->VAR25, VAR5 - VAR12,\nVAR13 + VAR12);\n}\n}\nVAR12 += VAR13;\nif (VAR6) {\nif (VAR19 || (VAR12 + VAR15 < 0)) {\nFUN4(VAR3->VAR23 + VAR13, VAR6, VAR15);\n} else if (VAR12 >= 0) {\nFUN4(VAR4->VAR24->VAR25 + VAR12, VAR6, VAR15);\n} else {\nFUN4(VAR3->VAR23 + VAR13, VAR6, -VAR12);\nFUN4(VAR4->VAR24->VAR25, VAR6 - VAR12,\nVAR15 + VAR12);\n}\n}\nVAR12 += VAR15;\nif (VAR19 || (VAR12 + VAR17 < 0)) {\nFUN5(VAR3->VAR23 + VAR13 + VAR15, VAR26, VAR17);\n} else if (VAR12 >= 0) {\nFUN5(VAR4->VAR24->VAR25 + VAR12, VAR26, VAR17);\n} else {\nFUN5(VAR3->VAR23 + VAR13 + VAR15, VAR26, -VAR12);\nFUN5(VAR4->VAR24->VAR25, VAR26, VAR17 + VAR12);\n}\nVAR11 = FUN6(0, (VAR22 *)VAR2 + sizeof(struct VAR10),\nsizeof(struct VAR18) - sizeof(struct VAR10));\nif (VAR4->VAR27 == VAR4->VAR24) {\nVAR11 = FUN6(VAR11, (VAR22 *)VAR3->VAR23,\nVAR7 + sizeof(struct VAR10) -\nsizeof(struct VAR18));\n} else if (sizeof(struct VAR18) >= -VAR4->VAR9) {\nVAR11 = FUN6(VAR11, VAR4->VAR24->VAR25 +\nsizeof(struct VAR18) +\nVAR4->VAR9,\nVAR7 + sizeof(struct VAR10) -\nsizeof(struct VAR18));\n} else {\nVAR11 = FUN6(VAR11, (VAR22 *)VAR3->VAR23,\n-VAR4->VAR9 - sizeof(struct VAR18));\nVAR11 = FUN6(VAR11, VAR4->VAR24->VAR25, VAR4->VAR8);\n}\nVAR2->VAR28.VAR29 = FUN7(VAR11);\nVAR2->VAR28.VAR30 = FUN7(VAR7);\nVAR2->VAR28.VAR31 = FUN8(&VAR2->VAR28);\nif (VAR19 || (sizeof(struct VAR18) <= -VAR4->VAR9)) {\nFUN4((VAR22 *)VAR3, (VAR22 *)VAR2,\nsizeof(struct VAR18));\n} else {\nFUN4((VAR22 *)VAR3, (VAR22 *)VAR2, -VAR4->VAR9);\nFUN4(VAR4->VAR24->VAR25, (VAR22 *)VAR2 - VAR4->VAR9,\nsizeof(struct VAR18) + VAR4->VAR9);\n}\nif (VAR19) {\nFUN9(VAR1);\n} else {\nif (VAR4->VAR27 != VAR4->VAR24)\nFUN10(VAR4->VAR24, VAR1);\nFUN10(VAR4->VAR27, VAR1);\n}\nFUN11(VAR1);\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct VAR1 *VAR1, struct fileIdentDesc *VAR2,\nstruct fileIdentDesc *VAR3, struct udf_fileident_bh *VAR4,\nuint8_t *VAR5, uint8_t *VAR6)\n{\nuint16_t VAR7 = VAR4->VAR8 - VAR4->VAR9 - sizeof(struct VAR10);\nuint16_t VAR11;\nint VAR12;\nuint16_t VAR13 = FUN2(VAR2->VAR14);\nuint8_t VAR15 = VAR2->VAR16;\nint VAR17 = VAR4->VAR8 - VAR4->VAR9 - VAR13 - VAR15 -\nsizeof(struct VAR18);\nint VAR19 = 0;\nif (FUN3(VAR1)->VAR20 == VAR21)\nVAR19 = 1;\nVAR12 = VAR4->VAR9 + sizeof(struct VAR18);\nif (VAR5) {\nif (VAR19 || (VAR12 + VAR13 < 0)) {\nFUN4((VAR22 *)VAR3->VAR23, VAR5, VAR13);\n} else if (VAR12 >= 0) {\nFUN4(VAR4->VAR24->VAR25 + VAR12, VAR5, VAR13);\n} else {\nFUN4((VAR22 *)VAR3->VAR23, VAR5, -VAR12);\nFUN4(VAR4->VAR24->VAR25, VAR5 - VAR12,\nVAR13 + VAR12);\n}\n}\nVAR12 += VAR13;\nif (VAR6) {\nif (VAR19 || (VAR12 + VAR15 < 0)) {\nFUN4(FUN5(VAR3), VAR6, VAR15);\n} else if (VAR12 >= 0) {\nFUN4(VAR4->VAR24->VAR25 + VAR12, VAR6, VAR15);\n} else {\nFUN4(FUN5(VAR3), VAR6, -VAR12);\nFUN4(VAR4->VAR24->VAR25, VAR6 - VAR12,\nVAR15 + VAR12);\n}\n}\nVAR12 += VAR15;\nif (VAR19 || (VAR12 + VAR17 < 0)) {\nFUN6(FUN5(VAR3) + VAR15, VAR26, VAR17);\n} else if (VAR12 >= 0) {\nFUN6(VAR4->VAR24->VAR25 + VAR12, VAR26, VAR17);\n} else {\nFUN6(FUN5(VAR3) + VAR15, VAR26, -VAR12);\nFUN6(VAR4->VAR24->VAR25, VAR26, VAR17 + VAR12);\n}\nVAR11 = FUN7(0, (VAR22 *)VAR2 + sizeof(struct VAR10),\nsizeof(struct VAR18) - sizeof(struct VAR10));\nif (VAR4->VAR27 == VAR4->VAR24) {\nVAR11 = FUN7(VAR11, (VAR22 *)VAR3->VAR23,\nVAR7 + sizeof(struct VAR10) -\nsizeof(struct VAR18));\n} else if (sizeof(struct VAR18) >= -VAR4->VAR9) {\nVAR11 = FUN7(VAR11, VAR4->VAR24->VAR25 +\nsizeof(struct VAR18) +\nVAR4->VAR9,\nVAR7 + sizeof(struct VAR10) -\nsizeof(struct VAR18));\n} else {\nVAR11 = FUN7(VAR11, (VAR22 *)VAR3->VAR23,\n-VAR4->VAR9 - sizeof(struct VAR18));\nVAR11 = FUN7(VAR11, VAR4->VAR24->VAR25, VAR4->VAR8);\n}\nVAR2->VAR28.VAR29 = FUN8(VAR11);\nVAR2->VAR28.VAR30 = FUN8(VAR7);\nVAR2->VAR28.VAR31 = FUN9(&VAR2->VAR28);\nif (VAR19 || (sizeof(struct VAR18) <= -VAR4->VAR9)) {\nFUN4((VAR22 *)VAR3, (VAR22 *)VAR2,\nsizeof(struct VAR18));\n} else {\nFUN4((VAR22 *)VAR3, (VAR22 *)VAR2, -VAR4->VAR9);\nFUN4(VAR4->VAR24->VAR25, (VAR22 *)VAR2 - VAR4->VAR9,\nsizeof(struct VAR18) + VAR4->VAR9);\n}\nif (VAR19) {\nFUN10(VAR1);\n} else {\nif (VAR4->VAR27 != VAR4->VAR24)\nFUN11(VAR4->VAR24, VAR1);\nFUN11(VAR4->VAR27, VAR1);\n}\nFUN12(VAR1);\nreturn 0;\n}\n",
      "code_after_change_raw": "int udf_write_fi(struct inode *inode, struct fileIdentDesc *cfi,\nstruct fileIdentDesc *sfi, struct udf_fileident_bh *fibh,\nuint8_t *impuse, uint8_t *fileident)\n{\nuint16_t crclen = fibh->eoffset - fibh->soffset - sizeof(struct tag);\nuint16_t crc;\nint offset;\nuint16_t liu = le16_to_cpu(cfi->lengthOfImpUse);\nuint8_t lfi = cfi->lengthFileIdent;\nint padlen = fibh->eoffset - fibh->soffset - liu - lfi -\nsizeof(struct fileIdentDesc);\nint adinicb = 0;\nif (UDF_I(inode)->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)\nadinicb = 1;\noffset = fibh->soffset + sizeof(struct fileIdentDesc);\nif (impuse) {\nif (adinicb || (offset + liu < 0)) {\nmemcpy((uint8_t *)sfi->impUse, impuse, liu);\n} else if (offset >= 0) {\nmemcpy(fibh->ebh->b_data + offset, impuse, liu);\n} else {\nmemcpy((uint8_t *)sfi->impUse, impuse, -offset);\nmemcpy(fibh->ebh->b_data, impuse - offset,\nliu + offset);\n}\n}\noffset += liu;\nif (fileident) {\nif (adinicb || (offset + lfi < 0)) {\nmemcpy(sfi->impUse + liu, fileident, lfi);\n} else if (offset >= 0) {\nmemcpy(fibh->ebh->b_data + offset, fileident, lfi);\n} else {\nmemcpy(sfi->impUse + liu, fileident, -offset);\nmemcpy(fibh->ebh->b_data, fileident - offset,\nlfi + offset);\n}\n}\noffset += lfi;\nif (adinicb || (offset + padlen < 0)) {\nmemset(sfi->impUse + liu + lfi, 0x00, padlen);\n} else if (offset >= 0) {\nmemset(fibh->ebh->b_data + offset, 0x00, padlen);\n} else {\nmemset(sfi->impUse + liu + lfi, 0x00, -offset);\nmemset(fibh->ebh->b_data, 0x00, padlen + offset);\n}\ncrc = crc_itu_t(0, (uint8_t *)cfi + sizeof(struct tag),\nsizeof(struct fileIdentDesc) - sizeof(struct tag));\nif (fibh->sbh == fibh->ebh) {\ncrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\ncrclen + sizeof(struct tag) -\nsizeof(struct fileIdentDesc));\n} else if (sizeof(struct fileIdentDesc) >= -fibh->soffset) {\ncrc = crc_itu_t(crc, fibh->ebh->b_data +\nsizeof(struct fileIdentDesc) +\nfibh->soffset,\ncrclen + sizeof(struct tag) -\nsizeof(struct fileIdentDesc));\n} else {\ncrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\n-fibh->soffset - sizeof(struct fileIdentDesc));\ncrc = crc_itu_t(crc, fibh->ebh->b_data, fibh->eoffset);\n}\ncfi->descTag.descCRC = cpu_to_le16(crc);\ncfi->descTag.descCRCLength = cpu_to_le16(crclen);\ncfi->descTag.tagChecksum = udf_tag_checksum(&cfi->descTag);\nif (adinicb || (sizeof(struct fileIdentDesc) <= -fibh->soffset)) {\nmemcpy((uint8_t *)sfi, (uint8_t *)cfi,\nsizeof(struct fileIdentDesc));\n} else {\nmemcpy((uint8_t *)sfi, (uint8_t *)cfi, -fibh->soffset);\nmemcpy(fibh->ebh->b_data, (uint8_t *)cfi - fibh->soffset,\nsizeof(struct fileIdentDesc) + fibh->soffset);\n}\nif (adinicb) {\nmark_inode_dirty(inode);\n} else {\nif (fibh->sbh != fibh->ebh)\nmark_buffer_dirty_inode(fibh->ebh, inode);\nmark_buffer_dirty_inode(fibh->sbh, inode);\n}\ninode_inc_iversion(inode);\nreturn 0;\n}\n",
      "code_before_change_raw": "int udf_write_fi(struct inode *inode, struct fileIdentDesc *cfi,\nstruct fileIdentDesc *sfi, struct udf_fileident_bh *fibh,\nuint8_t *impuse, uint8_t *fileident)\n{\nuint16_t crclen = fibh->eoffset - fibh->soffset - sizeof(struct tag);\nuint16_t crc;\nint offset;\nuint16_t liu = le16_to_cpu(cfi->lengthOfImpUse);\nuint8_t lfi = cfi->lengthFileIdent;\nint padlen = fibh->eoffset - fibh->soffset - liu - lfi -\nsizeof(struct fileIdentDesc);\nint adinicb = 0;\nif (UDF_I(inode)->i_alloc_type == ICBTAG_FLAG_AD_IN_ICB)\nadinicb = 1;\noffset = fibh->soffset + sizeof(struct fileIdentDesc);\nif (impuse) {\nif (adinicb || (offset + liu < 0)) {\nmemcpy((uint8_t *)sfi->impUse, impuse, liu);\n} else if (offset >= 0) {\nmemcpy(fibh->ebh->b_data + offset, impuse, liu);\n} else {\nmemcpy((uint8_t *)sfi->impUse, impuse, -offset);\nmemcpy(fibh->ebh->b_data, impuse - offset,\nliu + offset);\n}\n}\noffset += liu;\nif (fileident) {\nif (adinicb || (offset + lfi < 0)) {\nmemcpy(udf_get_fi_ident(sfi), fileident, lfi);\n} else if (offset >= 0) {\nmemcpy(fibh->ebh->b_data + offset, fileident, lfi);\n} else {\nmemcpy(udf_get_fi_ident(sfi), fileident, -offset);\nmemcpy(fibh->ebh->b_data, fileident - offset,\nlfi + offset);\n}\n}\noffset += lfi;\nif (adinicb || (offset + padlen < 0)) {\nmemset(udf_get_fi_ident(sfi) + lfi, 0x00, padlen);\n} else if (offset >= 0) {\nmemset(fibh->ebh->b_data + offset, 0x00, padlen);\n} else {\nmemset(udf_get_fi_ident(sfi) + lfi, 0x00, -offset);\nmemset(fibh->ebh->b_data, 0x00, padlen + offset);\n}\ncrc = crc_itu_t(0, (uint8_t *)cfi + sizeof(struct tag),\nsizeof(struct fileIdentDesc) - sizeof(struct tag));\nif (fibh->sbh == fibh->ebh) {\ncrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\ncrclen + sizeof(struct tag) -\nsizeof(struct fileIdentDesc));\n} else if (sizeof(struct fileIdentDesc) >= -fibh->soffset) {\ncrc = crc_itu_t(crc, fibh->ebh->b_data +\nsizeof(struct fileIdentDesc) +\nfibh->soffset,\ncrclen + sizeof(struct tag) -\nsizeof(struct fileIdentDesc));\n} else {\ncrc = crc_itu_t(crc, (uint8_t *)sfi->impUse,\n-fibh->soffset - sizeof(struct fileIdentDesc));\ncrc = crc_itu_t(crc, fibh->ebh->b_data, fibh->eoffset);\n}\ncfi->descTag.descCRC = cpu_to_le16(crc);\ncfi->descTag.descCRCLength = cpu_to_le16(crclen);\ncfi->descTag.tagChecksum = udf_tag_checksum(&cfi->descTag);\nif (adinicb || (sizeof(struct fileIdentDesc) <= -fibh->soffset)) {\nmemcpy((uint8_t *)sfi, (uint8_t *)cfi,\nsizeof(struct fileIdentDesc));\n} else {\nmemcpy((uint8_t *)sfi, (uint8_t *)cfi, -fibh->soffset);\nmemcpy(fibh->ebh->b_data, (uint8_t *)cfi - fibh->soffset,\nsizeof(struct fileIdentDesc) + fibh->soffset);\n}\nif (adinicb) {\nmark_inode_dirty(inode);\n} else {\nif (fibh->sbh != fibh->ebh)\nmark_buffer_dirty_inode(fibh->ebh, inode);\nmark_buffer_dirty_inode(fibh->sbh, inode);\n}\ninode_inc_iversion(inode);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2022-20369": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper input validation in the v4l2_m2m_qbuf function.",
        "trigger_condition": "An attacker provides malicious input that is not properly validated, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not adjust memory offsets for the CAPTURE queue, allowing for potential out-of-bounds writes if the input is not properly validated.",
        "solution": "To mitigate the vulnerability, it is necessary to adjust memory offsets for the CAPTURE queue to ensure that memory operations are performed within the bounds of allocated memory. This prevents potential out-of-bounds writes and enhances input validation to address the security issue."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability described in CVE-2022-20369, which involves a possible out-of-bounds write due to improper input validation in the `v4l2_m2m_qbuf` function of the `v4l2-mem2mem.c` file.\n\nThe added line `v4l2_m2m_adjust_mem_offset(vq, buf);` in the modified code snippet is crucial for adjusting the memory offsets for the CAPTURE queue. By adjusting the memory offsets properly, you can prevent potential out-of-bounds writes that could lead to security vulnerabilities, such as local escalation of privilege with system execution privileges.\n\nTherefore, by incorporating this adjustment in the code, you are enhancing the input validation and ensuring that memory operations are performed within the bounds of allocated memory, thereby mitigating the risk of exploitation related to the identified vulnerability.",
      "GPT_purpose": "Queue a buffer for processing in a Video4Linux2 (V4L2) memory-to-memory context, handling specific conditions related to capture buffers and buffer flags.",
      "GPT_function": "\n1. `v4l2_m2m_qbuf`: Handles queuing a buffer for video capture or output.\n2. `v4l2_m2m_get_vq`: Retrieves the video queue associated with a specific buffer type.\n3. `vb2_qbuf`: Queues a buffer in a Video Buffer 2 (VB2) queue.",
      "CVE_id": "CVE-2022-20369",
      "code_before_change": "int v4l2_m2m_qbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t  struct v4l2_buffer *buf)\n{\n\tstruct video_device *vdev = video_devdata(file);\n\tstruct vb2_queue *vq;\n\tint ret;\n\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\tif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\n\t    (buf->flags & V4L2_BUF_FLAG_REQUEST_FD)) {\n\t\tdprintk(\"%s: requests cannot be used with capture buffers\\n\",\n\t\t\t__func__);\n\t\treturn -EPERM;\n\t}\n\n\tret = vb2_qbuf(vq, vdev->v4l2_dev->mdev, buf);\n\tif (ret)\n\t\treturn ret;\n\n\t/*\n\t * If the capture queue is streaming, but streaming hasn't started\n\t * on the device, but was asked to stop, mark the previously queued\n\t * buffer as DONE with LAST flag since it won't be queued on the\n\t * device.\n\t */\n\tif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\n\t    vb2_is_streaming(vq) && !vb2_start_streaming_called(vq) &&\n\t   (v4l2_m2m_has_stopped(m2m_ctx) || v4l2_m2m_dst_buf_is_last(m2m_ctx)))\n\t\tv4l2_m2m_force_last_buf_done(m2m_ctx, vq);\n\telse if (!(buf->flags & V4L2_BUF_FLAG_IN_REQUEST))\n\t\tv4l2_m2m_try_schedule(m2m_ctx);\n\n\treturn 0;\n}",
      "code_after_change": "int v4l2_m2m_qbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t  struct v4l2_buffer *buf)\n{\n\tstruct video_device *vdev = video_devdata(file);\n\tstruct vb2_queue *vq;\n\tint ret;\n\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\tif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\n\t    (buf->flags & V4L2_BUF_FLAG_REQUEST_FD)) {\n\t\tdprintk(\"%s: requests cannot be used with capture buffers\\n\",\n\t\t\t__func__);\n\t\treturn -EPERM;\n\t}\n\n\tret = vb2_qbuf(vq, vdev->v4l2_dev->mdev, buf);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Adjust MMAP memory offsets for the CAPTURE queue */\n\tv4l2_m2m_adjust_mem_offset(vq, buf);\n\n\t/*\n\t * If the capture queue is streaming, but streaming hasn't started\n\t * on the device, but was asked to stop, mark the previously queued\n\t * buffer as DONE with LAST flag since it won't be queued on the\n\t * device.\n\t */\n\tif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\n\t    vb2_is_streaming(vq) && !vb2_start_streaming_called(vq) &&\n\t   (v4l2_m2m_has_stopped(m2m_ctx) || v4l2_m2m_dst_buf_is_last(m2m_ctx)))\n\t\tv4l2_m2m_force_last_buf_done(m2m_ctx, vq);\n\telse if (!(buf->flags & V4L2_BUF_FLAG_IN_REQUEST))\n\t\tv4l2_m2m_try_schedule(m2m_ctx);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t/* Adjust MMAP memory offsets for the CAPTURE queue */",
          "\tv4l2_m2m_adjust_mem_offset(vq, buf);",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Improper input validation in the v4l2_m2m_qbuf function.",
      "trigger_condition": "An attacker provides malicious input that is not properly validated, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not adjust memory offsets for the CAPTURE queue, allowing for potential out-of-bounds writes if the input is not properly validated.",
      "solution": "To mitigate the vulnerability, it is necessary to adjust memory offsets for the CAPTURE queue to ensure that memory operations are performed within the bounds of allocated memory. This prevents potential out-of-bounds writes and enhances input validation to address the security issue.",
      "id": 131,
      "code_after_change_normalized": "int FUN1(struct VAR1 *VAR1, struct v4l2_m2m_ctx *VAR2,\nstruct v4l2_buffer *VAR3)\n{\nstruct video_device *VAR4 = FUN2(VAR1);\nstruct vb2_queue *VAR5;\nint VAR6;\nVAR5 = FUN3(VAR2, VAR3->VAR7);\nif (FUN4(VAR5->VAR7) &&\n(VAR3->VAR8 & VAR9)) {\nFUN5(\"STR\",\nVAR10);\nreturn -VAR11;\n}\nVAR6 = FUN6(VAR5, VAR4->VAR12->VAR13, VAR3);\nif (VAR6)\nreturn VAR6;\nFUN7(VAR5, VAR3);\nif (FUN4(VAR5->VAR7) &&\nFUN8(VAR5) && !FUN9(VAR5) &&\n(FUN10(VAR2) || FUN11(VAR2)))\nFUN12(VAR2, VAR5);\nelse if (!(VAR3->VAR8 & VAR14))\nFUN13(VAR2);\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct VAR1 *VAR1, struct v4l2_m2m_ctx *VAR2,\nstruct v4l2_buffer *VAR3)\n{\nstruct video_device *VAR4 = FUN2(VAR1);\nstruct vb2_queue *VAR5;\nint VAR6;\nVAR5 = FUN3(VAR2, VAR3->VAR7);\nif (FUN4(VAR5->VAR7) &&\n(VAR3->VAR8 & VAR9)) {\nFUN5(\"STR\",\nVAR10);\nreturn -VAR11;\n}\nVAR6 = FUN6(VAR5, VAR4->VAR12->VAR13, VAR3);\nif (VAR6)\nreturn VAR6;\nif (FUN4(VAR5->VAR7) &&\nFUN7(VAR5) && !FUN8(VAR5) &&\n(FUN9(VAR2) || FUN10(VAR2)))\nFUN11(VAR2, VAR5);\nelse if (!(VAR3->VAR8 & VAR14))\nFUN12(VAR2);\nreturn 0;\n}\n",
      "code_after_change_raw": "int v4l2_m2m_qbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\nstruct v4l2_buffer *buf)\n{\nstruct video_device *vdev = video_devdata(file);\nstruct vb2_queue *vq;\nint ret;\nvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\nif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\n(buf->flags & V4L2_BUF_FLAG_REQUEST_FD)) {\ndprintk(\"%s: requests cannot be used with capture buffers\\n\",\n__func__);\nreturn -EPERM;\n}\nret = vb2_qbuf(vq, vdev->v4l2_dev->mdev, buf);\nif (ret)\nreturn ret;\nv4l2_m2m_adjust_mem_offset(vq, buf);\nif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\nvb2_is_streaming(vq) && !vb2_start_streaming_called(vq) &&\n(v4l2_m2m_has_stopped(m2m_ctx) || v4l2_m2m_dst_buf_is_last(m2m_ctx)))\nv4l2_m2m_force_last_buf_done(m2m_ctx, vq);\nelse if (!(buf->flags & V4L2_BUF_FLAG_IN_REQUEST))\nv4l2_m2m_try_schedule(m2m_ctx);\nreturn 0;\n}\n",
      "code_before_change_raw": "int v4l2_m2m_qbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\nstruct v4l2_buffer *buf)\n{\nstruct video_device *vdev = video_devdata(file);\nstruct vb2_queue *vq;\nint ret;\nvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\nif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\n(buf->flags & V4L2_BUF_FLAG_REQUEST_FD)) {\ndprintk(\"%s: requests cannot be used with capture buffers\\n\",\n__func__);\nreturn -EPERM;\n}\nret = vb2_qbuf(vq, vdev->v4l2_dev->mdev, buf);\nif (ret)\nreturn ret;\nif (V4L2_TYPE_IS_CAPTURE(vq->type) &&\nvb2_is_streaming(vq) && !vb2_start_streaming_called(vq) &&\n(v4l2_m2m_has_stopped(m2m_ctx) || v4l2_m2m_dst_buf_is_last(m2m_ctx)))\nv4l2_m2m_force_last_buf_done(m2m_ctx, vq);\nelse if (!(buf->flags & V4L2_BUF_FLAG_IN_REQUEST))\nv4l2_m2m_try_schedule(m2m_ctx);\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper input validation and error handling in the v4l2_m2m_dqbuf function.",
        "trigger_condition": "Calling the v4l2_m2m_dqbuf function with invalid or out-of-bounds data.",
        "specific_code_behavior_causing_vulnerability": "The original code did not properly check the return value of vb2_dqbuf, potentially allowing out-of-bounds writes due to improper input validation.",
        "specific_solution_to_fix_vulnerability": "The vulnerability can be mitigated by adding proper error handling and input validation. This includes checking the return value of vb2_dqbuf and returning the error code if it is non-zero. Additionally, adjusting memory offsets after dequeuing the buffer helps prevent memory-related vulnerabilities."
      },
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2022-20369 by adding proper error handling and input validation. By adding the check for the return value of `vb2_dqbuf` and returning the error code if it is non-zero, the code ensures that the function does not proceed with potentially invalid or out-of-bounds data.\n\nAdditionally, the modification includes a call to `v4l2_m2m_adjust_mem_offset` after successfully dequeuing the buffer. This adjustment is important for ensuring that memory offsets are correctly handled, which helps prevent potential out-of-bounds writes or other memory-related vulnerabilities.\n\nIn summary, the modification enhances the security and robustness of the code by addressing the vulnerability and adding necessary error handling and memory offset adjustments.",
      "GPT_purpose": "Dequeues a buffer from a Video4Linux2 (V4L2) memory-to-memory context.",
      "GPT_function": "\n1. `v4l2_m2m_dqbuf`: Dequeues a buffer from a Video4Linux2 (V4L2) memory-to-memory context.",
      "CVE_id": "CVE-2022-20369",
      "code_before_change": "int v4l2_m2m_dqbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t   struct v4l2_buffer *buf)\n{\n\tstruct vb2_queue *vq;\n\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\treturn vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);\n}",
      "code_after_change": "int v4l2_m2m_dqbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t   struct v4l2_buffer *buf)\n{\n\tstruct vb2_queue *vq;\n\tint ret;\n\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\tret = vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Adjust MMAP memory offsets for the CAPTURE queue */\n\tv4l2_m2m_adjust_mem_offset(vq, buf);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tint ret;",
          "\tret = vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);",
          "\tif (ret)",
          "\t\treturn ret;",
          "",
          "\t/* Adjust MMAP memory offsets for the CAPTURE queue */",
          "\tv4l2_m2m_adjust_mem_offset(vq, buf);",
          "",
          "\treturn 0;"
        ],
        "deleted": [
          "\treturn vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);"
        ]
      },
      "preconditions_for_vulnerability": "Improper input validation and error handling in the v4l2_m2m_dqbuf function.",
      "trigger_condition": "Calling the v4l2_m2m_dqbuf function with invalid or out-of-bounds data.",
      "specific_code_behavior_causing_vulnerability": "The original code did not properly check the return value of vb2_dqbuf, potentially allowing out-of-bounds writes due to improper input validation.",
      "id": 132,
      "code_after_change_normalized": "int FUN1(struct VAR1 *VAR1, struct v4l2_m2m_ctx *VAR2,\nstruct v4l2_buffer *VAR3)\n{\nstruct vb2_queue *VAR4;\nint VAR5;\nVAR4 = FUN2(VAR2, VAR3->VAR6);\nVAR5 = FUN3(VAR4, VAR3, VAR1->VAR7 & VAR8);\nif (VAR5)\nreturn VAR5;\nFUN4(VAR4, VAR3);\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct VAR1 *VAR1, struct v4l2_m2m_ctx *VAR2,\nstruct v4l2_buffer *VAR3)\n{\nstruct vb2_queue *VAR4;\nVAR4 = FUN2(VAR2, VAR3->VAR5);\nreturn FUN3(VAR4, VAR3, VAR1->VAR6 & VAR7);\n}\n",
      "code_after_change_raw": "int v4l2_m2m_dqbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\nstruct v4l2_buffer *buf)\n{\nstruct vb2_queue *vq;\nint ret;\nvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\nret = vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);\nif (ret)\nreturn ret;\nv4l2_m2m_adjust_mem_offset(vq, buf);\nreturn 0;\n}\n",
      "code_before_change_raw": "int v4l2_m2m_dqbuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\nstruct v4l2_buffer *buf)\n{\nstruct vb2_queue *vq;\nvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\nreturn vb2_dqbuf(vq, buf, file->f_flags & O_NONBLOCK);\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper input validation and error handling in the buffer query process.",
        "trigger_condition": "Improper input validation allows for out-of-bounds write when adjusting memory offsets for the CAPTURE queue.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the return value of the buffer query function, leading to a possible out-of-bounds write when adjusting memory offsets for the CAPTURE queue."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add input validation by checking the return value of the buffer query function. If an error occurs, the function should return the error code immediately. Additionally, the memory offset adjustment logic should be moved to a separate function to handle the adjustment of memory offsets for the CAPTURE queue. This separation of concerns improves code readability and ensures proper error handling.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2022-20369, which involves a possible out-of-bounds write due to improper input validation in the original code snippet. By making the following changes:\n\n1. Adding input validation check for the return value of `vb2_querybuf` function (`if (ret)`), and returning the error code if it is non-zero. This ensures that the function exits early if an error occurs during the buffer query process.\n\n2. Moving the memory offset adjustment logic to a separate function `v4l2_m2m_adjust_mem_offset` to handle the adjustment of memory offsets for the CAPTURE queue. This separation of concerns improves code readability and maintainability.\n\nBy implementing these modifications, the code becomes more robust and less susceptible to the vulnerability described, as it ensures proper handling of errors and separates the memory offset adjustment logic into a dedicated function.",
      "GPT_purpose": "Query buffer information and adjust memory offsets for the CAPTURE queue in a V4L2 memory-to-memory context.",
      "GPT_function": "\n1. Query a buffer in a Video4Linux2 (V4L2) memory-to-memory context.\n2. Adjust memory offsets for the CAPTURE queue if the memory type is V4L2_MEMORY_MMAP and the queue type is CAPTURE.\n3. Return the result of the buffer query operation.",
      "CVE_id": "CVE-2022-20369",
      "code_before_change": "int v4l2_m2m_querybuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t      struct v4l2_buffer *buf)\n{\n\tstruct vb2_queue *vq;\n\tint ret = 0;\n\tunsigned int i;\n\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\tret = vb2_querybuf(vq, buf);\n\n\t/* Adjust MMAP memory offsets for the CAPTURE queue */\n\tif (buf->memory == V4L2_MEMORY_MMAP && V4L2_TYPE_IS_CAPTURE(vq->type)) {\n\t\tif (V4L2_TYPE_IS_MULTIPLANAR(vq->type)) {\n\t\t\tfor (i = 0; i < buf->length; ++i)\n\t\t\t\tbuf->m.planes[i].m.mem_offset\n\t\t\t\t\t+= DST_QUEUE_OFF_BASE;\n\t\t} else {\n\t\t\tbuf->m.offset += DST_QUEUE_OFF_BASE;\n\t\t}\n\t}\n\n\treturn ret;\n}",
      "code_after_change": "int v4l2_m2m_querybuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\n\t\t      struct v4l2_buffer *buf)\n{\n\tstruct vb2_queue *vq;\n\tint ret;\n\n\tvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\n\tret = vb2_querybuf(vq, buf);\n\tif (ret)\n\t\treturn ret;\n\n\t/* Adjust MMAP memory offsets for the CAPTURE queue */\n\tv4l2_m2m_adjust_mem_offset(vq, buf);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tint ret;",
          "\tif (ret)",
          "\t\treturn ret;",
          "\tv4l2_m2m_adjust_mem_offset(vq, buf);",
          "\treturn 0;"
        ],
        "deleted": [
          "\tint ret = 0;",
          "\tunsigned int i;",
          "\tif (buf->memory == V4L2_MEMORY_MMAP && V4L2_TYPE_IS_CAPTURE(vq->type)) {",
          "\t\tif (V4L2_TYPE_IS_MULTIPLANAR(vq->type)) {",
          "\t\t\tfor (i = 0; i < buf->length; ++i)",
          "\t\t\t\tbuf->m.planes[i].m.mem_offset",
          "\t\t\t\t\t+= DST_QUEUE_OFF_BASE;",
          "\t\t} else {",
          "\t\t\tbuf->m.offset += DST_QUEUE_OFF_BASE;",
          "\t\t}",
          "\t}",
          "\treturn ret;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper input validation and error handling in the buffer query process.",
      "trigger_condition": "Improper input validation allows for out-of-bounds write when adjusting memory offsets for the CAPTURE queue.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the return value of the buffer query function, leading to a possible out-of-bounds write when adjusting memory offsets for the CAPTURE queue.",
      "id": 133,
      "code_after_change_normalized": "int FUN1(struct VAR1 *VAR1, struct v4l2_m2m_ctx *VAR2,\nstruct v4l2_buffer *VAR3)\n{\nstruct vb2_queue *VAR4;\nint VAR5;\nVAR4 = FUN2(VAR2, VAR3->VAR6);\nVAR5 = FUN3(VAR4, VAR3);\nif (VAR5)\nreturn VAR5;\nFUN4(VAR4, VAR3);\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct VAR1 *VAR1, struct v4l2_m2m_ctx *VAR2,\nstruct v4l2_buffer *VAR3)\n{\nstruct vb2_queue *VAR4;\nint VAR5 = 0;\nunsigned int VAR6;\nVAR4 = FUN2(VAR2, VAR3->VAR7);\nVAR5 = FUN3(VAR4, VAR3);\nif (VAR3->VAR8 == VAR9 && FUN4(VAR4->VAR7)) {\nif (FUN5(VAR4->VAR7)) {\nfor (VAR6 = 0; VAR6 < VAR3->VAR10; ++VAR6)\nVAR3->VAR11.VAR12[VAR6].VAR11.VAR13\n+= VAR14;\n} else {\nVAR3->VAR11.VAR15 += VAR14;\n}\n}\nreturn VAR5;\n}\n",
      "code_after_change_raw": "int v4l2_m2m_querybuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\nstruct v4l2_buffer *buf)\n{\nstruct vb2_queue *vq;\nint ret;\nvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\nret = vb2_querybuf(vq, buf);\nif (ret)\nreturn ret;\nv4l2_m2m_adjust_mem_offset(vq, buf);\nreturn 0;\n}\n",
      "code_before_change_raw": "int v4l2_m2m_querybuf(struct file *file, struct v4l2_m2m_ctx *m2m_ctx,\nstruct v4l2_buffer *buf)\n{\nstruct vb2_queue *vq;\nint ret = 0;\nunsigned int i;\nvq = v4l2_m2m_get_vq(m2m_ctx, buf->type);\nret = vb2_querybuf(vq, buf);\nif (buf->memory == V4L2_MEMORY_MMAP && V4L2_TYPE_IS_CAPTURE(vq->type)) {\nif (V4L2_TYPE_IS_MULTIPLANAR(vq->type)) {\nfor (i = 0; i < buf->length; ++i)\nbuf->m.planes[i].m.mem_offset\n+= DST_QUEUE_OFF_BASE;\n} else {\nbuf->m.offset += DST_QUEUE_OFF_BASE;\n}\n}\nreturn ret;\n}\n"
    }
  ],
  "CVE-2022-21499": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper lockdown mode handling in the debugger code, allowing unauthorized read and write access to kernel memory.",
        "trigger_condition": "An attacker with access to a serial port triggers the debugger, bypassing lockdown mode and gaining unauthorized access to kernel memory.",
        "specific_code_behavior_causing_vulnerability": "The code does not include a security check to respect lockdown mode when the debugger is triggered, leading to unauthorized access to kernel memory. This can result in confidentiality, integrity, and availability impacts.",
        "solution": "To mitigate the vulnerability, a security check must be added to ensure that the debugger respects lockdown mode when triggered. This check should prevent unauthorized access to kernel memory and enforce security measures. The modified code snippet includes a security check to handle lockdown mode appropriately and prevent unauthorized access to kernel memory."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability described in CVE-2022-21499. The vulnerability allows unauthorized read and write access to kernel memory through KGDB and KDB, which could lead to confidentiality, integrity, and availability impacts. \n\nThe modification specifically adds a security check to prevent unauthorized access to kernel memory when the debugger is triggered. By checking if the system is in lockdown mode and restricting access accordingly, the code ensures that the debugger respects the security measures in place. This helps mitigate the risk of exploitation by an attacker with access to a serial port triggering the debugger.\n\nBy adding the security check as shown in the modified code snippet, the system can prevent unauthorized access to kernel memory and reduce the risk associated with the vulnerability.",
      "GPT_purpose": "This function is responsible for handling the entry of a CPU into the KGDB debugger for debugging purposes.",
      "GPT_function": "\n1. Entering the KGDB debugger on a specific CPU.\n2. Handling various states and conditions for the CPU entering the debugger.\n3. Managing single-stepping behavior and interactions with other CPUs.\n4. Coordinating debugger activities between master and slave CPUs.\n5. Executing debugger operations such as handling breakpoints and I/O connections.\n6. Controlling the flow of debugger operations and interactions with other CPUs.\n7. Restoring the system state after debugger operations are completed.",
      "CVE_id": "CVE-2022-21499",
      "code_before_change": "static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,\n\t\tint exception_state)\n{\n\tunsigned long flags;\n\tint sstep_tries = 100;\n\tint error;\n\tint cpu;\n\tint trace_on = 0;\n\tint online_cpus = num_online_cpus();\n\tu64 time_left;\n\n\tkgdb_info[ks->cpu].enter_kgdb++;\n\tkgdb_info[ks->cpu].exception_state |= exception_state;\n\n\tif (exception_state == DCPU_WANT_MASTER)\n\t\tatomic_inc(&masters_in_kgdb);\n\telse\n\t\tatomic_inc(&slaves_in_kgdb);\n\n\tif (arch_kgdb_ops.disable_hw_break)\n\t\tarch_kgdb_ops.disable_hw_break(regs);\n\nacquirelock:\n\trcu_read_lock();\n\t/*\n\t * Interrupts will be restored by the 'trap return' code, except when\n\t * single stepping.\n\t */\n\tlocal_irq_save(flags);\n\n\tcpu = ks->cpu;\n\tkgdb_info[cpu].debuggerinfo = regs;\n\tkgdb_info[cpu].task = current;\n\tkgdb_info[cpu].ret_state = 0;\n\tkgdb_info[cpu].irq_depth = hardirq_count() >> HARDIRQ_SHIFT;\n\n\t/* Make sure the above info reaches the primary CPU */\n\tsmp_mb();\n\n\tif (exception_level == 1) {\n\t\tif (raw_spin_trylock(&dbg_master_lock))\n\t\t\tatomic_xchg(&kgdb_active, cpu);\n\t\tgoto cpu_master_loop;\n\t}\n\n\t/*\n\t * CPU will loop if it is a slave or request to become a kgdb\n\t * master cpu and acquire the kgdb_active lock:\n\t */\n\twhile (1) {\ncpu_loop:\n\t\tif (kgdb_info[cpu].exception_state & DCPU_NEXT_MASTER) {\n\t\t\tkgdb_info[cpu].exception_state &= ~DCPU_NEXT_MASTER;\n\t\t\tgoto cpu_master_loop;\n\t\t} else if (kgdb_info[cpu].exception_state & DCPU_WANT_MASTER) {\n\t\t\tif (raw_spin_trylock(&dbg_master_lock)) {\n\t\t\t\tatomic_xchg(&kgdb_active, cpu);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (kgdb_info[cpu].exception_state & DCPU_WANT_BT) {\n\t\t\tdump_stack();\n\t\t\tkgdb_info[cpu].exception_state &= ~DCPU_WANT_BT;\n\t\t} else if (kgdb_info[cpu].exception_state & DCPU_IS_SLAVE) {\n\t\t\tif (!raw_spin_is_locked(&dbg_slave_lock))\n\t\t\t\tgoto return_normal;\n\t\t} else {\nreturn_normal:\n\t\t\t/* Return to normal operation by executing any\n\t\t\t * hw breakpoint fixup.\n\t\t\t */\n\t\t\tif (arch_kgdb_ops.correct_hw_break)\n\t\t\t\tarch_kgdb_ops.correct_hw_break();\n\t\t\tif (trace_on)\n\t\t\t\ttracing_on();\n\t\t\tkgdb_info[cpu].debuggerinfo = NULL;\n\t\t\tkgdb_info[cpu].task = NULL;\n\t\t\tkgdb_info[cpu].exception_state &=\n\t\t\t\t~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\n\t\t\tkgdb_info[cpu].enter_kgdb--;\n\t\t\tsmp_mb__before_atomic();\n\t\t\tatomic_dec(&slaves_in_kgdb);\n\t\t\tdbg_touch_watchdogs();\n\t\t\tlocal_irq_restore(flags);\n\t\t\trcu_read_unlock();\n\t\t\treturn 0;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\t/*\n\t * For single stepping, try to only enter on the processor\n\t * that was single stepping.  To guard against a deadlock, the\n\t * kernel will only try for the value of sstep_tries before\n\t * giving up and continuing on.\n\t */\n\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1 &&\n\t    (kgdb_info[cpu].task &&\n\t     kgdb_info[cpu].task->pid != kgdb_sstep_pid) && --sstep_tries) {\n\t\tatomic_set(&kgdb_active, -1);\n\t\traw_spin_unlock(&dbg_master_lock);\n\t\tdbg_touch_watchdogs();\n\t\tlocal_irq_restore(flags);\n\t\trcu_read_unlock();\n\n\t\tgoto acquirelock;\n\t}\n\n\tif (!kgdb_io_ready(1)) {\n\t\tkgdb_info[cpu].ret_state = 1;\n\t\tgoto kgdb_restore; /* No I/O connection, resume the system */\n\t}\n\n\t/*\n\t * Don't enter if we have hit a removed breakpoint.\n\t */\n\tif (kgdb_skipexception(ks->ex_vector, ks->linux_regs))\n\t\tgoto kgdb_restore;\n\n\tatomic_inc(&ignore_console_lock_warning);\n\n\t/* Call the I/O driver's pre_exception routine */\n\tif (dbg_io_ops->pre_exception)\n\t\tdbg_io_ops->pre_exception();\n\n\t/*\n\t * Get the passive CPU lock which will hold all the non-primary\n\t * CPU in a spin state while the debugger is active\n\t */\n\tif (!kgdb_single_step)\n\t\traw_spin_lock(&dbg_slave_lock);\n\n#ifdef CONFIG_SMP\n\t/* If send_ready set, slaves are already waiting */\n\tif (ks->send_ready)\n\t\tatomic_set(ks->send_ready, 1);\n\n\t/* Signal the other CPUs to enter kgdb_wait() */\n\telse if ((!kgdb_single_step) && kgdb_do_roundup)\n\t\tkgdb_roundup_cpus();\n#endif\n\n\t/*\n\t * Wait for the other CPUs to be notified and be waiting for us:\n\t */\n\ttime_left = MSEC_PER_SEC;\n\twhile (kgdb_do_roundup && --time_left &&\n\t       (atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=\n\t\t   online_cpus)\n\t\tudelay(1000);\n\tif (!time_left)\n\t\tpr_crit(\"Timed out waiting for secondary CPUs.\\n\");\n\n\t/*\n\t * At this point the primary processor is completely\n\t * in the debugger and all secondary CPUs are quiescent\n\t */\n\tdbg_deactivate_sw_breakpoints();\n\tkgdb_single_step = 0;\n\tkgdb_contthread = current;\n\texception_level = 0;\n\ttrace_on = tracing_is_on();\n\tif (trace_on)\n\t\ttracing_off();\n\n\twhile (1) {\ncpu_master_loop:\n\t\tif (dbg_kdb_mode) {\n\t\t\tkgdb_connected = 1;\n\t\t\terror = kdb_stub(ks);\n\t\t\tif (error == -1)\n\t\t\t\tcontinue;\n\t\t\tkgdb_connected = 0;\n\t\t} else {\n\t\t\terror = gdb_serial_stub(ks);\n\t\t}\n\n\t\tif (error == DBG_PASS_EVENT) {\n\t\t\tdbg_kdb_mode = !dbg_kdb_mode;\n\t\t} else if (error == DBG_SWITCH_CPU_EVENT) {\n\t\t\tkgdb_info[dbg_switch_cpu].exception_state |=\n\t\t\t\tDCPU_NEXT_MASTER;\n\t\t\tgoto cpu_loop;\n\t\t} else {\n\t\t\tkgdb_info[cpu].ret_state = error;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdbg_activate_sw_breakpoints();\n\n\t/* Call the I/O driver's post_exception routine */\n\tif (dbg_io_ops->post_exception)\n\t\tdbg_io_ops->post_exception();\n\n\tatomic_dec(&ignore_console_lock_warning);\n\n\tif (!kgdb_single_step) {\n\t\traw_spin_unlock(&dbg_slave_lock);\n\t\t/* Wait till all the CPUs have quit from the debugger. */\n\t\twhile (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))\n\t\t\tcpu_relax();\n\t}\n\nkgdb_restore:\n\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\n\t\tint sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);\n\t\tif (kgdb_info[sstep_cpu].task)\n\t\t\tkgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;\n\t\telse\n\t\t\tkgdb_sstep_pid = 0;\n\t}\n\tif (arch_kgdb_ops.correct_hw_break)\n\t\tarch_kgdb_ops.correct_hw_break();\n\tif (trace_on)\n\t\ttracing_on();\n\n\tkgdb_info[cpu].debuggerinfo = NULL;\n\tkgdb_info[cpu].task = NULL;\n\tkgdb_info[cpu].exception_state &=\n\t\t~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\n\tkgdb_info[cpu].enter_kgdb--;\n\tsmp_mb__before_atomic();\n\tatomic_dec(&masters_in_kgdb);\n\t/* Free kgdb_active */\n\tatomic_set(&kgdb_active, -1);\n\traw_spin_unlock(&dbg_master_lock);\n\tdbg_touch_watchdogs();\n\tlocal_irq_restore(flags);\n\trcu_read_unlock();\n\n\treturn kgdb_info[cpu].ret_state;\n}",
      "code_after_change": "static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,\n\t\tint exception_state)\n{\n\tunsigned long flags;\n\tint sstep_tries = 100;\n\tint error;\n\tint cpu;\n\tint trace_on = 0;\n\tint online_cpus = num_online_cpus();\n\tu64 time_left;\n\n\tkgdb_info[ks->cpu].enter_kgdb++;\n\tkgdb_info[ks->cpu].exception_state |= exception_state;\n\n\tif (exception_state == DCPU_WANT_MASTER)\n\t\tatomic_inc(&masters_in_kgdb);\n\telse\n\t\tatomic_inc(&slaves_in_kgdb);\n\n\tif (arch_kgdb_ops.disable_hw_break)\n\t\tarch_kgdb_ops.disable_hw_break(regs);\n\nacquirelock:\n\trcu_read_lock();\n\t/*\n\t * Interrupts will be restored by the 'trap return' code, except when\n\t * single stepping.\n\t */\n\tlocal_irq_save(flags);\n\n\tcpu = ks->cpu;\n\tkgdb_info[cpu].debuggerinfo = regs;\n\tkgdb_info[cpu].task = current;\n\tkgdb_info[cpu].ret_state = 0;\n\tkgdb_info[cpu].irq_depth = hardirq_count() >> HARDIRQ_SHIFT;\n\n\t/* Make sure the above info reaches the primary CPU */\n\tsmp_mb();\n\n\tif (exception_level == 1) {\n\t\tif (raw_spin_trylock(&dbg_master_lock))\n\t\t\tatomic_xchg(&kgdb_active, cpu);\n\t\tgoto cpu_master_loop;\n\t}\n\n\t/*\n\t * CPU will loop if it is a slave or request to become a kgdb\n\t * master cpu and acquire the kgdb_active lock:\n\t */\n\twhile (1) {\ncpu_loop:\n\t\tif (kgdb_info[cpu].exception_state & DCPU_NEXT_MASTER) {\n\t\t\tkgdb_info[cpu].exception_state &= ~DCPU_NEXT_MASTER;\n\t\t\tgoto cpu_master_loop;\n\t\t} else if (kgdb_info[cpu].exception_state & DCPU_WANT_MASTER) {\n\t\t\tif (raw_spin_trylock(&dbg_master_lock)) {\n\t\t\t\tatomic_xchg(&kgdb_active, cpu);\n\t\t\t\tbreak;\n\t\t\t}\n\t\t} else if (kgdb_info[cpu].exception_state & DCPU_WANT_BT) {\n\t\t\tdump_stack();\n\t\t\tkgdb_info[cpu].exception_state &= ~DCPU_WANT_BT;\n\t\t} else if (kgdb_info[cpu].exception_state & DCPU_IS_SLAVE) {\n\t\t\tif (!raw_spin_is_locked(&dbg_slave_lock))\n\t\t\t\tgoto return_normal;\n\t\t} else {\nreturn_normal:\n\t\t\t/* Return to normal operation by executing any\n\t\t\t * hw breakpoint fixup.\n\t\t\t */\n\t\t\tif (arch_kgdb_ops.correct_hw_break)\n\t\t\t\tarch_kgdb_ops.correct_hw_break();\n\t\t\tif (trace_on)\n\t\t\t\ttracing_on();\n\t\t\tkgdb_info[cpu].debuggerinfo = NULL;\n\t\t\tkgdb_info[cpu].task = NULL;\n\t\t\tkgdb_info[cpu].exception_state &=\n\t\t\t\t~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\n\t\t\tkgdb_info[cpu].enter_kgdb--;\n\t\t\tsmp_mb__before_atomic();\n\t\t\tatomic_dec(&slaves_in_kgdb);\n\t\t\tdbg_touch_watchdogs();\n\t\t\tlocal_irq_restore(flags);\n\t\t\trcu_read_unlock();\n\t\t\treturn 0;\n\t\t}\n\t\tcpu_relax();\n\t}\n\n\t/*\n\t * For single stepping, try to only enter on the processor\n\t * that was single stepping.  To guard against a deadlock, the\n\t * kernel will only try for the value of sstep_tries before\n\t * giving up and continuing on.\n\t */\n\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1 &&\n\t    (kgdb_info[cpu].task &&\n\t     kgdb_info[cpu].task->pid != kgdb_sstep_pid) && --sstep_tries) {\n\t\tatomic_set(&kgdb_active, -1);\n\t\traw_spin_unlock(&dbg_master_lock);\n\t\tdbg_touch_watchdogs();\n\t\tlocal_irq_restore(flags);\n\t\trcu_read_unlock();\n\n\t\tgoto acquirelock;\n\t}\n\n\tif (!kgdb_io_ready(1)) {\n\t\tkgdb_info[cpu].ret_state = 1;\n\t\tgoto kgdb_restore; /* No I/O connection, resume the system */\n\t}\n\n\t/*\n\t * Don't enter if we have hit a removed breakpoint.\n\t */\n\tif (kgdb_skipexception(ks->ex_vector, ks->linux_regs))\n\t\tgoto kgdb_restore;\n\n\tatomic_inc(&ignore_console_lock_warning);\n\n\t/* Call the I/O driver's pre_exception routine */\n\tif (dbg_io_ops->pre_exception)\n\t\tdbg_io_ops->pre_exception();\n\n\t/*\n\t * Get the passive CPU lock which will hold all the non-primary\n\t * CPU in a spin state while the debugger is active\n\t */\n\tif (!kgdb_single_step)\n\t\traw_spin_lock(&dbg_slave_lock);\n\n#ifdef CONFIG_SMP\n\t/* If send_ready set, slaves are already waiting */\n\tif (ks->send_ready)\n\t\tatomic_set(ks->send_ready, 1);\n\n\t/* Signal the other CPUs to enter kgdb_wait() */\n\telse if ((!kgdb_single_step) && kgdb_do_roundup)\n\t\tkgdb_roundup_cpus();\n#endif\n\n\t/*\n\t * Wait for the other CPUs to be notified and be waiting for us:\n\t */\n\ttime_left = MSEC_PER_SEC;\n\twhile (kgdb_do_roundup && --time_left &&\n\t       (atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=\n\t\t   online_cpus)\n\t\tudelay(1000);\n\tif (!time_left)\n\t\tpr_crit(\"Timed out waiting for secondary CPUs.\\n\");\n\n\t/*\n\t * At this point the primary processor is completely\n\t * in the debugger and all secondary CPUs are quiescent\n\t */\n\tdbg_deactivate_sw_breakpoints();\n\tkgdb_single_step = 0;\n\tkgdb_contthread = current;\n\texception_level = 0;\n\ttrace_on = tracing_is_on();\n\tif (trace_on)\n\t\ttracing_off();\n\n\twhile (1) {\ncpu_master_loop:\n\t\tif (dbg_kdb_mode) {\n\t\t\tkgdb_connected = 1;\n\t\t\terror = kdb_stub(ks);\n\t\t\tif (error == -1)\n\t\t\t\tcontinue;\n\t\t\tkgdb_connected = 0;\n\t\t} else {\n\t\t\t/*\n\t\t\t * This is a brutal way to interfere with the debugger\n\t\t\t * and prevent gdb being used to poke at kernel memory.\n\t\t\t * This could cause trouble if lockdown is applied when\n\t\t\t * there is already an active gdb session. For now the\n\t\t\t * answer is simply \"don't do that\". Typically lockdown\n\t\t\t * *will* be applied before the debug core gets started\n\t\t\t * so only developers using kgdb for fairly advanced\n\t\t\t * early kernel debug can be biten by this. Hopefully\n\t\t\t * they are sophisticated enough to take care of\n\t\t\t * themselves, especially with help from the lockdown\n\t\t\t * message printed on the console!\n\t\t\t */\n\t\t\tif (security_locked_down(LOCKDOWN_DBG_WRITE_KERNEL)) {\n\t\t\t\tif (IS_ENABLED(CONFIG_KGDB_KDB)) {\n\t\t\t\t\t/* Switch back to kdb if possible... */\n\t\t\t\t\tdbg_kdb_mode = 1;\n\t\t\t\t\tcontinue;\n\t\t\t\t} else {\n\t\t\t\t\t/* ... otherwise just bail */\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\terror = gdb_serial_stub(ks);\n\t\t}\n\n\t\tif (error == DBG_PASS_EVENT) {\n\t\t\tdbg_kdb_mode = !dbg_kdb_mode;\n\t\t} else if (error == DBG_SWITCH_CPU_EVENT) {\n\t\t\tkgdb_info[dbg_switch_cpu].exception_state |=\n\t\t\t\tDCPU_NEXT_MASTER;\n\t\t\tgoto cpu_loop;\n\t\t} else {\n\t\t\tkgdb_info[cpu].ret_state = error;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tdbg_activate_sw_breakpoints();\n\n\t/* Call the I/O driver's post_exception routine */\n\tif (dbg_io_ops->post_exception)\n\t\tdbg_io_ops->post_exception();\n\n\tatomic_dec(&ignore_console_lock_warning);\n\n\tif (!kgdb_single_step) {\n\t\traw_spin_unlock(&dbg_slave_lock);\n\t\t/* Wait till all the CPUs have quit from the debugger. */\n\t\twhile (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))\n\t\t\tcpu_relax();\n\t}\n\nkgdb_restore:\n\tif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\n\t\tint sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);\n\t\tif (kgdb_info[sstep_cpu].task)\n\t\t\tkgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;\n\t\telse\n\t\t\tkgdb_sstep_pid = 0;\n\t}\n\tif (arch_kgdb_ops.correct_hw_break)\n\t\tarch_kgdb_ops.correct_hw_break();\n\tif (trace_on)\n\t\ttracing_on();\n\n\tkgdb_info[cpu].debuggerinfo = NULL;\n\tkgdb_info[cpu].task = NULL;\n\tkgdb_info[cpu].exception_state &=\n\t\t~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\n\tkgdb_info[cpu].enter_kgdb--;\n\tsmp_mb__before_atomic();\n\tatomic_dec(&masters_in_kgdb);\n\t/* Free kgdb_active */\n\tatomic_set(&kgdb_active, -1);\n\traw_spin_unlock(&dbg_master_lock);\n\tdbg_touch_watchdogs();\n\tlocal_irq_restore(flags);\n\trcu_read_unlock();\n\n\treturn kgdb_info[cpu].ret_state;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t/*",
          "\t\t\t * This is a brutal way to interfere with the debugger",
          "\t\t\t * and prevent gdb being used to poke at kernel memory.",
          "\t\t\t * This could cause trouble if lockdown is applied when",
          "\t\t\t * there is already an active gdb session. For now the",
          "\t\t\t * answer is simply \"don't do that\". Typically lockdown",
          "\t\t\t * *will* be applied before the debug core gets started",
          "\t\t\t * so only developers using kgdb for fairly advanced",
          "\t\t\t * early kernel debug can be biten by this. Hopefully",
          "\t\t\t * they are sophisticated enough to take care of",
          "\t\t\t * themselves, especially with help from the lockdown",
          "\t\t\t * message printed on the console!",
          "\t\t\t */",
          "\t\t\tif (security_locked_down(LOCKDOWN_DBG_WRITE_KERNEL)) {",
          "\t\t\t\tif (IS_ENABLED(CONFIG_KGDB_KDB)) {",
          "\t\t\t\t\t/* Switch back to kdb if possible... */",
          "\t\t\t\t\tdbg_kdb_mode = 1;",
          "\t\t\t\t\tcontinue;",
          "\t\t\t\t} else {",
          "\t\t\t\t\t/* ... otherwise just bail */",
          "\t\t\t\t\tbreak;",
          "\t\t\t\t}",
          "\t\t\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper lockdown mode handling in the debugger code, allowing unauthorized read and write access to kernel memory.",
      "trigger_condition": "An attacker with access to a serial port triggers the debugger, bypassing lockdown mode and gaining unauthorized access to kernel memory.",
      "specific_code_behavior_causing_vulnerability": "The code does not include a security check to respect lockdown mode when the debugger is triggered, leading to unauthorized access to kernel memory. This can result in confidentiality, integrity, and availability impacts.",
      "solution": "To mitigate the vulnerability, a security check must be added to ensure that the debugger respects lockdown mode when triggered. This check should prevent unauthorized access to kernel memory and enforce security measures. The modified code snippet includes a security check to handle lockdown mode appropriately and prevent unauthorized access to kernel memory.",
      "id": 134,
      "code_after_change_normalized": "static int FUN1(struct kgdb_state *VAR1, struct pt_regs *VAR2,\nint VAR3)\n{\nunsigned long VAR4;\nint VAR5 = 100;\nint VAR6;\nint VAR7;\nint VAR8 = 0;\nint VAR9 = FUN2();\nu64 VAR10;\nVAR11[VAR1->VAR7].VAR12++;\nVAR11[VAR1->VAR7].VAR3 |= VAR3;\nif (VAR3 == VAR13)\nFUN3(&VAR14);\nelse\nFUN3(&VAR15);\nif (VAR16.VAR17)\nVAR16.FUN4(VAR2);\nVAR18:\nFUN5();\nFUN6(VAR4);\nVAR7 = VAR1->VAR7;\nVAR11[VAR7].VAR19 = VAR2;\nVAR11[VAR7].VAR20 = VAR21;\nVAR11[VAR7].VAR22 = 0;\nVAR11[VAR7].VAR23 = FUN7() >> VAR24;\nFUN8();\nif (VAR25 == 1) {\nif (FUN9(&VAR26))\nFUN10(&VAR27, VAR7);\ngoto VAR28;\n}\nwhile (1) {\nVAR29:\nif (VAR11[VAR7].VAR3 & VAR30) {\nVAR11[VAR7].VAR3 &= ~VAR30;\ngoto VAR28;\n} else if (VAR11[VAR7].VAR3 & VAR13) {\nif (FUN9(&VAR26)) {\nFUN10(&VAR27, VAR7);\nbreak;\n}\n} else if (VAR11[VAR7].VAR3 & VAR31) {\nFUN11();\nVAR11[VAR7].VAR3 &= ~VAR31;\n} else if (VAR11[VAR7].VAR3 & VAR32) {\nif (!FUN12(&VAR33))\ngoto VAR34;\n} else {\nVAR34:\nif (VAR16.VAR35)\nVAR16.FUN13();\nif (VAR8)\nFUN14();\nVAR11[VAR7].VAR19 = NULL;\nVAR11[VAR7].VAR20 = NULL;\nVAR11[VAR7].VAR3 &=\n~(VAR13 | VAR32);\nVAR11[VAR7].VAR12--;\nFUN15();\nFUN16(&VAR15);\nFUN17();\nFUN18(VAR4);\nFUN19();\nreturn 0;\n}\nFUN20();\n}\nif (FUN21(&VAR36) != -1 &&\n(VAR11[VAR7].VAR20 &&\nVAR11[VAR7].VAR20->VAR37 != VAR38) && --VAR5) {\nFUN22(&VAR27, -1);\nFUN23(&VAR26);\nFUN17();\nFUN18(VAR4);\nFUN19();\ngoto VAR18;\n}\nif (!FUN24(1)) {\nVAR11[VAR7].VAR22 = 1;\ngoto VAR39; \n}\nif (FUN25(VAR1->VAR40, VAR1->VAR41))\ngoto VAR39;\nFUN3(&VAR42);\nif (VAR43->VAR44)\nVAR43->FUN26();\nif (!VAR45)\nFUN27(&VAR33);\n#ifdef VAR46\nif (VAR1->VAR47)\nFUN22(VAR1->VAR47, 1);\nelse if ((!VAR45) && VAR48)\nFUN28();\n#VAR49\nVAR10 = VAR50;\nwhile (VAR48 && --VAR10 &&\n(FUN21(&VAR14) + FUN21(&VAR15)) !=\nVAR9)\nFUN29(1000);\nif (!VAR10)\nFUN30(\"STR\");\nFUN31();\nVAR45 = 0;\nVAR51 = VAR21;\nVAR25 = 0;\nVAR8 = FUN32();\nif (VAR8)\nFUN33();\nwhile (1) {\nVAR28:\nif (VAR52) {\nVAR53 = 1;\nVAR6 = FUN34(VAR1);\nif (VAR6 == -1)\ncontinue;\nVAR53 = 0;\n} else {\nif (FUN35(VAR54)) {\nif (FUN36(VAR55)) {\nVAR52 = 1;\ncontinue;\n} else {\nbreak;\n}\n}\nVAR6 = FUN37(VAR1);\n}\nif (VAR6 == VAR56) {\nVAR52 = !VAR52;\n} else if (VAR6 == VAR57) {\nVAR11[VAR58].VAR3 |=\nVAR30;\ngoto VAR29;\n} else {\nVAR11[VAR7].VAR22 = VAR6;\nbreak;\n}\n}\nFUN38();\nif (VAR43->VAR59)\nVAR43->FUN39();\nFUN16(&VAR42);\nif (!VAR45) {\nFUN23(&VAR33);\nwhile (VAR48 && FUN21(&VAR15))\nFUN20();\n}\nVAR39:\nif (FUN21(&VAR36) != -1) {\nint VAR60 = FUN21(&VAR36);\nif (VAR11[VAR60].VAR20)\nVAR38 = VAR11[VAR60].VAR20->VAR37;\nelse\nVAR38 = 0;\n}\nif (VAR16.VAR35)\nVAR16.FUN13();\nif (VAR8)\nFUN14();\nVAR11[VAR7].VAR19 = NULL;\nVAR11[VAR7].VAR20 = NULL;\nVAR11[VAR7].VAR3 &=\n~(VAR13 | VAR32);\nVAR11[VAR7].VAR12--;\nFUN15();\nFUN16(&VAR14);\nFUN22(&VAR27, -1);\nFUN23(&VAR26);\nFUN17();\nFUN18(VAR4);\nFUN19();\nreturn VAR11[VAR7].VAR22;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct kgdb_state *VAR1, struct pt_regs *VAR2,\nint VAR3)\n{\nunsigned long VAR4;\nint VAR5 = 100;\nint VAR6;\nint VAR7;\nint VAR8 = 0;\nint VAR9 = FUN2();\nu64 VAR10;\nVAR11[VAR1->VAR7].VAR12++;\nVAR11[VAR1->VAR7].VAR3 |= VAR3;\nif (VAR3 == VAR13)\nFUN3(&VAR14);\nelse\nFUN3(&VAR15);\nif (VAR16.VAR17)\nVAR16.FUN4(VAR2);\nVAR18:\nFUN5();\nFUN6(VAR4);\nVAR7 = VAR1->VAR7;\nVAR11[VAR7].VAR19 = VAR2;\nVAR11[VAR7].VAR20 = VAR21;\nVAR11[VAR7].VAR22 = 0;\nVAR11[VAR7].VAR23 = FUN7() >> VAR24;\nFUN8();\nif (VAR25 == 1) {\nif (FUN9(&VAR26))\nFUN10(&VAR27, VAR7);\ngoto VAR28;\n}\nwhile (1) {\nVAR29:\nif (VAR11[VAR7].VAR3 & VAR30) {\nVAR11[VAR7].VAR3 &= ~VAR30;\ngoto VAR28;\n} else if (VAR11[VAR7].VAR3 & VAR13) {\nif (FUN9(&VAR26)) {\nFUN10(&VAR27, VAR7);\nbreak;\n}\n} else if (VAR11[VAR7].VAR3 & VAR31) {\nFUN11();\nVAR11[VAR7].VAR3 &= ~VAR31;\n} else if (VAR11[VAR7].VAR3 & VAR32) {\nif (!FUN12(&VAR33))\ngoto VAR34;\n} else {\nVAR34:\nif (VAR16.VAR35)\nVAR16.FUN13();\nif (VAR8)\nFUN14();\nVAR11[VAR7].VAR19 = NULL;\nVAR11[VAR7].VAR20 = NULL;\nVAR11[VAR7].VAR3 &=\n~(VAR13 | VAR32);\nVAR11[VAR7].VAR12--;\nFUN15();\nFUN16(&VAR15);\nFUN17();\nFUN18(VAR4);\nFUN19();\nreturn 0;\n}\nFUN20();\n}\nif (FUN21(&VAR36) != -1 &&\n(VAR11[VAR7].VAR20 &&\nVAR11[VAR7].VAR20->VAR37 != VAR38) && --VAR5) {\nFUN22(&VAR27, -1);\nFUN23(&VAR26);\nFUN17();\nFUN18(VAR4);\nFUN19();\ngoto VAR18;\n}\nif (!FUN24(1)) {\nVAR11[VAR7].VAR22 = 1;\ngoto VAR39; \n}\nif (FUN25(VAR1->VAR40, VAR1->VAR41))\ngoto VAR39;\nFUN3(&VAR42);\nif (VAR43->VAR44)\nVAR43->FUN26();\nif (!VAR45)\nFUN27(&VAR33);\n#ifdef VAR46\nif (VAR1->VAR47)\nFUN22(VAR1->VAR47, 1);\nelse if ((!VAR45) && VAR48)\nFUN28();\n#VAR49\nVAR10 = VAR50;\nwhile (VAR48 && --VAR10 &&\n(FUN21(&VAR14) + FUN21(&VAR15)) !=\nVAR9)\nFUN29(1000);\nif (!VAR10)\nFUN30(\"STR\");\nFUN31();\nVAR45 = 0;\nVAR51 = VAR21;\nVAR25 = 0;\nVAR8 = FUN32();\nif (VAR8)\nFUN33();\nwhile (1) {\nVAR28:\nif (VAR52) {\nVAR53 = 1;\nVAR6 = FUN34(VAR1);\nif (VAR6 == -1)\ncontinue;\nVAR53 = 0;\n} else {\nVAR6 = FUN35(VAR1);\n}\nif (VAR6 == VAR54) {\nVAR52 = !VAR52;\n} else if (VAR6 == VAR55) {\nVAR11[VAR56].VAR3 |=\nVAR30;\ngoto VAR29;\n} else {\nVAR11[VAR7].VAR22 = VAR6;\nbreak;\n}\n}\nFUN36();\nif (VAR43->VAR57)\nVAR43->FUN37();\nFUN16(&VAR42);\nif (!VAR45) {\nFUN23(&VAR33);\nwhile (VAR48 && FUN21(&VAR15))\nFUN20();\n}\nVAR39:\nif (FUN21(&VAR36) != -1) {\nint VAR58 = FUN21(&VAR36);\nif (VAR11[VAR58].VAR20)\nVAR38 = VAR11[VAR58].VAR20->VAR37;\nelse\nVAR38 = 0;\n}\nif (VAR16.VAR35)\nVAR16.FUN13();\nif (VAR8)\nFUN14();\nVAR11[VAR7].VAR19 = NULL;\nVAR11[VAR7].VAR20 = NULL;\nVAR11[VAR7].VAR3 &=\n~(VAR13 | VAR32);\nVAR11[VAR7].VAR12--;\nFUN15();\nFUN16(&VAR14);\nFUN22(&VAR27, -1);\nFUN23(&VAR26);\nFUN17();\nFUN18(VAR4);\nFUN19();\nreturn VAR11[VAR7].VAR22;\n}\n",
      "code_after_change_raw": "static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,\nint exception_state)\n{\nunsigned long flags;\nint sstep_tries = 100;\nint error;\nint cpu;\nint trace_on = 0;\nint online_cpus = num_online_cpus();\nu64 time_left;\nkgdb_info[ks->cpu].enter_kgdb++;\nkgdb_info[ks->cpu].exception_state |= exception_state;\nif (exception_state == DCPU_WANT_MASTER)\natomic_inc(&masters_in_kgdb);\nelse\natomic_inc(&slaves_in_kgdb);\nif (arch_kgdb_ops.disable_hw_break)\narch_kgdb_ops.disable_hw_break(regs);\nacquirelock:\nrcu_read_lock();\nlocal_irq_save(flags);\ncpu = ks->cpu;\nkgdb_info[cpu].debuggerinfo = regs;\nkgdb_info[cpu].task = current;\nkgdb_info[cpu].ret_state = 0;\nkgdb_info[cpu].irq_depth = hardirq_count() >> HARDIRQ_SHIFT;\nsmp_mb();\nif (exception_level == 1) {\nif (raw_spin_trylock(&dbg_master_lock))\natomic_xchg(&kgdb_active, cpu);\ngoto cpu_master_loop;\n}\nwhile (1) {\ncpu_loop:\nif (kgdb_info[cpu].exception_state & DCPU_NEXT_MASTER) {\nkgdb_info[cpu].exception_state &= ~DCPU_NEXT_MASTER;\ngoto cpu_master_loop;\n} else if (kgdb_info[cpu].exception_state & DCPU_WANT_MASTER) {\nif (raw_spin_trylock(&dbg_master_lock)) {\natomic_xchg(&kgdb_active, cpu);\nbreak;\n}\n} else if (kgdb_info[cpu].exception_state & DCPU_WANT_BT) {\ndump_stack();\nkgdb_info[cpu].exception_state &= ~DCPU_WANT_BT;\n} else if (kgdb_info[cpu].exception_state & DCPU_IS_SLAVE) {\nif (!raw_spin_is_locked(&dbg_slave_lock))\ngoto return_normal;\n} else {\nreturn_normal:\nif (arch_kgdb_ops.correct_hw_break)\narch_kgdb_ops.correct_hw_break();\nif (trace_on)\ntracing_on();\nkgdb_info[cpu].debuggerinfo = NULL;\nkgdb_info[cpu].task = NULL;\nkgdb_info[cpu].exception_state &=\n~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\nkgdb_info[cpu].enter_kgdb--;\nsmp_mb__before_atomic();\natomic_dec(&slaves_in_kgdb);\ndbg_touch_watchdogs();\nlocal_irq_restore(flags);\nrcu_read_unlock();\nreturn 0;\n}\ncpu_relax();\n}\nif (atomic_read(&kgdb_cpu_doing_single_step) != -1 &&\n(kgdb_info[cpu].task &&\nkgdb_info[cpu].task->pid != kgdb_sstep_pid) && --sstep_tries) {\natomic_set(&kgdb_active, -1);\nraw_spin_unlock(&dbg_master_lock);\ndbg_touch_watchdogs();\nlocal_irq_restore(flags);\nrcu_read_unlock();\ngoto acquirelock;\n}\nif (!kgdb_io_ready(1)) {\nkgdb_info[cpu].ret_state = 1;\ngoto kgdb_restore; \n}\nif (kgdb_skipexception(ks->ex_vector, ks->linux_regs))\ngoto kgdb_restore;\natomic_inc(&ignore_console_lock_warning);\nif (dbg_io_ops->pre_exception)\ndbg_io_ops->pre_exception();\nif (!kgdb_single_step)\nraw_spin_lock(&dbg_slave_lock);\n#ifdef CONFIG_SMP\nif (ks->send_ready)\natomic_set(ks->send_ready, 1);\nelse if ((!kgdb_single_step) && kgdb_do_roundup)\nkgdb_roundup_cpus();\n#endif\ntime_left = MSEC_PER_SEC;\nwhile (kgdb_do_roundup && --time_left &&\n(atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=\nonline_cpus)\nudelay(1000);\nif (!time_left)\npr_crit(\"Timed out waiting for secondary CPUs.\\n\");\ndbg_deactivate_sw_breakpoints();\nkgdb_single_step = 0;\nkgdb_contthread = current;\nexception_level = 0;\ntrace_on = tracing_is_on();\nif (trace_on)\ntracing_off();\nwhile (1) {\ncpu_master_loop:\nif (dbg_kdb_mode) {\nkgdb_connected = 1;\nerror = kdb_stub(ks);\nif (error == -1)\ncontinue;\nkgdb_connected = 0;\n} else {\nif (security_locked_down(LOCKDOWN_DBG_WRITE_KERNEL)) {\nif (IS_ENABLED(CONFIG_KGDB_KDB)) {\ndbg_kdb_mode = 1;\ncontinue;\n} else {\nbreak;\n}\n}\nerror = gdb_serial_stub(ks);\n}\nif (error == DBG_PASS_EVENT) {\ndbg_kdb_mode = !dbg_kdb_mode;\n} else if (error == DBG_SWITCH_CPU_EVENT) {\nkgdb_info[dbg_switch_cpu].exception_state |=\nDCPU_NEXT_MASTER;\ngoto cpu_loop;\n} else {\nkgdb_info[cpu].ret_state = error;\nbreak;\n}\n}\ndbg_activate_sw_breakpoints();\nif (dbg_io_ops->post_exception)\ndbg_io_ops->post_exception();\natomic_dec(&ignore_console_lock_warning);\nif (!kgdb_single_step) {\nraw_spin_unlock(&dbg_slave_lock);\nwhile (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))\ncpu_relax();\n}\nkgdb_restore:\nif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\nint sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);\nif (kgdb_info[sstep_cpu].task)\nkgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;\nelse\nkgdb_sstep_pid = 0;\n}\nif (arch_kgdb_ops.correct_hw_break)\narch_kgdb_ops.correct_hw_break();\nif (trace_on)\ntracing_on();\nkgdb_info[cpu].debuggerinfo = NULL;\nkgdb_info[cpu].task = NULL;\nkgdb_info[cpu].exception_state &=\n~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\nkgdb_info[cpu].enter_kgdb--;\nsmp_mb__before_atomic();\natomic_dec(&masters_in_kgdb);\natomic_set(&kgdb_active, -1);\nraw_spin_unlock(&dbg_master_lock);\ndbg_touch_watchdogs();\nlocal_irq_restore(flags);\nrcu_read_unlock();\nreturn kgdb_info[cpu].ret_state;\n}\n",
      "code_before_change_raw": "static int kgdb_cpu_enter(struct kgdb_state *ks, struct pt_regs *regs,\nint exception_state)\n{\nunsigned long flags;\nint sstep_tries = 100;\nint error;\nint cpu;\nint trace_on = 0;\nint online_cpus = num_online_cpus();\nu64 time_left;\nkgdb_info[ks->cpu].enter_kgdb++;\nkgdb_info[ks->cpu].exception_state |= exception_state;\nif (exception_state == DCPU_WANT_MASTER)\natomic_inc(&masters_in_kgdb);\nelse\natomic_inc(&slaves_in_kgdb);\nif (arch_kgdb_ops.disable_hw_break)\narch_kgdb_ops.disable_hw_break(regs);\nacquirelock:\nrcu_read_lock();\nlocal_irq_save(flags);\ncpu = ks->cpu;\nkgdb_info[cpu].debuggerinfo = regs;\nkgdb_info[cpu].task = current;\nkgdb_info[cpu].ret_state = 0;\nkgdb_info[cpu].irq_depth = hardirq_count() >> HARDIRQ_SHIFT;\nsmp_mb();\nif (exception_level == 1) {\nif (raw_spin_trylock(&dbg_master_lock))\natomic_xchg(&kgdb_active, cpu);\ngoto cpu_master_loop;\n}\nwhile (1) {\ncpu_loop:\nif (kgdb_info[cpu].exception_state & DCPU_NEXT_MASTER) {\nkgdb_info[cpu].exception_state &= ~DCPU_NEXT_MASTER;\ngoto cpu_master_loop;\n} else if (kgdb_info[cpu].exception_state & DCPU_WANT_MASTER) {\nif (raw_spin_trylock(&dbg_master_lock)) {\natomic_xchg(&kgdb_active, cpu);\nbreak;\n}\n} else if (kgdb_info[cpu].exception_state & DCPU_WANT_BT) {\ndump_stack();\nkgdb_info[cpu].exception_state &= ~DCPU_WANT_BT;\n} else if (kgdb_info[cpu].exception_state & DCPU_IS_SLAVE) {\nif (!raw_spin_is_locked(&dbg_slave_lock))\ngoto return_normal;\n} else {\nreturn_normal:\nif (arch_kgdb_ops.correct_hw_break)\narch_kgdb_ops.correct_hw_break();\nif (trace_on)\ntracing_on();\nkgdb_info[cpu].debuggerinfo = NULL;\nkgdb_info[cpu].task = NULL;\nkgdb_info[cpu].exception_state &=\n~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\nkgdb_info[cpu].enter_kgdb--;\nsmp_mb__before_atomic();\natomic_dec(&slaves_in_kgdb);\ndbg_touch_watchdogs();\nlocal_irq_restore(flags);\nrcu_read_unlock();\nreturn 0;\n}\ncpu_relax();\n}\nif (atomic_read(&kgdb_cpu_doing_single_step) != -1 &&\n(kgdb_info[cpu].task &&\nkgdb_info[cpu].task->pid != kgdb_sstep_pid) && --sstep_tries) {\natomic_set(&kgdb_active, -1);\nraw_spin_unlock(&dbg_master_lock);\ndbg_touch_watchdogs();\nlocal_irq_restore(flags);\nrcu_read_unlock();\ngoto acquirelock;\n}\nif (!kgdb_io_ready(1)) {\nkgdb_info[cpu].ret_state = 1;\ngoto kgdb_restore; \n}\nif (kgdb_skipexception(ks->ex_vector, ks->linux_regs))\ngoto kgdb_restore;\natomic_inc(&ignore_console_lock_warning);\nif (dbg_io_ops->pre_exception)\ndbg_io_ops->pre_exception();\nif (!kgdb_single_step)\nraw_spin_lock(&dbg_slave_lock);\n#ifdef CONFIG_SMP\nif (ks->send_ready)\natomic_set(ks->send_ready, 1);\nelse if ((!kgdb_single_step) && kgdb_do_roundup)\nkgdb_roundup_cpus();\n#endif\ntime_left = MSEC_PER_SEC;\nwhile (kgdb_do_roundup && --time_left &&\n(atomic_read(&masters_in_kgdb) + atomic_read(&slaves_in_kgdb)) !=\nonline_cpus)\nudelay(1000);\nif (!time_left)\npr_crit(\"Timed out waiting for secondary CPUs.\\n\");\ndbg_deactivate_sw_breakpoints();\nkgdb_single_step = 0;\nkgdb_contthread = current;\nexception_level = 0;\ntrace_on = tracing_is_on();\nif (trace_on)\ntracing_off();\nwhile (1) {\ncpu_master_loop:\nif (dbg_kdb_mode) {\nkgdb_connected = 1;\nerror = kdb_stub(ks);\nif (error == -1)\ncontinue;\nkgdb_connected = 0;\n} else {\nerror = gdb_serial_stub(ks);\n}\nif (error == DBG_PASS_EVENT) {\ndbg_kdb_mode = !dbg_kdb_mode;\n} else if (error == DBG_SWITCH_CPU_EVENT) {\nkgdb_info[dbg_switch_cpu].exception_state |=\nDCPU_NEXT_MASTER;\ngoto cpu_loop;\n} else {\nkgdb_info[cpu].ret_state = error;\nbreak;\n}\n}\ndbg_activate_sw_breakpoints();\nif (dbg_io_ops->post_exception)\ndbg_io_ops->post_exception();\natomic_dec(&ignore_console_lock_warning);\nif (!kgdb_single_step) {\nraw_spin_unlock(&dbg_slave_lock);\nwhile (kgdb_do_roundup && atomic_read(&slaves_in_kgdb))\ncpu_relax();\n}\nkgdb_restore:\nif (atomic_read(&kgdb_cpu_doing_single_step) != -1) {\nint sstep_cpu = atomic_read(&kgdb_cpu_doing_single_step);\nif (kgdb_info[sstep_cpu].task)\nkgdb_sstep_pid = kgdb_info[sstep_cpu].task->pid;\nelse\nkgdb_sstep_pid = 0;\n}\nif (arch_kgdb_ops.correct_hw_break)\narch_kgdb_ops.correct_hw_break();\nif (trace_on)\ntracing_on();\nkgdb_info[cpu].debuggerinfo = NULL;\nkgdb_info[cpu].task = NULL;\nkgdb_info[cpu].exception_state &=\n~(DCPU_WANT_MASTER | DCPU_IS_SLAVE);\nkgdb_info[cpu].enter_kgdb--;\nsmp_mb__before_atomic();\natomic_dec(&masters_in_kgdb);\natomic_set(&kgdb_active, -1);\nraw_spin_unlock(&dbg_master_lock);\ndbg_touch_watchdogs();\nlocal_irq_restore(flags);\nrcu_read_unlock();\nreturn kgdb_info[cpu].ret_state;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of lockdown mode enforcement in the debugger code, specifically during kernel memory access.",
        "trigger_condition": "An attacker with access to a serial port triggers the debugger, leading to unauthorized read and write access to kernel memory.",
        "specific_code_behavior_causing_vulnerability": "The debugger code does not check for lockdown mode before allowing access to kernel memory, creating a security vulnerability that can be exploited by an attacker with serial port access."
      },
      "solution": "To mitigate the vulnerability, it is necessary to enforce lockdown mode in the debugger code to restrict access to kernel memory when triggered. This involves adding a function call to check for lockdown mode before allowing any read or write operations on kernel memory. By implementing this check, the debugger will respect the lockdown mode and prevent unauthorized access to sensitive kernel memory areas.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2022-21499, which involves KGDB and KDB allowing read and write access to kernel memory. To mitigate this vulnerability, it is important to restrict KGDB and KDB during lockdown. By adding the line `kdb_check_for_lockdown();` in the `kdb_local` function, the debugger will respect the lockdown mode when triggered, thus preventing unauthorized access to kernel memory via KGDB and KDB. This modification enhances the security of the system by ensuring that the debugger behaves appropriately in lockdown scenarios, reducing the risk of unauthorized access and potential exploits.",
      "GPT_purpose": "Handle local kernel debugger operations and user interactions.",
      "GPT_function": "\n1. Handling various reasons for entering the kdb debugger.\n2. Processing user input commands for debugging purposes.\n3. Checking and responding to different debug trap results.\n4. Managing the debugger state and history of commands entered.",
      "CVE_id": "CVE-2022-21499",
      "code_before_change": "static int kdb_local(kdb_reason_t reason, int error, struct pt_regs *regs,\n\t\t     kdb_dbtrap_t db_result)\n{\n\tchar *cmdbuf;\n\tint diag;\n\tstruct task_struct *kdb_current =\n\t\tkdb_curr_task(raw_smp_processor_id());\n\n\tKDB_DEBUG_STATE(\"kdb_local 1\", reason);\n\tkdb_go_count = 0;\n\tif (reason == KDB_REASON_DEBUG) {\n\t\t/* special case below */\n\t} else {\n\t\tkdb_printf(\"\\nEntering kdb (current=0x%px, pid %d) \",\n\t\t\t   kdb_current, kdb_current ? kdb_current->pid : 0);\n#if defined(CONFIG_SMP)\n\t\tkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\n\t}\n\n\tswitch (reason) {\n\tcase KDB_REASON_DEBUG:\n\t{\n\t\t/*\n\t\t * If re-entering kdb after a single step\n\t\t * command, don't print the message.\n\t\t */\n\t\tswitch (db_result) {\n\t\tcase KDB_DB_BPT:\n\t\t\tkdb_printf(\"\\nEntering kdb (0x%px, pid %d) \",\n\t\t\t\t   kdb_current, kdb_current->pid);\n#if defined(CONFIG_SMP)\n\t\t\tkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\n\t\t\tkdb_printf(\"due to Debug @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t\t   instruction_pointer(regs));\n\t\t\tbreak;\n\t\tcase KDB_DB_SS:\n\t\t\tbreak;\n\t\tcase KDB_DB_SSBPT:\n\t\t\tKDB_DEBUG_STATE(\"kdb_local 4\", reason);\n\t\t\treturn 1;\t/* kdba_db_trap did the work */\n\t\tdefault:\n\t\t\tkdb_printf(\"kdb: Bad result from kdba_db_trap: %d\\n\",\n\t\t\t\t   db_result);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\t\tbreak;\n\tcase KDB_REASON_ENTER:\n\t\tif (KDB_STATE(KEYBOARD))\n\t\t\tkdb_printf(\"due to Keyboard Entry\\n\");\n\t\telse\n\t\t\tkdb_printf(\"due to KDB_ENTER()\\n\");\n\t\tbreak;\n\tcase KDB_REASON_KEYBOARD:\n\t\tKDB_STATE_SET(KEYBOARD);\n\t\tkdb_printf(\"due to Keyboard Entry\\n\");\n\t\tbreak;\n\tcase KDB_REASON_ENTER_SLAVE:\n\t\t/* drop through, slaves only get released via cpu switch */\n\tcase KDB_REASON_SWITCH:\n\t\tkdb_printf(\"due to cpu switch\\n\");\n\t\tbreak;\n\tcase KDB_REASON_OOPS:\n\t\tkdb_printf(\"Oops: %s\\n\", kdb_diemsg);\n\t\tkdb_printf(\"due to oops @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t   instruction_pointer(regs));\n\t\tkdb_dumpregs(regs);\n\t\tbreak;\n\tcase KDB_REASON_SYSTEM_NMI:\n\t\tkdb_printf(\"due to System NonMaskable Interrupt\\n\");\n\t\tbreak;\n\tcase KDB_REASON_NMI:\n\t\tkdb_printf(\"due to NonMaskable Interrupt @ \"\n\t\t\t   kdb_machreg_fmt \"\\n\",\n\t\t\t   instruction_pointer(regs));\n\t\tbreak;\n\tcase KDB_REASON_SSTEP:\n\tcase KDB_REASON_BREAK:\n\t\tkdb_printf(\"due to %s @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t   reason == KDB_REASON_BREAK ?\n\t\t\t   \"Breakpoint\" : \"SS trap\", instruction_pointer(regs));\n\t\t/*\n\t\t * Determine if this breakpoint is one that we\n\t\t * are interested in.\n\t\t */\n\t\tif (db_result != KDB_DB_BPT) {\n\t\t\tkdb_printf(\"kdb: error return from kdba_bp_trap: %d\\n\",\n\t\t\t\t   db_result);\n\t\t\tKDB_DEBUG_STATE(\"kdb_local 6\", reason);\n\t\t\treturn 0;\t/* Not for us, dismiss it */\n\t\t}\n\t\tbreak;\n\tcase KDB_REASON_RECURSE:\n\t\tkdb_printf(\"due to Recursion @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t   instruction_pointer(regs));\n\t\tbreak;\n\tdefault:\n\t\tkdb_printf(\"kdb: unexpected reason code: %d\\n\", reason);\n\t\tKDB_DEBUG_STATE(\"kdb_local 8\", reason);\n\t\treturn 0;\t/* Not for us, dismiss it */\n\t}\n\n\twhile (1) {\n\t\t/*\n\t\t * Initialize pager context.\n\t\t */\n\t\tkdb_nextline = 1;\n\t\tKDB_STATE_CLEAR(SUPPRESS);\n\t\tkdb_grepping_flag = 0;\n\t\t/* ensure the old search does not leak into '/' commands */\n\t\tkdb_grep_string[0] = '\\0';\n\n\t\tcmdbuf = cmd_cur;\n\t\t*cmdbuf = '\\0';\n\t\t*(cmd_hist[cmd_head]) = '\\0';\n\ndo_full_getstr:\n\t\t/* PROMPT can only be set if we have MEM_READ permission. */\n\t\tsnprintf(kdb_prompt_str, CMD_BUFLEN, kdbgetenv(\"PROMPT\"),\n\t\t\t raw_smp_processor_id());\n\t\tif (defcmd_in_progress)\n\t\t\tstrncat(kdb_prompt_str, \"[defcmd]\", CMD_BUFLEN);\n\n\t\t/*\n\t\t * Fetch command from keyboard\n\t\t */\n\t\tcmdbuf = kdb_getstr(cmdbuf, CMD_BUFLEN, kdb_prompt_str);\n\t\tif (*cmdbuf != '\\n') {\n\t\t\tif (*cmdbuf < 32) {\n\t\t\t\tif (cmdptr == cmd_head) {\n\t\t\t\t\tstrscpy(cmd_hist[cmd_head], cmd_cur,\n\t\t\t\t\t\tCMD_BUFLEN);\n\t\t\t\t\t*(cmd_hist[cmd_head] +\n\t\t\t\t\t  strlen(cmd_hist[cmd_head])-1) = '\\0';\n\t\t\t\t}\n\t\t\t\tif (!handle_ctrl_cmd(cmdbuf))\n\t\t\t\t\t*(cmd_cur+strlen(cmd_cur)-1) = '\\0';\n\t\t\t\tcmdbuf = cmd_cur;\n\t\t\t\tgoto do_full_getstr;\n\t\t\t} else {\n\t\t\t\tstrscpy(cmd_hist[cmd_head], cmd_cur,\n\t\t\t\t\tCMD_BUFLEN);\n\t\t\t}\n\n\t\t\tcmd_head = (cmd_head+1) % KDB_CMD_HISTORY_COUNT;\n\t\t\tif (cmd_head == cmd_tail)\n\t\t\t\tcmd_tail = (cmd_tail+1) % KDB_CMD_HISTORY_COUNT;\n\t\t}\n\n\t\tcmdptr = cmd_head;\n\t\tdiag = kdb_parse(cmdbuf);\n\t\tif (diag == KDB_NOTFOUND) {\n\t\t\tdrop_newline(cmdbuf);\n\t\t\tkdb_printf(\"Unknown kdb command: '%s'\\n\", cmdbuf);\n\t\t\tdiag = 0;\n\t\t}\n\t\tif (diag == KDB_CMD_GO\n\t\t || diag == KDB_CMD_CPU\n\t\t || diag == KDB_CMD_SS\n\t\t || diag == KDB_CMD_KGDB)\n\t\t\tbreak;\n\n\t\tif (diag)\n\t\t\tkdb_cmderror(diag);\n\t}\n\tKDB_DEBUG_STATE(\"kdb_local 9\", diag);\n\treturn diag;\n}",
      "code_after_change": "static int kdb_local(kdb_reason_t reason, int error, struct pt_regs *regs,\n\t\t     kdb_dbtrap_t db_result)\n{\n\tchar *cmdbuf;\n\tint diag;\n\tstruct task_struct *kdb_current =\n\t\tkdb_curr_task(raw_smp_processor_id());\n\n\tKDB_DEBUG_STATE(\"kdb_local 1\", reason);\n\n\tkdb_check_for_lockdown();\n\n\tkdb_go_count = 0;\n\tif (reason == KDB_REASON_DEBUG) {\n\t\t/* special case below */\n\t} else {\n\t\tkdb_printf(\"\\nEntering kdb (current=0x%px, pid %d) \",\n\t\t\t   kdb_current, kdb_current ? kdb_current->pid : 0);\n#if defined(CONFIG_SMP)\n\t\tkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\n\t}\n\n\tswitch (reason) {\n\tcase KDB_REASON_DEBUG:\n\t{\n\t\t/*\n\t\t * If re-entering kdb after a single step\n\t\t * command, don't print the message.\n\t\t */\n\t\tswitch (db_result) {\n\t\tcase KDB_DB_BPT:\n\t\t\tkdb_printf(\"\\nEntering kdb (0x%px, pid %d) \",\n\t\t\t\t   kdb_current, kdb_current->pid);\n#if defined(CONFIG_SMP)\n\t\t\tkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\n\t\t\tkdb_printf(\"due to Debug @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t\t   instruction_pointer(regs));\n\t\t\tbreak;\n\t\tcase KDB_DB_SS:\n\t\t\tbreak;\n\t\tcase KDB_DB_SSBPT:\n\t\t\tKDB_DEBUG_STATE(\"kdb_local 4\", reason);\n\t\t\treturn 1;\t/* kdba_db_trap did the work */\n\t\tdefault:\n\t\t\tkdb_printf(\"kdb: Bad result from kdba_db_trap: %d\\n\",\n\t\t\t\t   db_result);\n\t\t\tbreak;\n\t\t}\n\n\t}\n\t\tbreak;\n\tcase KDB_REASON_ENTER:\n\t\tif (KDB_STATE(KEYBOARD))\n\t\t\tkdb_printf(\"due to Keyboard Entry\\n\");\n\t\telse\n\t\t\tkdb_printf(\"due to KDB_ENTER()\\n\");\n\t\tbreak;\n\tcase KDB_REASON_KEYBOARD:\n\t\tKDB_STATE_SET(KEYBOARD);\n\t\tkdb_printf(\"due to Keyboard Entry\\n\");\n\t\tbreak;\n\tcase KDB_REASON_ENTER_SLAVE:\n\t\t/* drop through, slaves only get released via cpu switch */\n\tcase KDB_REASON_SWITCH:\n\t\tkdb_printf(\"due to cpu switch\\n\");\n\t\tbreak;\n\tcase KDB_REASON_OOPS:\n\t\tkdb_printf(\"Oops: %s\\n\", kdb_diemsg);\n\t\tkdb_printf(\"due to oops @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t   instruction_pointer(regs));\n\t\tkdb_dumpregs(regs);\n\t\tbreak;\n\tcase KDB_REASON_SYSTEM_NMI:\n\t\tkdb_printf(\"due to System NonMaskable Interrupt\\n\");\n\t\tbreak;\n\tcase KDB_REASON_NMI:\n\t\tkdb_printf(\"due to NonMaskable Interrupt @ \"\n\t\t\t   kdb_machreg_fmt \"\\n\",\n\t\t\t   instruction_pointer(regs));\n\t\tbreak;\n\tcase KDB_REASON_SSTEP:\n\tcase KDB_REASON_BREAK:\n\t\tkdb_printf(\"due to %s @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t   reason == KDB_REASON_BREAK ?\n\t\t\t   \"Breakpoint\" : \"SS trap\", instruction_pointer(regs));\n\t\t/*\n\t\t * Determine if this breakpoint is one that we\n\t\t * are interested in.\n\t\t */\n\t\tif (db_result != KDB_DB_BPT) {\n\t\t\tkdb_printf(\"kdb: error return from kdba_bp_trap: %d\\n\",\n\t\t\t\t   db_result);\n\t\t\tKDB_DEBUG_STATE(\"kdb_local 6\", reason);\n\t\t\treturn 0;\t/* Not for us, dismiss it */\n\t\t}\n\t\tbreak;\n\tcase KDB_REASON_RECURSE:\n\t\tkdb_printf(\"due to Recursion @ \" kdb_machreg_fmt \"\\n\",\n\t\t\t   instruction_pointer(regs));\n\t\tbreak;\n\tdefault:\n\t\tkdb_printf(\"kdb: unexpected reason code: %d\\n\", reason);\n\t\tKDB_DEBUG_STATE(\"kdb_local 8\", reason);\n\t\treturn 0;\t/* Not for us, dismiss it */\n\t}\n\n\twhile (1) {\n\t\t/*\n\t\t * Initialize pager context.\n\t\t */\n\t\tkdb_nextline = 1;\n\t\tKDB_STATE_CLEAR(SUPPRESS);\n\t\tkdb_grepping_flag = 0;\n\t\t/* ensure the old search does not leak into '/' commands */\n\t\tkdb_grep_string[0] = '\\0';\n\n\t\tcmdbuf = cmd_cur;\n\t\t*cmdbuf = '\\0';\n\t\t*(cmd_hist[cmd_head]) = '\\0';\n\ndo_full_getstr:\n\t\t/* PROMPT can only be set if we have MEM_READ permission. */\n\t\tsnprintf(kdb_prompt_str, CMD_BUFLEN, kdbgetenv(\"PROMPT\"),\n\t\t\t raw_smp_processor_id());\n\t\tif (defcmd_in_progress)\n\t\t\tstrncat(kdb_prompt_str, \"[defcmd]\", CMD_BUFLEN);\n\n\t\t/*\n\t\t * Fetch command from keyboard\n\t\t */\n\t\tcmdbuf = kdb_getstr(cmdbuf, CMD_BUFLEN, kdb_prompt_str);\n\t\tif (*cmdbuf != '\\n') {\n\t\t\tif (*cmdbuf < 32) {\n\t\t\t\tif (cmdptr == cmd_head) {\n\t\t\t\t\tstrscpy(cmd_hist[cmd_head], cmd_cur,\n\t\t\t\t\t\tCMD_BUFLEN);\n\t\t\t\t\t*(cmd_hist[cmd_head] +\n\t\t\t\t\t  strlen(cmd_hist[cmd_head])-1) = '\\0';\n\t\t\t\t}\n\t\t\t\tif (!handle_ctrl_cmd(cmdbuf))\n\t\t\t\t\t*(cmd_cur+strlen(cmd_cur)-1) = '\\0';\n\t\t\t\tcmdbuf = cmd_cur;\n\t\t\t\tgoto do_full_getstr;\n\t\t\t} else {\n\t\t\t\tstrscpy(cmd_hist[cmd_head], cmd_cur,\n\t\t\t\t\tCMD_BUFLEN);\n\t\t\t}\n\n\t\t\tcmd_head = (cmd_head+1) % KDB_CMD_HISTORY_COUNT;\n\t\t\tif (cmd_head == cmd_tail)\n\t\t\t\tcmd_tail = (cmd_tail+1) % KDB_CMD_HISTORY_COUNT;\n\t\t}\n\n\t\tcmdptr = cmd_head;\n\t\tdiag = kdb_parse(cmdbuf);\n\t\tif (diag == KDB_NOTFOUND) {\n\t\t\tdrop_newline(cmdbuf);\n\t\t\tkdb_printf(\"Unknown kdb command: '%s'\\n\", cmdbuf);\n\t\t\tdiag = 0;\n\t\t}\n\t\tif (diag == KDB_CMD_GO\n\t\t || diag == KDB_CMD_CPU\n\t\t || diag == KDB_CMD_SS\n\t\t || diag == KDB_CMD_KGDB)\n\t\t\tbreak;\n\n\t\tif (diag)\n\t\t\tkdb_cmderror(diag);\n\t}\n\tKDB_DEBUG_STATE(\"kdb_local 9\", diag);\n\treturn diag;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tkdb_check_for_lockdown();",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of lockdown mode enforcement in the debugger code, specifically during kernel memory access.",
      "trigger_condition": "An attacker with access to a serial port triggers the debugger, leading to unauthorized read and write access to kernel memory.",
      "specific_code_behavior_causing_vulnerability": "The debugger code does not check for lockdown mode before allowing access to kernel memory, creating a security vulnerability that can be exploited by an attacker with serial port access.",
      "id": 135,
      "code_after_change_normalized": "static int FUN1(kdb_reason_t VAR1, int VAR2, struct pt_regs *VAR3,\nkdb_dbtrap_t VAR4)\n{\nchar *VAR5;\nint VAR6;\nstruct task_struct *VAR7 =\nFUN2(FUN3());\nFUN4(\"STR\", VAR1);\nFUN5();\nVAR8 = 0;\nif (VAR1 == VAR9) {\n} else {\nFUN6(\"STR\",\nVAR7, VAR7 ? VAR7->VAR10 : 0);\n#if FUN7(VAR11)\nFUN6(\"STR\", FUN3());\n#VAR12\n}\nswitch (VAR1) {\ncase VAR9:\n{\nswitch (VAR4) {\ncase VAR13:\nFUN6(\"STR\",\nVAR7, VAR7->VAR10);\n#if FUN7(VAR11)\nFUN6(\"STR\", FUN3());\n#VAR12\nFUN6(\"STR\" VAR14 \"STR\",\nFUN8(VAR3));\nbreak;\ncase VAR15:\nbreak;\ncase VAR16:\nFUN4(\"STR\", VAR1);\nreturn 1;\t\ndefault:\nFUN6(\"STR\",\nVAR4);\nbreak;\n}\n}\nbreak;\ncase VAR17:\nif (FUN9(VAR18))\nFUN6(\"STR\");\nelse\nFUN6(\"STR\");\nbreak;\ncase VAR19:\nFUN10(VAR18);\nFUN6(\"STR\");\nbreak;\ncase VAR20:\ncase VAR21:\nFUN6(\"STR\");\nbreak;\ncase VAR22:\nFUN6(\"STR\", VAR23);\nFUN6(\"STR\" VAR14 \"STR\",\nFUN8(VAR3));\nFUN11(VAR3);\nbreak;\ncase VAR24:\nFUN6(\"STR\");\nbreak;\ncase VAR25:\nFUN6(\"STR\"\nVAR14 \"STR\",\nFUN8(VAR3));\nbreak;\ncase VAR26:\ncase VAR27:\nFUN6(\"STR\" VAR14 \"STR\",\nVAR1 == VAR27 ?\n\"STR\" : \"STR\", FUN8(VAR3));\nif (VAR4 != VAR13) {\nFUN6(\"STR\",\nVAR4);\nFUN4(\"STR\", VAR1);\nreturn 0;\t\n}\nbreak;\ncase VAR28:\nFUN6(\"STR\" VAR14 \"STR\",\nFUN8(VAR3));\nbreak;\ndefault:\nFUN6(\"STR\", VAR1);\nFUN4(\"STR\", VAR1);\nreturn 0;\t\n}\nwhile (1) {\nVAR29 = 1;\nFUN12(VAR30);\nVAR31 = 0;\nVAR32[0] = ;\nVAR5 = VAR33;\n*VAR5 = ;\n*(VAR34[VAR35]) = ;\nVAR36:\nFUN13(VAR37, VAR38, FUN14(\"STR\"),\nFUN3());\nif (VAR39)\nFUN15(VAR37, \"STR\", VAR38);\nVAR5 = FUN16(VAR5, VAR38, VAR37);\nif (*VAR5 != ) {\nif (*VAR5 < 32) {\nif (VAR40 == VAR35) {\nFUN17(VAR34[VAR35], VAR33,\nVAR38);\n*(VAR34[VAR35] +\nFUN18(VAR34[VAR35])-1) = ;\n}\nif (!FUN19(VAR5))\n*(VAR33+FUN18(VAR33)-1) = ;\nVAR5 = VAR33;\ngoto VAR36;\n} else {\nFUN17(VAR34[VAR35], VAR33,\nVAR38);\n}\nVAR35 = (VAR35+1) % VAR41;\nif (VAR35 == VAR42)\nVAR42 = (VAR42+1) % VAR41;\n}\nVAR40 = VAR35;\nVAR6 = FUN20(VAR5);\nif (VAR6 == VAR43) {\nFUN21(VAR5);\nFUN6(\"STR\", VAR5);\nVAR6 = 0;\n}\nif (VAR6 == VAR44\n|| VAR6 == VAR45\n|| VAR6 == VAR46\n|| VAR6 == VAR47)\nbreak;\nif (VAR6)\nFUN22(VAR6);\n}\nFUN4(\"STR\", VAR6);\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "static int FUN1(kdb_reason_t VAR1, int VAR2, struct pt_regs *VAR3,\nkdb_dbtrap_t VAR4)\n{\nchar *VAR5;\nint VAR6;\nstruct task_struct *VAR7 =\nFUN2(FUN3());\nFUN4(\"STR\", VAR1);\nVAR8 = 0;\nif (VAR1 == VAR9) {\n} else {\nFUN5(\"STR\",\nVAR7, VAR7 ? VAR7->VAR10 : 0);\n#if FUN6(VAR11)\nFUN5(\"STR\", FUN3());\n#VAR12\n}\nswitch (VAR1) {\ncase VAR9:\n{\nswitch (VAR4) {\ncase VAR13:\nFUN5(\"STR\",\nVAR7, VAR7->VAR10);\n#if FUN6(VAR11)\nFUN5(\"STR\", FUN3());\n#VAR12\nFUN5(\"STR\" VAR14 \"STR\",\nFUN7(VAR3));\nbreak;\ncase VAR15:\nbreak;\ncase VAR16:\nFUN4(\"STR\", VAR1);\nreturn 1;\t\ndefault:\nFUN5(\"STR\",\nVAR4);\nbreak;\n}\n}\nbreak;\ncase VAR17:\nif (FUN8(VAR18))\nFUN5(\"STR\");\nelse\nFUN5(\"STR\");\nbreak;\ncase VAR19:\nFUN9(VAR18);\nFUN5(\"STR\");\nbreak;\ncase VAR20:\ncase VAR21:\nFUN5(\"STR\");\nbreak;\ncase VAR22:\nFUN5(\"STR\", VAR23);\nFUN5(\"STR\" VAR14 \"STR\",\nFUN7(VAR3));\nFUN10(VAR3);\nbreak;\ncase VAR24:\nFUN5(\"STR\");\nbreak;\ncase VAR25:\nFUN5(\"STR\"\nVAR14 \"STR\",\nFUN7(VAR3));\nbreak;\ncase VAR26:\ncase VAR27:\nFUN5(\"STR\" VAR14 \"STR\",\nVAR1 == VAR27 ?\n\"STR\" : \"STR\", FUN7(VAR3));\nif (VAR4 != VAR13) {\nFUN5(\"STR\",\nVAR4);\nFUN4(\"STR\", VAR1);\nreturn 0;\t\n}\nbreak;\ncase VAR28:\nFUN5(\"STR\" VAR14 \"STR\",\nFUN7(VAR3));\nbreak;\ndefault:\nFUN5(\"STR\", VAR1);\nFUN4(\"STR\", VAR1);\nreturn 0;\t\n}\nwhile (1) {\nVAR29 = 1;\nFUN11(VAR30);\nVAR31 = 0;\nVAR32[0] = ;\nVAR5 = VAR33;\n*VAR5 = ;\n*(VAR34[VAR35]) = ;\nVAR36:\nFUN12(VAR37, VAR38, FUN13(\"STR\"),\nFUN3());\nif (VAR39)\nFUN14(VAR37, \"STR\", VAR38);\nVAR5 = FUN15(VAR5, VAR38, VAR37);\nif (*VAR5 != ) {\nif (*VAR5 < 32) {\nif (VAR40 == VAR35) {\nFUN16(VAR34[VAR35], VAR33,\nVAR38);\n*(VAR34[VAR35] +\nFUN17(VAR34[VAR35])-1) = ;\n}\nif (!FUN18(VAR5))\n*(VAR33+FUN17(VAR33)-1) = ;\nVAR5 = VAR33;\ngoto VAR36;\n} else {\nFUN16(VAR34[VAR35], VAR33,\nVAR38);\n}\nVAR35 = (VAR35+1) % VAR41;\nif (VAR35 == VAR42)\nVAR42 = (VAR42+1) % VAR41;\n}\nVAR40 = VAR35;\nVAR6 = FUN19(VAR5);\nif (VAR6 == VAR43) {\nFUN20(VAR5);\nFUN5(\"STR\", VAR5);\nVAR6 = 0;\n}\nif (VAR6 == VAR44\n|| VAR6 == VAR45\n|| VAR6 == VAR46\n|| VAR6 == VAR47)\nbreak;\nif (VAR6)\nFUN21(VAR6);\n}\nFUN4(\"STR\", VAR6);\nreturn VAR6;\n}\n",
      "code_after_change_raw": "static int kdb_local(kdb_reason_t reason, int error, struct pt_regs *regs,\nkdb_dbtrap_t db_result)\n{\nchar *cmdbuf;\nint diag;\nstruct task_struct *kdb_current =\nkdb_curr_task(raw_smp_processor_id());\nKDB_DEBUG_STATE(\"kdb_local 1\", reason);\nkdb_check_for_lockdown();\nkdb_go_count = 0;\nif (reason == KDB_REASON_DEBUG) {\n} else {\nkdb_printf(\"\\nEntering kdb (current=0x%px, pid %d) \",\nkdb_current, kdb_current ? kdb_current->pid : 0);\n#if defined(CONFIG_SMP)\nkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\n}\nswitch (reason) {\ncase KDB_REASON_DEBUG:\n{\nswitch (db_result) {\ncase KDB_DB_BPT:\nkdb_printf(\"\\nEntering kdb (0x%px, pid %d) \",\nkdb_current, kdb_current->pid);\n#if defined(CONFIG_SMP)\nkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\nkdb_printf(\"due to Debug @ \" kdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nbreak;\ncase KDB_DB_SS:\nbreak;\ncase KDB_DB_SSBPT:\nKDB_DEBUG_STATE(\"kdb_local 4\", reason);\nreturn 1;\t\ndefault:\nkdb_printf(\"kdb: Bad result from kdba_db_trap: %d\\n\",\ndb_result);\nbreak;\n}\n}\nbreak;\ncase KDB_REASON_ENTER:\nif (KDB_STATE(KEYBOARD))\nkdb_printf(\"due to Keyboard Entry\\n\");\nelse\nkdb_printf(\"due to KDB_ENTER()\\n\");\nbreak;\ncase KDB_REASON_KEYBOARD:\nKDB_STATE_SET(KEYBOARD);\nkdb_printf(\"due to Keyboard Entry\\n\");\nbreak;\ncase KDB_REASON_ENTER_SLAVE:\ncase KDB_REASON_SWITCH:\nkdb_printf(\"due to cpu switch\\n\");\nbreak;\ncase KDB_REASON_OOPS:\nkdb_printf(\"Oops: %s\\n\", kdb_diemsg);\nkdb_printf(\"due to oops @ \" kdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nkdb_dumpregs(regs);\nbreak;\ncase KDB_REASON_SYSTEM_NMI:\nkdb_printf(\"due to System NonMaskable Interrupt\\n\");\nbreak;\ncase KDB_REASON_NMI:\nkdb_printf(\"due to NonMaskable Interrupt @ \"\nkdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nbreak;\ncase KDB_REASON_SSTEP:\ncase KDB_REASON_BREAK:\nkdb_printf(\"due to %s @ \" kdb_machreg_fmt \"\\n\",\nreason == KDB_REASON_BREAK ?\n\"Breakpoint\" : \"SS trap\", instruction_pointer(regs));\nif (db_result != KDB_DB_BPT) {\nkdb_printf(\"kdb: error return from kdba_bp_trap: %d\\n\",\ndb_result);\nKDB_DEBUG_STATE(\"kdb_local 6\", reason);\nreturn 0;\t\n}\nbreak;\ncase KDB_REASON_RECURSE:\nkdb_printf(\"due to Recursion @ \" kdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nbreak;\ndefault:\nkdb_printf(\"kdb: unexpected reason code: %d\\n\", reason);\nKDB_DEBUG_STATE(\"kdb_local 8\", reason);\nreturn 0;\t\n}\nwhile (1) {\nkdb_nextline = 1;\nKDB_STATE_CLEAR(SUPPRESS);\nkdb_grepping_flag = 0;\nkdb_grep_string[0] = '\\0';\ncmdbuf = cmd_cur;\n*cmdbuf = '\\0';\n*(cmd_hist[cmd_head]) = '\\0';\ndo_full_getstr:\nsnprintf(kdb_prompt_str, CMD_BUFLEN, kdbgetenv(\"PROMPT\"),\nraw_smp_processor_id());\nif (defcmd_in_progress)\nstrncat(kdb_prompt_str, \"[defcmd]\", CMD_BUFLEN);\ncmdbuf = kdb_getstr(cmdbuf, CMD_BUFLEN, kdb_prompt_str);\nif (*cmdbuf != '\\n') {\nif (*cmdbuf < 32) {\nif (cmdptr == cmd_head) {\nstrscpy(cmd_hist[cmd_head], cmd_cur,\nCMD_BUFLEN);\n*(cmd_hist[cmd_head] +\nstrlen(cmd_hist[cmd_head])-1) = '\\0';\n}\nif (!handle_ctrl_cmd(cmdbuf))\n*(cmd_cur+strlen(cmd_cur)-1) = '\\0';\ncmdbuf = cmd_cur;\ngoto do_full_getstr;\n} else {\nstrscpy(cmd_hist[cmd_head], cmd_cur,\nCMD_BUFLEN);\n}\ncmd_head = (cmd_head+1) % KDB_CMD_HISTORY_COUNT;\nif (cmd_head == cmd_tail)\ncmd_tail = (cmd_tail+1) % KDB_CMD_HISTORY_COUNT;\n}\ncmdptr = cmd_head;\ndiag = kdb_parse(cmdbuf);\nif (diag == KDB_NOTFOUND) {\ndrop_newline(cmdbuf);\nkdb_printf(\"Unknown kdb command: '%s'\\n\", cmdbuf);\ndiag = 0;\n}\nif (diag == KDB_CMD_GO\n|| diag == KDB_CMD_CPU\n|| diag == KDB_CMD_SS\n|| diag == KDB_CMD_KGDB)\nbreak;\nif (diag)\nkdb_cmderror(diag);\n}\nKDB_DEBUG_STATE(\"kdb_local 9\", diag);\nreturn diag;\n}\n",
      "code_before_change_raw": "static int kdb_local(kdb_reason_t reason, int error, struct pt_regs *regs,\nkdb_dbtrap_t db_result)\n{\nchar *cmdbuf;\nint diag;\nstruct task_struct *kdb_current =\nkdb_curr_task(raw_smp_processor_id());\nKDB_DEBUG_STATE(\"kdb_local 1\", reason);\nkdb_go_count = 0;\nif (reason == KDB_REASON_DEBUG) {\n} else {\nkdb_printf(\"\\nEntering kdb (current=0x%px, pid %d) \",\nkdb_current, kdb_current ? kdb_current->pid : 0);\n#if defined(CONFIG_SMP)\nkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\n}\nswitch (reason) {\ncase KDB_REASON_DEBUG:\n{\nswitch (db_result) {\ncase KDB_DB_BPT:\nkdb_printf(\"\\nEntering kdb (0x%px, pid %d) \",\nkdb_current, kdb_current->pid);\n#if defined(CONFIG_SMP)\nkdb_printf(\"on processor %d \", raw_smp_processor_id());\n#endif\nkdb_printf(\"due to Debug @ \" kdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nbreak;\ncase KDB_DB_SS:\nbreak;\ncase KDB_DB_SSBPT:\nKDB_DEBUG_STATE(\"kdb_local 4\", reason);\nreturn 1;\t\ndefault:\nkdb_printf(\"kdb: Bad result from kdba_db_trap: %d\\n\",\ndb_result);\nbreak;\n}\n}\nbreak;\ncase KDB_REASON_ENTER:\nif (KDB_STATE(KEYBOARD))\nkdb_printf(\"due to Keyboard Entry\\n\");\nelse\nkdb_printf(\"due to KDB_ENTER()\\n\");\nbreak;\ncase KDB_REASON_KEYBOARD:\nKDB_STATE_SET(KEYBOARD);\nkdb_printf(\"due to Keyboard Entry\\n\");\nbreak;\ncase KDB_REASON_ENTER_SLAVE:\ncase KDB_REASON_SWITCH:\nkdb_printf(\"due to cpu switch\\n\");\nbreak;\ncase KDB_REASON_OOPS:\nkdb_printf(\"Oops: %s\\n\", kdb_diemsg);\nkdb_printf(\"due to oops @ \" kdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nkdb_dumpregs(regs);\nbreak;\ncase KDB_REASON_SYSTEM_NMI:\nkdb_printf(\"due to System NonMaskable Interrupt\\n\");\nbreak;\ncase KDB_REASON_NMI:\nkdb_printf(\"due to NonMaskable Interrupt @ \"\nkdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nbreak;\ncase KDB_REASON_SSTEP:\ncase KDB_REASON_BREAK:\nkdb_printf(\"due to %s @ \" kdb_machreg_fmt \"\\n\",\nreason == KDB_REASON_BREAK ?\n\"Breakpoint\" : \"SS trap\", instruction_pointer(regs));\nif (db_result != KDB_DB_BPT) {\nkdb_printf(\"kdb: error return from kdba_bp_trap: %d\\n\",\ndb_result);\nKDB_DEBUG_STATE(\"kdb_local 6\", reason);\nreturn 0;\t\n}\nbreak;\ncase KDB_REASON_RECURSE:\nkdb_printf(\"due to Recursion @ \" kdb_machreg_fmt \"\\n\",\ninstruction_pointer(regs));\nbreak;\ndefault:\nkdb_printf(\"kdb: unexpected reason code: %d\\n\", reason);\nKDB_DEBUG_STATE(\"kdb_local 8\", reason);\nreturn 0;\t\n}\nwhile (1) {\nkdb_nextline = 1;\nKDB_STATE_CLEAR(SUPPRESS);\nkdb_grepping_flag = 0;\nkdb_grep_string[0] = '\\0';\ncmdbuf = cmd_cur;\n*cmdbuf = '\\0';\n*(cmd_hist[cmd_head]) = '\\0';\ndo_full_getstr:\nsnprintf(kdb_prompt_str, CMD_BUFLEN, kdbgetenv(\"PROMPT\"),\nraw_smp_processor_id());\nif (defcmd_in_progress)\nstrncat(kdb_prompt_str, \"[defcmd]\", CMD_BUFLEN);\ncmdbuf = kdb_getstr(cmdbuf, CMD_BUFLEN, kdb_prompt_str);\nif (*cmdbuf != '\\n') {\nif (*cmdbuf < 32) {\nif (cmdptr == cmd_head) {\nstrscpy(cmd_hist[cmd_head], cmd_cur,\nCMD_BUFLEN);\n*(cmd_hist[cmd_head] +\nstrlen(cmd_hist[cmd_head])-1) = '\\0';\n}\nif (!handle_ctrl_cmd(cmdbuf))\n*(cmd_cur+strlen(cmd_cur)-1) = '\\0';\ncmdbuf = cmd_cur;\ngoto do_full_getstr;\n} else {\nstrscpy(cmd_hist[cmd_head], cmd_cur,\nCMD_BUFLEN);\n}\ncmd_head = (cmd_head+1) % KDB_CMD_HISTORY_COUNT;\nif (cmd_head == cmd_tail)\ncmd_tail = (cmd_tail+1) % KDB_CMD_HISTORY_COUNT;\n}\ncmdptr = cmd_head;\ndiag = kdb_parse(cmdbuf);\nif (diag == KDB_NOTFOUND) {\ndrop_newline(cmdbuf);\nkdb_printf(\"Unknown kdb command: '%s'\\n\", cmdbuf);\ndiag = 0;\n}\nif (diag == KDB_CMD_GO\n|| diag == KDB_CMD_CPU\n|| diag == KDB_CMD_SS\n|| diag == KDB_CMD_KGDB)\nbreak;\nif (diag)\nkdb_cmderror(diag);\n}\nKDB_DEBUG_STATE(\"kdb_local 9\", diag);\nreturn diag;\n}\n"
    }
  ],
  "CVE-2022-2380": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Dynamic memory allocation based on user-provided count parameter without proper bounds checking.",
        "trigger_condition": "User-supplied count parameter exceeding PAGE_SIZE leads to out-of-bounds memory access.",
        "specific_code_behavior_causing_vulnerability": "The code dynamically allocates memory for the buffer based on the count parameter without ensuring that the buffer size does not exceed PAGE_SIZE. This can result in buffer overflow and out-of-bounds memory access if the count parameter is larger than PAGE_SIZE."
      },
      "solution": "To mitigate the vulnerability, it is necessary to allocate a fixed-size buffer of PAGE_SIZE to prevent buffer overflows and out-of-bounds memory access. Additionally, the code should ensure that data is correctly processed before being written to the buffer to prevent potential issues related to incorrect data handling. In this case, the solution involves modifying the code to allocate a fixed-size buffer of PAGE_SIZE and correctly process the data before writing it to the buffer.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2022-2380 in the `smtcfb_read` function. Here's why the modification is required:\n\n1. **Buffer Allocation Size**: The original code dynamically allocates memory for the `buffer` based on the `count` parameter, which can lead to an out-of-bounds memory access if `count` exceeds `PAGE_SIZE`. By modifying the code to allocate a fixed size buffer of `PAGE_SIZE`, we ensure that the buffer size is always within bounds and prevent potential buffer overflows.\n\n2. **Reading Data**: In the original code, the data is read from the source using `fb_readl` and then immediately written to the destination buffer after performing a byte swap. This can lead to potential issues if the source data is not read correctly or if the destination buffer is not properly handled. In the modified code, the data is read into a temporary variable `val`, byte-swapped, and then written to the destination buffer. This ensures that the data is correctly processed before being written to the buffer.\n\n3. **Loop Iteration**: The loop iteration in the modified code `(c + 3) >> 2` ensures that the loop processes the correct number of elements based on the size of `c`. This modification helps in handling the data more accurately and prevents potential issues related to incorrect loop iterations.\n\nOverall, these modifications help in improving the code's robustness, ensuring proper memory handling, and addressing the vulnerability that could lead to out-of-bounds memory access and kernel crashes.",
      "GPT_purpose": "Read data from a framebuffer device and copy it to a user buffer.",
      "GPT_function": "\n1. Read data from a framebuffer device.\n2. Handle memory allocation and deallocation.\n3. Perform data copying to user space.\n4. Check for device and permissions status before proceeding.\n5. Perform byte swapping operations.\n6. Handle potential error conditions.",
      "CVE_id": "CVE-2022-2380",
      "code_before_change": "static ssize_t smtcfb_read(struct fb_info *info, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\n\tu32 *buffer, *dst;\n\tu32 __iomem *src;\n\tint c, i, cnt = 0, err = 0;\n\tunsigned long total_size;\n\n\tif (!info || !info->screen_base)\n\t\treturn -ENODEV;\n\n\tif (info->state != FBINFO_STATE_RUNNING)\n\t\treturn -EPERM;\n\n\ttotal_size = info->screen_size;\n\n\tif (total_size == 0)\n\t\ttotal_size = info->fix.smem_len;\n\n\tif (p >= total_size)\n\t\treturn 0;\n\n\tif (count >= total_size)\n\t\tcount = total_size;\n\n\tif (count + p > total_size)\n\t\tcount = total_size - p;\n\n\tbuffer = kmalloc((count > PAGE_SIZE) ? PAGE_SIZE : count, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\tsrc = (u32 __iomem *)(info->screen_base + p);\n\n\tif (info->fbops->fb_sync)\n\t\tinfo->fbops->fb_sync(info);\n\n\twhile (count) {\n\t\tc = (count > PAGE_SIZE) ? PAGE_SIZE : count;\n\t\tdst = buffer;\n\t\tfor (i = c >> 2; i--;) {\n\t\t\t*dst = fb_readl(src++);\n\t\t\t*dst = big_swap(*dst);\n\t\t\tdst++;\n\t\t}\n\t\tif (c & 3) {\n\t\t\tu8 *dst8 = (u8 *)dst;\n\t\t\tu8 __iomem *src8 = (u8 __iomem *)src;\n\n\t\t\tfor (i = c & 3; i--;) {\n\t\t\t\tif (i & 1) {\n\t\t\t\t\t*dst8++ = fb_readb(++src8);\n\t\t\t\t} else {\n\t\t\t\t\t*dst8++ = fb_readb(--src8);\n\t\t\t\t\tsrc8 += 2;\n\t\t\t\t}\n\t\t\t}\n\t\t\tsrc = (u32 __iomem *)src8;\n\t\t}\n\n\t\tif (copy_to_user(buf, buffer, c)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\t*ppos += c;\n\t\tbuf += c;\n\t\tcnt += c;\n\t\tcount -= c;\n\t}\n\n\tkfree(buffer);\n\n\treturn (err) ? err : cnt;\n}",
      "code_after_change": "static ssize_t smtcfb_read(struct fb_info *info, char __user *buf,\n\t\t\t   size_t count, loff_t *ppos)\n{\n\tunsigned long p = *ppos;\n\n\tu32 *buffer, *dst;\n\tu32 __iomem *src;\n\tint c, i, cnt = 0, err = 0;\n\tunsigned long total_size;\n\n\tif (!info || !info->screen_base)\n\t\treturn -ENODEV;\n\n\tif (info->state != FBINFO_STATE_RUNNING)\n\t\treturn -EPERM;\n\n\ttotal_size = info->screen_size;\n\n\tif (total_size == 0)\n\t\ttotal_size = info->fix.smem_len;\n\n\tif (p >= total_size)\n\t\treturn 0;\n\n\tif (count >= total_size)\n\t\tcount = total_size;\n\n\tif (count + p > total_size)\n\t\tcount = total_size - p;\n\n\tbuffer = kmalloc(PAGE_SIZE, GFP_KERNEL);\n\tif (!buffer)\n\t\treturn -ENOMEM;\n\n\tsrc = (u32 __iomem *)(info->screen_base + p);\n\n\tif (info->fbops->fb_sync)\n\t\tinfo->fbops->fb_sync(info);\n\n\twhile (count) {\n\t\tc = (count > PAGE_SIZE) ? PAGE_SIZE : count;\n\t\tdst = buffer;\n\t\tfor (i = (c + 3) >> 2; i--;) {\n\t\t\tu32 val;\n\n\t\t\tval = fb_readl(src);\n\t\t\t*dst = big_swap(val);\n\t\t\tsrc++;\n\t\t\tdst++;\n\t\t}\n\n\t\tif (copy_to_user(buf, buffer, c)) {\n\t\t\terr = -EFAULT;\n\t\t\tbreak;\n\t\t}\n\t\t*ppos += c;\n\t\tbuf += c;\n\t\tcnt += c;\n\t\tcount -= c;\n\t}\n\n\tkfree(buffer);\n\n\treturn (err) ? err : cnt;\n}",
      "modified_lines": {
        "added": [
          "\tbuffer = kmalloc(PAGE_SIZE, GFP_KERNEL);",
          "\t\tfor (i = (c + 3) >> 2; i--;) {",
          "\t\t\tu32 val;",
          "",
          "\t\t\tval = fb_readl(src);",
          "\t\t\t*dst = big_swap(val);",
          "\t\t\tsrc++;"
        ],
        "deleted": [
          "\tbuffer = kmalloc((count > PAGE_SIZE) ? PAGE_SIZE : count, GFP_KERNEL);",
          "\t\tfor (i = c >> 2; i--;) {",
          "\t\t\t*dst = fb_readl(src++);",
          "\t\t\t*dst = big_swap(*dst);",
          "\t\t}",
          "\t\tif (c & 3) {",
          "\t\t\tu8 *dst8 = (u8 *)dst;",
          "\t\t\tu8 __iomem *src8 = (u8 __iomem *)src;",
          "",
          "\t\t\tfor (i = c & 3; i--;) {",
          "\t\t\t\tif (i & 1) {",
          "\t\t\t\t\t*dst8++ = fb_readb(++src8);",
          "\t\t\t\t} else {",
          "\t\t\t\t\t*dst8++ = fb_readb(--src8);",
          "\t\t\t\t\tsrc8 += 2;",
          "\t\t\t\t}",
          "\t\t\t}",
          "\t\t\tsrc = (u32 __iomem *)src8;"
        ]
      },
      "preconditions_for_vulnerability": "Dynamic memory allocation based on user-provided count parameter without proper bounds checking.",
      "trigger_condition": "User-supplied count parameter exceeding PAGE_SIZE leads to out-of-bounds memory access.",
      "specific_code_behavior_causing_vulnerability": "The code dynamically allocates memory for the buffer based on the count parameter without ensuring that the buffer size does not exceed PAGE_SIZE. This can result in buffer overflow and out-of-bounds memory access if the count parameter is larger than PAGE_SIZE.",
      "id": 136,
      "code_after_change_normalized": "static ssize_t FUN1(struct fb_info *VAR1, char __user *VAR2,\nsize_t VAR3, loff_t *VAR4)\n{\nunsigned long VAR5 = *VAR4;\nu32 *VAR6, *VAR7;\nu32 __iomem *VAR8;\nint VAR9, VAR10, VAR11 = 0, VAR12 = 0;\nunsigned long VAR13;\nif (!VAR1 || !VAR1->VAR14)\nreturn -VAR15;\nif (VAR1->VAR16 != VAR17)\nreturn -VAR18;\nVAR13 = VAR1->VAR19;\nif (VAR13 == 0)\nVAR13 = VAR1->VAR20.VAR21;\nif (VAR5 >= VAR13)\nreturn 0;\nif (VAR3 >= VAR13)\nVAR3 = VAR13;\nif (VAR3 + VAR5 > VAR13)\nVAR3 = VAR13 - VAR5;\nVAR6 = FUN2(VAR22, VAR23);\nif (!VAR6)\nreturn -VAR24;\nVAR8 = (u32 VAR25 *)(VAR1->VAR14 + VAR5);\nif (VAR1->VAR26->VAR27)\nVAR1->VAR26->FUN3(VAR1);\nwhile (VAR3) {\nVAR9 = (VAR3 > VAR22) ? VAR22 : VAR3;\nVAR7 = VAR6;\nfor (VAR10 = (VAR9 + 3) >> 2; VAR10--;) {\nu32 VAR28;\nVAR28 = FUN4(VAR8);\n*VAR7 = FUN5(VAR28);\nVAR8++;\nVAR7++;\n}\nif (FUN6(VAR2, VAR6, VAR9)) {\nVAR12 = -VAR29;\nbreak;\n}\n*VAR4 += VAR9;\nVAR2 += VAR9;\nVAR11 += VAR9;\nVAR3 -= VAR9;\n}\nFUN7(VAR6);\nreturn (VAR12) ? VAR12 : VAR11;\n}\n",
      "code_before_change_normalized": "static ssize_t FUN1(struct fb_info *VAR1, char __user *VAR2,\nsize_t VAR3, loff_t *VAR4)\n{\nunsigned long VAR5 = *VAR4;\nu32 *VAR6, *VAR7;\nu32 __iomem *VAR8;\nint VAR9, VAR10, VAR11 = 0, VAR12 = 0;\nunsigned long VAR13;\nif (!VAR1 || !VAR1->VAR14)\nreturn -VAR15;\nif (VAR1->VAR16 != VAR17)\nreturn -VAR18;\nVAR13 = VAR1->VAR19;\nif (VAR13 == 0)\nVAR13 = VAR1->VAR20.VAR21;\nif (VAR5 >= VAR13)\nreturn 0;\nif (VAR3 >= VAR13)\nVAR3 = VAR13;\nif (VAR3 + VAR5 > VAR13)\nVAR3 = VAR13 - VAR5;\nVAR6 = FUN2((VAR3 > VAR22) ? VAR22 : VAR3, VAR23);\nif (!VAR6)\nreturn -VAR24;\nVAR8 = (u32 VAR25 *)(VAR1->VAR14 + VAR5);\nif (VAR1->VAR26->VAR27)\nVAR1->VAR26->FUN3(VAR1);\nwhile (VAR3) {\nVAR9 = (VAR3 > VAR22) ? VAR22 : VAR3;\nVAR7 = VAR6;\nfor (VAR10 = VAR9 >> 2; VAR10--;) {\n*VAR7 = FUN4(VAR8++);\n*VAR7 = FUN5(*VAR7);\nVAR7++;\n}\nif (VAR9 & 3) {\nVAR29 *VAR28 = (VAR29 *)VAR7;\nu8 VAR25 *VAR30 = (u8 VAR25 *)VAR8;\nfor (VAR10 = VAR9 & 3; VAR10--;) {\nif (VAR10 & 1) {\n*VAR28++ = FUN6(++VAR30);\n} else {\n*VAR28++ = FUN6(--VAR30);\nVAR30 += 2;\n}\n}\nVAR8 = (u32 VAR25 *)VAR30;\n}\nif (FUN7(VAR2, VAR6, VAR9)) {\nVAR12 = -VAR31;\nbreak;\n}\n*VAR4 += VAR9;\nVAR2 += VAR9;\nVAR11 += VAR9;\nVAR3 -= VAR9;\n}\nFUN8(VAR6);\nreturn (VAR12) ? VAR12 : VAR11;\n}\n",
      "code_after_change_raw": "static ssize_t smtcfb_read(struct fb_info *info, char __user *buf,\nsize_t count, loff_t *ppos)\n{\nunsigned long p = *ppos;\nu32 *buffer, *dst;\nu32 __iomem *src;\nint c, i, cnt = 0, err = 0;\nunsigned long total_size;\nif (!info || !info->screen_base)\nreturn -ENODEV;\nif (info->state != FBINFO_STATE_RUNNING)\nreturn -EPERM;\ntotal_size = info->screen_size;\nif (total_size == 0)\ntotal_size = info->fix.smem_len;\nif (p >= total_size)\nreturn 0;\nif (count >= total_size)\ncount = total_size;\nif (count + p > total_size)\ncount = total_size - p;\nbuffer = kmalloc(PAGE_SIZE, GFP_KERNEL);\nif (!buffer)\nreturn -ENOMEM;\nsrc = (u32 __iomem *)(info->screen_base + p);\nif (info->fbops->fb_sync)\ninfo->fbops->fb_sync(info);\nwhile (count) {\nc = (count > PAGE_SIZE) ? PAGE_SIZE : count;\ndst = buffer;\nfor (i = (c + 3) >> 2; i--;) {\nu32 val;\nval = fb_readl(src);\n*dst = big_swap(val);\nsrc++;\ndst++;\n}\nif (copy_to_user(buf, buffer, c)) {\nerr = -EFAULT;\nbreak;\n}\n*ppos += c;\nbuf += c;\ncnt += c;\ncount -= c;\n}\nkfree(buffer);\nreturn (err) ? err : cnt;\n}\n",
      "code_before_change_raw": "static ssize_t smtcfb_read(struct fb_info *info, char __user *buf,\nsize_t count, loff_t *ppos)\n{\nunsigned long p = *ppos;\nu32 *buffer, *dst;\nu32 __iomem *src;\nint c, i, cnt = 0, err = 0;\nunsigned long total_size;\nif (!info || !info->screen_base)\nreturn -ENODEV;\nif (info->state != FBINFO_STATE_RUNNING)\nreturn -EPERM;\ntotal_size = info->screen_size;\nif (total_size == 0)\ntotal_size = info->fix.smem_len;\nif (p >= total_size)\nreturn 0;\nif (count >= total_size)\ncount = total_size;\nif (count + p > total_size)\ncount = total_size - p;\nbuffer = kmalloc((count > PAGE_SIZE) ? PAGE_SIZE : count, GFP_KERNEL);\nif (!buffer)\nreturn -ENOMEM;\nsrc = (u32 __iomem *)(info->screen_base + p);\nif (info->fbops->fb_sync)\ninfo->fbops->fb_sync(info);\nwhile (count) {\nc = (count > PAGE_SIZE) ? PAGE_SIZE : count;\ndst = buffer;\nfor (i = c >> 2; i--;) {\n*dst = fb_readl(src++);\n*dst = big_swap(*dst);\ndst++;\n}\nif (c & 3) {\nu8 *dst8 = (u8 *)dst;\nu8 __iomem *src8 = (u8 __iomem *)src;\nfor (i = c & 3; i--;) {\nif (i & 1) {\n*dst8++ = fb_readb(++src8);\n} else {\n*dst8++ = fb_readb(--src8);\nsrc8 += 2;\n}\n}\nsrc = (u32 __iomem *)src8;\n}\nif (copy_to_user(buf, buffer, c)) {\nerr = -EFAULT;\nbreak;\n}\n*ppos += c;\nbuf += c;\ncnt += c;\ncount -= c;\n}\nkfree(buffer);\nreturn (err) ? err : cnt;\n}\n"
    }
  ],
  "CVE-2022-2991": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation of the length of user-supplied data before copying it to a fixed-length heap-based buffer.",
        "trigger_condition": "An attacker provides malicious input data that exceeds the expected buffer size, leading to a heap-based buffer overflow.",
        "specific_code_behavior_causing_vulnerability": "The code snippet copies user-supplied data to a fixed-length heap-based buffer without verifying the length of the data, allowing a potential buffer overflow if the input data is larger than the buffer size."
      },
      "solution": "To mitigate the vulnerability, it is necessary to implement proper length validation checks before copying user-supplied data to fixed-length buffers. This involves ensuring that the data being copied does not exceed the allocated buffer size to prevent buffer overflows. Specifically, the code should validate the length of the input data and perform bounds checking before copying it to the buffer. By enforcing these checks, the code can prevent buffer overflows and enhance security.",
      "GPT_analysis": "The modification is necessary to address the identified heap-based buffer overflow vulnerability in the code snippet. Specifically, the vulnerability arises from copying user-supplied data to a fixed-length heap-based buffer without proper validation of the data length. This lack of validation can lead to a situation where an attacker can provide malicious input that exceeds the expected buffer size, causing a buffer overflow.\n\nTo mitigate this vulnerability and prevent potential exploitation by an attacker, it is crucial to ensure that data copying operations are performed safely within the bounds of the allocated memory. By modifying the code to include proper length validation checks before copying data to buffers, you can prevent buffer overflows and enhance the security of the system.\n\nThe specific changes needed to address the vulnerability may involve adjusting the size of buffers, implementing bounds checking, or utilizing safer functions for copying data to buffers. These modifications help to ensure that only valid and properly sized data is copied, reducing the risk of buffer overflows and associated security threats.",
      "GPT_purpose": "Allocate and initialize a new NVMe namespace on a controller, including setting up a disk, queue, and other necessary data structures.",
      "GPT_function": "\n1. Allocate memory for a new NVMe namespace.\n2. Initialize a block queue for the namespace.\n3. Initialize a disk structure for the namespace.\n4. Register the namespace with the NVMe controller.\n5. Add the namespace to the list of namespaces associated with the controller.",
      "CVE_id": "CVE-2022-2991",
      "code_before_change": "static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid,\n\t\tstruct nvme_ns_ids *ids)\n{\n\tstruct nvme_ns *ns;\n\tstruct gendisk *disk;\n\tstruct nvme_id_ns *id;\n\tint node = ctrl->numa_node;\n\n\tif (nvme_identify_ns(ctrl, nsid, ids, &id))\n\t\treturn;\n\n\tns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);\n\tif (!ns)\n\t\tgoto out_free_id;\n\n\tns->queue = blk_mq_init_queue(ctrl->tagset);\n\tif (IS_ERR(ns->queue))\n\t\tgoto out_free_ns;\n\n\tif (ctrl->opts && ctrl->opts->data_digest)\n\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, ns->queue);\n\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, ns->queue);\n\tif (ctrl->ops->flags & NVME_F_PCI_P2PDMA)\n\t\tblk_queue_flag_set(QUEUE_FLAG_PCI_P2PDMA, ns->queue);\n\n\tns->queue->queuedata = ns;\n\tns->ctrl = ctrl;\n\tkref_init(&ns->kref);\n\n\tif (nvme_init_ns_head(ns, nsid, ids, id->nmic & NVME_NS_NMIC_SHARED))\n\t\tgoto out_free_queue;\n\n\tdisk = alloc_disk_node(0, node);\n\tif (!disk)\n\t\tgoto out_unlink_ns;\n\n\tdisk->fops = &nvme_bdev_ops;\n\tdisk->private_data = ns;\n\tdisk->queue = ns->queue;\n\t/*\n\t * Without the multipath code enabled, multiple controller per\n\t * subsystems are visible as devices and thus we cannot use the\n\t * subsystem instance.\n\t */\n\tif (!nvme_mpath_set_disk_name(ns, disk->disk_name, &disk->flags))\n\t\tsprintf(disk->disk_name, \"nvme%dn%d\", ctrl->instance,\n\t\t\tns->head->instance);\n\tns->disk = disk;\n\n\tif (nvme_update_ns_info(ns, id))\n\t\tgoto out_put_disk;\n\n\tif ((ctrl->quirks & NVME_QUIRK_LIGHTNVM) && id->vs[0] == 0x1) {\n\t\tif (nvme_nvm_register(ns, disk->disk_name, node)) {\n\t\t\tdev_warn(ctrl->device, \"LightNVM init failure\\n\");\n\t\t\tgoto out_put_disk;\n\t\t}\n\t}\n\n\tdown_write(&ctrl->namespaces_rwsem);\n\tlist_add_tail(&ns->list, &ctrl->namespaces);\n\tup_write(&ctrl->namespaces_rwsem);\n\n\tnvme_get_ctrl(ctrl);\n\n\tdevice_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);\n\tif (!nvme_ns_head_multipath(ns->head))\n\t\tnvme_add_ns_cdev(ns);\n\n\tnvme_mpath_add_disk(ns, id);\n\tnvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);\n\tkfree(id);\n\n\treturn;\n out_put_disk:\n\t/* prevent double queue cleanup */\n\tns->disk->queue = NULL;\n\tput_disk(ns->disk);\n out_unlink_ns:\n\tmutex_lock(&ctrl->subsys->lock);\n\tlist_del_rcu(&ns->siblings);\n\tif (list_empty(&ns->head->list))\n\t\tlist_del_init(&ns->head->entry);\n\tmutex_unlock(&ctrl->subsys->lock);\n\tnvme_put_ns_head(ns->head);\n out_free_queue:\n\tblk_cleanup_queue(ns->queue);\n out_free_ns:\n\tkfree(ns);\n out_free_id:\n\tkfree(id);\n}",
      "code_after_change": "static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid,\n\t\tstruct nvme_ns_ids *ids)\n{\n\tstruct nvme_ns *ns;\n\tstruct gendisk *disk;\n\tstruct nvme_id_ns *id;\n\tint node = ctrl->numa_node;\n\n\tif (nvme_identify_ns(ctrl, nsid, ids, &id))\n\t\treturn;\n\n\tns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);\n\tif (!ns)\n\t\tgoto out_free_id;\n\n\tns->queue = blk_mq_init_queue(ctrl->tagset);\n\tif (IS_ERR(ns->queue))\n\t\tgoto out_free_ns;\n\n\tif (ctrl->opts && ctrl->opts->data_digest)\n\t\tblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, ns->queue);\n\n\tblk_queue_flag_set(QUEUE_FLAG_NONROT, ns->queue);\n\tif (ctrl->ops->flags & NVME_F_PCI_P2PDMA)\n\t\tblk_queue_flag_set(QUEUE_FLAG_PCI_P2PDMA, ns->queue);\n\n\tns->queue->queuedata = ns;\n\tns->ctrl = ctrl;\n\tkref_init(&ns->kref);\n\n\tif (nvme_init_ns_head(ns, nsid, ids, id->nmic & NVME_NS_NMIC_SHARED))\n\t\tgoto out_free_queue;\n\n\tdisk = alloc_disk_node(0, node);\n\tif (!disk)\n\t\tgoto out_unlink_ns;\n\n\tdisk->fops = &nvme_bdev_ops;\n\tdisk->private_data = ns;\n\tdisk->queue = ns->queue;\n\t/*\n\t * Without the multipath code enabled, multiple controller per\n\t * subsystems are visible as devices and thus we cannot use the\n\t * subsystem instance.\n\t */\n\tif (!nvme_mpath_set_disk_name(ns, disk->disk_name, &disk->flags))\n\t\tsprintf(disk->disk_name, \"nvme%dn%d\", ctrl->instance,\n\t\t\tns->head->instance);\n\tns->disk = disk;\n\n\tif (nvme_update_ns_info(ns, id))\n\t\tgoto out_put_disk;\n\n\tdown_write(&ctrl->namespaces_rwsem);\n\tlist_add_tail(&ns->list, &ctrl->namespaces);\n\tup_write(&ctrl->namespaces_rwsem);\n\n\tnvme_get_ctrl(ctrl);\n\n\tdevice_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);\n\tif (!nvme_ns_head_multipath(ns->head))\n\t\tnvme_add_ns_cdev(ns);\n\n\tnvme_mpath_add_disk(ns, id);\n\tnvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);\n\tkfree(id);\n\n\treturn;\n out_put_disk:\n\t/* prevent double queue cleanup */\n\tns->disk->queue = NULL;\n\tput_disk(ns->disk);\n out_unlink_ns:\n\tmutex_lock(&ctrl->subsys->lock);\n\tlist_del_rcu(&ns->siblings);\n\tif (list_empty(&ns->head->list))\n\t\tlist_del_init(&ns->head->entry);\n\tmutex_unlock(&ctrl->subsys->lock);\n\tnvme_put_ns_head(ns->head);\n out_free_queue:\n\tblk_cleanup_queue(ns->queue);\n out_free_ns:\n\tkfree(ns);\n out_free_id:\n\tkfree(id);\n}",
      "modified_lines": {
        "added": [],
        "deleted": [
          "\tif ((ctrl->quirks & NVME_QUIRK_LIGHTNVM) && id->vs[0] == 0x1) {",
          "\t\tif (nvme_nvm_register(ns, disk->disk_name, node)) {",
          "\t\t\tdev_warn(ctrl->device, \"LightNVM init failure\\n\");",
          "\t\t\tgoto out_put_disk;",
          "\t\t}",
          "\t}",
          ""
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation of the length of user-supplied data before copying it to a fixed-length heap-based buffer.",
      "trigger_condition": "An attacker provides malicious input data that exceeds the expected buffer size, leading to a heap-based buffer overflow.",
      "specific_code_behavior_causing_vulnerability": "The code snippet copies user-supplied data to a fixed-length heap-based buffer without verifying the length of the data, allowing a potential buffer overflow if the input data is larger than the buffer size.",
      "id": 137,
      "code_after_change_normalized": "static void FUN1(struct nvme_ctrl *VAR1, unsigned VAR2,\nstruct nvme_ns_ids *VAR3)\n{\nstruct nvme_ns *VAR4;\nstruct gendisk *VAR5;\nstruct nvme_id_ns *VAR6;\nint VAR7 = VAR1->VAR8;\nif (FUN2(VAR1, VAR2, VAR3, &VAR6))\nreturn;\nVAR4 = FUN3(sizeof(*VAR4), VAR9, VAR7);\nif (!VAR4)\ngoto VAR10;\nVAR4->VAR11 = FUN4(VAR1->VAR12);\nif (FUN5(VAR4->VAR11))\ngoto VAR13;\nif (VAR1->VAR14 && VAR1->VAR14->VAR15)\nFUN6(VAR16, VAR4->VAR11);\nFUN6(VAR17, VAR4->VAR11);\nif (VAR1->VAR18->VAR19 & VAR20)\nFUN6(VAR21, VAR4->VAR11);\nVAR4->VAR11->VAR22 = VAR4;\nVAR4->VAR1 = VAR1;\nFUN7(&VAR4->VAR23);\nif (FUN8(VAR4, VAR2, VAR3, VAR6->VAR24 & VAR25))\ngoto VAR26;\nVAR5 = FUN9(0, VAR7);\nif (!VAR5)\ngoto VAR27;\nVAR5->VAR28 = &VAR29;\nVAR5->VAR30 = VAR4;\nVAR5->VAR11 = VAR4->VAR11;\nif (!FUN10(VAR4, VAR5->VAR31, &VAR5->VAR19))\nFUN11(VAR5->VAR31, \"STR\", VAR1->VAR32,\nVAR4->VAR33->VAR32);\nVAR4->VAR5 = VAR5;\nif (FUN12(VAR4, VAR6))\ngoto VAR34;\nFUN13(&VAR1->VAR35);\nFUN14(&VAR4->VAR36, &VAR1->VAR37);\nFUN15(&VAR1->VAR35);\nFUN16(VAR1);\nFUN17(VAR1->VAR38, VAR4->VAR5, VAR39);\nif (!FUN18(VAR4->VAR33))\nFUN19(VAR4);\nFUN20(VAR4, VAR6);\nFUN21(&VAR4->VAR40, VAR4->VAR5->VAR31);\nFUN22(VAR6);\nreturn;\nVAR34:\nVAR4->VAR5->VAR11 = NULL;\nFUN23(VAR4->VAR5);\nVAR27:\nFUN24(&VAR1->VAR41->VAR42);\nFUN25(&VAR4->VAR43);\nif (FUN26(&VAR4->VAR33->VAR36))\nFUN27(&VAR4->VAR33->VAR44);\nFUN28(&VAR1->VAR41->VAR42);\nFUN29(VAR4->VAR33);\nVAR26:\nFUN30(VAR4->VAR11);\nVAR13:\nFUN22(VAR4);\nVAR10:\nFUN22(VAR6);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct nvme_ctrl *VAR1, unsigned VAR2,\nstruct nvme_ns_ids *VAR3)\n{\nstruct nvme_ns *VAR4;\nstruct gendisk *VAR5;\nstruct nvme_id_ns *VAR6;\nint VAR7 = VAR1->VAR8;\nif (FUN2(VAR1, VAR2, VAR3, &VAR6))\nreturn;\nVAR4 = FUN3(sizeof(*VAR4), VAR9, VAR7);\nif (!VAR4)\ngoto VAR10;\nVAR4->VAR11 = FUN4(VAR1->VAR12);\nif (FUN5(VAR4->VAR11))\ngoto VAR13;\nif (VAR1->VAR14 && VAR1->VAR14->VAR15)\nFUN6(VAR16, VAR4->VAR11);\nFUN6(VAR17, VAR4->VAR11);\nif (VAR1->VAR18->VAR19 & VAR20)\nFUN6(VAR21, VAR4->VAR11);\nVAR4->VAR11->VAR22 = VAR4;\nVAR4->VAR1 = VAR1;\nFUN7(&VAR4->VAR23);\nif (FUN8(VAR4, VAR2, VAR3, VAR6->VAR24 & VAR25))\ngoto VAR26;\nVAR5 = FUN9(0, VAR7);\nif (!VAR5)\ngoto VAR27;\nVAR5->VAR28 = &VAR29;\nVAR5->VAR30 = VAR4;\nVAR5->VAR11 = VAR4->VAR11;\nif (!FUN10(VAR4, VAR5->VAR31, &VAR5->VAR19))\nFUN11(VAR5->VAR31, \"STR\", VAR1->VAR32,\nVAR4->VAR33->VAR32);\nVAR4->VAR5 = VAR5;\nif (FUN12(VAR4, VAR6))\ngoto VAR34;\nif ((VAR1->VAR35 & VAR36) && VAR6->VAR37[0] == VAR38) {\nif (FUN13(VAR4, VAR5->VAR31, VAR7)) {\nFUN14(VAR1->VAR39, \"STR\");\ngoto VAR34;\n}\n}\nFUN15(&VAR1->VAR40);\nFUN16(&VAR4->VAR41, &VAR1->VAR42);\nFUN17(&VAR1->VAR40);\nFUN18(VAR1);\nFUN19(VAR1->VAR39, VAR4->VAR5, VAR43);\nif (!FUN20(VAR4->VAR33))\nFUN21(VAR4);\nFUN22(VAR4, VAR6);\nFUN23(&VAR4->VAR44, VAR4->VAR5->VAR31);\nFUN24(VAR6);\nreturn;\nVAR34:\nVAR4->VAR5->VAR11 = NULL;\nFUN25(VAR4->VAR5);\nVAR27:\nFUN26(&VAR1->VAR45->VAR46);\nFUN27(&VAR4->VAR47);\nif (FUN28(&VAR4->VAR33->VAR41))\nFUN29(&VAR4->VAR33->VAR48);\nFUN30(&VAR1->VAR45->VAR46);\nFUN31(VAR4->VAR33);\nVAR26:\nFUN32(VAR4->VAR11);\nVAR13:\nFUN24(VAR4);\nVAR10:\nFUN24(VAR6);\n}\n",
      "code_after_change_raw": "static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid,\nstruct nvme_ns_ids *ids)\n{\nstruct nvme_ns *ns;\nstruct gendisk *disk;\nstruct nvme_id_ns *id;\nint node = ctrl->numa_node;\nif (nvme_identify_ns(ctrl, nsid, ids, &id))\nreturn;\nns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);\nif (!ns)\ngoto out_free_id;\nns->queue = blk_mq_init_queue(ctrl->tagset);\nif (IS_ERR(ns->queue))\ngoto out_free_ns;\nif (ctrl->opts && ctrl->opts->data_digest)\nblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, ns->queue);\nblk_queue_flag_set(QUEUE_FLAG_NONROT, ns->queue);\nif (ctrl->ops->flags & NVME_F_PCI_P2PDMA)\nblk_queue_flag_set(QUEUE_FLAG_PCI_P2PDMA, ns->queue);\nns->queue->queuedata = ns;\nns->ctrl = ctrl;\nkref_init(&ns->kref);\nif (nvme_init_ns_head(ns, nsid, ids, id->nmic & NVME_NS_NMIC_SHARED))\ngoto out_free_queue;\ndisk = alloc_disk_node(0, node);\nif (!disk)\ngoto out_unlink_ns;\ndisk->fops = &nvme_bdev_ops;\ndisk->private_data = ns;\ndisk->queue = ns->queue;\nif (!nvme_mpath_set_disk_name(ns, disk->disk_name, &disk->flags))\nsprintf(disk->disk_name, \"nvme%dn%d\", ctrl->instance,\nns->head->instance);\nns->disk = disk;\nif (nvme_update_ns_info(ns, id))\ngoto out_put_disk;\ndown_write(&ctrl->namespaces_rwsem);\nlist_add_tail(&ns->list, &ctrl->namespaces);\nup_write(&ctrl->namespaces_rwsem);\nnvme_get_ctrl(ctrl);\ndevice_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);\nif (!nvme_ns_head_multipath(ns->head))\nnvme_add_ns_cdev(ns);\nnvme_mpath_add_disk(ns, id);\nnvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);\nkfree(id);\nreturn;\nout_put_disk:\nns->disk->queue = NULL;\nput_disk(ns->disk);\nout_unlink_ns:\nmutex_lock(&ctrl->subsys->lock);\nlist_del_rcu(&ns->siblings);\nif (list_empty(&ns->head->list))\nlist_del_init(&ns->head->entry);\nmutex_unlock(&ctrl->subsys->lock);\nnvme_put_ns_head(ns->head);\nout_free_queue:\nblk_cleanup_queue(ns->queue);\nout_free_ns:\nkfree(ns);\nout_free_id:\nkfree(id);\n}\n",
      "code_before_change_raw": "static void nvme_alloc_ns(struct nvme_ctrl *ctrl, unsigned nsid,\nstruct nvme_ns_ids *ids)\n{\nstruct nvme_ns *ns;\nstruct gendisk *disk;\nstruct nvme_id_ns *id;\nint node = ctrl->numa_node;\nif (nvme_identify_ns(ctrl, nsid, ids, &id))\nreturn;\nns = kzalloc_node(sizeof(*ns), GFP_KERNEL, node);\nif (!ns)\ngoto out_free_id;\nns->queue = blk_mq_init_queue(ctrl->tagset);\nif (IS_ERR(ns->queue))\ngoto out_free_ns;\nif (ctrl->opts && ctrl->opts->data_digest)\nblk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, ns->queue);\nblk_queue_flag_set(QUEUE_FLAG_NONROT, ns->queue);\nif (ctrl->ops->flags & NVME_F_PCI_P2PDMA)\nblk_queue_flag_set(QUEUE_FLAG_PCI_P2PDMA, ns->queue);\nns->queue->queuedata = ns;\nns->ctrl = ctrl;\nkref_init(&ns->kref);\nif (nvme_init_ns_head(ns, nsid, ids, id->nmic & NVME_NS_NMIC_SHARED))\ngoto out_free_queue;\ndisk = alloc_disk_node(0, node);\nif (!disk)\ngoto out_unlink_ns;\ndisk->fops = &nvme_bdev_ops;\ndisk->private_data = ns;\ndisk->queue = ns->queue;\nif (!nvme_mpath_set_disk_name(ns, disk->disk_name, &disk->flags))\nsprintf(disk->disk_name, \"nvme%dn%d\", ctrl->instance,\nns->head->instance);\nns->disk = disk;\nif (nvme_update_ns_info(ns, id))\ngoto out_put_disk;\nif ((ctrl->quirks & NVME_QUIRK_LIGHTNVM) && id->vs[0] == 0x1) {\nif (nvme_nvm_register(ns, disk->disk_name, node)) {\ndev_warn(ctrl->device, \"LightNVM init failure\\n\");\ngoto out_put_disk;\n}\n}\ndown_write(&ctrl->namespaces_rwsem);\nlist_add_tail(&ns->list, &ctrl->namespaces);\nup_write(&ctrl->namespaces_rwsem);\nnvme_get_ctrl(ctrl);\ndevice_add_disk(ctrl->device, ns->disk, nvme_ns_id_attr_groups);\nif (!nvme_ns_head_multipath(ns->head))\nnvme_add_ns_cdev(ns);\nnvme_mpath_add_disk(ns, id);\nnvme_fault_inject_init(&ns->fault_inject, ns->disk->disk_name);\nkfree(id);\nreturn;\nout_put_disk:\nns->disk->queue = NULL;\nput_disk(ns->disk);\nout_unlink_ns:\nmutex_lock(&ctrl->subsys->lock);\nlist_del_rcu(&ns->siblings);\nif (list_empty(&ns->head->list))\nlist_del_init(&ns->head->entry);\nmutex_unlock(&ctrl->subsys->lock);\nnvme_put_ns_head(ns->head);\nout_free_queue:\nblk_cleanup_queue(ns->queue);\nout_free_ns:\nkfree(ns);\nout_free_id:\nkfree(id);\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation of the length of user-supplied data before copying it to a fixed-length heap-based buffer.",
        "trigger_condition": "A local attacker provides malicious input data with a length exceeding the buffer size, leading to a heap-based buffer overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the length of user-supplied data before copying it to a fixed-length heap-based buffer, allowing an attacker to overwrite adjacent memory regions and potentially execute arbitrary code."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper validation of the length of user-supplied data before copying it to the fixed-length heap-based buffer. This can be achieved by adding input data length checks and bounds checking to prevent buffer overflow scenarios. In this case, the solution involves removing unnecessary operations that copy user-supplied data without proper validation and always returning an error code in the default case to prevent further processing of potentially malicious input.",
      "GPT_analysis": "The modification made to the code snippet is necessary to address the vulnerability CVE-2022-2991, which involves a heap-based buffer overflow in the Linux kernel's LightNVM subsystem. \n\nIn the original code snippet, there is a lack of proper validation of the length of user-supplied data before copying it to a fixed-length heap-based buffer. This lack of validation can lead to a buffer overflow vulnerability, which can be exploited by a local attacker to escalate privileges and execute arbitrary code in the context of the kernel.\n\nTo mitigate this vulnerability, the modification removes the conditional check for `ns->ndev` and always returns `-ENOTTY` in the default case of the switch statement. By doing so, the code ensures that if an unrecognized command is received, it will immediately return an error code without attempting any further operations that could potentially lead to a buffer overflow.\n\nBy making this modification, the code limits the potential attack surface and reduces the risk of exploitation through buffer overflow vulnerabilities.",
      "GPT_purpose": "Handle various NVMe namespace IOCTL commands including ID retrieval and I/O submission.",
      "GPT_function": "\n1. Handle NVME_IOCTL_ID command by returning the namespace ID.\n2. Handle NVME_IOCTL_IO_CMD command by calling nvme_user_cmd function.\n3. Handle NVME_IOCTL_SUBMIT_IO or NVME_IOCTL_SUBMIT_IO32 command by calling nvme_submit_io function.\n4. Handle NVME_IOCTL_IO64_CMD command by calling nvme_user_cmd64 function.\n5. For any other command, if the namespace device is not available, return -ENOTTY; otherwise, call nvme_nvm_ioctl function.",
      "CVE_id": "CVE-2022-2991",
      "code_before_change": "static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned int cmd,\n\t\tvoid __user *argp)\n{\n\tswitch (cmd) {\n\tcase NVME_IOCTL_ID:\n\t\tforce_successful_syscall_return();\n\t\treturn ns->head->ns_id;\n\tcase NVME_IOCTL_IO_CMD:\n\t\treturn nvme_user_cmd(ns->ctrl, ns, argp);\n\t/*\n\t * struct nvme_user_io can have different padding on some 32-bit ABIs.\n\t * Just accept the compat version as all fields that are used are the\n\t * same size and at the same offset.\n\t */\n#ifdef COMPAT_FOR_U64_ALIGNMENT\n\tcase NVME_IOCTL_SUBMIT_IO32:\n#endif\n\tcase NVME_IOCTL_SUBMIT_IO:\n\t\treturn nvme_submit_io(ns, argp);\n\tcase NVME_IOCTL_IO64_CMD:\n\t\treturn nvme_user_cmd64(ns->ctrl, ns, argp);\n\tdefault:\n\t\tif (!ns->ndev)\n\t\t\treturn -ENOTTY;\n\t\treturn nvme_nvm_ioctl(ns, cmd, argp);\n\t}\n}",
      "code_after_change": "static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned int cmd,\n\t\tvoid __user *argp)\n{\n\tswitch (cmd) {\n\tcase NVME_IOCTL_ID:\n\t\tforce_successful_syscall_return();\n\t\treturn ns->head->ns_id;\n\tcase NVME_IOCTL_IO_CMD:\n\t\treturn nvme_user_cmd(ns->ctrl, ns, argp);\n\t/*\n\t * struct nvme_user_io can have different padding on some 32-bit ABIs.\n\t * Just accept the compat version as all fields that are used are the\n\t * same size and at the same offset.\n\t */\n#ifdef COMPAT_FOR_U64_ALIGNMENT\n\tcase NVME_IOCTL_SUBMIT_IO32:\n#endif\n\tcase NVME_IOCTL_SUBMIT_IO:\n\t\treturn nvme_submit_io(ns, argp);\n\tcase NVME_IOCTL_IO64_CMD:\n\t\treturn nvme_user_cmd64(ns->ctrl, ns, argp);\n\tdefault:\n\t\treturn -ENOTTY;\n\t}\n}",
      "modified_lines": {
        "added": [
          "\t\treturn -ENOTTY;"
        ],
        "deleted": [
          "\t\tif (!ns->ndev)",
          "\t\t\treturn -ENOTTY;",
          "\t\treturn nvme_nvm_ioctl(ns, cmd, argp);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation of the length of user-supplied data before copying it to a fixed-length heap-based buffer.",
      "trigger_condition": "A local attacker provides malicious input data with a length exceeding the buffer size, leading to a heap-based buffer overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the length of user-supplied data before copying it to a fixed-length heap-based buffer, allowing an attacker to overwrite adjacent memory regions and potentially execute arbitrary code.",
      "id": 138,
      "code_after_change_normalized": "static int FUN1(struct nvme_ns *VAR1, unsigned int VAR2,\nvoid __user *VAR3)\n{\nswitch (VAR2) {\ncase VAR4:\nFUN2();\nreturn VAR1->VAR5->VAR6;\ncase VAR7:\nreturn FUN3(VAR1->VAR8, VAR1, VAR3);\n#ifdef VAR9\ncase VAR10:\n#VAR11\ncase VAR12:\nreturn FUN4(VAR1, VAR3);\ncase VAR13:\nreturn FUN5(VAR1->VAR8, VAR1, VAR3);\ndefault:\nreturn -VAR14;\n}\n}\n",
      "code_before_change_normalized": "static int FUN1(struct nvme_ns *VAR1, unsigned int VAR2,\nvoid __user *VAR3)\n{\nswitch (VAR2) {\ncase VAR4:\nFUN2();\nreturn VAR1->VAR5->VAR6;\ncase VAR7:\nreturn FUN3(VAR1->VAR8, VAR1, VAR3);\n#ifdef VAR9\ncase VAR10:\n#VAR11\ncase VAR12:\nreturn FUN4(VAR1, VAR3);\ncase VAR13:\nreturn FUN5(VAR1->VAR8, VAR1, VAR3);\ndefault:\nif (!VAR1->VAR14)\nreturn -VAR15;\nreturn FUN6(VAR1, VAR2, VAR3);\n}\n}\n",
      "code_after_change_raw": "static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned int cmd,\nvoid __user *argp)\n{\nswitch (cmd) {\ncase NVME_IOCTL_ID:\nforce_successful_syscall_return();\nreturn ns->head->ns_id;\ncase NVME_IOCTL_IO_CMD:\nreturn nvme_user_cmd(ns->ctrl, ns, argp);\n#ifdef COMPAT_FOR_U64_ALIGNMENT\ncase NVME_IOCTL_SUBMIT_IO32:\n#endif\ncase NVME_IOCTL_SUBMIT_IO:\nreturn nvme_submit_io(ns, argp);\ncase NVME_IOCTL_IO64_CMD:\nreturn nvme_user_cmd64(ns->ctrl, ns, argp);\ndefault:\nreturn -ENOTTY;\n}\n}\n",
      "code_before_change_raw": "static int nvme_ns_ioctl(struct nvme_ns *ns, unsigned int cmd,\nvoid __user *argp)\n{\nswitch (cmd) {\ncase NVME_IOCTL_ID:\nforce_successful_syscall_return();\nreturn ns->head->ns_id;\ncase NVME_IOCTL_IO_CMD:\nreturn nvme_user_cmd(ns->ctrl, ns, argp);\n#ifdef COMPAT_FOR_U64_ALIGNMENT\ncase NVME_IOCTL_SUBMIT_IO32:\n#endif\ncase NVME_IOCTL_SUBMIT_IO:\nreturn nvme_submit_io(ns, argp);\ncase NVME_IOCTL_IO64_CMD:\nreturn nvme_user_cmd64(ns->ctrl, ns, argp);\ndefault:\nif (!ns->ndev)\nreturn -ENOTTY;\nreturn nvme_nvm_ioctl(ns, cmd, argp);\n}\n}\n"
    }
  ],
  "CVE-2022-3028": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of synchronization mechanism to handle concurrent access to a critical section of code.",
        "trigger_condition": "Simultaneous calls to a specific function by multiple threads without proper synchronization, leading to a race condition.",
        "specific_code_behavior_causing_vulnerability": "The code allows multiple threads to execute the xfrm_probe_algs() function concurrently, which can result in a race condition due to lack of synchronization. This race condition may lead to out-of-bounds writes or memory leaks."
      },
      "solution": "To mitigate the vulnerability, a synchronization mechanism such as a mutex lock needs to be added to ensure that only one thread can execute the xfrm_probe_algs() function at a time. By introducing mutex locking and unlocking around the critical section of code, concurrent access is controlled, preventing the race condition from occurring. The solution involves adding mutex_lock before the xfrm_probe_algs() function call and mutex_unlock afterward to enforce mutual exclusion and protect the critical section of code.",
      "GPT_analysis": "The modification is necessary to address the race condition vulnerability (CVE-2022-3028) identified in the code snippet. By adding the mutex_lock and mutex_unlock calls around the xfrm_probe_algs() function call, we ensure that only one thread can execute the xfrm_probe_algs() function at a time. This prevents multiple simultaneous calls to xfrm_probe_algs() from causing a race condition, which could lead to out-of-bounds writes or memory leaks.\n\nUsing a mutex (mutual exclusion) lock ensures that the critical section of code, in this case, the xfrm_probe_algs() function call, is protected from concurrent access by multiple threads. By acquiring the lock before executing the function and releasing it afterward, we enforce mutual exclusion and prevent the race condition from occurring.\n\nTherefore, the addition of mutex locking and unlocking in the modified code snippet helps to mitigate the race condition vulnerability and ensures the safe execution of the xfrm_probe_algs() function in a multi-threaded environment.",
      "GPT_purpose": "Registering a new Security Association Database (SADB) message type in the PF_KEY socket and broadcasting the supported algorithms.",
      "GPT_function": "\n1. Check if the specified security association type is within the valid range.\n2. Register a security association type if it is not already registered.\n3. Probe cryptographic algorithms used in IPsec.\n4. Compose a message containing supported security association types.\n5. Broadcast the message to registered sockets.",
      "CVE_id": "CVE-2022-3028",
      "code_before_change": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *supp_skb;\n\n\tif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\n\t\tif (pfk->registered&(1<<hdr->sadb_msg_satype))\n\t\t\treturn -EEXIST;\n\t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n\t}\n\n\txfrm_probe_algs();\n\n\tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n\tif (!supp_skb) {\n\t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n\t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);\n\n\t\treturn -ENOBUFS;\n\t}\n\n\tpfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\n\t\t\tsock_net(sk));\n\treturn 0;\n}",
      "code_after_change": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\n\tstruct pfkey_sock *pfk = pfkey_sk(sk);\n\tstruct sk_buff *supp_skb;\n\n\tif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\n\t\treturn -EINVAL;\n\n\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\n\t\tif (pfk->registered&(1<<hdr->sadb_msg_satype))\n\t\t\treturn -EEXIST;\n\t\tpfk->registered |= (1<<hdr->sadb_msg_satype);\n\t}\n\n\tmutex_lock(&pfkey_mutex);\n\txfrm_probe_algs();\n\n\tsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\n\tmutex_unlock(&pfkey_mutex);\n\n\tif (!supp_skb) {\n\t\tif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\n\t\t\tpfk->registered &= ~(1<<hdr->sadb_msg_satype);\n\n\t\treturn -ENOBUFS;\n\t}\n\n\tpfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\n\t\t\tsock_net(sk));\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tmutex_lock(&pfkey_mutex);",
          "\tmutex_unlock(&pfkey_mutex);",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of synchronization mechanism to handle concurrent access to a critical section of code.",
      "trigger_condition": "Simultaneous calls to a specific function by multiple threads without proper synchronization, leading to a race condition.",
      "specific_code_behavior_causing_vulnerability": "The code allows multiple threads to execute the xfrm_probe_algs() function concurrently, which can result in a race condition due to lack of synchronization. This race condition may lead to out-of-bounds writes or memory leaks.",
      "id": 139,
      "code_after_change_normalized": "static int FUN1(struct sock *VAR1, struct sk_buff *VAR2, const struct sadb_msg *VAR3, void * const *VAR4)\n{\nstruct pfkey_sock *VAR5 = FUN2(VAR1);\nstruct sk_buff *VAR6;\nif (VAR3->VAR7 > VAR8)\nreturn -VAR9;\nif (VAR3->VAR7 != VAR10) {\nif (VAR5->VAR11&(1<<VAR3->VAR7))\nreturn -VAR12;\nVAR5->VAR11 |= (1<<VAR3->VAR7);\n}\nFUN3(&VAR13);\nFUN4();\nVAR6 = FUN5(VAR3, VAR14 | VAR15);\nFUN6(&VAR13);\nif (!VAR6) {\nif (VAR3->VAR7 != VAR10)\nVAR5->VAR11 &= ~(1<<VAR3->VAR7);\nreturn -VAR16;\n}\nFUN7(VAR6, VAR14, VAR17, VAR1,\nFUN8(VAR1));\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sock *VAR1, struct sk_buff *VAR2, const struct sadb_msg *VAR3, void * const *VAR4)\n{\nstruct pfkey_sock *VAR5 = FUN2(VAR1);\nstruct sk_buff *VAR6;\nif (VAR3->VAR7 > VAR8)\nreturn -VAR9;\nif (VAR3->VAR7 != VAR10) {\nif (VAR5->VAR11&(1<<VAR3->VAR7))\nreturn -VAR12;\nVAR5->VAR11 |= (1<<VAR3->VAR7);\n}\nFUN3();\nVAR6 = FUN4(VAR3, VAR13 | VAR14);\nif (!VAR6) {\nif (VAR3->VAR7 != VAR10)\nVAR5->VAR11 &= ~(1<<VAR3->VAR7);\nreturn -VAR15;\n}\nFUN5(VAR6, VAR13, VAR16, VAR1,\nFUN6(VAR1));\nreturn 0;\n}\n",
      "code_after_change_raw": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\nstruct pfkey_sock *pfk = pfkey_sk(sk);\nstruct sk_buff *supp_skb;\nif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\nreturn -EINVAL;\nif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\nif (pfk->registered&(1<<hdr->sadb_msg_satype))\nreturn -EEXIST;\npfk->registered |= (1<<hdr->sadb_msg_satype);\n}\nmutex_lock(&pfkey_mutex);\nxfrm_probe_algs();\nsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\nmutex_unlock(&pfkey_mutex);\nif (!supp_skb) {\nif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\npfk->registered &= ~(1<<hdr->sadb_msg_satype);\nreturn -ENOBUFS;\n}\npfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\nsock_net(sk));\nreturn 0;\n}\n",
      "code_before_change_raw": "static int pfkey_register(struct sock *sk, struct sk_buff *skb, const struct sadb_msg *hdr, void * const *ext_hdrs)\n{\nstruct pfkey_sock *pfk = pfkey_sk(sk);\nstruct sk_buff *supp_skb;\nif (hdr->sadb_msg_satype > SADB_SATYPE_MAX)\nreturn -EINVAL;\nif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC) {\nif (pfk->registered&(1<<hdr->sadb_msg_satype))\nreturn -EEXIST;\npfk->registered |= (1<<hdr->sadb_msg_satype);\n}\nxfrm_probe_algs();\nsupp_skb = compose_sadb_supported(hdr, GFP_KERNEL | __GFP_ZERO);\nif (!supp_skb) {\nif (hdr->sadb_msg_satype != SADB_SATYPE_UNSPEC)\npfk->registered &= ~(1<<hdr->sadb_msg_satype);\nreturn -ENOBUFS;\n}\npfkey_broadcast(supp_skb, GFP_KERNEL, BROADCAST_REGISTERED, sk,\nsock_net(sk));\nreturn 0;\n}\n"
    }
  ],
  "CVE-2022-3577": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Incorrect assumption that all devices have inputs, leading to out-of-bounds memory write vulnerability.",
        "trigger_condition": "Malicious devices can violate the assumption, causing the code to write out of bounds.",
        "specific_code_behavior_causing_vulnerability": "The code assumes all devices have inputs without verifying, resulting in accessing memory beyond the allocated bounds when handling inputs.",
        "solution": "To mitigate the vulnerability, it is necessary to validate the presence of inputs before proceeding with operations that assume their existence. By adding a check to verify the availability of inputs, the code can prevent the out-of-bounds memory write vulnerability. In this case, the solution involves checking if inputs are empty and handling the scenario where no inputs are found."
      },
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2022-3577 in the Linux kernel's Kid-friendly Wired Controller driver. The vulnerability arises from an incorrect assumption in the original code that all bigben devices have inputs. However, malicious devices can violate this assumption, leading to an out-of-bounds memory write issue.\n\nBy adding the check `if (list_empty(&hid->inputs))` in the modified code, we ensure that the driver verifies the presence of inputs before proceeding with operations that assume their existence. If no inputs are found, the driver will log an error and return `-ENODEV`, preventing the out-of-bounds memory write vulnerability from being exploited.\n\nIn summary, the modification is necessary to enhance the security of the driver by validating the presence of inputs and handling the scenario where inputs are not available, thus mitigating the risk of the vulnerability being exploited.",
      "GPT_purpose": "Initialize a BigBen gamepad device, set up LED and force feedback support, and handle potential errors during initialization.",
      "GPT_function": "\n1. Initialize the bigben device structure and set up HID device data.\n2. Parse the HID device and start hardware.\n3. Create input force feedback effects and LED devices.\n4. Set initial state for LEDs and force feedback.\n5. Schedule work for LED and force feedback handling.",
      "CVE_id": "CVE-2022-3577",
      "code_before_change": "static int bigben_probe(struct hid_device *hid,\n\tconst struct hid_device_id *id)\n{\n\tstruct bigben_device *bigben;\n\tstruct hid_input *hidinput;\n\tstruct list_head *report_list;\n\tstruct led_classdev *led;\n\tchar *name;\n\tsize_t name_sz;\n\tint n, error;\n\n\tbigben = devm_kzalloc(&hid->dev, sizeof(*bigben), GFP_KERNEL);\n\tif (!bigben)\n\t\treturn -ENOMEM;\n\thid_set_drvdata(hid, bigben);\n\tbigben->hid = hid;\n\tbigben->removed = false;\n\n\terror = hid_parse(hid);\n\tif (error) {\n\t\thid_err(hid, \"parse failed\\n\");\n\t\treturn error;\n\t}\n\n\terror = hid_hw_start(hid, HID_CONNECT_DEFAULT & ~HID_CONNECT_FF);\n\tif (error) {\n\t\thid_err(hid, \"hw start failed\\n\");\n\t\treturn error;\n\t}\n\n\treport_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tbigben->report = list_entry(report_list->next,\n\t\tstruct hid_report, list);\n\n\thidinput = list_first_entry(&hid->inputs, struct hid_input, list);\n\tset_bit(FF_RUMBLE, hidinput->input->ffbit);\n\n\tINIT_WORK(&bigben->worker, bigben_worker);\n\n\terror = input_ff_create_memless(hidinput->input, NULL,\n\t\thid_bigben_play_effect);\n\tif (error)\n\t\tgoto error_hw_stop;\n\n\tname_sz = strlen(dev_name(&hid->dev)) + strlen(\":red:bigben#\") + 1;\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tled = devm_kzalloc(\n\t\t\t&hid->dev,\n\t\t\tsizeof(struct led_classdev) + name_sz,\n\t\t\tGFP_KERNEL\n\t\t);\n\t\tif (!led) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto error_hw_stop;\n\t\t}\n\t\tname = (void *)(&led[1]);\n\t\tsnprintf(name, name_sz,\n\t\t\t\"%s:red:bigben%d\",\n\t\t\tdev_name(&hid->dev), n + 1\n\t\t);\n\t\tled->name = name;\n\t\tled->brightness = (n == 0) ? LED_ON : LED_OFF;\n\t\tled->max_brightness = 1;\n\t\tled->brightness_get = bigben_get_led;\n\t\tled->brightness_set = bigben_set_led;\n\t\tbigben->leds[n] = led;\n\t\terror = devm_led_classdev_register(&hid->dev, led);\n\t\tif (error)\n\t\t\tgoto error_hw_stop;\n\t}\n\n\t/* initial state: LED1 is on, no rumble effect */\n\tbigben->led_state = BIT(0);\n\tbigben->right_motor_on = 0;\n\tbigben->left_motor_force = 0;\n\tbigben->work_led = true;\n\tbigben->work_ff = true;\n\tschedule_work(&bigben->worker);\n\n\thid_info(hid, \"LED and force feedback support for BigBen gamepad\\n\");\n\n\treturn 0;\n\nerror_hw_stop:\n\thid_hw_stop(hid);\n\treturn error;\n}",
      "code_after_change": "static int bigben_probe(struct hid_device *hid,\n\tconst struct hid_device_id *id)\n{\n\tstruct bigben_device *bigben;\n\tstruct hid_input *hidinput;\n\tstruct list_head *report_list;\n\tstruct led_classdev *led;\n\tchar *name;\n\tsize_t name_sz;\n\tint n, error;\n\n\tbigben = devm_kzalloc(&hid->dev, sizeof(*bigben), GFP_KERNEL);\n\tif (!bigben)\n\t\treturn -ENOMEM;\n\thid_set_drvdata(hid, bigben);\n\tbigben->hid = hid;\n\tbigben->removed = false;\n\n\terror = hid_parse(hid);\n\tif (error) {\n\t\thid_err(hid, \"parse failed\\n\");\n\t\treturn error;\n\t}\n\n\terror = hid_hw_start(hid, HID_CONNECT_DEFAULT & ~HID_CONNECT_FF);\n\tif (error) {\n\t\thid_err(hid, \"hw start failed\\n\");\n\t\treturn error;\n\t}\n\n\treport_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\n\tbigben->report = list_entry(report_list->next,\n\t\tstruct hid_report, list);\n\n\tif (list_empty(&hid->inputs)) {\n\t\thid_err(hid, \"no inputs found\\n\");\n\t\terror = -ENODEV;\n\t\tgoto error_hw_stop;\n\t}\n\n\thidinput = list_first_entry(&hid->inputs, struct hid_input, list);\n\tset_bit(FF_RUMBLE, hidinput->input->ffbit);\n\n\tINIT_WORK(&bigben->worker, bigben_worker);\n\n\terror = input_ff_create_memless(hidinput->input, NULL,\n\t\thid_bigben_play_effect);\n\tif (error)\n\t\tgoto error_hw_stop;\n\n\tname_sz = strlen(dev_name(&hid->dev)) + strlen(\":red:bigben#\") + 1;\n\n\tfor (n = 0; n < NUM_LEDS; n++) {\n\t\tled = devm_kzalloc(\n\t\t\t&hid->dev,\n\t\t\tsizeof(struct led_classdev) + name_sz,\n\t\t\tGFP_KERNEL\n\t\t);\n\t\tif (!led) {\n\t\t\terror = -ENOMEM;\n\t\t\tgoto error_hw_stop;\n\t\t}\n\t\tname = (void *)(&led[1]);\n\t\tsnprintf(name, name_sz,\n\t\t\t\"%s:red:bigben%d\",\n\t\t\tdev_name(&hid->dev), n + 1\n\t\t);\n\t\tled->name = name;\n\t\tled->brightness = (n == 0) ? LED_ON : LED_OFF;\n\t\tled->max_brightness = 1;\n\t\tled->brightness_get = bigben_get_led;\n\t\tled->brightness_set = bigben_set_led;\n\t\tbigben->leds[n] = led;\n\t\terror = devm_led_classdev_register(&hid->dev, led);\n\t\tif (error)\n\t\t\tgoto error_hw_stop;\n\t}\n\n\t/* initial state: LED1 is on, no rumble effect */\n\tbigben->led_state = BIT(0);\n\tbigben->right_motor_on = 0;\n\tbigben->left_motor_force = 0;\n\tbigben->work_led = true;\n\tbigben->work_ff = true;\n\tschedule_work(&bigben->worker);\n\n\thid_info(hid, \"LED and force feedback support for BigBen gamepad\\n\");\n\n\treturn 0;\n\nerror_hw_stop:\n\thid_hw_stop(hid);\n\treturn error;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (list_empty(&hid->inputs)) {",
          "\t\thid_err(hid, \"no inputs found\\n\");",
          "\t\terror = -ENODEV;",
          "\t\tgoto error_hw_stop;",
          "\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Incorrect assumption that all devices have inputs, leading to out-of-bounds memory write vulnerability.",
      "trigger_condition": "Malicious devices can violate the assumption, causing the code to write out of bounds.",
      "specific_code_behavior_causing_vulnerability": "The code assumes all devices have inputs without verifying, resulting in accessing memory beyond the allocated bounds when handling inputs.",
      "solution": "To mitigate the vulnerability, it is necessary to validate the presence of inputs before proceeding with operations that assume their existence. By adding a check to verify the availability of inputs, the code can prevent the out-of-bounds memory write vulnerability. In this case, the solution involves checking if inputs are empty and handling the scenario where no inputs are found.",
      "id": 140,
      "code_after_change_normalized": "static int FUN1(struct hid_device *VAR1,\nconst struct hid_device_id *VAR2)\n{\nstruct bigben_device *VAR3;\nstruct hid_input *VAR4;\nstruct list_head *VAR5;\nstruct led_classdev *VAR6;\nchar *VAR7;\nsize_t VAR8;\nint VAR9, VAR10;\nVAR3 = FUN2(&VAR1->VAR11, sizeof(*VAR3), VAR12);\nif (!VAR3)\nreturn -VAR13;\nFUN3(VAR1, VAR3);\nVAR3->VAR1 = VAR1;\nVAR3->VAR14 = false;\nVAR10 = FUN4(VAR1);\nif (VAR10) {\nFUN5(VAR1, \"STR\");\nreturn VAR10;\n}\nVAR10 = FUN6(VAR1, VAR15 & ~VAR16);\nif (VAR10) {\nFUN5(VAR1, \"STR\");\nreturn VAR10;\n}\nVAR5 = &VAR1->VAR17[VAR18].VAR5;\nVAR3->VAR19 = FUN7(VAR5->VAR20,\nstruct VAR21, VAR22);\nif (FUN8(&VAR1->VAR23)) {\nFUN5(VAR1, \"STR\");\nVAR10 = -VAR24;\ngoto VAR25;\n}\nVAR4 = FUN9(&VAR1->VAR23, struct VAR26, VAR22);\nFUN10(VAR27, VAR4->VAR28->VAR29);\nFUN11(&VAR3->VAR30, VAR31);\nVAR10 = FUN12(VAR4->VAR28, NULL,\nVAR32);\nif (VAR10)\ngoto VAR25;\nVAR8 = FUN13(FUN14(&VAR1->VAR11)) + FUN13(\"STR\") + 1;\nfor (VAR9 = 0; VAR9 < VAR33; VAR9++) {\nVAR6 = FUN2(\n&VAR1->VAR11,\nsizeof(struct VAR34) + VAR8,\nVAR12\n);\nif (!VAR6) {\nVAR10 = -VAR13;\ngoto VAR25;\n}\nVAR7 = (void *)(&VAR6[1]);\nFUN15(VAR7, VAR8,\n\"STR\",\nFUN14(&VAR1->VAR11), VAR9 + 1\n);\nVAR6->VAR7 = VAR7;\nVAR6->VAR35 = (VAR9 == 0) ? VAR36 : VAR37;\nVAR6->VAR38 = 1;\nVAR6->VAR39 = VAR40;\nVAR6->VAR41 = VAR42;\nVAR3->VAR43[VAR9] = VAR6;\nVAR10 = FUN16(&VAR1->VAR11, VAR6);\nif (VAR10)\ngoto VAR25;\n}\nVAR3->VAR44 = FUN17(0);\nVAR3->VAR45 = 0;\nVAR3->VAR46 = 0;\nVAR3->VAR47 = true;\nVAR3->VAR48 = true;\nFUN18(&VAR3->VAR30);\nFUN19(VAR1, \"STR\");\nreturn 0;\nVAR25:\nFUN20(VAR1);\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct hid_device *VAR1,\nconst struct hid_device_id *VAR2)\n{\nstruct bigben_device *VAR3;\nstruct hid_input *VAR4;\nstruct list_head *VAR5;\nstruct led_classdev *VAR6;\nchar *VAR7;\nsize_t VAR8;\nint VAR9, VAR10;\nVAR3 = FUN2(&VAR1->VAR11, sizeof(*VAR3), VAR12);\nif (!VAR3)\nreturn -VAR13;\nFUN3(VAR1, VAR3);\nVAR3->VAR1 = VAR1;\nVAR3->VAR14 = false;\nVAR10 = FUN4(VAR1);\nif (VAR10) {\nFUN5(VAR1, \"STR\");\nreturn VAR10;\n}\nVAR10 = FUN6(VAR1, VAR15 & ~VAR16);\nif (VAR10) {\nFUN5(VAR1, \"STR\");\nreturn VAR10;\n}\nVAR5 = &VAR1->VAR17[VAR18].VAR5;\nVAR3->VAR19 = FUN7(VAR5->VAR20,\nstruct VAR21, VAR22);\nVAR4 = FUN8(&VAR1->VAR23, struct VAR24, VAR22);\nFUN9(VAR25, VAR4->VAR26->VAR27);\nFUN10(&VAR3->VAR28, VAR29);\nVAR10 = FUN11(VAR4->VAR26, NULL,\nVAR30);\nif (VAR10)\ngoto VAR31;\nVAR8 = FUN12(FUN13(&VAR1->VAR11)) + FUN12(\"STR\") + 1;\nfor (VAR9 = 0; VAR9 < VAR32; VAR9++) {\nVAR6 = FUN2(\n&VAR1->VAR11,\nsizeof(struct VAR33) + VAR8,\nVAR12\n);\nif (!VAR6) {\nVAR10 = -VAR13;\ngoto VAR31;\n}\nVAR7 = (void *)(&VAR6[1]);\nFUN14(VAR7, VAR8,\n\"STR\",\nFUN13(&VAR1->VAR11), VAR9 + 1\n);\nVAR6->VAR7 = VAR7;\nVAR6->VAR34 = (VAR9 == 0) ? VAR35 : VAR36;\nVAR6->VAR37 = 1;\nVAR6->VAR38 = VAR39;\nVAR6->VAR40 = VAR41;\nVAR3->VAR42[VAR9] = VAR6;\nVAR10 = FUN15(&VAR1->VAR11, VAR6);\nif (VAR10)\ngoto VAR31;\n}\nVAR3->VAR43 = FUN16(0);\nVAR3->VAR44 = 0;\nVAR3->VAR45 = 0;\nVAR3->VAR46 = true;\nVAR3->VAR47 = true;\nFUN17(&VAR3->VAR28);\nFUN18(VAR1, \"STR\");\nreturn 0;\nVAR31:\nFUN19(VAR1);\nreturn VAR10;\n}\n",
      "code_after_change_raw": "static int bigben_probe(struct hid_device *hid,\nconst struct hid_device_id *id)\n{\nstruct bigben_device *bigben;\nstruct hid_input *hidinput;\nstruct list_head *report_list;\nstruct led_classdev *led;\nchar *name;\nsize_t name_sz;\nint n, error;\nbigben = devm_kzalloc(&hid->dev, sizeof(*bigben), GFP_KERNEL);\nif (!bigben)\nreturn -ENOMEM;\nhid_set_drvdata(hid, bigben);\nbigben->hid = hid;\nbigben->removed = false;\nerror = hid_parse(hid);\nif (error) {\nhid_err(hid, \"parse failed\\n\");\nreturn error;\n}\nerror = hid_hw_start(hid, HID_CONNECT_DEFAULT & ~HID_CONNECT_FF);\nif (error) {\nhid_err(hid, \"hw start failed\\n\");\nreturn error;\n}\nreport_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\nbigben->report = list_entry(report_list->next,\nstruct hid_report, list);\nif (list_empty(&hid->inputs)) {\nhid_err(hid, \"no inputs found\\n\");\nerror = -ENODEV;\ngoto error_hw_stop;\n}\nhidinput = list_first_entry(&hid->inputs, struct hid_input, list);\nset_bit(FF_RUMBLE, hidinput->input->ffbit);\nINIT_WORK(&bigben->worker, bigben_worker);\nerror = input_ff_create_memless(hidinput->input, NULL,\nhid_bigben_play_effect);\nif (error)\ngoto error_hw_stop;\nname_sz = strlen(dev_name(&hid->dev)) + strlen(\":red:bigben#\") + 1;\nfor (n = 0; n < NUM_LEDS; n++) {\nled = devm_kzalloc(\n&hid->dev,\nsizeof(struct led_classdev) + name_sz,\nGFP_KERNEL\n);\nif (!led) {\nerror = -ENOMEM;\ngoto error_hw_stop;\n}\nname = (void *)(&led[1]);\nsnprintf(name, name_sz,\n\"%s:red:bigben%d\",\ndev_name(&hid->dev), n + 1\n);\nled->name = name;\nled->brightness = (n == 0) ? LED_ON : LED_OFF;\nled->max_brightness = 1;\nled->brightness_get = bigben_get_led;\nled->brightness_set = bigben_set_led;\nbigben->leds[n] = led;\nerror = devm_led_classdev_register(&hid->dev, led);\nif (error)\ngoto error_hw_stop;\n}\nbigben->led_state = BIT(0);\nbigben->right_motor_on = 0;\nbigben->left_motor_force = 0;\nbigben->work_led = true;\nbigben->work_ff = true;\nschedule_work(&bigben->worker);\nhid_info(hid, \"LED and force feedback support for BigBen gamepad\\n\");\nreturn 0;\nerror_hw_stop:\nhid_hw_stop(hid);\nreturn error;\n}\n",
      "code_before_change_raw": "static int bigben_probe(struct hid_device *hid,\nconst struct hid_device_id *id)\n{\nstruct bigben_device *bigben;\nstruct hid_input *hidinput;\nstruct list_head *report_list;\nstruct led_classdev *led;\nchar *name;\nsize_t name_sz;\nint n, error;\nbigben = devm_kzalloc(&hid->dev, sizeof(*bigben), GFP_KERNEL);\nif (!bigben)\nreturn -ENOMEM;\nhid_set_drvdata(hid, bigben);\nbigben->hid = hid;\nbigben->removed = false;\nerror = hid_parse(hid);\nif (error) {\nhid_err(hid, \"parse failed\\n\");\nreturn error;\n}\nerror = hid_hw_start(hid, HID_CONNECT_DEFAULT & ~HID_CONNECT_FF);\nif (error) {\nhid_err(hid, \"hw start failed\\n\");\nreturn error;\n}\nreport_list = &hid->report_enum[HID_OUTPUT_REPORT].report_list;\nbigben->report = list_entry(report_list->next,\nstruct hid_report, list);\nhidinput = list_first_entry(&hid->inputs, struct hid_input, list);\nset_bit(FF_RUMBLE, hidinput->input->ffbit);\nINIT_WORK(&bigben->worker, bigben_worker);\nerror = input_ff_create_memless(hidinput->input, NULL,\nhid_bigben_play_effect);\nif (error)\ngoto error_hw_stop;\nname_sz = strlen(dev_name(&hid->dev)) + strlen(\":red:bigben#\") + 1;\nfor (n = 0; n < NUM_LEDS; n++) {\nled = devm_kzalloc(\n&hid->dev,\nsizeof(struct led_classdev) + name_sz,\nGFP_KERNEL\n);\nif (!led) {\nerror = -ENOMEM;\ngoto error_hw_stop;\n}\nname = (void *)(&led[1]);\nsnprintf(name, name_sz,\n\"%s:red:bigben%d\",\ndev_name(&hid->dev), n + 1\n);\nled->name = name;\nled->brightness = (n == 0) ? LED_ON : LED_OFF;\nled->max_brightness = 1;\nled->brightness_get = bigben_get_led;\nled->brightness_set = bigben_set_led;\nbigben->leds[n] = led;\nerror = devm_led_classdev_register(&hid->dev, led);\nif (error)\ngoto error_hw_stop;\n}\nbigben->led_state = BIT(0);\nbigben->right_motor_on = 0;\nbigben->left_motor_force = 0;\nbigben->work_led = true;\nbigben->work_ff = true;\nschedule_work(&bigben->worker);\nhid_info(hid, \"LED and force feedback support for BigBen gamepad\\n\");\nreturn 0;\nerror_hw_stop:\nhid_hw_stop(hid);\nreturn error;\n}\n"
    }
  ],
  "CVE-2022-36280": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for the dimensions of the copy box being processed in the code.",
        "trigger_condition": "An attacker provides crafted input with dimensions exceeding the expected bounds, leading to an out-of-bounds memory access vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the width or height of the copy box exceeds the expected limits of 64x64 pixels, allowing potential out-of-bounds memory access when processing the data."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add validation checks to ensure that the width and height of the copy box are within the expected bounds (64x64 pixels). By enforcing these limits, the code prevents potential out-of-bounds memory access issues and enhances the security of the system. In this case, the solution involves adding conditions to check if the width or height of the copy box exceeds 64 pixels.",
      "GPT_analysis": "The modification is necessary to address the out-of-bounds (OOB) memory access vulnerability identified in the code snippet. By adding the conditions `box->w > 64 || box->h > 64` to the existing validation checks, the code ensures that the width and height of the copy box are within the expected bounds (64x64 pixels). This prevents potential OOB memory access issues that could be exploited by a local attacker to gain privileges and cause a denial of service (DoS) on the system. By restricting the dimensions of the copy box, the code mitigates the vulnerability and enhances the security of the system.",
      "GPT_purpose": "Copy data from a DMA request to a surface snooper image for cursor handling.",
      "GPT_function": "\n1. Check if a snooper is installed and return if not.\n2. Validate the parameters of the DMA request for cursor.\n3. Copy data from a buffer object to a surface snooper image.\n4. Update the snooper age and release resources.",
      "CVE_id": "CVE-2022-36280",
      "code_before_change": "void vmw_kms_cursor_snoop(struct vmw_surface *srf,\n\t\t\t  struct ttm_object_file *tfile,\n\t\t\t  struct ttm_buffer_object *bo,\n\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct ttm_bo_kmap_obj map;\n\tunsigned long kmap_offset;\n\tunsigned long kmap_num;\n\tSVGA3dCopyBox *box;\n\tunsigned box_count;\n\tvoid *virtual;\n\tbool dummy;\n\tstruct vmw_dma_cmd {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdSurfaceDMA dma;\n\t} *cmd;\n\tint i, ret;\n\n\tcmd = container_of(header, struct vmw_dma_cmd, header);\n\n\t/* No snooper installed, nothing to copy */\n\tif (!srf->snooper.image)\n\t\treturn;\n\n\tif (cmd->dma.host.face != 0 || cmd->dma.host.mipmap != 0) {\n\t\tDRM_ERROR(\"face and mipmap for cursors should never != 0\\n\");\n\t\treturn;\n\t}\n\n\tif (cmd->header.size < 64) {\n\t\tDRM_ERROR(\"at least one full copy box must be given\\n\");\n\t\treturn;\n\t}\n\n\tbox = (SVGA3dCopyBox *)&cmd[1];\n\tbox_count = (cmd->header.size - sizeof(SVGA3dCmdSurfaceDMA)) /\n\t\t\tsizeof(SVGA3dCopyBox);\n\n\tif (cmd->dma.guest.ptr.offset % PAGE_SIZE ||\n\t    box->x != 0    || box->y != 0    || box->z != 0    ||\n\t    box->srcx != 0 || box->srcy != 0 || box->srcz != 0 ||\n\t    box->d != 1    || box_count != 1) {\n\t\t/* TODO handle none page aligned offsets */\n\t\t/* TODO handle more dst & src != 0 */\n\t\t/* TODO handle more then one copy */\n\t\tDRM_ERROR(\"Can't snoop dma request for cursor!\\n\");\n\t\tDRM_ERROR(\"(%u, %u, %u) (%u, %u, %u) (%ux%ux%u) %u %u\\n\",\n\t\t\t  box->srcx, box->srcy, box->srcz,\n\t\t\t  box->x, box->y, box->z,\n\t\t\t  box->w, box->h, box->d, box_count,\n\t\t\t  cmd->dma.guest.ptr.offset);\n\t\treturn;\n\t}\n\n\tkmap_offset = cmd->dma.guest.ptr.offset >> PAGE_SHIFT;\n\tkmap_num = (64*64*4) >> PAGE_SHIFT;\n\n\tret = ttm_bo_reserve(bo, true, false, NULL);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"reserve failed\\n\");\n\t\treturn;\n\t}\n\n\tret = ttm_bo_kmap(bo, kmap_offset, kmap_num, &map);\n\tif (unlikely(ret != 0))\n\t\tgoto err_unreserve;\n\n\tvirtual = ttm_kmap_obj_virtual(&map, &dummy);\n\n\tif (box->w == 64 && cmd->dma.guest.pitch == 64*4) {\n\t\tmemcpy(srf->snooper.image, virtual, 64*64*4);\n\t} else {\n\t\t/* Image is unsigned pointer. */\n\t\tfor (i = 0; i < box->h; i++)\n\t\t\tmemcpy(srf->snooper.image + i * 64,\n\t\t\t       virtual + i * cmd->dma.guest.pitch,\n\t\t\t       box->w * 4);\n\t}\n\n\tsrf->snooper.age++;\n\n\tttm_bo_kunmap(&map);\nerr_unreserve:\n\tttm_bo_unreserve(bo);\n}",
      "code_after_change": "void vmw_kms_cursor_snoop(struct vmw_surface *srf,\n\t\t\t  struct ttm_object_file *tfile,\n\t\t\t  struct ttm_buffer_object *bo,\n\t\t\t  SVGA3dCmdHeader *header)\n{\n\tstruct ttm_bo_kmap_obj map;\n\tunsigned long kmap_offset;\n\tunsigned long kmap_num;\n\tSVGA3dCopyBox *box;\n\tunsigned box_count;\n\tvoid *virtual;\n\tbool dummy;\n\tstruct vmw_dma_cmd {\n\t\tSVGA3dCmdHeader header;\n\t\tSVGA3dCmdSurfaceDMA dma;\n\t} *cmd;\n\tint i, ret;\n\n\tcmd = container_of(header, struct vmw_dma_cmd, header);\n\n\t/* No snooper installed, nothing to copy */\n\tif (!srf->snooper.image)\n\t\treturn;\n\n\tif (cmd->dma.host.face != 0 || cmd->dma.host.mipmap != 0) {\n\t\tDRM_ERROR(\"face and mipmap for cursors should never != 0\\n\");\n\t\treturn;\n\t}\n\n\tif (cmd->header.size < 64) {\n\t\tDRM_ERROR(\"at least one full copy box must be given\\n\");\n\t\treturn;\n\t}\n\n\tbox = (SVGA3dCopyBox *)&cmd[1];\n\tbox_count = (cmd->header.size - sizeof(SVGA3dCmdSurfaceDMA)) /\n\t\t\tsizeof(SVGA3dCopyBox);\n\n\tif (cmd->dma.guest.ptr.offset % PAGE_SIZE ||\n\t    box->x != 0    || box->y != 0    || box->z != 0    ||\n\t    box->srcx != 0 || box->srcy != 0 || box->srcz != 0 ||\n\t    box->d != 1    || box_count != 1 ||\n\t    box->w > 64 || box->h > 64) {\n\t\t/* TODO handle none page aligned offsets */\n\t\t/* TODO handle more dst & src != 0 */\n\t\t/* TODO handle more then one copy */\n\t\tDRM_ERROR(\"Can't snoop dma request for cursor!\\n\");\n\t\tDRM_ERROR(\"(%u, %u, %u) (%u, %u, %u) (%ux%ux%u) %u %u\\n\",\n\t\t\t  box->srcx, box->srcy, box->srcz,\n\t\t\t  box->x, box->y, box->z,\n\t\t\t  box->w, box->h, box->d, box_count,\n\t\t\t  cmd->dma.guest.ptr.offset);\n\t\treturn;\n\t}\n\n\tkmap_offset = cmd->dma.guest.ptr.offset >> PAGE_SHIFT;\n\tkmap_num = (64*64*4) >> PAGE_SHIFT;\n\n\tret = ttm_bo_reserve(bo, true, false, NULL);\n\tif (unlikely(ret != 0)) {\n\t\tDRM_ERROR(\"reserve failed\\n\");\n\t\treturn;\n\t}\n\n\tret = ttm_bo_kmap(bo, kmap_offset, kmap_num, &map);\n\tif (unlikely(ret != 0))\n\t\tgoto err_unreserve;\n\n\tvirtual = ttm_kmap_obj_virtual(&map, &dummy);\n\n\tif (box->w == 64 && cmd->dma.guest.pitch == 64*4) {\n\t\tmemcpy(srf->snooper.image, virtual, 64*64*4);\n\t} else {\n\t\t/* Image is unsigned pointer. */\n\t\tfor (i = 0; i < box->h; i++)\n\t\t\tmemcpy(srf->snooper.image + i * 64,\n\t\t\t       virtual + i * cmd->dma.guest.pitch,\n\t\t\t       box->w * 4);\n\t}\n\n\tsrf->snooper.age++;\n\n\tttm_bo_kunmap(&map);\nerr_unreserve:\n\tttm_bo_unreserve(bo);\n}",
      "modified_lines": {
        "added": [
          "\t    box->d != 1    || box_count != 1 ||",
          "\t    box->w > 64 || box->h > 64) {"
        ],
        "deleted": [
          "\t    box->d != 1    || box_count != 1) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for the dimensions of the copy box being processed in the code.",
      "trigger_condition": "An attacker provides crafted input with dimensions exceeding the expected bounds, leading to an out-of-bounds memory access vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the width or height of the copy box exceeds the expected limits of 64x64 pixels, allowing potential out-of-bounds memory access when processing the data.",
      "id": 141,
      "code_after_change_normalized": "void FUN1(struct vmw_surface *VAR1,\nstruct ttm_object_file *VAR2,\nstruct ttm_buffer_object *VAR3,\nSVGA3dCmdHeader *VAR4)\n{\nstruct ttm_bo_kmap_obj VAR5;\nunsigned long VAR6;\nunsigned long VAR7;\nSVGA3dCopyBox *VAR8;\nunsigned VAR9;\nvoid *virtual;\nbool VAR10;\nstruct VAR11 {\nSVGA3dCmdHeader VAR4;\nSVGA3dCmdSurfaceDMA VAR12;\n} *VAR13;\nint VAR14, VAR15;\nVAR13 = FUN2(VAR4, struct VAR11, VAR4);\nif (!VAR1->VAR16.VAR17)\nreturn;\nif (VAR13->VAR12.VAR18.VAR19 != 0 || VAR13->VAR12.VAR18.VAR20 != 0) {\nFUN3(\"STR\");\nreturn;\n}\nif (VAR13->VAR4.VAR21 < 64) {\nFUN3(\"STR\");\nreturn;\n}\nVAR8 = (VAR22 *)&VAR13[1];\nVAR9 = (VAR13->VAR4.VAR21 - sizeof(VAR23)) /\nsizeof(VAR22);\nif (VAR13->VAR12.VAR24.VAR25.VAR26 % VAR27 ||\nVAR8->VAR28 != 0    || VAR8->VAR29 != 0    || VAR8->VAR30 != 0    ||\nVAR8->VAR31 != 0 || VAR8->VAR32 != 0 || VAR8->VAR33 != 0 ||\nVAR8->VAR34 != 1    || VAR9 != 1 ||\nVAR8->VAR35 > 64 || VAR8->VAR36 > 64) {\nFUN3(\"STR\");\nFUN3(\"STR\",\nVAR8->VAR31, VAR8->VAR32, VAR8->VAR33,\nVAR8->VAR28, VAR8->VAR29, VAR8->VAR30,\nVAR8->VAR35, VAR8->VAR36, VAR8->VAR34, VAR9,\nVAR13->VAR12.VAR24.VAR25.VAR26);\nreturn;\n}\nVAR6 = VAR13->VAR12.VAR24.VAR25.VAR26 >> VAR37;\nVAR7 = (64*64*4) >> VAR37;\nVAR15 = FUN4(VAR3, true, false, NULL);\nif (FUN5(VAR15 != 0)) {\nFUN3(\"STR\");\nreturn;\n}\nVAR15 = FUN6(VAR3, VAR6, VAR7, &VAR5);\nif (FUN5(VAR15 != 0))\ngoto VAR38;\nvirtual = FUN7(&VAR5, &VAR10);\nif (VAR8->VAR35 == 64 && VAR13->VAR12.VAR24.VAR39 == 64*4) {\nFUN8(VAR1->VAR16.VAR17, virtual, 64*64*4);\n} else {\nfor (VAR14 = 0; VAR14 < VAR8->VAR36; VAR14++)\nFUN8(VAR1->VAR16.VAR17 + VAR14 * 64,\nvirtual + VAR14 * VAR13->VAR12.VAR24.VAR39,\nVAR8->VAR35 * 4);\n}\nVAR1->VAR16.VAR40++;\nFUN9(&VAR5);\nVAR38:\nFUN10(VAR3);\n}\n",
      "code_before_change_normalized": "void FUN1(struct vmw_surface *VAR1,\nstruct ttm_object_file *VAR2,\nstruct ttm_buffer_object *VAR3,\nSVGA3dCmdHeader *VAR4)\n{\nstruct ttm_bo_kmap_obj VAR5;\nunsigned long VAR6;\nunsigned long VAR7;\nSVGA3dCopyBox *VAR8;\nunsigned VAR9;\nvoid *virtual;\nbool VAR10;\nstruct VAR11 {\nSVGA3dCmdHeader VAR4;\nSVGA3dCmdSurfaceDMA VAR12;\n} *VAR13;\nint VAR14, VAR15;\nVAR13 = FUN2(VAR4, struct VAR11, VAR4);\nif (!VAR1->VAR16.VAR17)\nreturn;\nif (VAR13->VAR12.VAR18.VAR19 != 0 || VAR13->VAR12.VAR18.VAR20 != 0) {\nFUN3(\"STR\");\nreturn;\n}\nif (VAR13->VAR4.VAR21 < 64) {\nFUN3(\"STR\");\nreturn;\n}\nVAR8 = (VAR22 *)&VAR13[1];\nVAR9 = (VAR13->VAR4.VAR21 - sizeof(VAR23)) /\nsizeof(VAR22);\nif (VAR13->VAR12.VAR24.VAR25.VAR26 % VAR27 ||\nVAR8->VAR28 != 0    || VAR8->VAR29 != 0    || VAR8->VAR30 != 0    ||\nVAR8->VAR31 != 0 || VAR8->VAR32 != 0 || VAR8->VAR33 != 0 ||\nVAR8->VAR34 != 1    || VAR9 != 1) {\nFUN3(\"STR\");\nFUN3(\"STR\",\nVAR8->VAR31, VAR8->VAR32, VAR8->VAR33,\nVAR8->VAR28, VAR8->VAR29, VAR8->VAR30,\nVAR8->VAR35, VAR8->VAR36, VAR8->VAR34, VAR9,\nVAR13->VAR12.VAR24.VAR25.VAR26);\nreturn;\n}\nVAR6 = VAR13->VAR12.VAR24.VAR25.VAR26 >> VAR37;\nVAR7 = (64*64*4) >> VAR37;\nVAR15 = FUN4(VAR3, true, false, NULL);\nif (FUN5(VAR15 != 0)) {\nFUN3(\"STR\");\nreturn;\n}\nVAR15 = FUN6(VAR3, VAR6, VAR7, &VAR5);\nif (FUN5(VAR15 != 0))\ngoto VAR38;\nvirtual = FUN7(&VAR5, &VAR10);\nif (VAR8->VAR35 == 64 && VAR13->VAR12.VAR24.VAR39 == 64*4) {\nFUN8(VAR1->VAR16.VAR17, virtual, 64*64*4);\n} else {\nfor (VAR14 = 0; VAR14 < VAR8->VAR36; VAR14++)\nFUN8(VAR1->VAR16.VAR17 + VAR14 * 64,\nvirtual + VAR14 * VAR13->VAR12.VAR24.VAR39,\nVAR8->VAR35 * 4);\n}\nVAR1->VAR16.VAR40++;\nFUN9(&VAR5);\nVAR38:\nFUN10(VAR3);\n}\n",
      "code_after_change_raw": "void vmw_kms_cursor_snoop(struct vmw_surface *srf,\nstruct ttm_object_file *tfile,\nstruct ttm_buffer_object *bo,\nSVGA3dCmdHeader *header)\n{\nstruct ttm_bo_kmap_obj map;\nunsigned long kmap_offset;\nunsigned long kmap_num;\nSVGA3dCopyBox *box;\nunsigned box_count;\nvoid *virtual;\nbool dummy;\nstruct vmw_dma_cmd {\nSVGA3dCmdHeader header;\nSVGA3dCmdSurfaceDMA dma;\n} *cmd;\nint i, ret;\ncmd = container_of(header, struct vmw_dma_cmd, header);\nif (!srf->snooper.image)\nreturn;\nif (cmd->dma.host.face != 0 || cmd->dma.host.mipmap != 0) {\nDRM_ERROR(\"face and mipmap for cursors should never != 0\\n\");\nreturn;\n}\nif (cmd->header.size < 64) {\nDRM_ERROR(\"at least one full copy box must be given\\n\");\nreturn;\n}\nbox = (SVGA3dCopyBox *)&cmd[1];\nbox_count = (cmd->header.size - sizeof(SVGA3dCmdSurfaceDMA)) /\nsizeof(SVGA3dCopyBox);\nif (cmd->dma.guest.ptr.offset % PAGE_SIZE ||\nbox->x != 0    || box->y != 0    || box->z != 0    ||\nbox->srcx != 0 || box->srcy != 0 || box->srcz != 0 ||\nbox->d != 1    || box_count != 1 ||\nbox->w > 64 || box->h > 64) {\nDRM_ERROR(\"Can't snoop dma request for cursor!\\n\");\nDRM_ERROR(\"(%u, %u, %u) (%u, %u, %u) (%ux%ux%u) %u %u\\n\",\nbox->srcx, box->srcy, box->srcz,\nbox->x, box->y, box->z,\nbox->w, box->h, box->d, box_count,\ncmd->dma.guest.ptr.offset);\nreturn;\n}\nkmap_offset = cmd->dma.guest.ptr.offset >> PAGE_SHIFT;\nkmap_num = (64*64*4) >> PAGE_SHIFT;\nret = ttm_bo_reserve(bo, true, false, NULL);\nif (unlikely(ret != 0)) {\nDRM_ERROR(\"reserve failed\\n\");\nreturn;\n}\nret = ttm_bo_kmap(bo, kmap_offset, kmap_num, &map);\nif (unlikely(ret != 0))\ngoto err_unreserve;\nvirtual = ttm_kmap_obj_virtual(&map, &dummy);\nif (box->w == 64 && cmd->dma.guest.pitch == 64*4) {\nmemcpy(srf->snooper.image, virtual, 64*64*4);\n} else {\nfor (i = 0; i < box->h; i++)\nmemcpy(srf->snooper.image + i * 64,\nvirtual + i * cmd->dma.guest.pitch,\nbox->w * 4);\n}\nsrf->snooper.age++;\nttm_bo_kunmap(&map);\nerr_unreserve:\nttm_bo_unreserve(bo);\n}\n",
      "code_before_change_raw": "void vmw_kms_cursor_snoop(struct vmw_surface *srf,\nstruct ttm_object_file *tfile,\nstruct ttm_buffer_object *bo,\nSVGA3dCmdHeader *header)\n{\nstruct ttm_bo_kmap_obj map;\nunsigned long kmap_offset;\nunsigned long kmap_num;\nSVGA3dCopyBox *box;\nunsigned box_count;\nvoid *virtual;\nbool dummy;\nstruct vmw_dma_cmd {\nSVGA3dCmdHeader header;\nSVGA3dCmdSurfaceDMA dma;\n} *cmd;\nint i, ret;\ncmd = container_of(header, struct vmw_dma_cmd, header);\nif (!srf->snooper.image)\nreturn;\nif (cmd->dma.host.face != 0 || cmd->dma.host.mipmap != 0) {\nDRM_ERROR(\"face and mipmap for cursors should never != 0\\n\");\nreturn;\n}\nif (cmd->header.size < 64) {\nDRM_ERROR(\"at least one full copy box must be given\\n\");\nreturn;\n}\nbox = (SVGA3dCopyBox *)&cmd[1];\nbox_count = (cmd->header.size - sizeof(SVGA3dCmdSurfaceDMA)) /\nsizeof(SVGA3dCopyBox);\nif (cmd->dma.guest.ptr.offset % PAGE_SIZE ||\nbox->x != 0    || box->y != 0    || box->z != 0    ||\nbox->srcx != 0 || box->srcy != 0 || box->srcz != 0 ||\nbox->d != 1    || box_count != 1) {\nDRM_ERROR(\"Can't snoop dma request for cursor!\\n\");\nDRM_ERROR(\"(%u, %u, %u) (%u, %u, %u) (%ux%ux%u) %u %u\\n\",\nbox->srcx, box->srcy, box->srcz,\nbox->x, box->y, box->z,\nbox->w, box->h, box->d, box_count,\ncmd->dma.guest.ptr.offset);\nreturn;\n}\nkmap_offset = cmd->dma.guest.ptr.offset >> PAGE_SHIFT;\nkmap_num = (64*64*4) >> PAGE_SHIFT;\nret = ttm_bo_reserve(bo, true, false, NULL);\nif (unlikely(ret != 0)) {\nDRM_ERROR(\"reserve failed\\n\");\nreturn;\n}\nret = ttm_bo_kmap(bo, kmap_offset, kmap_num, &map);\nif (unlikely(ret != 0))\ngoto err_unreserve;\nvirtual = ttm_kmap_obj_virtual(&map, &dummy);\nif (box->w == 64 && cmd->dma.guest.pitch == 64*4) {\nmemcpy(srf->snooper.image, virtual, 64*64*4);\n} else {\nfor (i = 0; i < box->h; i++)\nmemcpy(srf->snooper.image + i * 64,\nvirtual + i * cmd->dma.guest.pitch,\nbox->w * 4);\n}\nsrf->snooper.age++;\nttm_bo_kunmap(&map);\nerr_unreserve:\nttm_bo_unreserve(bo);\n}\n"
    }
  ],
  "CVE-2022-41674": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and handling of buffer sizes in memory copy operations.",
        "trigger_condition": "Injection of WLAN frames by attackers can lead to buffer overflow in the ieee80211_bss_info_update function.",
        "specific_code_behavior_causing_vulnerability": "The code uses a u8 variable to store the length of data to be copied, which may not be sufficient to handle larger values, potentially leading to buffer overflow if the data size exceeds the capacity of u8."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that variables used to store data lengths in memory copy operations have sufficient capacity to handle larger values. In this case, changing the variable type from u8 to size_t allows for larger values to be stored, reducing the risk of buffer overflow. By making this modification, the code can safely handle memory operations and prevent potential security vulnerabilities related to buffer overflows.",
      "GPT_analysis": "The modification is necessary to address a buffer overflow vulnerability (CVE-2022-41674) in the Linux kernel. By changing the variable `u8 cpy_len;` to `size_t cpy_len;`, we ensure that the `cpy_len` variable can hold larger values, preventing potential buffer overflow issues that could be exploited by attackers injecting WLAN frames. This modification helps to ensure the safe handling of memory operations within the `cfg80211_update_notlisted_nontrans` function, reducing the risk of security vulnerabilities related to buffer overflows.",
      "GPT_purpose": "Update the information elements (IEs) of a non-transmitted BSS based on the received management frame in a wireless network.",
      "GPT_function": "\n1. Update nontransmitted BSS information in the wireless configuration.\n2. Generate a new information element (IE) for the nontransmitted BSS.\n3. Copy and update the IE data for the nontransmitted BSS.",
      "CVE_id": "CVE-2022-41674",
      "code_before_change": "static void\ncfg80211_update_notlisted_nontrans(struct wiphy *wiphy,\n\t\t\t\t   struct cfg80211_bss *nontrans_bss,\n\t\t\t\t   struct ieee80211_mgmt *mgmt, size_t len)\n{\n\tu8 *ie, *new_ie, *pos;\n\tconst struct element *nontrans_ssid;\n\tconst u8 *trans_ssid, *mbssid;\n\tsize_t ielen = len - offsetof(struct ieee80211_mgmt,\n\t\t\t\t      u.probe_resp.variable);\n\tsize_t new_ie_len;\n\tstruct cfg80211_bss_ies *new_ies;\n\tconst struct cfg80211_bss_ies *old;\n\tu8 cpy_len;\n\n\tlockdep_assert_held(&wiphy_to_rdev(wiphy)->bss_lock);\n\n\tie = mgmt->u.probe_resp.variable;\n\n\tnew_ie_len = ielen;\n\ttrans_ssid = cfg80211_find_ie(WLAN_EID_SSID, ie, ielen);\n\tif (!trans_ssid)\n\t\treturn;\n\tnew_ie_len -= trans_ssid[1];\n\tmbssid = cfg80211_find_ie(WLAN_EID_MULTIPLE_BSSID, ie, ielen);\n\t/*\n\t * It's not valid to have the MBSSID element before SSID\n\t * ignore if that happens - the code below assumes it is\n\t * after (while copying things inbetween).\n\t */\n\tif (!mbssid || mbssid < trans_ssid)\n\t\treturn;\n\tnew_ie_len -= mbssid[1];\n\n\tnontrans_ssid = ieee80211_bss_get_elem(nontrans_bss, WLAN_EID_SSID);\n\tif (!nontrans_ssid)\n\t\treturn;\n\n\tnew_ie_len += nontrans_ssid->datalen;\n\n\t/* generate new ie for nontrans BSS\n\t * 1. replace SSID with nontrans BSS' SSID\n\t * 2. skip MBSSID IE\n\t */\n\tnew_ie = kzalloc(new_ie_len, GFP_ATOMIC);\n\tif (!new_ie)\n\t\treturn;\n\n\tnew_ies = kzalloc(sizeof(*new_ies) + new_ie_len, GFP_ATOMIC);\n\tif (!new_ies)\n\t\tgoto out_free;\n\n\tpos = new_ie;\n\n\t/* copy the nontransmitted SSID */\n\tcpy_len = nontrans_ssid->datalen + 2;\n\tmemcpy(pos, nontrans_ssid, cpy_len);\n\tpos += cpy_len;\n\t/* copy the IEs between SSID and MBSSID */\n\tcpy_len = trans_ssid[1] + 2;\n\tmemcpy(pos, (trans_ssid + cpy_len), (mbssid - (trans_ssid + cpy_len)));\n\tpos += (mbssid - (trans_ssid + cpy_len));\n\t/* copy the IEs after MBSSID */\n\tcpy_len = mbssid[1] + 2;\n\tmemcpy(pos, mbssid + cpy_len, ((ie + ielen) - (mbssid + cpy_len)));\n\n\t/* update ie */\n\tnew_ies->len = new_ie_len;\n\tnew_ies->tsf = le64_to_cpu(mgmt->u.probe_resp.timestamp);\n\tnew_ies->from_beacon = ieee80211_is_beacon(mgmt->frame_control);\n\tmemcpy(new_ies->data, new_ie, new_ie_len);\n\tif (ieee80211_is_probe_resp(mgmt->frame_control)) {\n\t\told = rcu_access_pointer(nontrans_bss->proberesp_ies);\n\t\trcu_assign_pointer(nontrans_bss->proberesp_ies, new_ies);\n\t\trcu_assign_pointer(nontrans_bss->ies, new_ies);\n\t\tif (old)\n\t\t\tkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n\t} else {\n\t\told = rcu_access_pointer(nontrans_bss->beacon_ies);\n\t\trcu_assign_pointer(nontrans_bss->beacon_ies, new_ies);\n\t\trcu_assign_pointer(nontrans_bss->ies, new_ies);\n\t\tif (old)\n\t\t\tkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n\t}\n\nout_free:\n\tkfree(new_ie);\n}",
      "code_after_change": "static void\ncfg80211_update_notlisted_nontrans(struct wiphy *wiphy,\n\t\t\t\t   struct cfg80211_bss *nontrans_bss,\n\t\t\t\t   struct ieee80211_mgmt *mgmt, size_t len)\n{\n\tu8 *ie, *new_ie, *pos;\n\tconst struct element *nontrans_ssid;\n\tconst u8 *trans_ssid, *mbssid;\n\tsize_t ielen = len - offsetof(struct ieee80211_mgmt,\n\t\t\t\t      u.probe_resp.variable);\n\tsize_t new_ie_len;\n\tstruct cfg80211_bss_ies *new_ies;\n\tconst struct cfg80211_bss_ies *old;\n\tsize_t cpy_len;\n\n\tlockdep_assert_held(&wiphy_to_rdev(wiphy)->bss_lock);\n\n\tie = mgmt->u.probe_resp.variable;\n\n\tnew_ie_len = ielen;\n\ttrans_ssid = cfg80211_find_ie(WLAN_EID_SSID, ie, ielen);\n\tif (!trans_ssid)\n\t\treturn;\n\tnew_ie_len -= trans_ssid[1];\n\tmbssid = cfg80211_find_ie(WLAN_EID_MULTIPLE_BSSID, ie, ielen);\n\t/*\n\t * It's not valid to have the MBSSID element before SSID\n\t * ignore if that happens - the code below assumes it is\n\t * after (while copying things inbetween).\n\t */\n\tif (!mbssid || mbssid < trans_ssid)\n\t\treturn;\n\tnew_ie_len -= mbssid[1];\n\n\tnontrans_ssid = ieee80211_bss_get_elem(nontrans_bss, WLAN_EID_SSID);\n\tif (!nontrans_ssid)\n\t\treturn;\n\n\tnew_ie_len += nontrans_ssid->datalen;\n\n\t/* generate new ie for nontrans BSS\n\t * 1. replace SSID with nontrans BSS' SSID\n\t * 2. skip MBSSID IE\n\t */\n\tnew_ie = kzalloc(new_ie_len, GFP_ATOMIC);\n\tif (!new_ie)\n\t\treturn;\n\n\tnew_ies = kzalloc(sizeof(*new_ies) + new_ie_len, GFP_ATOMIC);\n\tif (!new_ies)\n\t\tgoto out_free;\n\n\tpos = new_ie;\n\n\t/* copy the nontransmitted SSID */\n\tcpy_len = nontrans_ssid->datalen + 2;\n\tmemcpy(pos, nontrans_ssid, cpy_len);\n\tpos += cpy_len;\n\t/* copy the IEs between SSID and MBSSID */\n\tcpy_len = trans_ssid[1] + 2;\n\tmemcpy(pos, (trans_ssid + cpy_len), (mbssid - (trans_ssid + cpy_len)));\n\tpos += (mbssid - (trans_ssid + cpy_len));\n\t/* copy the IEs after MBSSID */\n\tcpy_len = mbssid[1] + 2;\n\tmemcpy(pos, mbssid + cpy_len, ((ie + ielen) - (mbssid + cpy_len)));\n\n\t/* update ie */\n\tnew_ies->len = new_ie_len;\n\tnew_ies->tsf = le64_to_cpu(mgmt->u.probe_resp.timestamp);\n\tnew_ies->from_beacon = ieee80211_is_beacon(mgmt->frame_control);\n\tmemcpy(new_ies->data, new_ie, new_ie_len);\n\tif (ieee80211_is_probe_resp(mgmt->frame_control)) {\n\t\told = rcu_access_pointer(nontrans_bss->proberesp_ies);\n\t\trcu_assign_pointer(nontrans_bss->proberesp_ies, new_ies);\n\t\trcu_assign_pointer(nontrans_bss->ies, new_ies);\n\t\tif (old)\n\t\t\tkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n\t} else {\n\t\told = rcu_access_pointer(nontrans_bss->beacon_ies);\n\t\trcu_assign_pointer(nontrans_bss->beacon_ies, new_ies);\n\t\trcu_assign_pointer(nontrans_bss->ies, new_ies);\n\t\tif (old)\n\t\t\tkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n\t}\n\nout_free:\n\tkfree(new_ie);\n}",
      "modified_lines": {
        "added": [
          "\tsize_t cpy_len;"
        ],
        "deleted": [
          "\tu8 cpy_len;"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and handling of buffer sizes in memory copy operations.",
      "trigger_condition": "Injection of WLAN frames by attackers can lead to buffer overflow in the ieee80211_bss_info_update function.",
      "specific_code_behavior_causing_vulnerability": "The code uses a u8 variable to store the length of data to be copied, which may not be sufficient to handle larger values, potentially leading to buffer overflow if the data size exceeds the capacity of u8.",
      "id": 142,
      "code_after_change_normalized": "static void\nFUN1(struct VAR1 *VAR1,\nstruct cfg80211_bss *VAR2,\nstruct ieee80211_mgmt *VAR3, size_t VAR4)\n{\nu8 *VAR5, *VAR6, *VAR7;\nconst struct element *VAR8;\nconst u8 *VAR9, *VAR10;\nsize_t VAR11 = VAR4 - FUN2(struct VAR12,\nVAR13.VAR14.VAR15);\nsize_t VAR16;\nstruct cfg80211_bss_ies *VAR17;\nconst struct cfg80211_bss_ies *VAR18;\nsize_t VAR19;\nFUN3(&FUN4(VAR1)->VAR20);\nVAR5 = VAR3->VAR13.VAR14.VAR15;\nVAR16 = VAR11;\nVAR9 = FUN5(VAR21, VAR5, VAR11);\nif (!VAR9)\nreturn;\nVAR16 -= VAR9[1];\nVAR10 = FUN5(VAR22, VAR5, VAR11);\nif (!VAR10 || VAR10 < VAR9)\nreturn;\nVAR16 -= VAR10[1];\nVAR8 = FUN6(VAR2, VAR21);\nif (!VAR8)\nreturn;\nVAR16 += VAR8->VAR23;\nVAR6 = FUN7(VAR16, VAR24);\nif (!VAR6)\nreturn;\nVAR17 = FUN7(sizeof(*VAR17) + VAR16, VAR24);\nif (!VAR17)\ngoto VAR25;\nVAR7 = VAR6;\nVAR19 = VAR8->VAR23 + 2;\nFUN8(VAR7, VAR8, VAR19);\nVAR7 += VAR19;\nVAR19 = VAR9[1] + 2;\nFUN8(VAR7, (VAR9 + VAR19), (VAR10 - (VAR9 + VAR19)));\nVAR7 += (VAR10 - (VAR9 + VAR19));\nVAR19 = VAR10[1] + 2;\nFUN8(VAR7, VAR10 + VAR19, ((VAR5 + VAR11) - (VAR10 + VAR19)));\nVAR17->VAR4 = VAR16;\nVAR17->VAR26 = FUN9(VAR3->VAR13.VAR14.VAR27);\nVAR17->VAR28 = FUN10(VAR3->VAR29);\nFUN8(VAR17->VAR30, VAR6, VAR16);\nif (FUN11(VAR3->VAR29)) {\nVAR18 = FUN12(VAR2->VAR31);\nFUN13(VAR2->VAR31, VAR17);\nFUN13(VAR2->VAR32, VAR17);\nif (VAR18)\nFUN14((struct VAR33 *)VAR18, VAR34);\n} else {\nVAR18 = FUN12(VAR2->VAR35);\nFUN13(VAR2->VAR35, VAR17);\nFUN13(VAR2->VAR32, VAR17);\nif (VAR18)\nFUN14((struct VAR33 *)VAR18, VAR34);\n}\nVAR25:\nFUN15(VAR6);\n}\n",
      "code_before_change_normalized": "static void\nFUN1(struct VAR1 *VAR1,\nstruct cfg80211_bss *VAR2,\nstruct ieee80211_mgmt *VAR3, size_t VAR4)\n{\nu8 *VAR5, *VAR6, *VAR7;\nconst struct element *VAR8;\nconst u8 *VAR9, *VAR10;\nsize_t VAR11 = VAR4 - FUN2(struct VAR12,\nVAR13.VAR14.VAR15);\nsize_t VAR16;\nstruct cfg80211_bss_ies *VAR17;\nconst struct cfg80211_bss_ies *VAR18;\nu8 VAR19;\nFUN3(&FUN4(VAR1)->VAR20);\nVAR5 = VAR3->VAR13.VAR14.VAR15;\nVAR16 = VAR11;\nVAR9 = FUN5(VAR21, VAR5, VAR11);\nif (!VAR9)\nreturn;\nVAR16 -= VAR9[1];\nVAR10 = FUN5(VAR22, VAR5, VAR11);\nif (!VAR10 || VAR10 < VAR9)\nreturn;\nVAR16 -= VAR10[1];\nVAR8 = FUN6(VAR2, VAR21);\nif (!VAR8)\nreturn;\nVAR16 += VAR8->VAR23;\nVAR6 = FUN7(VAR16, VAR24);\nif (!VAR6)\nreturn;\nVAR17 = FUN7(sizeof(*VAR17) + VAR16, VAR24);\nif (!VAR17)\ngoto VAR25;\nVAR7 = VAR6;\nVAR19 = VAR8->VAR23 + 2;\nFUN8(VAR7, VAR8, VAR19);\nVAR7 += VAR19;\nVAR19 = VAR9[1] + 2;\nFUN8(VAR7, (VAR9 + VAR19), (VAR10 - (VAR9 + VAR19)));\nVAR7 += (VAR10 - (VAR9 + VAR19));\nVAR19 = VAR10[1] + 2;\nFUN8(VAR7, VAR10 + VAR19, ((VAR5 + VAR11) - (VAR10 + VAR19)));\nVAR17->VAR4 = VAR16;\nVAR17->VAR26 = FUN9(VAR3->VAR13.VAR14.VAR27);\nVAR17->VAR28 = FUN10(VAR3->VAR29);\nFUN8(VAR17->VAR30, VAR6, VAR16);\nif (FUN11(VAR3->VAR29)) {\nVAR18 = FUN12(VAR2->VAR31);\nFUN13(VAR2->VAR31, VAR17);\nFUN13(VAR2->VAR32, VAR17);\nif (VAR18)\nFUN14((struct VAR33 *)VAR18, VAR34);\n} else {\nVAR18 = FUN12(VAR2->VAR35);\nFUN13(VAR2->VAR35, VAR17);\nFUN13(VAR2->VAR32, VAR17);\nif (VAR18)\nFUN14((struct VAR33 *)VAR18, VAR34);\n}\nVAR25:\nFUN15(VAR6);\n}\n",
      "code_after_change_raw": "static void\ncfg80211_update_notlisted_nontrans(struct wiphy *wiphy,\nstruct cfg80211_bss *nontrans_bss,\nstruct ieee80211_mgmt *mgmt, size_t len)\n{\nu8 *ie, *new_ie, *pos;\nconst struct element *nontrans_ssid;\nconst u8 *trans_ssid, *mbssid;\nsize_t ielen = len - offsetof(struct ieee80211_mgmt,\nu.probe_resp.variable);\nsize_t new_ie_len;\nstruct cfg80211_bss_ies *new_ies;\nconst struct cfg80211_bss_ies *old;\nsize_t cpy_len;\nlockdep_assert_held(&wiphy_to_rdev(wiphy)->bss_lock);\nie = mgmt->u.probe_resp.variable;\nnew_ie_len = ielen;\ntrans_ssid = cfg80211_find_ie(WLAN_EID_SSID, ie, ielen);\nif (!trans_ssid)\nreturn;\nnew_ie_len -= trans_ssid[1];\nmbssid = cfg80211_find_ie(WLAN_EID_MULTIPLE_BSSID, ie, ielen);\nif (!mbssid || mbssid < trans_ssid)\nreturn;\nnew_ie_len -= mbssid[1];\nnontrans_ssid = ieee80211_bss_get_elem(nontrans_bss, WLAN_EID_SSID);\nif (!nontrans_ssid)\nreturn;\nnew_ie_len += nontrans_ssid->datalen;\nnew_ie = kzalloc(new_ie_len, GFP_ATOMIC);\nif (!new_ie)\nreturn;\nnew_ies = kzalloc(sizeof(*new_ies) + new_ie_len, GFP_ATOMIC);\nif (!new_ies)\ngoto out_free;\npos = new_ie;\ncpy_len = nontrans_ssid->datalen + 2;\nmemcpy(pos, nontrans_ssid, cpy_len);\npos += cpy_len;\ncpy_len = trans_ssid[1] + 2;\nmemcpy(pos, (trans_ssid + cpy_len), (mbssid - (trans_ssid + cpy_len)));\npos += (mbssid - (trans_ssid + cpy_len));\ncpy_len = mbssid[1] + 2;\nmemcpy(pos, mbssid + cpy_len, ((ie + ielen) - (mbssid + cpy_len)));\nnew_ies->len = new_ie_len;\nnew_ies->tsf = le64_to_cpu(mgmt->u.probe_resp.timestamp);\nnew_ies->from_beacon = ieee80211_is_beacon(mgmt->frame_control);\nmemcpy(new_ies->data, new_ie, new_ie_len);\nif (ieee80211_is_probe_resp(mgmt->frame_control)) {\nold = rcu_access_pointer(nontrans_bss->proberesp_ies);\nrcu_assign_pointer(nontrans_bss->proberesp_ies, new_ies);\nrcu_assign_pointer(nontrans_bss->ies, new_ies);\nif (old)\nkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n} else {\nold = rcu_access_pointer(nontrans_bss->beacon_ies);\nrcu_assign_pointer(nontrans_bss->beacon_ies, new_ies);\nrcu_assign_pointer(nontrans_bss->ies, new_ies);\nif (old)\nkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n}\nout_free:\nkfree(new_ie);\n}\n",
      "code_before_change_raw": "static void\ncfg80211_update_notlisted_nontrans(struct wiphy *wiphy,\nstruct cfg80211_bss *nontrans_bss,\nstruct ieee80211_mgmt *mgmt, size_t len)\n{\nu8 *ie, *new_ie, *pos;\nconst struct element *nontrans_ssid;\nconst u8 *trans_ssid, *mbssid;\nsize_t ielen = len - offsetof(struct ieee80211_mgmt,\nu.probe_resp.variable);\nsize_t new_ie_len;\nstruct cfg80211_bss_ies *new_ies;\nconst struct cfg80211_bss_ies *old;\nu8 cpy_len;\nlockdep_assert_held(&wiphy_to_rdev(wiphy)->bss_lock);\nie = mgmt->u.probe_resp.variable;\nnew_ie_len = ielen;\ntrans_ssid = cfg80211_find_ie(WLAN_EID_SSID, ie, ielen);\nif (!trans_ssid)\nreturn;\nnew_ie_len -= trans_ssid[1];\nmbssid = cfg80211_find_ie(WLAN_EID_MULTIPLE_BSSID, ie, ielen);\nif (!mbssid || mbssid < trans_ssid)\nreturn;\nnew_ie_len -= mbssid[1];\nnontrans_ssid = ieee80211_bss_get_elem(nontrans_bss, WLAN_EID_SSID);\nif (!nontrans_ssid)\nreturn;\nnew_ie_len += nontrans_ssid->datalen;\nnew_ie = kzalloc(new_ie_len, GFP_ATOMIC);\nif (!new_ie)\nreturn;\nnew_ies = kzalloc(sizeof(*new_ies) + new_ie_len, GFP_ATOMIC);\nif (!new_ies)\ngoto out_free;\npos = new_ie;\ncpy_len = nontrans_ssid->datalen + 2;\nmemcpy(pos, nontrans_ssid, cpy_len);\npos += cpy_len;\ncpy_len = trans_ssid[1] + 2;\nmemcpy(pos, (trans_ssid + cpy_len), (mbssid - (trans_ssid + cpy_len)));\npos += (mbssid - (trans_ssid + cpy_len));\ncpy_len = mbssid[1] + 2;\nmemcpy(pos, mbssid + cpy_len, ((ie + ielen) - (mbssid + cpy_len)));\nnew_ies->len = new_ie_len;\nnew_ies->tsf = le64_to_cpu(mgmt->u.probe_resp.timestamp);\nnew_ies->from_beacon = ieee80211_is_beacon(mgmt->frame_control);\nmemcpy(new_ies->data, new_ie, new_ie_len);\nif (ieee80211_is_probe_resp(mgmt->frame_control)) {\nold = rcu_access_pointer(nontrans_bss->proberesp_ies);\nrcu_assign_pointer(nontrans_bss->proberesp_ies, new_ies);\nrcu_assign_pointer(nontrans_bss->ies, new_ies);\nif (old)\nkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n} else {\nold = rcu_access_pointer(nontrans_bss->beacon_ies);\nrcu_assign_pointer(nontrans_bss->beacon_ies, new_ies);\nrcu_assign_pointer(nontrans_bss->ies, new_ies);\nif (old)\nkfree_rcu((struct cfg80211_bss_ies *)old, rcu_head);\n}\nout_free:\nkfree(new_ie);\n}\n"
    }
  ],
  "CVE-2022-43750": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for memory mapping operations, allowing user-space clients to corrupt internal memory.",
        "trigger_condition": "User-space client performs a memory mapping operation that is not properly restricted, enabling unauthorized write access to the monitor's internal memory.",
        "specific_code_behavior_causing_vulnerability": "The code does not check for write permissions before allowing memory mapping operations, leading to a vulnerability where user-space clients can corrupt the monitor's internal memory by writing to it."
      },
      "solution": "To mitigate the vulnerability, it is necessary to validate write permissions before allowing memory mapping operations. By adding code to check if writing is allowed and returning an error if not, the system can prevent unauthorized write access to the monitor's internal memory. Additionally, clearing the VM_MAYWRITE flag further restricts write access, enhancing the security of the system.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2022-43750, which allows a user-space client to corrupt the monitor's internal memory. By adding the following lines to the code snippet:\n\n```c\nif (vma->vm_flags & VM_WRITE)\n    return -EPERM;\n\nvma->vm_flags &= ~VM_MAYWRITE;\n```\n\nThe code now checks if the memory mapping operation allows writing (`VM_WRITE` flag set). If writing is allowed, the function returns an error (`-EPERM`), preventing the user-space client from corrupting the monitor's internal memory.\n\nAdditionally, by clearing the `VM_MAYWRITE` flag, the code ensures that the memory mapping operation does not allow writing, further enhancing the security of the system.\n\nIn summary, the modification restricts write access to the memory mapping operation, preventing potential memory corruption by unauthorized users.",
      "GPT_purpose": "Map a binary file into memory for monitoring purposes.",
      "GPT_function": "\n1. Set vm_ops to mon_bin_vm_ops.\n2. Set vm_flags to VM_DONTEXPAND | VM_DONTDUMP.\n3. Set vm_private_data to filp->private_data.\n4. Call mon_bin_vma_open(vma).\n5. Return 0.",
      "CVE_id": "CVE-2022-43750",
      "code_before_change": "static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\t/* don't do anything here: \"fault\" will set up page table entries */\n\tvma->vm_ops = &mon_bin_vm_ops;\n\tvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = filp->private_data;\n\tmon_bin_vma_open(vma);\n\treturn 0;\n}",
      "code_after_change": "static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)\n{\n\t/* don't do anything here: \"fault\" will set up page table entries */\n\tvma->vm_ops = &mon_bin_vm_ops;\n\n\tif (vma->vm_flags & VM_WRITE)\n\t\treturn -EPERM;\n\n\tvma->vm_flags &= ~VM_MAYWRITE;\n\tvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\n\tvma->vm_private_data = filp->private_data;\n\tmon_bin_vma_open(vma);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (vma->vm_flags & VM_WRITE)",
          "\t\treturn -EPERM;",
          "",
          "\tvma->vm_flags &= ~VM_MAYWRITE;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper validation for memory mapping operations, allowing user-space clients to corrupt internal memory.",
      "trigger_condition": "User-space client performs a memory mapping operation that is not properly restricted, enabling unauthorized write access to the monitor's internal memory.",
      "specific_code_behavior_causing_vulnerability": "The code does not check for write permissions before allowing memory mapping operations, leading to a vulnerability where user-space clients can corrupt the monitor's internal memory by writing to it.",
      "id": 143,
      "code_after_change_normalized": "static int FUN1(struct file *VAR1, struct vm_area_struct *VAR2)\n{\nVAR2->VAR3 = &VAR4;\nif (VAR2->VAR5 & VAR6)\nreturn -VAR7;\nVAR2->VAR5 &= ~VAR8;\nVAR2->VAR5 |= VAR9 | VAR10;\nVAR2->VAR11 = VAR1->VAR12;\nFUN2(VAR2);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct file *VAR1, struct vm_area_struct *VAR2)\n{\nVAR2->VAR3 = &VAR4;\nVAR2->VAR5 |= VAR6 | VAR7;\nVAR2->VAR8 = VAR1->VAR9;\nFUN2(VAR2);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)\n{\nvma->vm_ops = &mon_bin_vm_ops;\nif (vma->vm_flags & VM_WRITE)\nreturn -EPERM;\nvma->vm_flags &= ~VM_MAYWRITE;\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\nvma->vm_private_data = filp->private_data;\nmon_bin_vma_open(vma);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int mon_bin_mmap(struct file *filp, struct vm_area_struct *vma)\n{\nvma->vm_ops = &mon_bin_vm_ops;\nvma->vm_flags |= VM_DONTEXPAND | VM_DONTDUMP;\nvma->vm_private_data = filp->private_data;\nmon_bin_vma_open(vma);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2022-4378": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of passing 'left' parameter by reference to proc_skip_spaces function, leading to potential stack overflow.",
        "trigger_condition": "The code snippet uses proc_skip_spaces function without passing 'left' parameter by reference, causing a stack overflow vulnerability if the function modifies 'left' without updating the original value in the calling function.",
        "specific_code_behavior_causing_vulnerability": "The code snippet fails to pass the 'left' parameter by reference to proc_skip_spaces function, which can result in a stack overflow vulnerability due to potential modification of 'left' without updating the original value in the calling function."
      },
      "solution": "To mitigate the vulnerability, it is necessary to pass the 'left' parameter by reference to the proc_skip_spaces function in both the write and non-write branches. This ensures that the function can safely update the 'left' value without causing a stack overflow. By making this modification, the code addresses the vulnerability and prevents potential security risks.",
      "GPT_analysis": "The modification is necessary to address the stack overflow vulnerability in the code snippet. The vulnerability arises from the use of the `proc_skip_spaces` function without passing the `left` parameter by reference. This can lead to a stack overflow if the function modifies `left` without updating the original value in the calling function.\n\nBy modifying the code to pass the `left` parameter by reference to the `proc_skip_spaces` function in both the write and non-write branches, we ensure that the function can safely update the `left` value without causing a stack overflow. This modification helps prevent potential security risks and ensures the proper functioning of the code without the vulnerability described in CVE-2022-4378.",
      "GPT_purpose": "This function is responsible for handling the processing of unsigned long vector values within the Linux kernel's SYSCTL subsystem.",
      "GPT_function": "\n1. Process a long vector with minimum and maximum values.\n2. Handle reading and writing operations for kernel parameters.\n3. Check for valid values and boundaries.\n4. Update kernel parameters based on user input.\n5. Handle error conditions and buffer management.",
      "CVE_id": "CVE-2022-4378",
      "code_before_change": "static int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table,\n\t\tint write, void *buffer, size_t *lenp, loff_t *ppos,\n\t\tunsigned long convmul, unsigned long convdiv)\n{\n\tunsigned long *i, *min, *max;\n\tint vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *p;\n\n\tif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\ti = data;\n\tmin = table->extra1;\n\tmax = table->extra2;\n\tvleft = table->maxlen / sizeof(unsigned long);\n\tleft = *lenp;\n\n\tif (write) {\n\t\tif (proc_first_pos_non_zero_ignore(ppos, table))\n\t\t\tgoto out;\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = buffer;\n\t}\n\n\tfor (; left && vleft--; i++, first = 0) {\n\t\tunsigned long val;\n\n\t\tif (write) {\n\t\t\tbool neg;\n\n\t\t\tleft -= proc_skip_spaces(&p);\n\t\t\tif (!left)\n\t\t\t\tbreak;\n\n\t\t\terr = proc_get_long(&p, &left, &val, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err || neg) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tval = convmul * val / convdiv;\n\t\t\tif ((min && val < *min) || (max && val > *max)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tWRITE_ONCE(*i, val);\n\t\t} else {\n\t\t\tval = convdiv * READ_ONCE(*i) / convmul;\n\t\t\tif (!first)\n\t\t\t\tproc_put_char(&buffer, &left, '\\t');\n\t\t\tproc_put_long(&buffer, &left, val, false);\n\t\t}\n\t}\n\n\tif (!write && !first && left && !err)\n\t\tproc_put_char(&buffer, &left, '\\n');\n\tif (write && !err)\n\t\tleft -= proc_skip_spaces(&p);\n\tif (write && first)\n\t\treturn err ? : -EINVAL;\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}",
      "code_after_change": "static int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table,\n\t\tint write, void *buffer, size_t *lenp, loff_t *ppos,\n\t\tunsigned long convmul, unsigned long convdiv)\n{\n\tunsigned long *i, *min, *max;\n\tint vleft, first = 1, err = 0;\n\tsize_t left;\n\tchar *p;\n\n\tif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\n\t\t*lenp = 0;\n\t\treturn 0;\n\t}\n\n\ti = data;\n\tmin = table->extra1;\n\tmax = table->extra2;\n\tvleft = table->maxlen / sizeof(unsigned long);\n\tleft = *lenp;\n\n\tif (write) {\n\t\tif (proc_first_pos_non_zero_ignore(ppos, table))\n\t\t\tgoto out;\n\n\t\tif (left > PAGE_SIZE - 1)\n\t\t\tleft = PAGE_SIZE - 1;\n\t\tp = buffer;\n\t}\n\n\tfor (; left && vleft--; i++, first = 0) {\n\t\tunsigned long val;\n\n\t\tif (write) {\n\t\t\tbool neg;\n\n\t\t\tproc_skip_spaces(&p, &left);\n\t\t\tif (!left)\n\t\t\t\tbreak;\n\n\t\t\terr = proc_get_long(&p, &left, &val, &neg,\n\t\t\t\t\t     proc_wspace_sep,\n\t\t\t\t\t     sizeof(proc_wspace_sep), NULL);\n\t\t\tif (err || neg) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tval = convmul * val / convdiv;\n\t\t\tif ((min && val < *min) || (max && val > *max)) {\n\t\t\t\terr = -EINVAL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t\tWRITE_ONCE(*i, val);\n\t\t} else {\n\t\t\tval = convdiv * READ_ONCE(*i) / convmul;\n\t\t\tif (!first)\n\t\t\t\tproc_put_char(&buffer, &left, '\\t');\n\t\t\tproc_put_long(&buffer, &left, val, false);\n\t\t}\n\t}\n\n\tif (!write && !first && left && !err)\n\t\tproc_put_char(&buffer, &left, '\\n');\n\tif (write && !err)\n\t\tproc_skip_spaces(&p, &left);\n\tif (write && first)\n\t\treturn err ? : -EINVAL;\n\t*lenp -= left;\nout:\n\t*ppos += *lenp;\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\t\t\tproc_skip_spaces(&p, &left);",
          "\t\tproc_skip_spaces(&p, &left);"
        ],
        "deleted": [
          "\t\t\tleft -= proc_skip_spaces(&p);",
          "\t\tleft -= proc_skip_spaces(&p);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of passing 'left' parameter by reference to proc_skip_spaces function, leading to potential stack overflow.",
      "trigger_condition": "The code snippet uses proc_skip_spaces function without passing 'left' parameter by reference, causing a stack overflow vulnerability if the function modifies 'left' without updating the original value in the calling function.",
      "specific_code_behavior_causing_vulnerability": "The code snippet fails to pass the 'left' parameter by reference to proc_skip_spaces function, which can result in a stack overflow vulnerability due to potential modification of 'left' without updating the original value in the calling function.",
      "id": 144,
      "code_after_change_normalized": "static int FUN1(void *VAR1, struct ctl_table *VAR2,\nint VAR3, void *VAR4, size_t *VAR5, loff_t *VAR6,\nunsigned long VAR7, unsigned long VAR8)\n{\nunsigned long *VAR9, *VAR10, *VAR11;\nint VAR12, VAR13 = 1, VAR14 = 0;\nsize_t VAR15;\nchar *VAR16;\nif (!VAR1 || !VAR2->VAR17 || !*VAR5 || (*VAR6 && !VAR3)) {\n*VAR5 = 0;\nreturn 0;\n}\nVAR9 = VAR1;\nVAR10 = VAR2->VAR18;\nVAR11 = VAR2->VAR19;\nVAR12 = VAR2->VAR17 / sizeof(unsigned long);\nVAR15 = *VAR5;\nif (VAR3) {\nif (FUN2(VAR6, VAR2))\ngoto VAR20;\nif (VAR15 > VAR21 - 1)\nVAR15 = VAR21 - 1;\nVAR16 = VAR4;\n}\nfor (; VAR15 && VAR12--; VAR9++, VAR13 = 0) {\nunsigned long VAR22;\nif (VAR3) {\nbool VAR23;\nFUN3(&VAR16, &VAR15);\nif (!VAR15)\nbreak;\nVAR14 = FUN4(&VAR16, &VAR15, &VAR22, &VAR23,\nVAR24,\nsizeof(VAR24), NULL);\nif (VAR14 || VAR23) {\nVAR14 = -VAR25;\nbreak;\n}\nVAR22 = VAR7 * VAR22 / VAR8;\nif ((VAR10 && VAR22 < *VAR10) || (VAR11 && VAR22 > *VAR11)) {\nVAR14 = -VAR25;\nbreak;\n}\nFUN5(*VAR9, VAR22);\n} else {\nVAR22 = VAR8 * FUN6(*VAR9) / VAR7;\nif (!VAR13)\nFUN7(&VAR4, &VAR15, );\nFUN8(&VAR4, &VAR15, VAR22, false);\n}\n}\nif (!VAR3 && !VAR13 && VAR15 && !VAR14)\nFUN7(&VAR4, &VAR15, );\nif (VAR3 && !VAR14)\nFUN3(&VAR16, &VAR15);\nif (VAR3 && VAR13)\nreturn VAR14 ? : -VAR25;\n*VAR5 -= VAR15;\nVAR20:\n*VAR6 += *VAR5;\nreturn VAR14;\n}\n",
      "code_before_change_normalized": "static int FUN1(void *VAR1, struct ctl_table *VAR2,\nint VAR3, void *VAR4, size_t *VAR5, loff_t *VAR6,\nunsigned long VAR7, unsigned long VAR8)\n{\nunsigned long *VAR9, *VAR10, *VAR11;\nint VAR12, VAR13 = 1, VAR14 = 0;\nsize_t VAR15;\nchar *VAR16;\nif (!VAR1 || !VAR2->VAR17 || !*VAR5 || (*VAR6 && !VAR3)) {\n*VAR5 = 0;\nreturn 0;\n}\nVAR9 = VAR1;\nVAR10 = VAR2->VAR18;\nVAR11 = VAR2->VAR19;\nVAR12 = VAR2->VAR17 / sizeof(unsigned long);\nVAR15 = *VAR5;\nif (VAR3) {\nif (FUN2(VAR6, VAR2))\ngoto VAR20;\nif (VAR15 > VAR21 - 1)\nVAR15 = VAR21 - 1;\nVAR16 = VAR4;\n}\nfor (; VAR15 && VAR12--; VAR9++, VAR13 = 0) {\nunsigned long VAR22;\nif (VAR3) {\nbool VAR23;\nVAR15 -= FUN3(&VAR16);\nif (!VAR15)\nbreak;\nVAR14 = FUN4(&VAR16, &VAR15, &VAR22, &VAR23,\nVAR24,\nsizeof(VAR24), NULL);\nif (VAR14 || VAR23) {\nVAR14 = -VAR25;\nbreak;\n}\nVAR22 = VAR7 * VAR22 / VAR8;\nif ((VAR10 && VAR22 < *VAR10) || (VAR11 && VAR22 > *VAR11)) {\nVAR14 = -VAR25;\nbreak;\n}\nFUN5(*VAR9, VAR22);\n} else {\nVAR22 = VAR8 * FUN6(*VAR9) / VAR7;\nif (!VAR13)\nFUN7(&VAR4, &VAR15, );\nFUN8(&VAR4, &VAR15, VAR22, false);\n}\n}\nif (!VAR3 && !VAR13 && VAR15 && !VAR14)\nFUN7(&VAR4, &VAR15, );\nif (VAR3 && !VAR14)\nVAR15 -= FUN3(&VAR16);\nif (VAR3 && VAR13)\nreturn VAR14 ? : -VAR25;\n*VAR5 -= VAR15;\nVAR20:\n*VAR6 += *VAR5;\nreturn VAR14;\n}\n",
      "code_after_change_raw": "static int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table,\nint write, void *buffer, size_t *lenp, loff_t *ppos,\nunsigned long convmul, unsigned long convdiv)\n{\nunsigned long *i, *min, *max;\nint vleft, first = 1, err = 0;\nsize_t left;\nchar *p;\nif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\n*lenp = 0;\nreturn 0;\n}\ni = data;\nmin = table->extra1;\nmax = table->extra2;\nvleft = table->maxlen / sizeof(unsigned long);\nleft = *lenp;\nif (write) {\nif (proc_first_pos_non_zero_ignore(ppos, table))\ngoto out;\nif (left > PAGE_SIZE - 1)\nleft = PAGE_SIZE - 1;\np = buffer;\n}\nfor (; left && vleft--; i++, first = 0) {\nunsigned long val;\nif (write) {\nbool neg;\nproc_skip_spaces(&p, &left);\nif (!left)\nbreak;\nerr = proc_get_long(&p, &left, &val, &neg,\nproc_wspace_sep,\nsizeof(proc_wspace_sep), NULL);\nif (err || neg) {\nerr = -EINVAL;\nbreak;\n}\nval = convmul * val / convdiv;\nif ((min && val < *min) || (max && val > *max)) {\nerr = -EINVAL;\nbreak;\n}\nWRITE_ONCE(*i, val);\n} else {\nval = convdiv * READ_ONCE(*i) / convmul;\nif (!first)\nproc_put_char(&buffer, &left, '\\t');\nproc_put_long(&buffer, &left, val, false);\n}\n}\nif (!write && !first && left && !err)\nproc_put_char(&buffer, &left, '\\n');\nif (write && !err)\nproc_skip_spaces(&p, &left);\nif (write && first)\nreturn err ? : -EINVAL;\n*lenp -= left;\nout:\n*ppos += *lenp;\nreturn err;\n}\n",
      "code_before_change_raw": "static int __do_proc_doulongvec_minmax(void *data, struct ctl_table *table,\nint write, void *buffer, size_t *lenp, loff_t *ppos,\nunsigned long convmul, unsigned long convdiv)\n{\nunsigned long *i, *min, *max;\nint vleft, first = 1, err = 0;\nsize_t left;\nchar *p;\nif (!data || !table->maxlen || !*lenp || (*ppos && !write)) {\n*lenp = 0;\nreturn 0;\n}\ni = data;\nmin = table->extra1;\nmax = table->extra2;\nvleft = table->maxlen / sizeof(unsigned long);\nleft = *lenp;\nif (write) {\nif (proc_first_pos_non_zero_ignore(ppos, table))\ngoto out;\nif (left > PAGE_SIZE - 1)\nleft = PAGE_SIZE - 1;\np = buffer;\n}\nfor (; left && vleft--; i++, first = 0) {\nunsigned long val;\nif (write) {\nbool neg;\nleft -= proc_skip_spaces(&p);\nif (!left)\nbreak;\nerr = proc_get_long(&p, &left, &val, &neg,\nproc_wspace_sep,\nsizeof(proc_wspace_sep), NULL);\nif (err || neg) {\nerr = -EINVAL;\nbreak;\n}\nval = convmul * val / convdiv;\nif ((min && val < *min) || (max && val > *max)) {\nerr = -EINVAL;\nbreak;\n}\nWRITE_ONCE(*i, val);\n} else {\nval = convdiv * READ_ONCE(*i) / convmul;\nif (!first)\nproc_put_char(&buffer, &left, '\\t');\nproc_put_long(&buffer, &left, val, false);\n}\n}\nif (!write && !first && left && !err)\nproc_put_char(&buffer, &left, '\\n');\nif (write && !err)\nleft -= proc_skip_spaces(&p);\nif (write && first)\nreturn err ? : -EINVAL;\n*lenp -= left;\nout:\n*ppos += *lenp;\nreturn err;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of user input in the SYSCTL subsystem, specifically in a function that processes unsigned integer values.",
        "trigger_condition": "When a user provides malicious or unexpected input that is not properly validated or sanitized, it can lead to a stack overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The vulnerable code does not correctly manage the buffer size and pointer when skipping spaces in the user-provided input. This can result in a stack overflow if the input is crafted in a way that causes excessive space skipping and overflows the stack.",
        "solution": "To mitigate the vulnerability, the code needs to ensure proper handling of the buffer size and pointer when processing user input. Specifically, the function calls to skip spaces should be updated to provide the correct parameters, preventing potential stack overflow issues. By making this modification, the code becomes more robust and less susceptible to stack overflow vulnerabilities caused by improper input handling."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2022-4378, which is a stack overflow flaw in the Linux kernel's SYSCTL subsystem. The vulnerability occurs due to improper handling of user input, which can lead to a stack overflow and potentially allow a local user to crash the system or escalate their privileges.\n\nThe modification involves replacing the `proc_skip_spaces` function calls with `proc_skip_spaces(&p, &left)` to ensure that the function is provided with the correct parameters. This change helps prevent potential stack overflow issues by correctly managing the buffer size (`left`) and the buffer pointer (`p`) during space skipping operations.\n\nBy making this modification, the code is more robust and less susceptible to stack overflow vulnerabilities caused by improper handling of user input.",
      "GPT_purpose": "Write unsigned integer vector data to a proc file system entry.",
      "GPT_function": "\n1. Process unsigned integer vector data for a proc file entry.\n2. Check for valid input parameters and handle errors accordingly.\n3. Perform data conversion and manipulation based on user input.\n4. Handle potential stack overflow vulnerability by ensuring proper buffer size limits.",
      "CVE_id": "CVE-2022-4378",
      "code_before_change": "static int do_proc_douintvec_w(unsigned int *tbl_data,\n\t\t\t       struct ctl_table *table,\n\t\t\t       void *buffer,\n\t\t\t       size_t *lenp, loff_t *ppos,\n\t\t\t       int (*conv)(unsigned long *lvalp,\n\t\t\t\t\t   unsigned int *valp,\n\t\t\t\t\t   int write, void *data),\n\t\t\t       void *data)\n{\n\tunsigned long lval;\n\tint err = 0;\n\tsize_t left;\n\tbool neg;\n\tchar *p = buffer;\n\n\tleft = *lenp;\n\n\tif (proc_first_pos_non_zero_ignore(ppos, table))\n\t\tgoto bail_early;\n\n\tif (left > PAGE_SIZE - 1)\n\t\tleft = PAGE_SIZE - 1;\n\n\tleft -= proc_skip_spaces(&p);\n\tif (!left) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\terr = proc_get_long(&p, &left, &lval, &neg,\n\t\t\t     proc_wspace_sep,\n\t\t\t     sizeof(proc_wspace_sep), NULL);\n\tif (err || neg) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tif (conv(&lval, tbl_data, 1, data)) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tif (!err && left)\n\t\tleft -= proc_skip_spaces(&p);\n\nout_free:\n\tif (err)\n\t\treturn -EINVAL;\n\n\treturn 0;\n\n\t/* This is in keeping with old __do_proc_dointvec() */\nbail_early:\n\t*ppos += *lenp;\n\treturn err;\n}",
      "code_after_change": "static int do_proc_douintvec_w(unsigned int *tbl_data,\n\t\t\t       struct ctl_table *table,\n\t\t\t       void *buffer,\n\t\t\t       size_t *lenp, loff_t *ppos,\n\t\t\t       int (*conv)(unsigned long *lvalp,\n\t\t\t\t\t   unsigned int *valp,\n\t\t\t\t\t   int write, void *data),\n\t\t\t       void *data)\n{\n\tunsigned long lval;\n\tint err = 0;\n\tsize_t left;\n\tbool neg;\n\tchar *p = buffer;\n\n\tleft = *lenp;\n\n\tif (proc_first_pos_non_zero_ignore(ppos, table))\n\t\tgoto bail_early;\n\n\tif (left > PAGE_SIZE - 1)\n\t\tleft = PAGE_SIZE - 1;\n\n\tproc_skip_spaces(&p, &left);\n\tif (!left) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\terr = proc_get_long(&p, &left, &lval, &neg,\n\t\t\t     proc_wspace_sep,\n\t\t\t     sizeof(proc_wspace_sep), NULL);\n\tif (err || neg) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tif (conv(&lval, tbl_data, 1, data)) {\n\t\terr = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\tif (!err && left)\n\t\tproc_skip_spaces(&p, &left);\n\nout_free:\n\tif (err)\n\t\treturn -EINVAL;\n\n\treturn 0;\n\n\t/* This is in keeping with old __do_proc_dointvec() */\nbail_early:\n\t*ppos += *lenp;\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tproc_skip_spaces(&p, &left);",
          "\t\tproc_skip_spaces(&p, &left);"
        ],
        "deleted": [
          "\tleft -= proc_skip_spaces(&p);",
          "\t\tleft -= proc_skip_spaces(&p);"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of user input in the SYSCTL subsystem, specifically in a function that processes unsigned integer values.",
      "trigger_condition": "When a user provides malicious or unexpected input that is not properly validated or sanitized, it can lead to a stack overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The vulnerable code does not correctly manage the buffer size and pointer when skipping spaces in the user-provided input. This can result in a stack overflow if the input is crafted in a way that causes excessive space skipping and overflows the stack.",
      "solution": "To mitigate the vulnerability, the code needs to ensure proper handling of the buffer size and pointer when processing user input. Specifically, the function calls to skip spaces should be updated to provide the correct parameters, preventing potential stack overflow issues. By making this modification, the code becomes more robust and less susceptible to stack overflow vulnerabilities caused by improper input handling.",
      "id": 145,
      "code_after_change_normalized": "static int FUN1(unsigned int *VAR1,\nstruct ctl_table *VAR2,\nvoid *VAR3,\nsize_t *VAR4, loff_t *VAR5,\nint (*VAR6)(unsigned long *VAR7,\nunsigned int *VAR8,\nint VAR9, void *VAR10),\nvoid *VAR10)\n{\nunsigned long VAR11;\nint VAR12 = 0;\nsize_t VAR13;\nbool VAR14;\nchar *VAR15 = VAR3;\nVAR13 = *VAR4;\nif (FUN2(VAR5, VAR2))\ngoto VAR16;\nif (VAR13 > VAR17 - 1)\nVAR13 = VAR17 - 1;\nFUN3(&VAR15, &VAR13);\nif (!VAR13) {\nVAR12 = -VAR18;\ngoto VAR19;\n}\nVAR12 = FUN4(&VAR15, &VAR13, &VAR11, &VAR14,\nVAR20,\nsizeof(VAR20), NULL);\nif (VAR12 || VAR14) {\nVAR12 = -VAR18;\ngoto VAR19;\n}\nif (FUN5(&VAR11, VAR1, 1, VAR10)) {\nVAR12 = -VAR18;\ngoto VAR19;\n}\nif (!VAR12 && VAR13)\nFUN3(&VAR15, &VAR13);\nVAR19:\nif (VAR12)\nreturn -VAR18;\nreturn 0;\nVAR16:\n*VAR5 += *VAR4;\nreturn VAR12;\n}\n",
      "code_before_change_normalized": "static int FUN1(unsigned int *VAR1,\nstruct ctl_table *VAR2,\nvoid *VAR3,\nsize_t *VAR4, loff_t *VAR5,\nint (*VAR6)(unsigned long *VAR7,\nunsigned int *VAR8,\nint VAR9, void *VAR10),\nvoid *VAR10)\n{\nunsigned long VAR11;\nint VAR12 = 0;\nsize_t VAR13;\nbool VAR14;\nchar *VAR15 = VAR3;\nVAR13 = *VAR4;\nif (FUN2(VAR5, VAR2))\ngoto VAR16;\nif (VAR13 > VAR17 - 1)\nVAR13 = VAR17 - 1;\nVAR13 -= FUN3(&VAR15);\nif (!VAR13) {\nVAR12 = -VAR18;\ngoto VAR19;\n}\nVAR12 = FUN4(&VAR15, &VAR13, &VAR11, &VAR14,\nVAR20,\nsizeof(VAR20), NULL);\nif (VAR12 || VAR14) {\nVAR12 = -VAR18;\ngoto VAR19;\n}\nif (FUN5(&VAR11, VAR1, 1, VAR10)) {\nVAR12 = -VAR18;\ngoto VAR19;\n}\nif (!VAR12 && VAR13)\nVAR13 -= FUN3(&VAR15);\nVAR19:\nif (VAR12)\nreturn -VAR18;\nreturn 0;\nVAR16:\n*VAR5 += *VAR4;\nreturn VAR12;\n}\n",
      "code_after_change_raw": "static int do_proc_douintvec_w(unsigned int *tbl_data,\nstruct ctl_table *table,\nvoid *buffer,\nsize_t *lenp, loff_t *ppos,\nint (*conv)(unsigned long *lvalp,\nunsigned int *valp,\nint write, void *data),\nvoid *data)\n{\nunsigned long lval;\nint err = 0;\nsize_t left;\nbool neg;\nchar *p = buffer;\nleft = *lenp;\nif (proc_first_pos_non_zero_ignore(ppos, table))\ngoto bail_early;\nif (left > PAGE_SIZE - 1)\nleft = PAGE_SIZE - 1;\nproc_skip_spaces(&p, &left);\nif (!left) {\nerr = -EINVAL;\ngoto out_free;\n}\nerr = proc_get_long(&p, &left, &lval, &neg,\nproc_wspace_sep,\nsizeof(proc_wspace_sep), NULL);\nif (err || neg) {\nerr = -EINVAL;\ngoto out_free;\n}\nif (conv(&lval, tbl_data, 1, data)) {\nerr = -EINVAL;\ngoto out_free;\n}\nif (!err && left)\nproc_skip_spaces(&p, &left);\nout_free:\nif (err)\nreturn -EINVAL;\nreturn 0;\nbail_early:\n*ppos += *lenp;\nreturn err;\n}\n",
      "code_before_change_raw": "static int do_proc_douintvec_w(unsigned int *tbl_data,\nstruct ctl_table *table,\nvoid *buffer,\nsize_t *lenp, loff_t *ppos,\nint (*conv)(unsigned long *lvalp,\nunsigned int *valp,\nint write, void *data),\nvoid *data)\n{\nunsigned long lval;\nint err = 0;\nsize_t left;\nbool neg;\nchar *p = buffer;\nleft = *lenp;\nif (proc_first_pos_non_zero_ignore(ppos, table))\ngoto bail_early;\nif (left > PAGE_SIZE - 1)\nleft = PAGE_SIZE - 1;\nleft -= proc_skip_spaces(&p);\nif (!left) {\nerr = -EINVAL;\ngoto out_free;\n}\nerr = proc_get_long(&p, &left, &lval, &neg,\nproc_wspace_sep,\nsizeof(proc_wspace_sep), NULL);\nif (err || neg) {\nerr = -EINVAL;\ngoto out_free;\n}\nif (conv(&lval, tbl_data, 1, data)) {\nerr = -EINVAL;\ngoto out_free;\n}\nif (!err && left)\nleft -= proc_skip_spaces(&p);\nout_free:\nif (err)\nreturn -EINVAL;\nreturn 0;\nbail_early:\n*ppos += *lenp;\nreturn err;\n}\n"
    }
  ],
  "CVE-2022-47942": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of SMB2_QUERY_INFO_HE requests after a malformed SMB2_SET_INFO_HE command.",
        "trigger_condition": "Execution of SMB2_QUERY_INFO_HE requests without proper validation of the request parameters, leading to a potential buffer overflow.",
        "specific_code_behavior_causing_vulnerability": "The code does not adequately check for security-related SMB2_QUERY_INFO_HE requests, potentially allowing a buffer overflow due to incorrect buffer size allocation.",
        "specific_solution": "To mitigate the vulnerability, it is necessary to include an additional condition to check for security-related SMB2_QUERY_INFO_HE requests and adjust the response buffer size accordingly. By validating the request parameters and ensuring the correct buffer size allocation, the code can prevent buffer overflows and enhance security."
      },
      "GPT_analysis": "The above modification is necessary to address the heap-based buffer overflow vulnerability (CVE-2022-47942) in the ksmbd code snippet. The vulnerability occurs due to the use of SMB2_QUERY_INFO_HE after a malformed SMB2_SET_INFO_HE command, which can lead to a buffer overflow in the set_ntacl_dacl function.\n\nThe modification adds an additional condition in the if statement for handling SMB2_QUERY_INFO_HE requests. By including the condition `req->InfoType == SMB2_O_INFO_SECURITY`, the code ensures that when the request is related to security information, the response buffer size is set to large_sz to prevent potential buffer overflows.\n\nBy making this modification, the code is more robust and can prevent the exploitation of the vulnerability by properly handling SMB2_QUERY_INFO_HE requests related to security information.",
      "GPT_purpose": "Allocate response buffer based on the SMB2 command type and size for handling SMB requests in the ksmbd kernel module.",
      "GPT_function": "\n1. Allocate response buffer based on the size determined by the command type and request information.\n2. Check the command type to determine if a large response buffer is needed.\n3. Allocate a large response buffer for chained commands.",
      "CVE_id": "CVE-2022-47942",
      "code_before_change": "int smb2_allocate_rsp_buf(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *hdr = smb2_get_msg(work->request_buf);\n\tsize_t small_sz = MAX_CIFS_SMALL_BUFFER_SIZE;\n\tsize_t large_sz = small_sz + work->conn->vals->max_trans_size;\n\tsize_t sz = small_sz;\n\tint cmd = le16_to_cpu(hdr->Command);\n\n\tif (cmd == SMB2_IOCTL_HE || cmd == SMB2_QUERY_DIRECTORY_HE)\n\t\tsz = large_sz;\n\n\tif (cmd == SMB2_QUERY_INFO_HE) {\n\t\tstruct smb2_query_info_req *req;\n\n\t\treq = smb2_get_msg(work->request_buf);\n\t\tif (req->InfoType == SMB2_O_INFO_FILE &&\n\t\t    (req->FileInfoClass == FILE_FULL_EA_INFORMATION ||\n\t\t     req->FileInfoClass == FILE_ALL_INFORMATION))\n\t\t\tsz = large_sz;\n\t}\n\n\t/* allocate large response buf for chained commands */\n\tif (le32_to_cpu(hdr->NextCommand) > 0)\n\t\tsz = large_sz;\n\n\twork->response_buf = kvmalloc(sz, GFP_KERNEL | __GFP_ZERO);\n\tif (!work->response_buf)\n\t\treturn -ENOMEM;\n\n\twork->response_sz = sz;\n\treturn 0;\n}",
      "code_after_change": "int smb2_allocate_rsp_buf(struct ksmbd_work *work)\n{\n\tstruct smb2_hdr *hdr = smb2_get_msg(work->request_buf);\n\tsize_t small_sz = MAX_CIFS_SMALL_BUFFER_SIZE;\n\tsize_t large_sz = small_sz + work->conn->vals->max_trans_size;\n\tsize_t sz = small_sz;\n\tint cmd = le16_to_cpu(hdr->Command);\n\n\tif (cmd == SMB2_IOCTL_HE || cmd == SMB2_QUERY_DIRECTORY_HE)\n\t\tsz = large_sz;\n\n\tif (cmd == SMB2_QUERY_INFO_HE) {\n\t\tstruct smb2_query_info_req *req;\n\n\t\treq = smb2_get_msg(work->request_buf);\n\t\tif ((req->InfoType == SMB2_O_INFO_FILE &&\n\t\t     (req->FileInfoClass == FILE_FULL_EA_INFORMATION ||\n\t\t     req->FileInfoClass == FILE_ALL_INFORMATION)) ||\n\t\t    req->InfoType == SMB2_O_INFO_SECURITY)\n\t\t\tsz = large_sz;\n\t}\n\n\t/* allocate large response buf for chained commands */\n\tif (le32_to_cpu(hdr->NextCommand) > 0)\n\t\tsz = large_sz;\n\n\twork->response_buf = kvmalloc(sz, GFP_KERNEL | __GFP_ZERO);\n\tif (!work->response_buf)\n\t\treturn -ENOMEM;\n\n\twork->response_sz = sz;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\t\tif ((req->InfoType == SMB2_O_INFO_FILE &&",
          "\t\t     (req->FileInfoClass == FILE_FULL_EA_INFORMATION ||",
          "\t\t     req->FileInfoClass == FILE_ALL_INFORMATION)) ||",
          "\t\t    req->InfoType == SMB2_O_INFO_SECURITY)"
        ],
        "deleted": [
          "\t\tif (req->InfoType == SMB2_O_INFO_FILE &&",
          "\t\t    (req->FileInfoClass == FILE_FULL_EA_INFORMATION ||",
          "\t\t     req->FileInfoClass == FILE_ALL_INFORMATION))"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of SMB2_QUERY_INFO_HE requests after a malformed SMB2_SET_INFO_HE command.",
      "trigger_condition": "Execution of SMB2_QUERY_INFO_HE requests without proper validation of the request parameters, leading to a potential buffer overflow.",
      "specific_code_behavior_causing_vulnerability": "The code does not adequately check for security-related SMB2_QUERY_INFO_HE requests, potentially allowing a buffer overflow due to incorrect buffer size allocation.",
      "id": 146,
      "code_after_change_normalized": "int FUN1(struct ksmbd_work *VAR1)\n{\nstruct smb2_hdr *VAR2 = FUN2(VAR1->VAR3);\nsize_t VAR4 = VAR5;\nsize_t VAR6 = VAR4 + VAR1->VAR7->VAR8->VAR9;\nsize_t VAR10 = VAR4;\nint VAR11 = FUN3(VAR2->VAR12);\nif (VAR11 == VAR13 || VAR11 == VAR14)\nVAR10 = VAR6;\nif (VAR11 == VAR15) {\nstruct smb2_query_info_req *VAR16;\nVAR16 = FUN2(VAR1->VAR3);\nif ((VAR16->VAR17 == VAR18 &&\n(VAR16->VAR19 == VAR20 ||\nVAR16->VAR19 == VAR21)) ||\nVAR16->VAR17 == VAR22)\nVAR10 = VAR6;\n}\nif (FUN4(VAR2->VAR23) > 0)\nVAR10 = VAR6;\nVAR1->VAR24 = FUN5(VAR10, VAR25 | VAR26);\nif (!VAR1->VAR24)\nreturn -VAR27;\nVAR1->VAR28 = VAR10;\nreturn 0;\n}\n",
      "code_before_change_normalized": "int FUN1(struct ksmbd_work *VAR1)\n{\nstruct smb2_hdr *VAR2 = FUN2(VAR1->VAR3);\nsize_t VAR4 = VAR5;\nsize_t VAR6 = VAR4 + VAR1->VAR7->VAR8->VAR9;\nsize_t VAR10 = VAR4;\nint VAR11 = FUN3(VAR2->VAR12);\nif (VAR11 == VAR13 || VAR11 == VAR14)\nVAR10 = VAR6;\nif (VAR11 == VAR15) {\nstruct smb2_query_info_req *VAR16;\nVAR16 = FUN2(VAR1->VAR3);\nif (VAR16->VAR17 == VAR18 &&\n(VAR16->VAR19 == VAR20 ||\nVAR16->VAR19 == VAR21))\nVAR10 = VAR6;\n}\nif (FUN4(VAR2->VAR22) > 0)\nVAR10 = VAR6;\nVAR1->VAR23 = FUN5(VAR10, VAR24 | VAR25);\nif (!VAR1->VAR23)\nreturn -VAR26;\nVAR1->VAR27 = VAR10;\nreturn 0;\n}\n",
      "code_after_change_raw": "int smb2_allocate_rsp_buf(struct ksmbd_work *work)\n{\nstruct smb2_hdr *hdr = smb2_get_msg(work->request_buf);\nsize_t small_sz = MAX_CIFS_SMALL_BUFFER_SIZE;\nsize_t large_sz = small_sz + work->conn->vals->max_trans_size;\nsize_t sz = small_sz;\nint cmd = le16_to_cpu(hdr->Command);\nif (cmd == SMB2_IOCTL_HE || cmd == SMB2_QUERY_DIRECTORY_HE)\nsz = large_sz;\nif (cmd == SMB2_QUERY_INFO_HE) {\nstruct smb2_query_info_req *req;\nreq = smb2_get_msg(work->request_buf);\nif ((req->InfoType == SMB2_O_INFO_FILE &&\n(req->FileInfoClass == FILE_FULL_EA_INFORMATION ||\nreq->FileInfoClass == FILE_ALL_INFORMATION)) ||\nreq->InfoType == SMB2_O_INFO_SECURITY)\nsz = large_sz;\n}\nif (le32_to_cpu(hdr->NextCommand) > 0)\nsz = large_sz;\nwork->response_buf = kvmalloc(sz, GFP_KERNEL | __GFP_ZERO);\nif (!work->response_buf)\nreturn -ENOMEM;\nwork->response_sz = sz;\nreturn 0;\n}\n",
      "code_before_change_raw": "int smb2_allocate_rsp_buf(struct ksmbd_work *work)\n{\nstruct smb2_hdr *hdr = smb2_get_msg(work->request_buf);\nsize_t small_sz = MAX_CIFS_SMALL_BUFFER_SIZE;\nsize_t large_sz = small_sz + work->conn->vals->max_trans_size;\nsize_t sz = small_sz;\nint cmd = le16_to_cpu(hdr->Command);\nif (cmd == SMB2_IOCTL_HE || cmd == SMB2_QUERY_DIRECTORY_HE)\nsz = large_sz;\nif (cmd == SMB2_QUERY_INFO_HE) {\nstruct smb2_query_info_req *req;\nreq = smb2_get_msg(work->request_buf);\nif (req->InfoType == SMB2_O_INFO_FILE &&\n(req->FileInfoClass == FILE_FULL_EA_INFORMATION ||\nreq->FileInfoClass == FILE_ALL_INFORMATION))\nsz = large_sz;\n}\nif (le32_to_cpu(hdr->NextCommand) > 0)\nsz = large_sz;\nwork->response_buf = kvmalloc(sz, GFP_KERNEL | __GFP_ZERO);\nif (!work->response_buf)\nreturn -ENOMEM;\nwork->response_sz = sz;\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Inadequate bounds checking and size validation of Access Control Entries (ACEs) within the parent ACL.",
        "trigger_condition": "Processing malformed or oversized ACEs from the parent ACL can lead to heap-based buffer overflow vulnerabilities.",
        "specific_code_behavior_causing_vulnerability": "The code does not perform sufficient size validation of ACEs and the parent DACL, allowing potential buffer overflows when processing ACE structures."
      },
      "solution": "To mitigate the vulnerability, it is essential to implement proper bounds checking and size validation mechanisms for ACEs and the parent DACL. This involves checking the size of each ACE before processing it, ensuring the parent DACL size does not exceed available data, and verifying that the processed ACE size does not exceed the remaining ACL space. By accurately calculating and updating structure sizes, the code can prevent buffer overflows and enhance security.",
      "GPT_analysis": "The above modification is necessary to address the heap-based buffer overflow vulnerability in the `smb_inherit_dacl` function. The vulnerability arises due to potential buffer overflows when processing ACEs (Access Control Entries) from the parent ACL (Access Control List). By ensuring proper bounds checking and handling of ACE sizes, the modified code prevents the possibility of writing beyond the allocated memory for ACE structures.\n\nSpecifically, the modification includes:\n1. Checking the size of each ACE before processing it to prevent buffer overflows.\n2. Verifying that the size of the parent DACL (Discretionary Access Control List) does not exceed the available data size.\n3. Ensuring that the size of the processed ACE does not exceed the remaining available space in the ACL.\n4. Properly calculating and updating the sizes of structures to prevent writing beyond allocated memory.\n\nBy implementing these changes, the modified code mitigates the risk of heap-based buffer overflows and enhances the overall security and stability of the `smb_inherit_dacl` function, addressing the vulnerability identified as CVE-2022-47942 in the original code snippet.",
      "GPT_purpose": "Inherit discretionary access control list (DACL) entries from a parent directory to a child directory in an SMB server implementation.",
      "GPT_function": "\n1. Inheriting discretionary access control list (DACL) from a parent directory.\n2. Handling access control entries (ACEs) for inheritance.\n3. Setting ACEs based on specified conditions.\n4. Allocating memory for new security descriptors and ACLs.\n5. Copying and updating security descriptor information.\n6. Setting the updated security descriptor as an extended attribute.\n7. Freeing allocated memory before returning.",
      "CVE_id": "CVE-2022-47942",
      "code_before_change": "int smb_inherit_dacl(struct ksmbd_conn *conn,\n\t\t     struct path *path,\n\t\t     unsigned int uid, unsigned int gid)\n{\n\tconst struct smb_sid *psid, *creator = NULL;\n\tstruct smb_ace *parent_aces, *aces;\n\tstruct smb_acl *parent_pdacl;\n\tstruct smb_ntsd *parent_pntsd = NULL;\n\tstruct smb_sid owner_sid, group_sid;\n\tstruct dentry *parent = path->dentry->d_parent;\n\tstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\n\tint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0;\n\tint rc = 0, num_aces, dacloffset, pntsd_type, acl_len;\n\tchar *aces_base;\n\tbool is_dir = S_ISDIR(d_inode(path->dentry)->i_mode);\n\n\tacl_len = ksmbd_vfs_get_sd_xattr(conn, user_ns,\n\t\t\t\t\t parent, &parent_pntsd);\n\tif (acl_len <= 0)\n\t\treturn -ENOENT;\n\tdacloffset = le32_to_cpu(parent_pntsd->dacloffset);\n\tif (!dacloffset) {\n\t\trc = -EINVAL;\n\t\tgoto free_parent_pntsd;\n\t}\n\n\tparent_pdacl = (struct smb_acl *)((char *)parent_pntsd + dacloffset);\n\tnum_aces = le32_to_cpu(parent_pdacl->num_aces);\n\tpntsd_type = le16_to_cpu(parent_pntsd->type);\n\n\taces_base = kmalloc(sizeof(struct smb_ace) * num_aces * 2, GFP_KERNEL);\n\tif (!aces_base) {\n\t\trc = -ENOMEM;\n\t\tgoto free_parent_pntsd;\n\t}\n\n\taces = (struct smb_ace *)aces_base;\n\tparent_aces = (struct smb_ace *)((char *)parent_pdacl +\n\t\t\tsizeof(struct smb_acl));\n\n\tif (pntsd_type & DACL_AUTO_INHERITED)\n\t\tinherited_flags = INHERITED_ACE;\n\n\tfor (i = 0; i < num_aces; i++) {\n\t\tflags = parent_aces->flags;\n\t\tif (!smb_inherit_flags(flags, is_dir))\n\t\t\tgoto pass;\n\t\tif (is_dir) {\n\t\t\tflags &= ~(INHERIT_ONLY_ACE | INHERITED_ACE);\n\t\t\tif (!(flags & CONTAINER_INHERIT_ACE))\n\t\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tif (flags & NO_PROPAGATE_INHERIT_ACE)\n\t\t\t\tflags = 0;\n\t\t} else {\n\t\t\tflags = 0;\n\t\t}\n\n\t\tif (!compare_sids(&creator_owner, &parent_aces->sid)) {\n\t\t\tcreator = &creator_owner;\n\t\t\tid_to_sid(uid, SIDOWNER, &owner_sid);\n\t\t\tpsid = &owner_sid;\n\t\t} else if (!compare_sids(&creator_group, &parent_aces->sid)) {\n\t\t\tcreator = &creator_group;\n\t\t\tid_to_sid(gid, SIDUNIX_GROUP, &group_sid);\n\t\t\tpsid = &group_sid;\n\t\t} else {\n\t\t\tcreator = NULL;\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\n\t\tif (is_dir && creator && flags & CONTAINER_INHERIT_ACE) {\n\t\t\tsmb_set_ace(aces, psid, parent_aces->type, inherited_flags,\n\t\t\t\t    parent_aces->access_req);\n\t\t\tnt_size += le16_to_cpu(aces->size);\n\t\t\tace_cnt++;\n\t\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tpsid = creator;\n\t\t} else if (is_dir && !(parent_aces->flags & NO_PROPAGATE_INHERIT_ACE)) {\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\n\t\tsmb_set_ace(aces, psid, parent_aces->type, flags | inherited_flags,\n\t\t\t    parent_aces->access_req);\n\t\tnt_size += le16_to_cpu(aces->size);\n\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\tace_cnt++;\npass:\n\t\tparent_aces =\n\t\t\t(struct smb_ace *)((char *)parent_aces + le16_to_cpu(parent_aces->size));\n\t}\n\n\tif (nt_size > 0) {\n\t\tstruct smb_ntsd *pntsd;\n\t\tstruct smb_acl *pdacl;\n\t\tstruct smb_sid *powner_sid = NULL, *pgroup_sid = NULL;\n\t\tint powner_sid_size = 0, pgroup_sid_size = 0, pntsd_size;\n\n\t\tif (parent_pntsd->osidoffset) {\n\t\t\tpowner_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->osidoffset));\n\t\t\tpowner_sid_size = 1 + 1 + 6 + (powner_sid->num_subauth * 4);\n\t\t}\n\t\tif (parent_pntsd->gsidoffset) {\n\t\t\tpgroup_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->gsidoffset));\n\t\t\tpgroup_sid_size = 1 + 1 + 6 + (pgroup_sid->num_subauth * 4);\n\t\t}\n\n\t\tpntsd = kzalloc(sizeof(struct smb_ntsd) + powner_sid_size +\n\t\t\t\tpgroup_sid_size + sizeof(struct smb_acl) +\n\t\t\t\tnt_size, GFP_KERNEL);\n\t\tif (!pntsd) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_aces_base;\n\t\t}\n\n\t\tpntsd->revision = cpu_to_le16(1);\n\t\tpntsd->type = cpu_to_le16(SELF_RELATIVE | DACL_PRESENT);\n\t\tif (le16_to_cpu(parent_pntsd->type) & DACL_AUTO_INHERITED)\n\t\t\tpntsd->type |= cpu_to_le16(DACL_AUTO_INHERITED);\n\t\tpntsd_size = sizeof(struct smb_ntsd);\n\t\tpntsd->osidoffset = parent_pntsd->osidoffset;\n\t\tpntsd->gsidoffset = parent_pntsd->gsidoffset;\n\t\tpntsd->dacloffset = parent_pntsd->dacloffset;\n\n\t\tif (pntsd->osidoffset) {\n\t\t\tstruct smb_sid *owner_sid = (struct smb_sid *)((char *)pntsd +\n\t\t\t\t\tle32_to_cpu(pntsd->osidoffset));\n\t\t\tmemcpy(owner_sid, powner_sid, powner_sid_size);\n\t\t\tpntsd_size += powner_sid_size;\n\t\t}\n\n\t\tif (pntsd->gsidoffset) {\n\t\t\tstruct smb_sid *group_sid = (struct smb_sid *)((char *)pntsd +\n\t\t\t\t\tle32_to_cpu(pntsd->gsidoffset));\n\t\t\tmemcpy(group_sid, pgroup_sid, pgroup_sid_size);\n\t\t\tpntsd_size += pgroup_sid_size;\n\t\t}\n\n\t\tif (pntsd->dacloffset) {\n\t\t\tstruct smb_ace *pace;\n\n\t\t\tpdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\n\t\t\tpdacl->revision = cpu_to_le16(2);\n\t\t\tpdacl->size = cpu_to_le16(sizeof(struct smb_acl) + nt_size);\n\t\t\tpdacl->num_aces = cpu_to_le32(ace_cnt);\n\t\t\tpace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\n\t\t\tmemcpy(pace, aces_base, nt_size);\n\t\t\tpntsd_size += sizeof(struct smb_acl) + nt_size;\n\t\t}\n\n\t\tksmbd_vfs_set_sd_xattr(conn, user_ns,\n\t\t\t\t       path->dentry, pntsd, pntsd_size);\n\t\tkfree(pntsd);\n\t}\n\nfree_aces_base:\n\tkfree(aces_base);\nfree_parent_pntsd:\n\tkfree(parent_pntsd);\n\treturn rc;\n}",
      "code_after_change": "int smb_inherit_dacl(struct ksmbd_conn *conn,\n\t\t     struct path *path,\n\t\t     unsigned int uid, unsigned int gid)\n{\n\tconst struct smb_sid *psid, *creator = NULL;\n\tstruct smb_ace *parent_aces, *aces;\n\tstruct smb_acl *parent_pdacl;\n\tstruct smb_ntsd *parent_pntsd = NULL;\n\tstruct smb_sid owner_sid, group_sid;\n\tstruct dentry *parent = path->dentry->d_parent;\n\tstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\n\tint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0, pdacl_size;\n\tint rc = 0, num_aces, dacloffset, pntsd_type, pntsd_size, acl_len, aces_size;\n\tchar *aces_base;\n\tbool is_dir = S_ISDIR(d_inode(path->dentry)->i_mode);\n\n\tpntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\n\t\t\t\t\t    parent, &parent_pntsd);\n\tif (pntsd_size <= 0)\n\t\treturn -ENOENT;\n\tdacloffset = le32_to_cpu(parent_pntsd->dacloffset);\n\tif (!dacloffset || (dacloffset + sizeof(struct smb_acl) > pntsd_size)) {\n\t\trc = -EINVAL;\n\t\tgoto free_parent_pntsd;\n\t}\n\n\tparent_pdacl = (struct smb_acl *)((char *)parent_pntsd + dacloffset);\n\tacl_len = pntsd_size - dacloffset;\n\tnum_aces = le32_to_cpu(parent_pdacl->num_aces);\n\tpntsd_type = le16_to_cpu(parent_pntsd->type);\n\tpdacl_size = le16_to_cpu(parent_pdacl->size);\n\n\tif (pdacl_size > acl_len || pdacl_size < sizeof(struct smb_acl)) {\n\t\trc = -EINVAL;\n\t\tgoto free_parent_pntsd;\n\t}\n\n\taces_base = kmalloc(sizeof(struct smb_ace) * num_aces * 2, GFP_KERNEL);\n\tif (!aces_base) {\n\t\trc = -ENOMEM;\n\t\tgoto free_parent_pntsd;\n\t}\n\n\taces = (struct smb_ace *)aces_base;\n\tparent_aces = (struct smb_ace *)((char *)parent_pdacl +\n\t\t\tsizeof(struct smb_acl));\n\taces_size = acl_len - sizeof(struct smb_acl);\n\n\tif (pntsd_type & DACL_AUTO_INHERITED)\n\t\tinherited_flags = INHERITED_ACE;\n\n\tfor (i = 0; i < num_aces; i++) {\n\t\tint pace_size;\n\n\t\tif (offsetof(struct smb_ace, access_req) > aces_size)\n\t\t\tbreak;\n\n\t\tpace_size = le16_to_cpu(parent_aces->size);\n\t\tif (pace_size > aces_size)\n\t\t\tbreak;\n\n\t\taces_size -= pace_size;\n\n\t\tflags = parent_aces->flags;\n\t\tif (!smb_inherit_flags(flags, is_dir))\n\t\t\tgoto pass;\n\t\tif (is_dir) {\n\t\t\tflags &= ~(INHERIT_ONLY_ACE | INHERITED_ACE);\n\t\t\tif (!(flags & CONTAINER_INHERIT_ACE))\n\t\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tif (flags & NO_PROPAGATE_INHERIT_ACE)\n\t\t\t\tflags = 0;\n\t\t} else {\n\t\t\tflags = 0;\n\t\t}\n\n\t\tif (!compare_sids(&creator_owner, &parent_aces->sid)) {\n\t\t\tcreator = &creator_owner;\n\t\t\tid_to_sid(uid, SIDOWNER, &owner_sid);\n\t\t\tpsid = &owner_sid;\n\t\t} else if (!compare_sids(&creator_group, &parent_aces->sid)) {\n\t\t\tcreator = &creator_group;\n\t\t\tid_to_sid(gid, SIDUNIX_GROUP, &group_sid);\n\t\t\tpsid = &group_sid;\n\t\t} else {\n\t\t\tcreator = NULL;\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\n\t\tif (is_dir && creator && flags & CONTAINER_INHERIT_ACE) {\n\t\t\tsmb_set_ace(aces, psid, parent_aces->type, inherited_flags,\n\t\t\t\t    parent_aces->access_req);\n\t\t\tnt_size += le16_to_cpu(aces->size);\n\t\t\tace_cnt++;\n\t\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\t\tflags |= INHERIT_ONLY_ACE;\n\t\t\tpsid = creator;\n\t\t} else if (is_dir && !(parent_aces->flags & NO_PROPAGATE_INHERIT_ACE)) {\n\t\t\tpsid = &parent_aces->sid;\n\t\t}\n\n\t\tsmb_set_ace(aces, psid, parent_aces->type, flags | inherited_flags,\n\t\t\t    parent_aces->access_req);\n\t\tnt_size += le16_to_cpu(aces->size);\n\t\taces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\n\t\tace_cnt++;\npass:\n\t\tparent_aces = (struct smb_ace *)((char *)parent_aces + pace_size);\n\t}\n\n\tif (nt_size > 0) {\n\t\tstruct smb_ntsd *pntsd;\n\t\tstruct smb_acl *pdacl;\n\t\tstruct smb_sid *powner_sid = NULL, *pgroup_sid = NULL;\n\t\tint powner_sid_size = 0, pgroup_sid_size = 0, pntsd_size;\n\n\t\tif (parent_pntsd->osidoffset) {\n\t\t\tpowner_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->osidoffset));\n\t\t\tpowner_sid_size = 1 + 1 + 6 + (powner_sid->num_subauth * 4);\n\t\t}\n\t\tif (parent_pntsd->gsidoffset) {\n\t\t\tpgroup_sid = (struct smb_sid *)((char *)parent_pntsd +\n\t\t\t\t\tle32_to_cpu(parent_pntsd->gsidoffset));\n\t\t\tpgroup_sid_size = 1 + 1 + 6 + (pgroup_sid->num_subauth * 4);\n\t\t}\n\n\t\tpntsd = kzalloc(sizeof(struct smb_ntsd) + powner_sid_size +\n\t\t\t\tpgroup_sid_size + sizeof(struct smb_acl) +\n\t\t\t\tnt_size, GFP_KERNEL);\n\t\tif (!pntsd) {\n\t\t\trc = -ENOMEM;\n\t\t\tgoto free_aces_base;\n\t\t}\n\n\t\tpntsd->revision = cpu_to_le16(1);\n\t\tpntsd->type = cpu_to_le16(SELF_RELATIVE | DACL_PRESENT);\n\t\tif (le16_to_cpu(parent_pntsd->type) & DACL_AUTO_INHERITED)\n\t\t\tpntsd->type |= cpu_to_le16(DACL_AUTO_INHERITED);\n\t\tpntsd_size = sizeof(struct smb_ntsd);\n\t\tpntsd->osidoffset = parent_pntsd->osidoffset;\n\t\tpntsd->gsidoffset = parent_pntsd->gsidoffset;\n\t\tpntsd->dacloffset = parent_pntsd->dacloffset;\n\n\t\tif (pntsd->osidoffset) {\n\t\t\tstruct smb_sid *owner_sid = (struct smb_sid *)((char *)pntsd +\n\t\t\t\t\tle32_to_cpu(pntsd->osidoffset));\n\t\t\tmemcpy(owner_sid, powner_sid, powner_sid_size);\n\t\t\tpntsd_size += powner_sid_size;\n\t\t}\n\n\t\tif (pntsd->gsidoffset) {\n\t\t\tstruct smb_sid *group_sid = (struct smb_sid *)((char *)pntsd +\n\t\t\t\t\tle32_to_cpu(pntsd->gsidoffset));\n\t\t\tmemcpy(group_sid, pgroup_sid, pgroup_sid_size);\n\t\t\tpntsd_size += pgroup_sid_size;\n\t\t}\n\n\t\tif (pntsd->dacloffset) {\n\t\t\tstruct smb_ace *pace;\n\n\t\t\tpdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\n\t\t\tpdacl->revision = cpu_to_le16(2);\n\t\t\tpdacl->size = cpu_to_le16(sizeof(struct smb_acl) + nt_size);\n\t\t\tpdacl->num_aces = cpu_to_le32(ace_cnt);\n\t\t\tpace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\n\t\t\tmemcpy(pace, aces_base, nt_size);\n\t\t\tpntsd_size += sizeof(struct smb_acl) + nt_size;\n\t\t}\n\n\t\tksmbd_vfs_set_sd_xattr(conn, user_ns,\n\t\t\t\t       path->dentry, pntsd, pntsd_size);\n\t\tkfree(pntsd);\n\t}\n\nfree_aces_base:\n\tkfree(aces_base);\nfree_parent_pntsd:\n\tkfree(parent_pntsd);\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0, pdacl_size;",
          "\tint rc = 0, num_aces, dacloffset, pntsd_type, pntsd_size, acl_len, aces_size;",
          "\tpntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,",
          "\t\t\t\t\t    parent, &parent_pntsd);",
          "\tif (pntsd_size <= 0)",
          "\tif (!dacloffset || (dacloffset + sizeof(struct smb_acl) > pntsd_size)) {",
          "\tacl_len = pntsd_size - dacloffset;",
          "\tpdacl_size = le16_to_cpu(parent_pdacl->size);",
          "",
          "\tif (pdacl_size > acl_len || pdacl_size < sizeof(struct smb_acl)) {",
          "\t\trc = -EINVAL;",
          "\t\tgoto free_parent_pntsd;",
          "\t}",
          "\taces_size = acl_len - sizeof(struct smb_acl);",
          "\t\tint pace_size;",
          "",
          "\t\tif (offsetof(struct smb_ace, access_req) > aces_size)",
          "\t\t\tbreak;",
          "",
          "\t\tpace_size = le16_to_cpu(parent_aces->size);",
          "\t\tif (pace_size > aces_size)",
          "\t\t\tbreak;",
          "",
          "\t\taces_size -= pace_size;",
          "",
          "\t\tparent_aces = (struct smb_ace *)((char *)parent_aces + pace_size);"
        ],
        "deleted": [
          "\tint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0;",
          "\tint rc = 0, num_aces, dacloffset, pntsd_type, acl_len;",
          "\tacl_len = ksmbd_vfs_get_sd_xattr(conn, user_ns,",
          "\t\t\t\t\t parent, &parent_pntsd);",
          "\tif (acl_len <= 0)",
          "\tif (!dacloffset) {",
          "\t\tparent_aces =",
          "\t\t\t(struct smb_ace *)((char *)parent_aces + le16_to_cpu(parent_aces->size));"
        ]
      },
      "preconditions_for_vulnerability": "Inadequate bounds checking and size validation of Access Control Entries (ACEs) within the parent ACL.",
      "trigger_condition": "Processing malformed or oversized ACEs from the parent ACL can lead to heap-based buffer overflow vulnerabilities.",
      "specific_code_behavior_causing_vulnerability": "The code does not perform sufficient size validation of ACEs and the parent DACL, allowing potential buffer overflows when processing ACE structures.",
      "id": 147,
      "code_after_change_normalized": "int FUN1(struct ksmbd_conn *VAR1,\nstruct VAR2 *VAR2,\nunsigned int VAR3, unsigned int VAR4)\n{\nconst struct smb_sid *VAR5, *VAR6 = NULL;\nstruct smb_ace *VAR7, *VAR8;\nstruct smb_acl *VAR9;\nstruct smb_ntsd *VAR10 = NULL;\nstruct smb_sid VAR11, VAR12;\nstruct VAR14 *VAR13 = VAR2->VAR14->VAR15;\nstruct user_namespace *VAR16 = FUN2(VAR2->VAR17);\nint VAR18 = 0, VAR19 = 0, VAR20, VAR21 = 0, VAR22 = 0, VAR23;\nint VAR24 = 0, VAR25, VAR26, VAR27, VAR28, VAR29, VAR30;\nchar *VAR31;\nbool VAR32 = FUN3(FUN4(VAR2->VAR14)->VAR33);\nVAR28 = FUN5(VAR1, VAR16,\nVAR13, &VAR10);\nif (VAR28 <= 0)\nreturn -VAR34;\nVAR26 = FUN6(VAR10->VAR26);\nif (!VAR26 || (VAR26 + sizeof(struct VAR35) > VAR28)) {\nVAR24 = -VAR36;\ngoto VAR37;\n}\nVAR9 = (struct VAR35 *)((char *)VAR10 + VAR26);\nVAR29 = VAR28 - VAR26;\nVAR25 = FUN6(VAR9->VAR25);\nVAR27 = FUN7(VAR10->VAR38);\nVAR23 = FUN7(VAR9->VAR39);\nif (VAR23 > VAR29 || VAR23 < sizeof(struct VAR35)) {\nVAR24 = -VAR36;\ngoto VAR37;\n}\nVAR31 = FUN8(sizeof(struct VAR40) * VAR25 * 2, VAR41);\nif (!VAR31) {\nVAR24 = -VAR42;\ngoto VAR37;\n}\nVAR8 = (struct VAR40 *)VAR31;\nVAR7 = (struct VAR40 *)((char *)VAR9 +\nsizeof(struct VAR35));\nVAR30 = VAR29 - sizeof(struct VAR35);\nif (VAR27 & VAR43)\nVAR18 = VAR44;\nfor (VAR20 = 0; VAR20 < VAR25; VAR20++) {\nint VAR45;\nif (FUN9(struct VAR40, VAR46) > VAR30)\nbreak;\nVAR45 = FUN7(VAR7->VAR39);\nif (VAR45 > VAR30)\nbreak;\nVAR30 -= VAR45;\nVAR19 = VAR7->VAR19;\nif (!FUN10(VAR19, VAR32))\ngoto VAR47;\nif (VAR32) {\nVAR19 &= ~(VAR48 | VAR44);\nif (!(VAR19 & VAR49))\nVAR19 |= VAR48;\nif (VAR19 & VAR50)\nVAR19 = 0;\n} else {\nVAR19 = 0;\n}\nif (!FUN11(&VAR51, &VAR7->VAR52)) {\nVAR6 = &VAR51;\nFUN12(VAR3, VAR53, &VAR11);\nVAR5 = &VAR11;\n} else if (!FUN11(&VAR54, &VAR7->VAR52)) {\nVAR6 = &VAR54;\nFUN12(VAR4, VAR55, &VAR12);\nVAR5 = &VAR12;\n} else {\nVAR6 = NULL;\nVAR5 = &VAR7->VAR52;\n}\nif (VAR32 && VAR6 && VAR19 & VAR49) {\nFUN13(VAR8, VAR5, VAR7->VAR38, VAR18,\nVAR7->VAR46);\nVAR22 += FUN7(VAR8->VAR39);\nVAR21++;\nVAR8 = (struct VAR40 *)((char *)VAR8 + FUN7(VAR8->VAR39));\nVAR19 |= VAR48;\nVAR5 = VAR6;\n} else if (VAR32 && !(VAR7->VAR19 & VAR50)) {\nVAR5 = &VAR7->VAR52;\n}\nFUN13(VAR8, VAR5, VAR7->VAR38, VAR19 | VAR18,\nVAR7->VAR46);\nVAR22 += FUN7(VAR8->VAR39);\nVAR8 = (struct VAR40 *)((char *)VAR8 + FUN7(VAR8->VAR39));\nVAR21++;\nVAR47:\nVAR7 = (struct VAR40 *)((char *)VAR7 + VAR45);\n}\nif (VAR22 > 0) {\nstruct smb_ntsd *VAR56;\nstruct smb_acl *VAR57;\nstruct smb_sid *VAR58 = NULL, *VAR59 = NULL;\nint VAR60 = 0, VAR61 = 0, VAR28;\nif (VAR10->VAR62) {\nVAR58 = (struct VAR63 *)((char *)VAR10 +\nFUN6(VAR10->VAR62));\nVAR60 = 1 + 1 + 6 + (VAR58->VAR64 * 4);\n}\nif (VAR10->VAR65) {\nVAR59 = (struct VAR63 *)((char *)VAR10 +\nFUN6(VAR10->VAR65));\nVAR61 = 1 + 1 + 6 + (VAR59->VAR64 * 4);\n}\nVAR56 = FUN14(sizeof(struct VAR66) + VAR60 +\nVAR61 + sizeof(struct VAR35) +\nVAR22, VAR41);\nif (!VAR56) {\nVAR24 = -VAR42;\ngoto VAR67;\n}\nVAR56->VAR68 = FUN15(1);\nVAR56->VAR38 = FUN15(VAR69 | VAR70);\nif (FUN7(VAR10->VAR38) & VAR43)\nVAR56->VAR38 |= FUN15(VAR43);\nVAR28 = sizeof(struct VAR66);\nVAR56->VAR62 = VAR10->VAR62;\nVAR56->VAR65 = VAR10->VAR65;\nVAR56->VAR26 = VAR10->VAR26;\nif (VAR56->VAR62) {\nstruct VAR63 *VAR11 = (struct VAR63 *)((char *)VAR56 +\nFUN6(VAR56->VAR62));\nFUN16(VAR11, VAR58, VAR60);\nVAR28 += VAR60;\n}\nif (VAR56->VAR65) {\nstruct VAR63 *VAR12 = (struct VAR63 *)((char *)VAR56 +\nFUN6(VAR56->VAR65));\nFUN16(VAR12, VAR59, VAR61);\nVAR28 += VAR61;\n}\nif (VAR56->VAR26) {\nstruct smb_ace *VAR71;\nVAR57 = (struct VAR35 *)((char *)VAR56 + FUN6(VAR56->VAR26));\nVAR57->VAR68 = FUN15(2);\nVAR57->VAR39 = FUN15(sizeof(struct VAR35) + VAR22);\nVAR57->VAR25 = FUN17(VAR21);\nVAR71 = (struct VAR40 *)((char *)VAR57 + sizeof(struct VAR35));\nFUN16(VAR71, VAR31, VAR22);\nVAR28 += sizeof(struct VAR35) + VAR22;\n}\nFUN18(VAR1, VAR16,\nVAR2->VAR14, VAR56, VAR28);\nFUN19(VAR56);\n}\nVAR67:\nFUN19(VAR31);\nVAR37:\nFUN19(VAR10);\nreturn VAR24;\n}\n",
      "code_before_change_normalized": "int FUN1(struct ksmbd_conn *VAR1,\nstruct VAR2 *VAR2,\nunsigned int VAR3, unsigned int VAR4)\n{\nconst struct smb_sid *VAR5, *VAR6 = NULL;\nstruct smb_ace *VAR7, *VAR8;\nstruct smb_acl *VAR9;\nstruct smb_ntsd *VAR10 = NULL;\nstruct smb_sid VAR11, VAR12;\nstruct VAR14 *VAR13 = VAR2->VAR14->VAR15;\nstruct user_namespace *VAR16 = FUN2(VAR2->VAR17);\nint VAR18 = 0, VAR19 = 0, VAR20, VAR21 = 0, VAR22 = 0;\nint VAR23 = 0, VAR24, VAR25, VAR26, VAR27;\nchar *VAR28;\nbool VAR29 = FUN3(FUN4(VAR2->VAR14)->VAR30);\nVAR27 = FUN5(VAR1, VAR16,\nVAR13, &VAR10);\nif (VAR27 <= 0)\nreturn -VAR31;\nVAR25 = FUN6(VAR10->VAR25);\nif (!VAR25) {\nVAR23 = -VAR32;\ngoto VAR33;\n}\nVAR9 = (struct VAR34 *)((char *)VAR10 + VAR25);\nVAR24 = FUN6(VAR9->VAR24);\nVAR26 = FUN7(VAR10->VAR35);\nVAR28 = FUN8(sizeof(struct VAR36) * VAR24 * 2, VAR37);\nif (!VAR28) {\nVAR23 = -VAR38;\ngoto VAR33;\n}\nVAR8 = (struct VAR36 *)VAR28;\nVAR7 = (struct VAR36 *)((char *)VAR9 +\nsizeof(struct VAR34));\nif (VAR26 & VAR39)\nVAR18 = VAR40;\nfor (VAR20 = 0; VAR20 < VAR24; VAR20++) {\nVAR19 = VAR7->VAR19;\nif (!FUN9(VAR19, VAR29))\ngoto VAR41;\nif (VAR29) {\nVAR19 &= ~(VAR42 | VAR40);\nif (!(VAR19 & VAR43))\nVAR19 |= VAR42;\nif (VAR19 & VAR44)\nVAR19 = 0;\n} else {\nVAR19 = 0;\n}\nif (!FUN10(&VAR45, &VAR7->VAR46)) {\nVAR6 = &VAR45;\nFUN11(VAR3, VAR47, &VAR11);\nVAR5 = &VAR11;\n} else if (!FUN10(&VAR48, &VAR7->VAR46)) {\nVAR6 = &VAR48;\nFUN11(VAR4, VAR49, &VAR12);\nVAR5 = &VAR12;\n} else {\nVAR6 = NULL;\nVAR5 = &VAR7->VAR46;\n}\nif (VAR29 && VAR6 && VAR19 & VAR43) {\nFUN12(VAR8, VAR5, VAR7->VAR35, VAR18,\nVAR7->VAR50);\nVAR22 += FUN7(VAR8->VAR51);\nVAR21++;\nVAR8 = (struct VAR36 *)((char *)VAR8 + FUN7(VAR8->VAR51));\nVAR19 |= VAR42;\nVAR5 = VAR6;\n} else if (VAR29 && !(VAR7->VAR19 & VAR44)) {\nVAR5 = &VAR7->VAR46;\n}\nFUN12(VAR8, VAR5, VAR7->VAR35, VAR19 | VAR18,\nVAR7->VAR50);\nVAR22 += FUN7(VAR8->VAR51);\nVAR8 = (struct VAR36 *)((char *)VAR8 + FUN7(VAR8->VAR51));\nVAR21++;\nVAR41:\nVAR7 =\n(struct VAR36 *)((char *)VAR7 + FUN7(VAR7->VAR51));\n}\nif (VAR22 > 0) {\nstruct smb_ntsd *VAR52;\nstruct smb_acl *VAR53;\nstruct smb_sid *VAR54 = NULL, *VAR55 = NULL;\nint VAR56 = 0, VAR57 = 0, VAR58;\nif (VAR10->VAR59) {\nVAR54 = (struct VAR60 *)((char *)VAR10 +\nFUN6(VAR10->VAR59));\nVAR56 = 1 + 1 + 6 + (VAR54->VAR61 * 4);\n}\nif (VAR10->VAR62) {\nVAR55 = (struct VAR60 *)((char *)VAR10 +\nFUN6(VAR10->VAR62));\nVAR57 = 1 + 1 + 6 + (VAR55->VAR61 * 4);\n}\nVAR52 = FUN13(sizeof(struct VAR63) + VAR56 +\nVAR57 + sizeof(struct VAR34) +\nVAR22, VAR37);\nif (!VAR52) {\nVAR23 = -VAR38;\ngoto VAR64;\n}\nVAR52->VAR65 = FUN14(1);\nVAR52->VAR35 = FUN14(VAR66 | VAR67);\nif (FUN7(VAR10->VAR35) & VAR39)\nVAR52->VAR35 |= FUN14(VAR39);\nVAR58 = sizeof(struct VAR63);\nVAR52->VAR59 = VAR10->VAR59;\nVAR52->VAR62 = VAR10->VAR62;\nVAR52->VAR25 = VAR10->VAR25;\nif (VAR52->VAR59) {\nstruct VAR60 *VAR11 = (struct VAR60 *)((char *)VAR52 +\nFUN6(VAR52->VAR59));\nFUN15(VAR11, VAR54, VAR56);\nVAR58 += VAR56;\n}\nif (VAR52->VAR62) {\nstruct VAR60 *VAR12 = (struct VAR60 *)((char *)VAR52 +\nFUN6(VAR52->VAR62));\nFUN15(VAR12, VAR55, VAR57);\nVAR58 += VAR57;\n}\nif (VAR52->VAR25) {\nstruct smb_ace *VAR68;\nVAR53 = (struct VAR34 *)((char *)VAR52 + FUN6(VAR52->VAR25));\nVAR53->VAR65 = FUN14(2);\nVAR53->VAR51 = FUN14(sizeof(struct VAR34) + VAR22);\nVAR53->VAR24 = FUN16(VAR21);\nVAR68 = (struct VAR36 *)((char *)VAR53 + sizeof(struct VAR34));\nFUN15(VAR68, VAR28, VAR22);\nVAR58 += sizeof(struct VAR34) + VAR22;\n}\nFUN17(VAR1, VAR16,\nVAR2->VAR14, VAR52, VAR58);\nFUN18(VAR52);\n}\nVAR64:\nFUN18(VAR28);\nVAR33:\nFUN18(VAR10);\nreturn VAR23;\n}\n",
      "code_after_change_raw": "int smb_inherit_dacl(struct ksmbd_conn *conn,\nstruct path *path,\nunsigned int uid, unsigned int gid)\n{\nconst struct smb_sid *psid, *creator = NULL;\nstruct smb_ace *parent_aces, *aces;\nstruct smb_acl *parent_pdacl;\nstruct smb_ntsd *parent_pntsd = NULL;\nstruct smb_sid owner_sid, group_sid;\nstruct dentry *parent = path->dentry->d_parent;\nstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\nint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0, pdacl_size;\nint rc = 0, num_aces, dacloffset, pntsd_type, pntsd_size, acl_len, aces_size;\nchar *aces_base;\nbool is_dir = S_ISDIR(d_inode(path->dentry)->i_mode);\npntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\nparent, &parent_pntsd);\nif (pntsd_size <= 0)\nreturn -ENOENT;\ndacloffset = le32_to_cpu(parent_pntsd->dacloffset);\nif (!dacloffset || (dacloffset + sizeof(struct smb_acl) > pntsd_size)) {\nrc = -EINVAL;\ngoto free_parent_pntsd;\n}\nparent_pdacl = (struct smb_acl *)((char *)parent_pntsd + dacloffset);\nacl_len = pntsd_size - dacloffset;\nnum_aces = le32_to_cpu(parent_pdacl->num_aces);\npntsd_type = le16_to_cpu(parent_pntsd->type);\npdacl_size = le16_to_cpu(parent_pdacl->size);\nif (pdacl_size > acl_len || pdacl_size < sizeof(struct smb_acl)) {\nrc = -EINVAL;\ngoto free_parent_pntsd;\n}\naces_base = kmalloc(sizeof(struct smb_ace) * num_aces * 2, GFP_KERNEL);\nif (!aces_base) {\nrc = -ENOMEM;\ngoto free_parent_pntsd;\n}\naces = (struct smb_ace *)aces_base;\nparent_aces = (struct smb_ace *)((char *)parent_pdacl +\nsizeof(struct smb_acl));\naces_size = acl_len - sizeof(struct smb_acl);\nif (pntsd_type & DACL_AUTO_INHERITED)\ninherited_flags = INHERITED_ACE;\nfor (i = 0; i < num_aces; i++) {\nint pace_size;\nif (offsetof(struct smb_ace, access_req) > aces_size)\nbreak;\npace_size = le16_to_cpu(parent_aces->size);\nif (pace_size > aces_size)\nbreak;\naces_size -= pace_size;\nflags = parent_aces->flags;\nif (!smb_inherit_flags(flags, is_dir))\ngoto pass;\nif (is_dir) {\nflags &= ~(INHERIT_ONLY_ACE | INHERITED_ACE);\nif (!(flags & CONTAINER_INHERIT_ACE))\nflags |= INHERIT_ONLY_ACE;\nif (flags & NO_PROPAGATE_INHERIT_ACE)\nflags = 0;\n} else {\nflags = 0;\n}\nif (!compare_sids(&creator_owner, &parent_aces->sid)) {\ncreator = &creator_owner;\nid_to_sid(uid, SIDOWNER, &owner_sid);\npsid = &owner_sid;\n} else if (!compare_sids(&creator_group, &parent_aces->sid)) {\ncreator = &creator_group;\nid_to_sid(gid, SIDUNIX_GROUP, &group_sid);\npsid = &group_sid;\n} else {\ncreator = NULL;\npsid = &parent_aces->sid;\n}\nif (is_dir && creator && flags & CONTAINER_INHERIT_ACE) {\nsmb_set_ace(aces, psid, parent_aces->type, inherited_flags,\nparent_aces->access_req);\nnt_size += le16_to_cpu(aces->size);\nace_cnt++;\naces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\nflags |= INHERIT_ONLY_ACE;\npsid = creator;\n} else if (is_dir && !(parent_aces->flags & NO_PROPAGATE_INHERIT_ACE)) {\npsid = &parent_aces->sid;\n}\nsmb_set_ace(aces, psid, parent_aces->type, flags | inherited_flags,\nparent_aces->access_req);\nnt_size += le16_to_cpu(aces->size);\naces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\nace_cnt++;\npass:\nparent_aces = (struct smb_ace *)((char *)parent_aces + pace_size);\n}\nif (nt_size > 0) {\nstruct smb_ntsd *pntsd;\nstruct smb_acl *pdacl;\nstruct smb_sid *powner_sid = NULL, *pgroup_sid = NULL;\nint powner_sid_size = 0, pgroup_sid_size = 0, pntsd_size;\nif (parent_pntsd->osidoffset) {\npowner_sid = (struct smb_sid *)((char *)parent_pntsd +\nle32_to_cpu(parent_pntsd->osidoffset));\npowner_sid_size = 1 + 1 + 6 + (powner_sid->num_subauth * 4);\n}\nif (parent_pntsd->gsidoffset) {\npgroup_sid = (struct smb_sid *)((char *)parent_pntsd +\nle32_to_cpu(parent_pntsd->gsidoffset));\npgroup_sid_size = 1 + 1 + 6 + (pgroup_sid->num_subauth * 4);\n}\npntsd = kzalloc(sizeof(struct smb_ntsd) + powner_sid_size +\npgroup_sid_size + sizeof(struct smb_acl) +\nnt_size, GFP_KERNEL);\nif (!pntsd) {\nrc = -ENOMEM;\ngoto free_aces_base;\n}\npntsd->revision = cpu_to_le16(1);\npntsd->type = cpu_to_le16(SELF_RELATIVE | DACL_PRESENT);\nif (le16_to_cpu(parent_pntsd->type) & DACL_AUTO_INHERITED)\npntsd->type |= cpu_to_le16(DACL_AUTO_INHERITED);\npntsd_size = sizeof(struct smb_ntsd);\npntsd->osidoffset = parent_pntsd->osidoffset;\npntsd->gsidoffset = parent_pntsd->gsidoffset;\npntsd->dacloffset = parent_pntsd->dacloffset;\nif (pntsd->osidoffset) {\nstruct smb_sid *owner_sid = (struct smb_sid *)((char *)pntsd +\nle32_to_cpu(pntsd->osidoffset));\nmemcpy(owner_sid, powner_sid, powner_sid_size);\npntsd_size += powner_sid_size;\n}\nif (pntsd->gsidoffset) {\nstruct smb_sid *group_sid = (struct smb_sid *)((char *)pntsd +\nle32_to_cpu(pntsd->gsidoffset));\nmemcpy(group_sid, pgroup_sid, pgroup_sid_size);\npntsd_size += pgroup_sid_size;\n}\nif (pntsd->dacloffset) {\nstruct smb_ace *pace;\npdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\npdacl->revision = cpu_to_le16(2);\npdacl->size = cpu_to_le16(sizeof(struct smb_acl) + nt_size);\npdacl->num_aces = cpu_to_le32(ace_cnt);\npace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\nmemcpy(pace, aces_base, nt_size);\npntsd_size += sizeof(struct smb_acl) + nt_size;\n}\nksmbd_vfs_set_sd_xattr(conn, user_ns,\npath->dentry, pntsd, pntsd_size);\nkfree(pntsd);\n}\nfree_aces_base:\nkfree(aces_base);\nfree_parent_pntsd:\nkfree(parent_pntsd);\nreturn rc;\n}\n",
      "code_before_change_raw": "int smb_inherit_dacl(struct ksmbd_conn *conn,\nstruct path *path,\nunsigned int uid, unsigned int gid)\n{\nconst struct smb_sid *psid, *creator = NULL;\nstruct smb_ace *parent_aces, *aces;\nstruct smb_acl *parent_pdacl;\nstruct smb_ntsd *parent_pntsd = NULL;\nstruct smb_sid owner_sid, group_sid;\nstruct dentry *parent = path->dentry->d_parent;\nstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\nint inherited_flags = 0, flags = 0, i, ace_cnt = 0, nt_size = 0;\nint rc = 0, num_aces, dacloffset, pntsd_type, acl_len;\nchar *aces_base;\nbool is_dir = S_ISDIR(d_inode(path->dentry)->i_mode);\nacl_len = ksmbd_vfs_get_sd_xattr(conn, user_ns,\nparent, &parent_pntsd);\nif (acl_len <= 0)\nreturn -ENOENT;\ndacloffset = le32_to_cpu(parent_pntsd->dacloffset);\nif (!dacloffset) {\nrc = -EINVAL;\ngoto free_parent_pntsd;\n}\nparent_pdacl = (struct smb_acl *)((char *)parent_pntsd + dacloffset);\nnum_aces = le32_to_cpu(parent_pdacl->num_aces);\npntsd_type = le16_to_cpu(parent_pntsd->type);\naces_base = kmalloc(sizeof(struct smb_ace) * num_aces * 2, GFP_KERNEL);\nif (!aces_base) {\nrc = -ENOMEM;\ngoto free_parent_pntsd;\n}\naces = (struct smb_ace *)aces_base;\nparent_aces = (struct smb_ace *)((char *)parent_pdacl +\nsizeof(struct smb_acl));\nif (pntsd_type & DACL_AUTO_INHERITED)\ninherited_flags = INHERITED_ACE;\nfor (i = 0; i < num_aces; i++) {\nflags = parent_aces->flags;\nif (!smb_inherit_flags(flags, is_dir))\ngoto pass;\nif (is_dir) {\nflags &= ~(INHERIT_ONLY_ACE | INHERITED_ACE);\nif (!(flags & CONTAINER_INHERIT_ACE))\nflags |= INHERIT_ONLY_ACE;\nif (flags & NO_PROPAGATE_INHERIT_ACE)\nflags = 0;\n} else {\nflags = 0;\n}\nif (!compare_sids(&creator_owner, &parent_aces->sid)) {\ncreator = &creator_owner;\nid_to_sid(uid, SIDOWNER, &owner_sid);\npsid = &owner_sid;\n} else if (!compare_sids(&creator_group, &parent_aces->sid)) {\ncreator = &creator_group;\nid_to_sid(gid, SIDUNIX_GROUP, &group_sid);\npsid = &group_sid;\n} else {\ncreator = NULL;\npsid = &parent_aces->sid;\n}\nif (is_dir && creator && flags & CONTAINER_INHERIT_ACE) {\nsmb_set_ace(aces, psid, parent_aces->type, inherited_flags,\nparent_aces->access_req);\nnt_size += le16_to_cpu(aces->size);\nace_cnt++;\naces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\nflags |= INHERIT_ONLY_ACE;\npsid = creator;\n} else if (is_dir && !(parent_aces->flags & NO_PROPAGATE_INHERIT_ACE)) {\npsid = &parent_aces->sid;\n}\nsmb_set_ace(aces, psid, parent_aces->type, flags | inherited_flags,\nparent_aces->access_req);\nnt_size += le16_to_cpu(aces->size);\naces = (struct smb_ace *)((char *)aces + le16_to_cpu(aces->size));\nace_cnt++;\npass:\nparent_aces =\n(struct smb_ace *)((char *)parent_aces + le16_to_cpu(parent_aces->size));\n}\nif (nt_size > 0) {\nstruct smb_ntsd *pntsd;\nstruct smb_acl *pdacl;\nstruct smb_sid *powner_sid = NULL, *pgroup_sid = NULL;\nint powner_sid_size = 0, pgroup_sid_size = 0, pntsd_size;\nif (parent_pntsd->osidoffset) {\npowner_sid = (struct smb_sid *)((char *)parent_pntsd +\nle32_to_cpu(parent_pntsd->osidoffset));\npowner_sid_size = 1 + 1 + 6 + (powner_sid->num_subauth * 4);\n}\nif (parent_pntsd->gsidoffset) {\npgroup_sid = (struct smb_sid *)((char *)parent_pntsd +\nle32_to_cpu(parent_pntsd->gsidoffset));\npgroup_sid_size = 1 + 1 + 6 + (pgroup_sid->num_subauth * 4);\n}\npntsd = kzalloc(sizeof(struct smb_ntsd) + powner_sid_size +\npgroup_sid_size + sizeof(struct smb_acl) +\nnt_size, GFP_KERNEL);\nif (!pntsd) {\nrc = -ENOMEM;\ngoto free_aces_base;\n}\npntsd->revision = cpu_to_le16(1);\npntsd->type = cpu_to_le16(SELF_RELATIVE | DACL_PRESENT);\nif (le16_to_cpu(parent_pntsd->type) & DACL_AUTO_INHERITED)\npntsd->type |= cpu_to_le16(DACL_AUTO_INHERITED);\npntsd_size = sizeof(struct smb_ntsd);\npntsd->osidoffset = parent_pntsd->osidoffset;\npntsd->gsidoffset = parent_pntsd->gsidoffset;\npntsd->dacloffset = parent_pntsd->dacloffset;\nif (pntsd->osidoffset) {\nstruct smb_sid *owner_sid = (struct smb_sid *)((char *)pntsd +\nle32_to_cpu(pntsd->osidoffset));\nmemcpy(owner_sid, powner_sid, powner_sid_size);\npntsd_size += powner_sid_size;\n}\nif (pntsd->gsidoffset) {\nstruct smb_sid *group_sid = (struct smb_sid *)((char *)pntsd +\nle32_to_cpu(pntsd->gsidoffset));\nmemcpy(group_sid, pgroup_sid, pgroup_sid_size);\npntsd_size += pgroup_sid_size;\n}\nif (pntsd->dacloffset) {\nstruct smb_ace *pace;\npdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\npdacl->revision = cpu_to_le16(2);\npdacl->size = cpu_to_le16(sizeof(struct smb_acl) + nt_size);\npdacl->num_aces = cpu_to_le32(ace_cnt);\npace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\nmemcpy(pace, aces_base, nt_size);\npntsd_size += sizeof(struct smb_acl) + nt_size;\n}\nksmbd_vfs_set_sd_xattr(conn, user_ns,\npath->dentry, pntsd, pntsd_size);\nkfree(pntsd);\n}\nfree_aces_base:\nkfree(aces_base);\nfree_parent_pntsd:\nkfree(parent_pntsd);\nreturn rc;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of ACL entry sizes during processing of SMB2_SET_INFO_HE commands.",
        "trigger_condition": "When the size of ACL entries is not properly validated and exceeds the allocated buffer size, a heap-based buffer overflow vulnerability occurs.",
        "specific_code_behavior_causing_vulnerability": "The code does not adequately check and verify the sizes of ACL entries, allowing for a buffer overflow if the sizes are not within the expected bounds."
      },
      "solution": "To mitigate the vulnerability, it is essential to add additional checks and validations to ensure that the size of ACL entries is properly calculated and verified before accessing them. By verifying that the sizes of ACL entries do not exceed the allocated buffer size, the code can prevent potential buffer overflow issues. In this case, the solution involves adding checks for ACL entry sizes and ensuring that they are within the expected bounds to prevent buffer overflow vulnerabilities.",
      "GPT_analysis": "The modification is necessary to address the heap-based buffer overflow vulnerability (CVE-2022-47942) in the `smb_check_perm_dacl` function of the ksmbd code snippet. The vulnerability arises due to improper handling of the size of the ACL entries, which can lead to a buffer overflow when processing SMB2_SET_INFO_HE commands.\n\nThe modification includes additional checks and validations to ensure that the size of ACL entries is properly calculated and verified before accessing them. By checking the sizes of the ACL entries and ensuring that they do not exceed the allocated buffer size, the code prevents potential buffer overflow issues that could be exploited by malicious actors.\n\nIn summary, the modification helps to enhance the security and robustness of the code by addressing the identified vulnerability and reducing the risk of buffer overflow attacks.",
      "GPT_purpose": "Check permission using Windows ACL for a given user on a file or directory.",
      "GPT_function": "\n1. Check permission using Windows ACL.\n2. Handle various cases related to access control entries (ACEs) in the ACL.\n3. Verify access rights based on granted permissions and access requirements.\n4. Handle POSIX ACLs if enabled in the configuration.\n5. Identify and handle specific ACE types for access control.\n6. Check and compare access bits to determine if access should be granted or denied.\n7. Free allocated memory and return appropriate error codes as needed.",
      "CVE_id": "CVE-2022-47942",
      "code_before_change": "int smb_check_perm_dacl(struct ksmbd_conn *conn, struct path *path,\n\t\t\t__le32 *pdaccess, int uid)\n{\n\tstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\n\tstruct smb_ntsd *pntsd = NULL;\n\tstruct smb_acl *pdacl;\n\tstruct posix_acl *posix_acls;\n\tint rc = 0, acl_size;\n\tstruct smb_sid sid;\n\tint granted = le32_to_cpu(*pdaccess & ~FILE_MAXIMAL_ACCESS_LE);\n\tstruct smb_ace *ace;\n\tint i, found = 0;\n\tunsigned int access_bits = 0;\n\tstruct smb_ace *others_ace = NULL;\n\tstruct posix_acl_entry *pa_entry;\n\tunsigned int sid_type = SIDOWNER;\n\tchar *end_of_acl;\n\n\tksmbd_debug(SMB, \"check permission using windows acl\\n\");\n\tacl_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\n\t\t\t\t\t  path->dentry, &pntsd);\n\tif (acl_size <= 0 || !pntsd || !pntsd->dacloffset) {\n\t\tkfree(pntsd);\n\t\treturn 0;\n\t}\n\n\tpdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\n\tend_of_acl = ((char *)pntsd) + acl_size;\n\tif (end_of_acl <= (char *)pdacl) {\n\t\tkfree(pntsd);\n\t\treturn 0;\n\t}\n\n\tif (end_of_acl < (char *)pdacl + le16_to_cpu(pdacl->size) ||\n\t    le16_to_cpu(pdacl->size) < sizeof(struct smb_acl)) {\n\t\tkfree(pntsd);\n\t\treturn 0;\n\t}\n\n\tif (!pdacl->num_aces) {\n\t\tif (!(le16_to_cpu(pdacl->size) - sizeof(struct smb_acl)) &&\n\t\t    *pdaccess & ~(FILE_READ_CONTROL_LE | FILE_WRITE_DAC_LE)) {\n\t\t\trc = -EACCES;\n\t\t\tgoto err_out;\n\t\t}\n\t\tkfree(pntsd);\n\t\treturn 0;\n\t}\n\n\tif (*pdaccess & FILE_MAXIMAL_ACCESS_LE) {\n\t\tgranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\n\t\t\tDELETE;\n\n\t\tace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\n\t\tfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\n\t\t\tgranted |= le32_to_cpu(ace->access_req);\n\t\t\tace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\n\t\t\tif (end_of_acl < (char *)ace)\n\t\t\t\tgoto err_out;\n\t\t}\n\n\t\tif (!pdacl->num_aces)\n\t\t\tgranted = GENERIC_ALL_FLAGS;\n\t}\n\n\tif (!uid)\n\t\tsid_type = SIDUNIX_USER;\n\tid_to_sid(uid, sid_type, &sid);\n\n\tace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\n\tfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\n\t\tif (!compare_sids(&sid, &ace->sid) ||\n\t\t    !compare_sids(&sid_unix_NFS_mode, &ace->sid)) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (!compare_sids(&sid_everyone, &ace->sid))\n\t\t\tothers_ace = ace;\n\n\t\tace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\n\t\tif (end_of_acl < (char *)ace)\n\t\t\tgoto err_out;\n\t}\n\n\tif (*pdaccess & FILE_MAXIMAL_ACCESS_LE && found) {\n\t\tgranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\n\t\t\tDELETE;\n\n\t\tgranted |= le32_to_cpu(ace->access_req);\n\n\t\tif (!pdacl->num_aces)\n\t\t\tgranted = GENERIC_ALL_FLAGS;\n\t}\n\n\tif (IS_ENABLED(CONFIG_FS_POSIX_ACL)) {\n\t\tposix_acls = get_acl(d_inode(path->dentry), ACL_TYPE_ACCESS);\n\t\tif (posix_acls && !found) {\n\t\t\tunsigned int id = -1;\n\n\t\t\tpa_entry = posix_acls->a_entries;\n\t\t\tfor (i = 0; i < posix_acls->a_count; i++, pa_entry++) {\n\t\t\t\tif (pa_entry->e_tag == ACL_USER)\n\t\t\t\t\tid = posix_acl_uid_translate(user_ns, pa_entry);\n\t\t\t\telse if (pa_entry->e_tag == ACL_GROUP)\n\t\t\t\t\tid = posix_acl_gid_translate(user_ns, pa_entry);\n\t\t\t\telse\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (id == uid) {\n\t\t\t\t\tmode_to_access_flags(pa_entry->e_perm,\n\t\t\t\t\t\t\t     0777,\n\t\t\t\t\t\t\t     &access_bits);\n\t\t\t\t\tif (!access_bits)\n\t\t\t\t\t\taccess_bits =\n\t\t\t\t\t\t\tSET_MINIMUM_RIGHTS;\n\t\t\t\t\tposix_acl_release(posix_acls);\n\t\t\t\t\tgoto check_access_bits;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (posix_acls)\n\t\t\tposix_acl_release(posix_acls);\n\t}\n\n\tif (!found) {\n\t\tif (others_ace) {\n\t\t\tace = others_ace;\n\t\t} else {\n\t\t\tksmbd_debug(SMB, \"Can't find corresponding sid\\n\");\n\t\t\trc = -EACCES;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tswitch (ace->type) {\n\tcase ACCESS_ALLOWED_ACE_TYPE:\n\t\taccess_bits = le32_to_cpu(ace->access_req);\n\t\tbreak;\n\tcase ACCESS_DENIED_ACE_TYPE:\n\tcase ACCESS_DENIED_CALLBACK_ACE_TYPE:\n\t\taccess_bits = le32_to_cpu(~ace->access_req);\n\t\tbreak;\n\t}\n\ncheck_access_bits:\n\tif (granted &\n\t    ~(access_bits | FILE_READ_ATTRIBUTES | READ_CONTROL | WRITE_DAC | DELETE)) {\n\t\tksmbd_debug(SMB, \"Access denied with winACL, granted : %x, access_req : %x\\n\",\n\t\t\t    granted, le32_to_cpu(ace->access_req));\n\t\trc = -EACCES;\n\t\tgoto err_out;\n\t}\n\n\t*pdaccess = cpu_to_le32(granted);\nerr_out:\n\tkfree(pntsd);\n\treturn rc;\n}",
      "code_after_change": "int smb_check_perm_dacl(struct ksmbd_conn *conn, struct path *path,\n\t\t\t__le32 *pdaccess, int uid)\n{\n\tstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\n\tstruct smb_ntsd *pntsd = NULL;\n\tstruct smb_acl *pdacl;\n\tstruct posix_acl *posix_acls;\n\tint rc = 0, pntsd_size, acl_size, aces_size, pdacl_size, dacl_offset;\n\tstruct smb_sid sid;\n\tint granted = le32_to_cpu(*pdaccess & ~FILE_MAXIMAL_ACCESS_LE);\n\tstruct smb_ace *ace;\n\tint i, found = 0;\n\tunsigned int access_bits = 0;\n\tstruct smb_ace *others_ace = NULL;\n\tstruct posix_acl_entry *pa_entry;\n\tunsigned int sid_type = SIDOWNER;\n\tunsigned short ace_size;\n\n\tksmbd_debug(SMB, \"check permission using windows acl\\n\");\n\tpntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\n\t\t\t\t\t    path->dentry, &pntsd);\n\tif (pntsd_size <= 0 || !pntsd)\n\t\tgoto err_out;\n\n\tdacl_offset = le32_to_cpu(pntsd->dacloffset);\n\tif (!dacl_offset ||\n\t    (dacl_offset + sizeof(struct smb_acl) > pntsd_size))\n\t\tgoto err_out;\n\n\tpdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\n\tacl_size = pntsd_size - dacl_offset;\n\tpdacl_size = le16_to_cpu(pdacl->size);\n\n\tif (pdacl_size > acl_size || pdacl_size < sizeof(struct smb_acl))\n\t\tgoto err_out;\n\n\tif (!pdacl->num_aces) {\n\t\tif (!(pdacl_size - sizeof(struct smb_acl)) &&\n\t\t    *pdaccess & ~(FILE_READ_CONTROL_LE | FILE_WRITE_DAC_LE)) {\n\t\t\trc = -EACCES;\n\t\t\tgoto err_out;\n\t\t}\n\t\tgoto err_out;\n\t}\n\n\tif (*pdaccess & FILE_MAXIMAL_ACCESS_LE) {\n\t\tgranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\n\t\t\tDELETE;\n\n\t\tace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\n\t\taces_size = acl_size - sizeof(struct smb_acl);\n\t\tfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\n\t\t\tif (offsetof(struct smb_ace, access_req) > aces_size)\n\t\t\t\tbreak;\n\t\t\tace_size = le16_to_cpu(ace->size);\n\t\t\tif (ace_size > aces_size)\n\t\t\t\tbreak;\n\t\t\taces_size -= ace_size;\n\t\t\tgranted |= le32_to_cpu(ace->access_req);\n\t\t\tace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\n\t\t}\n\n\t\tif (!pdacl->num_aces)\n\t\t\tgranted = GENERIC_ALL_FLAGS;\n\t}\n\n\tif (!uid)\n\t\tsid_type = SIDUNIX_USER;\n\tid_to_sid(uid, sid_type, &sid);\n\n\tace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\n\taces_size = acl_size - sizeof(struct smb_acl);\n\tfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\n\t\tif (offsetof(struct smb_ace, access_req) > aces_size)\n\t\t\tbreak;\n\t\tace_size = le16_to_cpu(ace->size);\n\t\tif (ace_size > aces_size)\n\t\t\tbreak;\n\t\taces_size -= ace_size;\n\n\t\tif (!compare_sids(&sid, &ace->sid) ||\n\t\t    !compare_sids(&sid_unix_NFS_mode, &ace->sid)) {\n\t\t\tfound = 1;\n\t\t\tbreak;\n\t\t}\n\t\tif (!compare_sids(&sid_everyone, &ace->sid))\n\t\t\tothers_ace = ace;\n\n\t\tace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\n\t}\n\n\tif (*pdaccess & FILE_MAXIMAL_ACCESS_LE && found) {\n\t\tgranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\n\t\t\tDELETE;\n\n\t\tgranted |= le32_to_cpu(ace->access_req);\n\n\t\tif (!pdacl->num_aces)\n\t\t\tgranted = GENERIC_ALL_FLAGS;\n\t}\n\n\tif (IS_ENABLED(CONFIG_FS_POSIX_ACL)) {\n\t\tposix_acls = get_acl(d_inode(path->dentry), ACL_TYPE_ACCESS);\n\t\tif (posix_acls && !found) {\n\t\t\tunsigned int id = -1;\n\n\t\t\tpa_entry = posix_acls->a_entries;\n\t\t\tfor (i = 0; i < posix_acls->a_count; i++, pa_entry++) {\n\t\t\t\tif (pa_entry->e_tag == ACL_USER)\n\t\t\t\t\tid = posix_acl_uid_translate(user_ns, pa_entry);\n\t\t\t\telse if (pa_entry->e_tag == ACL_GROUP)\n\t\t\t\t\tid = posix_acl_gid_translate(user_ns, pa_entry);\n\t\t\t\telse\n\t\t\t\t\tcontinue;\n\n\t\t\t\tif (id == uid) {\n\t\t\t\t\tmode_to_access_flags(pa_entry->e_perm,\n\t\t\t\t\t\t\t     0777,\n\t\t\t\t\t\t\t     &access_bits);\n\t\t\t\t\tif (!access_bits)\n\t\t\t\t\t\taccess_bits =\n\t\t\t\t\t\t\tSET_MINIMUM_RIGHTS;\n\t\t\t\t\tposix_acl_release(posix_acls);\n\t\t\t\t\tgoto check_access_bits;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif (posix_acls)\n\t\t\tposix_acl_release(posix_acls);\n\t}\n\n\tif (!found) {\n\t\tif (others_ace) {\n\t\t\tace = others_ace;\n\t\t} else {\n\t\t\tksmbd_debug(SMB, \"Can't find corresponding sid\\n\");\n\t\t\trc = -EACCES;\n\t\t\tgoto err_out;\n\t\t}\n\t}\n\n\tswitch (ace->type) {\n\tcase ACCESS_ALLOWED_ACE_TYPE:\n\t\taccess_bits = le32_to_cpu(ace->access_req);\n\t\tbreak;\n\tcase ACCESS_DENIED_ACE_TYPE:\n\tcase ACCESS_DENIED_CALLBACK_ACE_TYPE:\n\t\taccess_bits = le32_to_cpu(~ace->access_req);\n\t\tbreak;\n\t}\n\ncheck_access_bits:\n\tif (granted &\n\t    ~(access_bits | FILE_READ_ATTRIBUTES | READ_CONTROL | WRITE_DAC | DELETE)) {\n\t\tksmbd_debug(SMB, \"Access denied with winACL, granted : %x, access_req : %x\\n\",\n\t\t\t    granted, le32_to_cpu(ace->access_req));\n\t\trc = -EACCES;\n\t\tgoto err_out;\n\t}\n\n\t*pdaccess = cpu_to_le32(granted);\nerr_out:\n\tkfree(pntsd);\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tint rc = 0, pntsd_size, acl_size, aces_size, pdacl_size, dacl_offset;",
          "\tunsigned short ace_size;",
          "\tpntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,",
          "\t\t\t\t\t    path->dentry, &pntsd);",
          "\tif (pntsd_size <= 0 || !pntsd)",
          "\t\tgoto err_out;",
          "",
          "\tdacl_offset = le32_to_cpu(pntsd->dacloffset);",
          "\tif (!dacl_offset ||",
          "\t    (dacl_offset + sizeof(struct smb_acl) > pntsd_size))",
          "\t\tgoto err_out;",
          "\tacl_size = pntsd_size - dacl_offset;",
          "\tpdacl_size = le16_to_cpu(pdacl->size);",
          "\tif (pdacl_size > acl_size || pdacl_size < sizeof(struct smb_acl))",
          "\t\tgoto err_out;",
          "\t\tif (!(pdacl_size - sizeof(struct smb_acl)) &&",
          "\t\tgoto err_out;",
          "\t\taces_size = acl_size - sizeof(struct smb_acl);",
          "\t\t\tif (offsetof(struct smb_ace, access_req) > aces_size)",
          "\t\t\t\tbreak;",
          "\t\t\tace_size = le16_to_cpu(ace->size);",
          "\t\t\tif (ace_size > aces_size)",
          "\t\t\t\tbreak;",
          "\t\t\taces_size -= ace_size;",
          "\taces_size = acl_size - sizeof(struct smb_acl);",
          "\t\tif (offsetof(struct smb_ace, access_req) > aces_size)",
          "\t\t\tbreak;",
          "\t\tace_size = le16_to_cpu(ace->size);",
          "\t\tif (ace_size > aces_size)",
          "\t\t\tbreak;",
          "\t\taces_size -= ace_size;",
          ""
        ],
        "deleted": [
          "\tint rc = 0, acl_size;",
          "\tchar *end_of_acl;",
          "\tacl_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,",
          "\t\t\t\t\t  path->dentry, &pntsd);",
          "\tif (acl_size <= 0 || !pntsd || !pntsd->dacloffset) {",
          "\t\tkfree(pntsd);",
          "\t\treturn 0;",
          "\t}",
          "\tend_of_acl = ((char *)pntsd) + acl_size;",
          "\tif (end_of_acl <= (char *)pdacl) {",
          "\t\tkfree(pntsd);",
          "\t\treturn 0;",
          "\t}",
          "\tif (end_of_acl < (char *)pdacl + le16_to_cpu(pdacl->size) ||",
          "\t    le16_to_cpu(pdacl->size) < sizeof(struct smb_acl)) {",
          "\t\tkfree(pntsd);",
          "\t\treturn 0;",
          "\t}",
          "\t\tif (!(le16_to_cpu(pdacl->size) - sizeof(struct smb_acl)) &&",
          "\t\tkfree(pntsd);",
          "\t\treturn 0;",
          "\t\t\tif (end_of_acl < (char *)ace)",
          "\t\t\t\tgoto err_out;",
          "\t\tif (end_of_acl < (char *)ace)",
          "\t\t\tgoto err_out;"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of ACL entry sizes during processing of SMB2_SET_INFO_HE commands.",
      "trigger_condition": "When the size of ACL entries is not properly validated and exceeds the allocated buffer size, a heap-based buffer overflow vulnerability occurs.",
      "specific_code_behavior_causing_vulnerability": "The code does not adequately check and verify the sizes of ACL entries, allowing for a buffer overflow if the sizes are not within the expected bounds.",
      "id": 148,
      "code_after_change_normalized": "int FUN1(struct ksmbd_conn *VAR1, struct VAR2 *VAR2,\n__le32 *VAR3, int VAR4)\n{\nstruct user_namespace *VAR5 = FUN2(VAR2->VAR6);\nstruct smb_ntsd *VAR7 = NULL;\nstruct smb_acl *VAR8;\nstruct posix_acl *VAR9;\nint VAR10 = 0, VAR11, VAR12, VAR13, VAR14, VAR15;\nstruct smb_sid VAR16;\nint VAR17 = FUN3(*VAR3 & ~VAR18);\nstruct smb_ace *VAR19;\nint VAR20, VAR21 = 0;\nunsigned int VAR22 = 0;\nstruct smb_ace *VAR23 = NULL;\nstruct posix_acl_entry *VAR24;\nunsigned int VAR25 = VAR26;\nunsigned short VAR27;\nFUN4(VAR28, \"STR\");\nVAR11 = FUN5(VAR1, VAR5,\nVAR2->VAR29, &VAR7);\nif (VAR11 <= 0 || !VAR7)\ngoto VAR30;\nVAR15 = FUN3(VAR7->VAR31);\nif (!VAR15 ||\n(VAR15 + sizeof(struct VAR32) > VAR11))\ngoto VAR30;\nVAR8 = (struct VAR32 *)((char *)VAR7 + FUN3(VAR7->VAR31));\nVAR12 = VAR11 - VAR15;\nVAR14 = FUN6(VAR8->VAR33);\nif (VAR14 > VAR12 || VAR14 < sizeof(struct VAR32))\ngoto VAR30;\nif (!VAR8->VAR34) {\nif (!(VAR14 - sizeof(struct VAR32)) &&\n*VAR3 & ~(VAR35 | VAR36)) {\nVAR10 = -VAR37;\ngoto VAR30;\n}\ngoto VAR30;\n}\nif (*VAR3 & VAR18) {\nVAR17 = VAR38 | VAR39 | VAR40 |\nVAR41;\nVAR19 = (struct VAR42 *)((char *)VAR8 + sizeof(struct VAR32));\nVAR13 = VAR12 - sizeof(struct VAR32);\nfor (VAR20 = 0; VAR20 < FUN3(VAR8->VAR34); VAR20++) {\nif (FUN7(struct VAR42, VAR43) > VAR13)\nbreak;\nVAR27 = FUN6(VAR19->VAR33);\nif (VAR27 > VAR13)\nbreak;\nVAR13 -= VAR27;\nVAR17 |= FUN3(VAR19->VAR43);\nVAR19 = (struct VAR42 *)((char *)VAR19 + FUN6(VAR19->VAR33));\n}\nif (!VAR8->VAR34)\nVAR17 = VAR44;\n}\nif (!VAR4)\nVAR25 = VAR45;\nFUN8(VAR4, VAR25, &VAR16);\nVAR19 = (struct VAR42 *)((char *)VAR8 + sizeof(struct VAR32));\nVAR13 = VAR12 - sizeof(struct VAR32);\nfor (VAR20 = 0; VAR20 < FUN3(VAR8->VAR34); VAR20++) {\nif (FUN7(struct VAR42, VAR43) > VAR13)\nbreak;\nVAR27 = FUN6(VAR19->VAR33);\nif (VAR27 > VAR13)\nbreak;\nVAR13 -= VAR27;\nif (!FUN9(&VAR16, &VAR19->VAR16) ||\n!FUN9(&VAR46, &VAR19->VAR16)) {\nVAR21 = 1;\nbreak;\n}\nif (!FUN9(&VAR47, &VAR19->VAR16))\nVAR23 = VAR19;\nVAR19 = (struct VAR42 *)((char *)VAR19 + FUN6(VAR19->VAR33));\n}\nif (*VAR3 & VAR18 && VAR21) {\nVAR17 = VAR38 | VAR39 | VAR40 |\nVAR41;\nVAR17 |= FUN3(VAR19->VAR43);\nif (!VAR8->VAR34)\nVAR17 = VAR44;\n}\nif (FUN10(VAR48)) {\nVAR9 = FUN11(FUN12(VAR2->VAR29), VAR49);\nif (VAR9 && !VAR21) {\nunsigned int VAR50 = -1;\nVAR24 = VAR9->VAR51;\nfor (VAR20 = 0; VAR20 < VAR9->VAR52; VAR20++, VAR24++) {\nif (VAR24->VAR53 == VAR54)\nVAR50 = FUN13(VAR5, VAR24);\nelse if (VAR24->VAR53 == VAR55)\nVAR50 = FUN14(VAR5, VAR24);\nelse\ncontinue;\nif (VAR50 == VAR4) {\nFUN15(VAR24->VAR56,\n0777,\n&VAR22);\nif (!VAR22)\nVAR22 =\nVAR57;\nFUN16(VAR9);\ngoto VAR58;\n}\n}\n}\nif (VAR9)\nFUN16(VAR9);\n}\nif (!VAR21) {\nif (VAR23) {\nVAR19 = VAR23;\n} else {\nFUN4(VAR28, \"STR\");\nVAR10 = -VAR37;\ngoto VAR30;\n}\n}\nswitch (VAR19->VAR59) {\ncase VAR60:\nVAR22 = FUN3(VAR19->VAR43);\nbreak;\ncase VAR61:\ncase VAR62:\nVAR22 = FUN3(~VAR19->VAR43);\nbreak;\n}\nVAR58:\nif (VAR17 &\n~(VAR22 | VAR40 | VAR38 | VAR39 | VAR41)) {\nFUN4(VAR28, \"STR\",\nVAR17, FUN3(VAR19->VAR43));\nVAR10 = -VAR37;\ngoto VAR30;\n}\n*VAR3 = FUN17(VAR17);\nVAR30:\nFUN18(VAR7);\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "int FUN1(struct ksmbd_conn *VAR1, struct VAR2 *VAR2,\n__le32 *VAR3, int VAR4)\n{\nstruct user_namespace *VAR5 = FUN2(VAR2->VAR6);\nstruct smb_ntsd *VAR7 = NULL;\nstruct smb_acl *VAR8;\nstruct posix_acl *VAR9;\nint VAR10 = 0, VAR11;\nstruct smb_sid VAR12;\nint VAR13 = FUN3(*VAR3 & ~VAR14);\nstruct smb_ace *VAR15;\nint VAR16, VAR17 = 0;\nunsigned int VAR18 = 0;\nstruct smb_ace *VAR19 = NULL;\nstruct posix_acl_entry *VAR20;\nunsigned int VAR21 = VAR22;\nchar *VAR23;\nFUN4(VAR24, \"STR\");\nVAR11 = FUN5(VAR1, VAR5,\nVAR2->VAR25, &VAR7);\nif (VAR11 <= 0 || !VAR7 || !VAR7->VAR26) {\nFUN6(VAR7);\nreturn 0;\n}\nVAR8 = (struct VAR27 *)((char *)VAR7 + FUN3(VAR7->VAR26));\nVAR23 = ((char *)VAR7) + VAR11;\nif (VAR23 <= (char *)VAR8) {\nFUN6(VAR7);\nreturn 0;\n}\nif (VAR23 < (char *)VAR8 + FUN7(VAR8->VAR28) ||\nFUN7(VAR8->VAR28) < sizeof(struct VAR27)) {\nFUN6(VAR7);\nreturn 0;\n}\nif (!VAR8->VAR29) {\nif (!(FUN7(VAR8->VAR28) - sizeof(struct VAR27)) &&\n*VAR3 & ~(VAR30 | VAR31)) {\nVAR10 = -VAR32;\ngoto VAR33;\n}\nFUN6(VAR7);\nreturn 0;\n}\nif (*VAR3 & VAR14) {\nVAR13 = VAR34 | VAR35 | VAR36 |\nVAR37;\nVAR15 = (struct VAR38 *)((char *)VAR8 + sizeof(struct VAR27));\nfor (VAR16 = 0; VAR16 < FUN3(VAR8->VAR29); VAR16++) {\nVAR13 |= FUN3(VAR15->VAR39);\nVAR15 = (struct VAR38 *)((char *)VAR15 + FUN7(VAR15->VAR28));\nif (VAR23 < (char *)VAR15)\ngoto VAR33;\n}\nif (!VAR8->VAR29)\nVAR13 = VAR40;\n}\nif (!VAR4)\nVAR21 = VAR41;\nFUN8(VAR4, VAR21, &VAR12);\nVAR15 = (struct VAR38 *)((char *)VAR8 + sizeof(struct VAR27));\nfor (VAR16 = 0; VAR16 < FUN3(VAR8->VAR29); VAR16++) {\nif (!FUN9(&VAR12, &VAR15->VAR12) ||\n!FUN9(&VAR42, &VAR15->VAR12)) {\nVAR17 = 1;\nbreak;\n}\nif (!FUN9(&VAR43, &VAR15->VAR12))\nVAR19 = VAR15;\nVAR15 = (struct VAR38 *)((char *)VAR15 + FUN7(VAR15->VAR28));\nif (VAR23 < (char *)VAR15)\ngoto VAR33;\n}\nif (*VAR3 & VAR14 && VAR17) {\nVAR13 = VAR34 | VAR35 | VAR36 |\nVAR37;\nVAR13 |= FUN3(VAR15->VAR39);\nif (!VAR8->VAR29)\nVAR13 = VAR40;\n}\nif (FUN10(VAR44)) {\nVAR9 = FUN11(FUN12(VAR2->VAR25), VAR45);\nif (VAR9 && !VAR17) {\nunsigned int VAR46 = -1;\nVAR20 = VAR9->VAR47;\nfor (VAR16 = 0; VAR16 < VAR9->VAR48; VAR16++, VAR20++) {\nif (VAR20->VAR49 == VAR50)\nVAR46 = FUN13(VAR5, VAR20);\nelse if (VAR20->VAR49 == VAR51)\nVAR46 = FUN14(VAR5, VAR20);\nelse\ncontinue;\nif (VAR46 == VAR4) {\nFUN15(VAR20->VAR52,\n0777,\n&VAR18);\nif (!VAR18)\nVAR18 =\nVAR53;\nFUN16(VAR9);\ngoto VAR54;\n}\n}\n}\nif (VAR9)\nFUN16(VAR9);\n}\nif (!VAR17) {\nif (VAR19) {\nVAR15 = VAR19;\n} else {\nFUN4(VAR24, \"STR\");\nVAR10 = -VAR32;\ngoto VAR33;\n}\n}\nswitch (VAR15->VAR55) {\ncase VAR56:\nVAR18 = FUN3(VAR15->VAR39);\nbreak;\ncase VAR57:\ncase VAR58:\nVAR18 = FUN3(~VAR15->VAR39);\nbreak;\n}\nVAR54:\nif (VAR13 &\n~(VAR18 | VAR36 | VAR34 | VAR35 | VAR37)) {\nFUN4(VAR24, \"STR\",\nVAR13, FUN3(VAR15->VAR39));\nVAR10 = -VAR32;\ngoto VAR33;\n}\n*VAR3 = FUN17(VAR13);\nVAR33:\nFUN6(VAR7);\nreturn VAR10;\n}\n",
      "code_after_change_raw": "int smb_check_perm_dacl(struct ksmbd_conn *conn, struct path *path,\n__le32 *pdaccess, int uid)\n{\nstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\nstruct smb_ntsd *pntsd = NULL;\nstruct smb_acl *pdacl;\nstruct posix_acl *posix_acls;\nint rc = 0, pntsd_size, acl_size, aces_size, pdacl_size, dacl_offset;\nstruct smb_sid sid;\nint granted = le32_to_cpu(*pdaccess & ~FILE_MAXIMAL_ACCESS_LE);\nstruct smb_ace *ace;\nint i, found = 0;\nunsigned int access_bits = 0;\nstruct smb_ace *others_ace = NULL;\nstruct posix_acl_entry *pa_entry;\nunsigned int sid_type = SIDOWNER;\nunsigned short ace_size;\nksmbd_debug(SMB, \"check permission using windows acl\\n\");\npntsd_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\npath->dentry, &pntsd);\nif (pntsd_size <= 0 || !pntsd)\ngoto err_out;\ndacl_offset = le32_to_cpu(pntsd->dacloffset);\nif (!dacl_offset ||\n(dacl_offset + sizeof(struct smb_acl) > pntsd_size))\ngoto err_out;\npdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\nacl_size = pntsd_size - dacl_offset;\npdacl_size = le16_to_cpu(pdacl->size);\nif (pdacl_size > acl_size || pdacl_size < sizeof(struct smb_acl))\ngoto err_out;\nif (!pdacl->num_aces) {\nif (!(pdacl_size - sizeof(struct smb_acl)) &&\n*pdaccess & ~(FILE_READ_CONTROL_LE | FILE_WRITE_DAC_LE)) {\nrc = -EACCES;\ngoto err_out;\n}\ngoto err_out;\n}\nif (*pdaccess & FILE_MAXIMAL_ACCESS_LE) {\ngranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\nDELETE;\nace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\naces_size = acl_size - sizeof(struct smb_acl);\nfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\nif (offsetof(struct smb_ace, access_req) > aces_size)\nbreak;\nace_size = le16_to_cpu(ace->size);\nif (ace_size > aces_size)\nbreak;\naces_size -= ace_size;\ngranted |= le32_to_cpu(ace->access_req);\nace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\n}\nif (!pdacl->num_aces)\ngranted = GENERIC_ALL_FLAGS;\n}\nif (!uid)\nsid_type = SIDUNIX_USER;\nid_to_sid(uid, sid_type, &sid);\nace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\naces_size = acl_size - sizeof(struct smb_acl);\nfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\nif (offsetof(struct smb_ace, access_req) > aces_size)\nbreak;\nace_size = le16_to_cpu(ace->size);\nif (ace_size > aces_size)\nbreak;\naces_size -= ace_size;\nif (!compare_sids(&sid, &ace->sid) ||\n!compare_sids(&sid_unix_NFS_mode, &ace->sid)) {\nfound = 1;\nbreak;\n}\nif (!compare_sids(&sid_everyone, &ace->sid))\nothers_ace = ace;\nace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\n}\nif (*pdaccess & FILE_MAXIMAL_ACCESS_LE && found) {\ngranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\nDELETE;\ngranted |= le32_to_cpu(ace->access_req);\nif (!pdacl->num_aces)\ngranted = GENERIC_ALL_FLAGS;\n}\nif (IS_ENABLED(CONFIG_FS_POSIX_ACL)) {\nposix_acls = get_acl(d_inode(path->dentry), ACL_TYPE_ACCESS);\nif (posix_acls && !found) {\nunsigned int id = -1;\npa_entry = posix_acls->a_entries;\nfor (i = 0; i < posix_acls->a_count; i++, pa_entry++) {\nif (pa_entry->e_tag == ACL_USER)\nid = posix_acl_uid_translate(user_ns, pa_entry);\nelse if (pa_entry->e_tag == ACL_GROUP)\nid = posix_acl_gid_translate(user_ns, pa_entry);\nelse\ncontinue;\nif (id == uid) {\nmode_to_access_flags(pa_entry->e_perm,\n0777,\n&access_bits);\nif (!access_bits)\naccess_bits =\nSET_MINIMUM_RIGHTS;\nposix_acl_release(posix_acls);\ngoto check_access_bits;\n}\n}\n}\nif (posix_acls)\nposix_acl_release(posix_acls);\n}\nif (!found) {\nif (others_ace) {\nace = others_ace;\n} else {\nksmbd_debug(SMB, \"Can't find corresponding sid\\n\");\nrc = -EACCES;\ngoto err_out;\n}\n}\nswitch (ace->type) {\ncase ACCESS_ALLOWED_ACE_TYPE:\naccess_bits = le32_to_cpu(ace->access_req);\nbreak;\ncase ACCESS_DENIED_ACE_TYPE:\ncase ACCESS_DENIED_CALLBACK_ACE_TYPE:\naccess_bits = le32_to_cpu(~ace->access_req);\nbreak;\n}\ncheck_access_bits:\nif (granted &\n~(access_bits | FILE_READ_ATTRIBUTES | READ_CONTROL | WRITE_DAC | DELETE)) {\nksmbd_debug(SMB, \"Access denied with winACL, granted : %x, access_req : %x\\n\",\ngranted, le32_to_cpu(ace->access_req));\nrc = -EACCES;\ngoto err_out;\n}\n*pdaccess = cpu_to_le32(granted);\nerr_out:\nkfree(pntsd);\nreturn rc;\n}\n",
      "code_before_change_raw": "int smb_check_perm_dacl(struct ksmbd_conn *conn, struct path *path,\n__le32 *pdaccess, int uid)\n{\nstruct user_namespace *user_ns = mnt_user_ns(path->mnt);\nstruct smb_ntsd *pntsd = NULL;\nstruct smb_acl *pdacl;\nstruct posix_acl *posix_acls;\nint rc = 0, acl_size;\nstruct smb_sid sid;\nint granted = le32_to_cpu(*pdaccess & ~FILE_MAXIMAL_ACCESS_LE);\nstruct smb_ace *ace;\nint i, found = 0;\nunsigned int access_bits = 0;\nstruct smb_ace *others_ace = NULL;\nstruct posix_acl_entry *pa_entry;\nunsigned int sid_type = SIDOWNER;\nchar *end_of_acl;\nksmbd_debug(SMB, \"check permission using windows acl\\n\");\nacl_size = ksmbd_vfs_get_sd_xattr(conn, user_ns,\npath->dentry, &pntsd);\nif (acl_size <= 0 || !pntsd || !pntsd->dacloffset) {\nkfree(pntsd);\nreturn 0;\n}\npdacl = (struct smb_acl *)((char *)pntsd + le32_to_cpu(pntsd->dacloffset));\nend_of_acl = ((char *)pntsd) + acl_size;\nif (end_of_acl <= (char *)pdacl) {\nkfree(pntsd);\nreturn 0;\n}\nif (end_of_acl < (char *)pdacl + le16_to_cpu(pdacl->size) ||\nle16_to_cpu(pdacl->size) < sizeof(struct smb_acl)) {\nkfree(pntsd);\nreturn 0;\n}\nif (!pdacl->num_aces) {\nif (!(le16_to_cpu(pdacl->size) - sizeof(struct smb_acl)) &&\n*pdaccess & ~(FILE_READ_CONTROL_LE | FILE_WRITE_DAC_LE)) {\nrc = -EACCES;\ngoto err_out;\n}\nkfree(pntsd);\nreturn 0;\n}\nif (*pdaccess & FILE_MAXIMAL_ACCESS_LE) {\ngranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\nDELETE;\nace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\nfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\ngranted |= le32_to_cpu(ace->access_req);\nace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\nif (end_of_acl < (char *)ace)\ngoto err_out;\n}\nif (!pdacl->num_aces)\ngranted = GENERIC_ALL_FLAGS;\n}\nif (!uid)\nsid_type = SIDUNIX_USER;\nid_to_sid(uid, sid_type, &sid);\nace = (struct smb_ace *)((char *)pdacl + sizeof(struct smb_acl));\nfor (i = 0; i < le32_to_cpu(pdacl->num_aces); i++) {\nif (!compare_sids(&sid, &ace->sid) ||\n!compare_sids(&sid_unix_NFS_mode, &ace->sid)) {\nfound = 1;\nbreak;\n}\nif (!compare_sids(&sid_everyone, &ace->sid))\nothers_ace = ace;\nace = (struct smb_ace *)((char *)ace + le16_to_cpu(ace->size));\nif (end_of_acl < (char *)ace)\ngoto err_out;\n}\nif (*pdaccess & FILE_MAXIMAL_ACCESS_LE && found) {\ngranted = READ_CONTROL | WRITE_DAC | FILE_READ_ATTRIBUTES |\nDELETE;\ngranted |= le32_to_cpu(ace->access_req);\nif (!pdacl->num_aces)\ngranted = GENERIC_ALL_FLAGS;\n}\nif (IS_ENABLED(CONFIG_FS_POSIX_ACL)) {\nposix_acls = get_acl(d_inode(path->dentry), ACL_TYPE_ACCESS);\nif (posix_acls && !found) {\nunsigned int id = -1;\npa_entry = posix_acls->a_entries;\nfor (i = 0; i < posix_acls->a_count; i++, pa_entry++) {\nif (pa_entry->e_tag == ACL_USER)\nid = posix_acl_uid_translate(user_ns, pa_entry);\nelse if (pa_entry->e_tag == ACL_GROUP)\nid = posix_acl_gid_translate(user_ns, pa_entry);\nelse\ncontinue;\nif (id == uid) {\nmode_to_access_flags(pa_entry->e_perm,\n0777,\n&access_bits);\nif (!access_bits)\naccess_bits =\nSET_MINIMUM_RIGHTS;\nposix_acl_release(posix_acls);\ngoto check_access_bits;\n}\n}\n}\nif (posix_acls)\nposix_acl_release(posix_acls);\n}\nif (!found) {\nif (others_ace) {\nace = others_ace;\n} else {\nksmbd_debug(SMB, \"Can't find corresponding sid\\n\");\nrc = -EACCES;\ngoto err_out;\n}\n}\nswitch (ace->type) {\ncase ACCESS_ALLOWED_ACE_TYPE:\naccess_bits = le32_to_cpu(ace->access_req);\nbreak;\ncase ACCESS_DENIED_ACE_TYPE:\ncase ACCESS_DENIED_CALLBACK_ACE_TYPE:\naccess_bits = le32_to_cpu(~ace->access_req);\nbreak;\n}\ncheck_access_bits:\nif (granted &\n~(access_bits | FILE_READ_ATTRIBUTES | READ_CONTROL | WRITE_DAC | DELETE)) {\nksmbd_debug(SMB, \"Access denied with winACL, granted : %x, access_req : %x\\n\",\ngranted, le32_to_cpu(ace->access_req));\nrc = -EACCES;\ngoto err_out;\n}\n*pdaccess = cpu_to_le32(granted);\nerr_out:\nkfree(pntsd);\nreturn rc;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for the size of the security descriptor buffer before assigning it to an output parameter.",
        "trigger_condition": "The security descriptor buffer size is smaller than the expected size of the output parameter structure, leading to a heap-based buffer overflow vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code directly assigns the security descriptor buffer to the output parameter without verifying that the size of the buffer is valid, allowing a potential buffer overflow if the size is insufficient."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the size of the security descriptor buffer is at least the size of the output parameter structure before assigning it. If the size is invalid, the function should return an error and free resources to prevent a heap-based buffer overflow. In this case, the solution involves adding a validation check for the size of the security descriptor buffer before assigning it to the output parameter.",
      "GPT_analysis": "The above modification is necessary to address a heap-based buffer overflow vulnerability in the `ksmbd_vfs_get_sd_xattr` function. The vulnerability is related to the handling of the `acl.sd_size` field, which is used to determine the size of the security descriptor buffer `acl.sd_buf`. \n\nIn the vulnerable code snippet, the security descriptor buffer `acl.sd_buf` is directly assigned to the output parameter `*pntsd`, without checking if the size of `acl.sd_buf` is valid. This can lead to a heap-based buffer overflow if the size of `acl.sd_buf` is smaller than the expected size of `struct smb_ntsd`.\n\nThe modification adds a check to ensure that the size of `acl.sd_buf` is at least the size of `struct smb_ntsd` before proceeding with further operations on `*pntsd`. If the size is invalid, the function will return an error and free resources to prevent potential buffer overflow.\n\nBy making this modification, the code is more robust and secure, mitigating the risk of heap-based buffer overflow vulnerabilities in the `ksmbd_vfs_get_sd_xattr` function.",
      "GPT_purpose": "Retrieve security descriptor extended attribute (xattr) for a file in the SMB server.",
      "GPT_function": "\n1. Retrieve security descriptor extended attribute (xattr) for a given dentry.\n2. Decode the retrieved security descriptor using ndr_decode_v4_ntacl.\n3. Generate a hash for the decoded security descriptor and compare it with the stored hash value.\n4. Adjust offsets in the security descriptor buffer.\n5. Free allocated memory based on the outcome of the operations.",
      "CVE_id": "CVE-2022-47942",
      "code_before_change": "int ksmbd_vfs_get_sd_xattr(struct ksmbd_conn *conn,\n\t\t\t   struct user_namespace *user_ns,\n\t\t\t   struct dentry *dentry,\n\t\t\t   struct smb_ntsd **pntsd)\n{\n\tint rc;\n\tstruct ndr n;\n\tstruct inode *inode = d_inode(dentry);\n\tstruct ndr acl_ndr = {0};\n\tstruct xattr_ntacl acl;\n\tstruct xattr_smb_acl *smb_acl = NULL, *def_smb_acl = NULL;\n\t__u8 cmp_hash[XATTR_SD_HASH_SIZE] = {0};\n\n\trc = ksmbd_vfs_getxattr(user_ns, dentry, XATTR_NAME_SD, &n.data);\n\tif (rc <= 0)\n\t\treturn rc;\n\n\tn.length = rc;\n\trc = ndr_decode_v4_ntacl(&n, &acl);\n\tif (rc)\n\t\tgoto free_n_data;\n\n\tsmb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\n\t\t\t\t\t\t ACL_TYPE_ACCESS);\n\tif (S_ISDIR(inode->i_mode))\n\t\tdef_smb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\n\t\t\t\t\t\t\t     ACL_TYPE_DEFAULT);\n\n\trc = ndr_encode_posix_acl(&acl_ndr, user_ns, inode, smb_acl,\n\t\t\t\t  def_smb_acl);\n\tif (rc) {\n\t\tpr_err(\"failed to encode ndr to posix acl\\n\");\n\t\tgoto out_free;\n\t}\n\n\trc = ksmbd_gen_sd_hash(conn, acl_ndr.data, acl_ndr.offset, cmp_hash);\n\tif (rc) {\n\t\tpr_err(\"failed to generate hash for ndr acl\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (memcmp(cmp_hash, acl.posix_acl_hash, XATTR_SD_HASH_SIZE)) {\n\t\tpr_err(\"hash value diff\\n\");\n\t\trc = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\t*pntsd = acl.sd_buf;\n\t(*pntsd)->osidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->osidoffset) -\n\t\t\t\t\t   NDR_NTSD_OFFSETOF);\n\t(*pntsd)->gsidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->gsidoffset) -\n\t\t\t\t\t   NDR_NTSD_OFFSETOF);\n\t(*pntsd)->dacloffset = cpu_to_le32(le32_to_cpu((*pntsd)->dacloffset) -\n\t\t\t\t\t   NDR_NTSD_OFFSETOF);\n\n\trc = acl.sd_size;\nout_free:\n\tkfree(acl_ndr.data);\n\tkfree(smb_acl);\n\tkfree(def_smb_acl);\n\tif (rc < 0) {\n\t\tkfree(acl.sd_buf);\n\t\t*pntsd = NULL;\n\t}\n\nfree_n_data:\n\tkfree(n.data);\n\treturn rc;\n}",
      "code_after_change": "int ksmbd_vfs_get_sd_xattr(struct ksmbd_conn *conn,\n\t\t\t   struct user_namespace *user_ns,\n\t\t\t   struct dentry *dentry,\n\t\t\t   struct smb_ntsd **pntsd)\n{\n\tint rc;\n\tstruct ndr n;\n\tstruct inode *inode = d_inode(dentry);\n\tstruct ndr acl_ndr = {0};\n\tstruct xattr_ntacl acl;\n\tstruct xattr_smb_acl *smb_acl = NULL, *def_smb_acl = NULL;\n\t__u8 cmp_hash[XATTR_SD_HASH_SIZE] = {0};\n\n\trc = ksmbd_vfs_getxattr(user_ns, dentry, XATTR_NAME_SD, &n.data);\n\tif (rc <= 0)\n\t\treturn rc;\n\n\tn.length = rc;\n\trc = ndr_decode_v4_ntacl(&n, &acl);\n\tif (rc)\n\t\tgoto free_n_data;\n\n\tsmb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\n\t\t\t\t\t\t ACL_TYPE_ACCESS);\n\tif (S_ISDIR(inode->i_mode))\n\t\tdef_smb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\n\t\t\t\t\t\t\t     ACL_TYPE_DEFAULT);\n\n\trc = ndr_encode_posix_acl(&acl_ndr, user_ns, inode, smb_acl,\n\t\t\t\t  def_smb_acl);\n\tif (rc) {\n\t\tpr_err(\"failed to encode ndr to posix acl\\n\");\n\t\tgoto out_free;\n\t}\n\n\trc = ksmbd_gen_sd_hash(conn, acl_ndr.data, acl_ndr.offset, cmp_hash);\n\tif (rc) {\n\t\tpr_err(\"failed to generate hash for ndr acl\\n\");\n\t\tgoto out_free;\n\t}\n\n\tif (memcmp(cmp_hash, acl.posix_acl_hash, XATTR_SD_HASH_SIZE)) {\n\t\tpr_err(\"hash value diff\\n\");\n\t\trc = -EINVAL;\n\t\tgoto out_free;\n\t}\n\n\t*pntsd = acl.sd_buf;\n\tif (acl.sd_size < sizeof(struct smb_ntsd)) {\n\t\tpr_err(\"sd size is invalid\\n\");\n\t\tgoto out_free;\n\t}\n\n\t(*pntsd)->osidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->osidoffset) -\n\t\t\t\t\t   NDR_NTSD_OFFSETOF);\n\t(*pntsd)->gsidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->gsidoffset) -\n\t\t\t\t\t   NDR_NTSD_OFFSETOF);\n\t(*pntsd)->dacloffset = cpu_to_le32(le32_to_cpu((*pntsd)->dacloffset) -\n\t\t\t\t\t   NDR_NTSD_OFFSETOF);\n\n\trc = acl.sd_size;\nout_free:\n\tkfree(acl_ndr.data);\n\tkfree(smb_acl);\n\tkfree(def_smb_acl);\n\tif (rc < 0) {\n\t\tkfree(acl.sd_buf);\n\t\t*pntsd = NULL;\n\t}\n\nfree_n_data:\n\tkfree(n.data);\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "\tif (acl.sd_size < sizeof(struct smb_ntsd)) {",
          "\t\tpr_err(\"sd size is invalid\\n\");",
          "\t\tgoto out_free;",
          "\t}",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of validation for the size of the security descriptor buffer before assigning it to an output parameter.",
      "trigger_condition": "The security descriptor buffer size is smaller than the expected size of the output parameter structure, leading to a heap-based buffer overflow vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code directly assigns the security descriptor buffer to the output parameter without verifying that the size of the buffer is valid, allowing a potential buffer overflow if the size is insufficient.",
      "id": 149,
      "code_after_change_normalized": "int FUN1(struct ksmbd_conn *VAR1,\nstruct user_namespace *VAR2,\nstruct VAR3 *VAR3,\nstruct smb_ntsd **VAR4)\n{\nint VAR5;\nstruct ndr VAR6;\nstruct VAR7 *VAR7 = FUN2(VAR3);\nstruct ndr VAR8 = {0};\nstruct xattr_ntacl VAR9;\nstruct xattr_smb_acl *VAR10 = NULL, *VAR11 = NULL;\n__u8 VAR12[VAR13] = {0};\nVAR5 = FUN3(VAR2, VAR3, VAR14, &VAR6.VAR15);\nif (VAR5 <= 0)\nreturn VAR5;\nVAR6.VAR16 = VAR5;\nVAR5 = FUN4(&VAR6, &VAR9);\nif (VAR5)\ngoto VAR17;\nVAR10 = FUN5(VAR2, VAR7,\nVAR18);\nif (FUN6(VAR7->VAR19))\nVAR11 = FUN5(VAR2, VAR7,\nVAR20);\nVAR5 = FUN7(&VAR8, VAR2, VAR7, VAR10,\nVAR11);\nif (VAR5) {\nFUN8(\"STR\");\ngoto VAR21;\n}\nVAR5 = FUN9(VAR1, VAR8.VAR15, VAR8.VAR22, VAR12);\nif (VAR5) {\nFUN8(\"STR\");\ngoto VAR21;\n}\nif (FUN10(VAR12, VAR9.VAR23, VAR13)) {\nFUN8(\"STR\");\nVAR5 = -VAR24;\ngoto VAR21;\n}\n*VAR4 = VAR9.VAR25;\nif (VAR9.VAR26 < sizeof(struct VAR27)) {\nFUN8(\"STR\");\ngoto VAR21;\n}\n(*VAR4)->VAR28 = FUN11(FUN12((*VAR4)->VAR28) -\nVAR29);\n(*VAR4)->VAR30 = FUN11(FUN12((*VAR4)->VAR30) -\nVAR29);\n(*VAR4)->VAR31 = FUN11(FUN12((*VAR4)->VAR31) -\nVAR29);\nVAR5 = VAR9.VAR26;\nVAR21:\nFUN13(VAR8.VAR15);\nFUN13(VAR10);\nFUN13(VAR11);\nif (VAR5 < 0) {\nFUN13(VAR9.VAR25);\n*VAR4 = NULL;\n}\nVAR17:\nFUN13(VAR6.VAR15);\nreturn VAR5;\n}\n",
      "code_before_change_normalized": "int FUN1(struct ksmbd_conn *VAR1,\nstruct user_namespace *VAR2,\nstruct VAR3 *VAR3,\nstruct smb_ntsd **VAR4)\n{\nint VAR5;\nstruct ndr VAR6;\nstruct VAR7 *VAR7 = FUN2(VAR3);\nstruct ndr VAR8 = {0};\nstruct xattr_ntacl VAR9;\nstruct xattr_smb_acl *VAR10 = NULL, *VAR11 = NULL;\n__u8 VAR12[VAR13] = {0};\nVAR5 = FUN3(VAR2, VAR3, VAR14, &VAR6.VAR15);\nif (VAR5 <= 0)\nreturn VAR5;\nVAR6.VAR16 = VAR5;\nVAR5 = FUN4(&VAR6, &VAR9);\nif (VAR5)\ngoto VAR17;\nVAR10 = FUN5(VAR2, VAR7,\nVAR18);\nif (FUN6(VAR7->VAR19))\nVAR11 = FUN5(VAR2, VAR7,\nVAR20);\nVAR5 = FUN7(&VAR8, VAR2, VAR7, VAR10,\nVAR11);\nif (VAR5) {\nFUN8(\"STR\");\ngoto VAR21;\n}\nVAR5 = FUN9(VAR1, VAR8.VAR15, VAR8.VAR22, VAR12);\nif (VAR5) {\nFUN8(\"STR\");\ngoto VAR21;\n}\nif (FUN10(VAR12, VAR9.VAR23, VAR13)) {\nFUN8(\"STR\");\nVAR5 = -VAR24;\ngoto VAR21;\n}\n*VAR4 = VAR9.VAR25;\n(*VAR4)->VAR26 = FUN11(FUN12((*VAR4)->VAR26) -\nVAR27);\n(*VAR4)->VAR28 = FUN11(FUN12((*VAR4)->VAR28) -\nVAR27);\n(*VAR4)->VAR29 = FUN11(FUN12((*VAR4)->VAR29) -\nVAR27);\nVAR5 = VAR9.VAR30;\nVAR21:\nFUN13(VAR8.VAR15);\nFUN13(VAR10);\nFUN13(VAR11);\nif (VAR5 < 0) {\nFUN13(VAR9.VAR25);\n*VAR4 = NULL;\n}\nVAR17:\nFUN13(VAR6.VAR15);\nreturn VAR5;\n}\n",
      "code_after_change_raw": "int ksmbd_vfs_get_sd_xattr(struct ksmbd_conn *conn,\nstruct user_namespace *user_ns,\nstruct dentry *dentry,\nstruct smb_ntsd **pntsd)\n{\nint rc;\nstruct ndr n;\nstruct inode *inode = d_inode(dentry);\nstruct ndr acl_ndr = {0};\nstruct xattr_ntacl acl;\nstruct xattr_smb_acl *smb_acl = NULL, *def_smb_acl = NULL;\n__u8 cmp_hash[XATTR_SD_HASH_SIZE] = {0};\nrc = ksmbd_vfs_getxattr(user_ns, dentry, XATTR_NAME_SD, &n.data);\nif (rc <= 0)\nreturn rc;\nn.length = rc;\nrc = ndr_decode_v4_ntacl(&n, &acl);\nif (rc)\ngoto free_n_data;\nsmb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\nACL_TYPE_ACCESS);\nif (S_ISDIR(inode->i_mode))\ndef_smb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\nACL_TYPE_DEFAULT);\nrc = ndr_encode_posix_acl(&acl_ndr, user_ns, inode, smb_acl,\ndef_smb_acl);\nif (rc) {\npr_err(\"failed to encode ndr to posix acl\\n\");\ngoto out_free;\n}\nrc = ksmbd_gen_sd_hash(conn, acl_ndr.data, acl_ndr.offset, cmp_hash);\nif (rc) {\npr_err(\"failed to generate hash for ndr acl\\n\");\ngoto out_free;\n}\nif (memcmp(cmp_hash, acl.posix_acl_hash, XATTR_SD_HASH_SIZE)) {\npr_err(\"hash value diff\\n\");\nrc = -EINVAL;\ngoto out_free;\n}\n*pntsd = acl.sd_buf;\nif (acl.sd_size < sizeof(struct smb_ntsd)) {\npr_err(\"sd size is invalid\\n\");\ngoto out_free;\n}\n(*pntsd)->osidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->osidoffset) -\nNDR_NTSD_OFFSETOF);\n(*pntsd)->gsidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->gsidoffset) -\nNDR_NTSD_OFFSETOF);\n(*pntsd)->dacloffset = cpu_to_le32(le32_to_cpu((*pntsd)->dacloffset) -\nNDR_NTSD_OFFSETOF);\nrc = acl.sd_size;\nout_free:\nkfree(acl_ndr.data);\nkfree(smb_acl);\nkfree(def_smb_acl);\nif (rc < 0) {\nkfree(acl.sd_buf);\n*pntsd = NULL;\n}\nfree_n_data:\nkfree(n.data);\nreturn rc;\n}\n",
      "code_before_change_raw": "int ksmbd_vfs_get_sd_xattr(struct ksmbd_conn *conn,\nstruct user_namespace *user_ns,\nstruct dentry *dentry,\nstruct smb_ntsd **pntsd)\n{\nint rc;\nstruct ndr n;\nstruct inode *inode = d_inode(dentry);\nstruct ndr acl_ndr = {0};\nstruct xattr_ntacl acl;\nstruct xattr_smb_acl *smb_acl = NULL, *def_smb_acl = NULL;\n__u8 cmp_hash[XATTR_SD_HASH_SIZE] = {0};\nrc = ksmbd_vfs_getxattr(user_ns, dentry, XATTR_NAME_SD, &n.data);\nif (rc <= 0)\nreturn rc;\nn.length = rc;\nrc = ndr_decode_v4_ntacl(&n, &acl);\nif (rc)\ngoto free_n_data;\nsmb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\nACL_TYPE_ACCESS);\nif (S_ISDIR(inode->i_mode))\ndef_smb_acl = ksmbd_vfs_make_xattr_posix_acl(user_ns, inode,\nACL_TYPE_DEFAULT);\nrc = ndr_encode_posix_acl(&acl_ndr, user_ns, inode, smb_acl,\ndef_smb_acl);\nif (rc) {\npr_err(\"failed to encode ndr to posix acl\\n\");\ngoto out_free;\n}\nrc = ksmbd_gen_sd_hash(conn, acl_ndr.data, acl_ndr.offset, cmp_hash);\nif (rc) {\npr_err(\"failed to generate hash for ndr acl\\n\");\ngoto out_free;\n}\nif (memcmp(cmp_hash, acl.posix_acl_hash, XATTR_SD_HASH_SIZE)) {\npr_err(\"hash value diff\\n\");\nrc = -EINVAL;\ngoto out_free;\n}\n*pntsd = acl.sd_buf;\n(*pntsd)->osidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->osidoffset) -\nNDR_NTSD_OFFSETOF);\n(*pntsd)->gsidoffset = cpu_to_le32(le32_to_cpu((*pntsd)->gsidoffset) -\nNDR_NTSD_OFFSETOF);\n(*pntsd)->dacloffset = cpu_to_le32(le32_to_cpu((*pntsd)->dacloffset) -\nNDR_NTSD_OFFSETOF);\nrc = acl.sd_size;\nout_free:\nkfree(acl_ndr.data);\nkfree(smb_acl);\nkfree(def_smb_acl);\nif (rc < 0) {\nkfree(acl.sd_buf);\n*pntsd = NULL;\n}\nfree_n_data:\nkfree(n.data);\nreturn rc;\n}\n"
    }
  ],
  "CVE-2022-48423": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for resident attribute names in the code.",
        "trigger_condition": "An out-of-bounds write vulnerability occurs when processing resident attribute names due to the absence of proper bounds checking.",
        "specific_code_behavior_causing_vulnerability": "The code fails to validate resident attribute names, allowing the possibility of writing data beyond the intended boundaries, potentially leading to security vulnerabilities."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a bounds check for resident attribute names to ensure they do not exceed the valid range. By incorporating this validation, the code can prevent out-of-bounds writes and enhance security. In this case, the solution involves adding a specific check for resident attribute names to prevent potential exploitation of the vulnerability.",
      "GPT_analysis": "The above modification is necessary to address the vulnerability described in CVE-2022-48423, which states that in the Linux kernel before version 6.1.3, the code does not validate resident attribute names, potentially leading to an out-of-bounds write vulnerability.\n\nBy adding the following line to the code:\n```\nif (attr->name_len && le16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len > t16) {\n    return NULL;\n}\n```\nwe are performing a bounds check on the resident attribute names to ensure that they do not exceed the valid range. This check helps prevent an out-of-bounds write scenario that could be exploited by malicious actors to manipulate memory beyond the intended boundaries, potentially leading to security vulnerabilities.\n\nTherefore, the modification is necessary to enhance the security of the code and prevent potential exploitation of the vulnerability.",
      "GPT_purpose": "Enumerate and validate attributes within an MFT inode structure in the NTFS filesystem.",
      "GPT_function": "\n1. Enumerate attributes within a given MFT inode.\n2. Validate the input attribute and its size.\n3. Check for attribute type and boundary conditions.\n4. Validate resident and non-resident attributes.\n5. Perform checks on nonresident fields.",
      "CVE_id": "CVE-2022-48423",
      "code_before_change": "struct ATTRIB *mi_enum_attr(struct mft_inode *mi, struct ATTRIB *attr)\n{\n\tconst struct MFT_REC *rec = mi->mrec;\n\tu32 used = le32_to_cpu(rec->used);\n\tu32 t32, off, asize;\n\tu16 t16;\n\n\tif (!attr) {\n\t\tu32 total = le32_to_cpu(rec->total);\n\n\t\toff = le16_to_cpu(rec->attr_off);\n\n\t\tif (used > total)\n\t\t\treturn NULL;\n\n\t\tif (off >= used || off < MFTRECORD_FIXUP_OFFSET_1 ||\n\t\t    !IS_ALIGNED(off, 4)) {\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/* Skip non-resident records. */\n\t\tif (!is_rec_inuse(rec))\n\t\t\treturn NULL;\n\n\t\tattr = Add2Ptr(rec, off);\n\t} else {\n\t\t/* Check if input attr inside record. */\n\t\toff = PtrOffset(rec, attr);\n\t\tif (off >= used)\n\t\t\treturn NULL;\n\n\t\tasize = le32_to_cpu(attr->size);\n\t\tif (asize < SIZEOF_RESIDENT) {\n\t\t\t/* Impossible 'cause we should not return such attribute. */\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (off + asize < off) {\n\t\t\t/* overflow check */\n\t\t\treturn NULL;\n\t\t}\n\n\t\tattr = Add2Ptr(attr, asize);\n\t\toff += asize;\n\t}\n\n\tasize = le32_to_cpu(attr->size);\n\n\t/* Can we use the first field (attr->type). */\n\tif (off + 8 > used) {\n\t\tstatic_assert(ALIGN(sizeof(enum ATTR_TYPE), 8) == 8);\n\t\treturn NULL;\n\t}\n\n\tif (attr->type == ATTR_END) {\n\t\t/* End of enumeration. */\n\t\treturn NULL;\n\t}\n\n\t/* 0x100 is last known attribute for now. */\n\tt32 = le32_to_cpu(attr->type);\n\tif ((t32 & 0xf) || (t32 > 0x100))\n\t\treturn NULL;\n\n\t/* Check boundary. */\n\tif (off + asize > used)\n\t\treturn NULL;\n\n\t/* Check size of attribute. */\n\tif (!attr->non_res) {\n\t\tif (asize < SIZEOF_RESIDENT)\n\t\t\treturn NULL;\n\n\t\tt16 = le16_to_cpu(attr->res.data_off);\n\n\t\tif (t16 > asize)\n\t\t\treturn NULL;\n\n\t\tt32 = le32_to_cpu(attr->res.data_size);\n\t\tif (t16 + t32 > asize)\n\t\t\treturn NULL;\n\n\t\treturn attr;\n\t}\n\n\t/* Check some nonresident fields. */\n\tif (attr->name_len &&\n\t    le16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len >\n\t\t    le16_to_cpu(attr->nres.run_off)) {\n\t\treturn NULL;\n\t}\n\n\tif (attr->nres.svcn || !is_attr_ext(attr)) {\n\t\tif (asize + 8 < SIZEOF_NONRESIDENT)\n\t\t\treturn NULL;\n\n\t\tif (attr->nres.c_unit)\n\t\t\treturn NULL;\n\t} else if (asize + 8 < SIZEOF_NONRESIDENT_EX)\n\t\treturn NULL;\n\n\treturn attr;\n}",
      "code_after_change": "struct ATTRIB *mi_enum_attr(struct mft_inode *mi, struct ATTRIB *attr)\n{\n\tconst struct MFT_REC *rec = mi->mrec;\n\tu32 used = le32_to_cpu(rec->used);\n\tu32 t32, off, asize;\n\tu16 t16;\n\n\tif (!attr) {\n\t\tu32 total = le32_to_cpu(rec->total);\n\n\t\toff = le16_to_cpu(rec->attr_off);\n\n\t\tif (used > total)\n\t\t\treturn NULL;\n\n\t\tif (off >= used || off < MFTRECORD_FIXUP_OFFSET_1 ||\n\t\t    !IS_ALIGNED(off, 4)) {\n\t\t\treturn NULL;\n\t\t}\n\n\t\t/* Skip non-resident records. */\n\t\tif (!is_rec_inuse(rec))\n\t\t\treturn NULL;\n\n\t\tattr = Add2Ptr(rec, off);\n\t} else {\n\t\t/* Check if input attr inside record. */\n\t\toff = PtrOffset(rec, attr);\n\t\tif (off >= used)\n\t\t\treturn NULL;\n\n\t\tasize = le32_to_cpu(attr->size);\n\t\tif (asize < SIZEOF_RESIDENT) {\n\t\t\t/* Impossible 'cause we should not return such attribute. */\n\t\t\treturn NULL;\n\t\t}\n\n\t\tif (off + asize < off) {\n\t\t\t/* overflow check */\n\t\t\treturn NULL;\n\t\t}\n\n\t\tattr = Add2Ptr(attr, asize);\n\t\toff += asize;\n\t}\n\n\tasize = le32_to_cpu(attr->size);\n\n\t/* Can we use the first field (attr->type). */\n\tif (off + 8 > used) {\n\t\tstatic_assert(ALIGN(sizeof(enum ATTR_TYPE), 8) == 8);\n\t\treturn NULL;\n\t}\n\n\tif (attr->type == ATTR_END) {\n\t\t/* End of enumeration. */\n\t\treturn NULL;\n\t}\n\n\t/* 0x100 is last known attribute for now. */\n\tt32 = le32_to_cpu(attr->type);\n\tif ((t32 & 0xf) || (t32 > 0x100))\n\t\treturn NULL;\n\n\t/* Check boundary. */\n\tif (off + asize > used)\n\t\treturn NULL;\n\n\t/* Check size of attribute. */\n\tif (!attr->non_res) {\n\t\tif (asize < SIZEOF_RESIDENT)\n\t\t\treturn NULL;\n\n\t\tt16 = le16_to_cpu(attr->res.data_off);\n\n\t\tif (t16 > asize)\n\t\t\treturn NULL;\n\n\t\tt32 = le32_to_cpu(attr->res.data_size);\n\t\tif (t16 + t32 > asize)\n\t\t\treturn NULL;\n\n\t\tif (attr->name_len &&\n\t\t    le16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len > t16) {\n\t\t\treturn NULL;\n\t\t}\n\n\t\treturn attr;\n\t}\n\n\t/* Check some nonresident fields. */\n\tif (attr->name_len &&\n\t    le16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len >\n\t\t    le16_to_cpu(attr->nres.run_off)) {\n\t\treturn NULL;\n\t}\n\n\tif (attr->nres.svcn || !is_attr_ext(attr)) {\n\t\tif (asize + 8 < SIZEOF_NONRESIDENT)\n\t\t\treturn NULL;\n\n\t\tif (attr->nres.c_unit)\n\t\t\treturn NULL;\n\t} else if (asize + 8 < SIZEOF_NONRESIDENT_EX)\n\t\treturn NULL;\n\n\treturn attr;\n}",
      "modified_lines": {
        "added": [
          "\t\tif (attr->name_len &&",
          "\t\t    le16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len > t16) {",
          "\t\t\treturn NULL;",
          "\t\t}",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of validation for resident attribute names in the code.",
      "trigger_condition": "An out-of-bounds write vulnerability occurs when processing resident attribute names due to the absence of proper bounds checking.",
      "specific_code_behavior_causing_vulnerability": "The code fails to validate resident attribute names, allowing the possibility of writing data beyond the intended boundaries, potentially leading to security vulnerabilities.",
      "id": 150,
      "code_after_change_normalized": "struct ATTRIB *FUN1(struct mft_inode *VAR1, struct ATTRIB *VAR2)\n{\nconst struct MFT_REC *VAR3 = VAR1->VAR4;\nu32 VAR5 = FUN2(VAR3->VAR5);\nu32 VAR6, VAR7, VAR8;\nu16 VAR9;\nif (!VAR2) {\nu32 VAR10 = FUN2(VAR3->VAR10);\nVAR7 = FUN3(VAR3->VAR11);\nif (VAR5 > VAR10)\nreturn NULL;\nif (VAR7 >= VAR5 || VAR7 < VAR12 ||\n!FUN4(VAR7, 4)) {\nreturn NULL;\n}\nif (!FUN5(VAR3))\nreturn NULL;\nVAR2 = FUN6(VAR3, VAR7);\n} else {\nVAR7 = FUN7(VAR3, VAR2);\nif (VAR7 >= VAR5)\nreturn NULL;\nVAR8 = FUN2(VAR2->VAR13);\nif (VAR8 < VAR14) {\nreturn NULL;\n}\nif (VAR7 + VAR8 < VAR7) {\nreturn NULL;\n}\nVAR2 = FUN6(VAR2, VAR8);\nVAR7 += VAR8;\n}\nVAR8 = FUN2(VAR2->VAR13);\nif (VAR7 + 8 > VAR5) {\nstatic_assert(FUN8(sizeof(enum VAR15), 8) == 8);\nreturn NULL;\n}\nif (VAR2->VAR16 == VAR17) {\nreturn NULL;\n}\nVAR6 = FUN2(VAR2->VAR16);\nif ((VAR6 & VAR18) || (VAR6 > VAR18))\nreturn NULL;\nif (VAR7 + VAR8 > VAR5)\nreturn NULL;\nif (!VAR2->VAR19) {\nif (VAR8 < VAR14)\nreturn NULL;\nVAR9 = FUN3(VAR2->VAR20.VAR21);\nif (VAR9 > VAR8)\nreturn NULL;\nVAR6 = FUN2(VAR2->VAR20.VAR22);\nif (VAR9 + VAR6 > VAR8)\nreturn NULL;\nif (VAR2->VAR23 &&\nFUN3(VAR2->VAR24) + sizeof(short) * VAR2->VAR23 > VAR9) {\nreturn NULL;\n}\nreturn VAR2;\n}\nif (VAR2->VAR23 &&\nFUN3(VAR2->VAR24) + sizeof(short) * VAR2->VAR23 >\nFUN3(VAR2->VAR25.VAR26)) {\nreturn NULL;\n}\nif (VAR2->VAR25.VAR27 || !FUN9(VAR2)) {\nif (VAR8 + 8 < VAR28)\nreturn NULL;\nif (VAR2->VAR25.VAR29)\nreturn NULL;\n} else if (VAR8 + 8 < VAR30)\nreturn NULL;\nreturn VAR2;\n}\n",
      "code_before_change_normalized": "struct ATTRIB *FUN1(struct mft_inode *VAR1, struct ATTRIB *VAR2)\n{\nconst struct MFT_REC *VAR3 = VAR1->VAR4;\nu32 VAR5 = FUN2(VAR3->VAR5);\nu32 VAR6, VAR7, VAR8;\nu16 VAR9;\nif (!VAR2) {\nu32 VAR10 = FUN2(VAR3->VAR10);\nVAR7 = FUN3(VAR3->VAR11);\nif (VAR5 > VAR10)\nreturn NULL;\nif (VAR7 >= VAR5 || VAR7 < VAR12 ||\n!FUN4(VAR7, 4)) {\nreturn NULL;\n}\nif (!FUN5(VAR3))\nreturn NULL;\nVAR2 = FUN6(VAR3, VAR7);\n} else {\nVAR7 = FUN7(VAR3, VAR2);\nif (VAR7 >= VAR5)\nreturn NULL;\nVAR8 = FUN2(VAR2->VAR13);\nif (VAR8 < VAR14) {\nreturn NULL;\n}\nif (VAR7 + VAR8 < VAR7) {\nreturn NULL;\n}\nVAR2 = FUN6(VAR2, VAR8);\nVAR7 += VAR8;\n}\nVAR8 = FUN2(VAR2->VAR13);\nif (VAR7 + 8 > VAR5) {\nstatic_assert(FUN8(sizeof(enum VAR15), 8) == 8);\nreturn NULL;\n}\nif (VAR2->VAR16 == VAR17) {\nreturn NULL;\n}\nVAR6 = FUN2(VAR2->VAR16);\nif ((VAR6 & VAR18) || (VAR6 > VAR18))\nreturn NULL;\nif (VAR7 + VAR8 > VAR5)\nreturn NULL;\nif (!VAR2->VAR19) {\nif (VAR8 < VAR14)\nreturn NULL;\nVAR9 = FUN3(VAR2->VAR20.VAR21);\nif (VAR9 > VAR8)\nreturn NULL;\nVAR6 = FUN2(VAR2->VAR20.VAR22);\nif (VAR9 + VAR6 > VAR8)\nreturn NULL;\nreturn VAR2;\n}\nif (VAR2->VAR23 &&\nFUN3(VAR2->VAR24) + sizeof(short) * VAR2->VAR23 >\nFUN3(VAR2->VAR25.VAR26)) {\nreturn NULL;\n}\nif (VAR2->VAR25.VAR27 || !FUN9(VAR2)) {\nif (VAR8 + 8 < VAR28)\nreturn NULL;\nif (VAR2->VAR25.VAR29)\nreturn NULL;\n} else if (VAR8 + 8 < VAR30)\nreturn NULL;\nreturn VAR2;\n}\n",
      "code_after_change_raw": "struct ATTRIB *mi_enum_attr(struct mft_inode *mi, struct ATTRIB *attr)\n{\nconst struct MFT_REC *rec = mi->mrec;\nu32 used = le32_to_cpu(rec->used);\nu32 t32, off, asize;\nu16 t16;\nif (!attr) {\nu32 total = le32_to_cpu(rec->total);\noff = le16_to_cpu(rec->attr_off);\nif (used > total)\nreturn NULL;\nif (off >= used || off < MFTRECORD_FIXUP_OFFSET_1 ||\n!IS_ALIGNED(off, 4)) {\nreturn NULL;\n}\nif (!is_rec_inuse(rec))\nreturn NULL;\nattr = Add2Ptr(rec, off);\n} else {\noff = PtrOffset(rec, attr);\nif (off >= used)\nreturn NULL;\nasize = le32_to_cpu(attr->size);\nif (asize < SIZEOF_RESIDENT) {\nreturn NULL;\n}\nif (off + asize < off) {\nreturn NULL;\n}\nattr = Add2Ptr(attr, asize);\noff += asize;\n}\nasize = le32_to_cpu(attr->size);\nif (off + 8 > used) {\nstatic_assert(ALIGN(sizeof(enum ATTR_TYPE), 8) == 8);\nreturn NULL;\n}\nif (attr->type == ATTR_END) {\nreturn NULL;\n}\nt32 = le32_to_cpu(attr->type);\nif ((t32 & 0xf) || (t32 > 0x100))\nreturn NULL;\nif (off + asize > used)\nreturn NULL;\nif (!attr->non_res) {\nif (asize < SIZEOF_RESIDENT)\nreturn NULL;\nt16 = le16_to_cpu(attr->res.data_off);\nif (t16 > asize)\nreturn NULL;\nt32 = le32_to_cpu(attr->res.data_size);\nif (t16 + t32 > asize)\nreturn NULL;\nif (attr->name_len &&\nle16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len > t16) {\nreturn NULL;\n}\nreturn attr;\n}\nif (attr->name_len &&\nle16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len >\nle16_to_cpu(attr->nres.run_off)) {\nreturn NULL;\n}\nif (attr->nres.svcn || !is_attr_ext(attr)) {\nif (asize + 8 < SIZEOF_NONRESIDENT)\nreturn NULL;\nif (attr->nres.c_unit)\nreturn NULL;\n} else if (asize + 8 < SIZEOF_NONRESIDENT_EX)\nreturn NULL;\nreturn attr;\n}\n",
      "code_before_change_raw": "struct ATTRIB *mi_enum_attr(struct mft_inode *mi, struct ATTRIB *attr)\n{\nconst struct MFT_REC *rec = mi->mrec;\nu32 used = le32_to_cpu(rec->used);\nu32 t32, off, asize;\nu16 t16;\nif (!attr) {\nu32 total = le32_to_cpu(rec->total);\noff = le16_to_cpu(rec->attr_off);\nif (used > total)\nreturn NULL;\nif (off >= used || off < MFTRECORD_FIXUP_OFFSET_1 ||\n!IS_ALIGNED(off, 4)) {\nreturn NULL;\n}\nif (!is_rec_inuse(rec))\nreturn NULL;\nattr = Add2Ptr(rec, off);\n} else {\noff = PtrOffset(rec, attr);\nif (off >= used)\nreturn NULL;\nasize = le32_to_cpu(attr->size);\nif (asize < SIZEOF_RESIDENT) {\nreturn NULL;\n}\nif (off + asize < off) {\nreturn NULL;\n}\nattr = Add2Ptr(attr, asize);\noff += asize;\n}\nasize = le32_to_cpu(attr->size);\nif (off + 8 > used) {\nstatic_assert(ALIGN(sizeof(enum ATTR_TYPE), 8) == 8);\nreturn NULL;\n}\nif (attr->type == ATTR_END) {\nreturn NULL;\n}\nt32 = le32_to_cpu(attr->type);\nif ((t32 & 0xf) || (t32 > 0x100))\nreturn NULL;\nif (off + asize > used)\nreturn NULL;\nif (!attr->non_res) {\nif (asize < SIZEOF_RESIDENT)\nreturn NULL;\nt16 = le16_to_cpu(attr->res.data_off);\nif (t16 > asize)\nreturn NULL;\nt32 = le32_to_cpu(attr->res.data_size);\nif (t16 + t32 > asize)\nreturn NULL;\nreturn attr;\n}\nif (attr->name_len &&\nle16_to_cpu(attr->name_off) + sizeof(short) * attr->name_len >\nle16_to_cpu(attr->nres.run_off)) {\nreturn NULL;\n}\nif (attr->nres.svcn || !is_attr_ext(attr)) {\nif (asize + 8 < SIZEOF_NONRESIDENT)\nreturn NULL;\nif (attr->nres.c_unit)\nreturn NULL;\n} else if (asize + 8 < SIZEOF_NONRESIDENT_EX)\nreturn NULL;\nreturn attr;\n}\n"
    }
  ],
  "CVE-2023-0210": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Insufficient validation of the NTLMv2 response length before processing it, leading to a potential buffer overflow.",
        "trigger_condition": "The NTLMv2 response length (nt_len) is not properly validated against the minimum required length (CIFS_ENCPWD_SIZE), allowing for the possibility of a buffer overflow if nt_len is less than CIFS_ENCPWD_SIZE.",
        "specific_code_behavior_causing_vulnerability": "The code fails to check that the NTLMv2 response length (nt_len) is at least CIFS_ENCPWD_SIZE before processing it, which can result in a buffer overflow if nt_len is insufficient.",
        "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure that the NTLMv2 response length (nt_len) is at least CIFS_ENCPWD_SIZE before proceeding with processing. This validation step helps prevent buffer overflows by ensuring that the NTLMv2 response length is sufficient for safe processing."
      },
      "GPT_analysis": "The above modification is necessary to address the vulnerability CVE-2023-0210 in the code snippet related to the ksmbd NTLMv2 authentication. The vulnerability arises from the following issue:\n\nIn the original code snippet, the function `ksmbd_decode_ntlmssp_auth_blob` processes NTLMv2 authentication by extracting the NTLMv2 response from the provided authentication blob. However, there is a vulnerability in the calculation of the `nt_len` (NTLMv2 response length) where it is not properly validated against the minimum required length (`CIFS_ENCPWD_SIZE`). This can lead to a buffer overflow if the `nt_len` is less than `CIFS_ENCPWD_SIZE`, potentially causing a crash or other security issues.\n\nTo address this vulnerability, the modification adds an additional check to ensure that `nt_len` is at least `CIFS_ENCPWD_SIZE`. By validating that the NTLMv2 response length is sufficient before processing it, the code prevents potential buffer overflows and improves the overall security of the authentication process.",
      "GPT_purpose": "Decode and process NTLMv2 authentication blobs for SMB connections in the ksmbd kernel module.",
      "GPT_function": "\n1. Decode NTLMSSP authentication blob.\n2. Check blob signature correctness.\n3. Process NTLMv2 authentication.\n4. Recover secondary session key if negotiated.",
      "CVE_id": "CVE-2023-0210",
      "code_before_change": "int ksmbd_decode_ntlmssp_auth_blob(struct authenticate_message *authblob,\n\t\t\t\t   int blob_len, struct ksmbd_conn *conn,\n\t\t\t\t   struct ksmbd_session *sess)\n{\n\tchar *domain_name;\n\tunsigned int nt_off, dn_off;\n\tunsigned short nt_len, dn_len;\n\tint ret;\n\n\tif (blob_len < sizeof(struct authenticate_message)) {\n\t\tksmbd_debug(AUTH, \"negotiate blob len %d too small\\n\",\n\t\t\t    blob_len);\n\t\treturn -EINVAL;\n\t}\n\n\tif (memcmp(authblob->Signature, \"NTLMSSP\", 8)) {\n\t\tksmbd_debug(AUTH, \"blob signature incorrect %s\\n\",\n\t\t\t    authblob->Signature);\n\t\treturn -EINVAL;\n\t}\n\n\tnt_off = le32_to_cpu(authblob->NtChallengeResponse.BufferOffset);\n\tnt_len = le16_to_cpu(authblob->NtChallengeResponse.Length);\n\tdn_off = le32_to_cpu(authblob->DomainName.BufferOffset);\n\tdn_len = le16_to_cpu(authblob->DomainName.Length);\n\n\tif (blob_len < (u64)dn_off + dn_len || blob_len < (u64)nt_off + nt_len)\n\t\treturn -EINVAL;\n\n\t/* TODO : use domain name that imported from configuration file */\n\tdomain_name = smb_strndup_from_utf16((const char *)authblob + dn_off,\n\t\t\t\t\t     dn_len, true, conn->local_nls);\n\tif (IS_ERR(domain_name))\n\t\treturn PTR_ERR(domain_name);\n\n\t/* process NTLMv2 authentication */\n\tksmbd_debug(AUTH, \"decode_ntlmssp_authenticate_blob dname%s\\n\",\n\t\t    domain_name);\n\tret = ksmbd_auth_ntlmv2(conn, sess,\n\t\t\t\t(struct ntlmv2_resp *)((char *)authblob + nt_off),\n\t\t\t\tnt_len - CIFS_ENCPWD_SIZE,\n\t\t\t\tdomain_name, conn->ntlmssp.cryptkey);\n\tkfree(domain_name);\n\n\t/* The recovered secondary session key */\n\tif (conn->ntlmssp.client_flags & NTLMSSP_NEGOTIATE_KEY_XCH) {\n\t\tstruct arc4_ctx *ctx_arc4;\n\t\tunsigned int sess_key_off, sess_key_len;\n\n\t\tsess_key_off = le32_to_cpu(authblob->SessionKey.BufferOffset);\n\t\tsess_key_len = le16_to_cpu(authblob->SessionKey.Length);\n\n\t\tif (blob_len < (u64)sess_key_off + sess_key_len)\n\t\t\treturn -EINVAL;\n\n\t\tctx_arc4 = kmalloc(sizeof(*ctx_arc4), GFP_KERNEL);\n\t\tif (!ctx_arc4)\n\t\t\treturn -ENOMEM;\n\n\t\tcifs_arc4_setkey(ctx_arc4, sess->sess_key,\n\t\t\t\t SMB2_NTLMV2_SESSKEY_SIZE);\n\t\tcifs_arc4_crypt(ctx_arc4, sess->sess_key,\n\t\t\t\t(char *)authblob + sess_key_off, sess_key_len);\n\t\tkfree_sensitive(ctx_arc4);\n\t}\n\n\treturn ret;\n}",
      "code_after_change": "int ksmbd_decode_ntlmssp_auth_blob(struct authenticate_message *authblob,\n\t\t\t\t   int blob_len, struct ksmbd_conn *conn,\n\t\t\t\t   struct ksmbd_session *sess)\n{\n\tchar *domain_name;\n\tunsigned int nt_off, dn_off;\n\tunsigned short nt_len, dn_len;\n\tint ret;\n\n\tif (blob_len < sizeof(struct authenticate_message)) {\n\t\tksmbd_debug(AUTH, \"negotiate blob len %d too small\\n\",\n\t\t\t    blob_len);\n\t\treturn -EINVAL;\n\t}\n\n\tif (memcmp(authblob->Signature, \"NTLMSSP\", 8)) {\n\t\tksmbd_debug(AUTH, \"blob signature incorrect %s\\n\",\n\t\t\t    authblob->Signature);\n\t\treturn -EINVAL;\n\t}\n\n\tnt_off = le32_to_cpu(authblob->NtChallengeResponse.BufferOffset);\n\tnt_len = le16_to_cpu(authblob->NtChallengeResponse.Length);\n\tdn_off = le32_to_cpu(authblob->DomainName.BufferOffset);\n\tdn_len = le16_to_cpu(authblob->DomainName.Length);\n\n\tif (blob_len < (u64)dn_off + dn_len || blob_len < (u64)nt_off + nt_len ||\n\t    nt_len < CIFS_ENCPWD_SIZE)\n\t\treturn -EINVAL;\n\n\t/* TODO : use domain name that imported from configuration file */\n\tdomain_name = smb_strndup_from_utf16((const char *)authblob + dn_off,\n\t\t\t\t\t     dn_len, true, conn->local_nls);\n\tif (IS_ERR(domain_name))\n\t\treturn PTR_ERR(domain_name);\n\n\t/* process NTLMv2 authentication */\n\tksmbd_debug(AUTH, \"decode_ntlmssp_authenticate_blob dname%s\\n\",\n\t\t    domain_name);\n\tret = ksmbd_auth_ntlmv2(conn, sess,\n\t\t\t\t(struct ntlmv2_resp *)((char *)authblob + nt_off),\n\t\t\t\tnt_len - CIFS_ENCPWD_SIZE,\n\t\t\t\tdomain_name, conn->ntlmssp.cryptkey);\n\tkfree(domain_name);\n\n\t/* The recovered secondary session key */\n\tif (conn->ntlmssp.client_flags & NTLMSSP_NEGOTIATE_KEY_XCH) {\n\t\tstruct arc4_ctx *ctx_arc4;\n\t\tunsigned int sess_key_off, sess_key_len;\n\n\t\tsess_key_off = le32_to_cpu(authblob->SessionKey.BufferOffset);\n\t\tsess_key_len = le16_to_cpu(authblob->SessionKey.Length);\n\n\t\tif (blob_len < (u64)sess_key_off + sess_key_len)\n\t\t\treturn -EINVAL;\n\n\t\tctx_arc4 = kmalloc(sizeof(*ctx_arc4), GFP_KERNEL);\n\t\tif (!ctx_arc4)\n\t\t\treturn -ENOMEM;\n\n\t\tcifs_arc4_setkey(ctx_arc4, sess->sess_key,\n\t\t\t\t SMB2_NTLMV2_SESSKEY_SIZE);\n\t\tcifs_arc4_crypt(ctx_arc4, sess->sess_key,\n\t\t\t\t(char *)authblob + sess_key_off, sess_key_len);\n\t\tkfree_sensitive(ctx_arc4);\n\t}\n\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\tif (blob_len < (u64)dn_off + dn_len || blob_len < (u64)nt_off + nt_len ||",
          "\t    nt_len < CIFS_ENCPWD_SIZE)"
        ],
        "deleted": [
          "\tif (blob_len < (u64)dn_off + dn_len || blob_len < (u64)nt_off + nt_len)"
        ]
      },
      "preconditions_for_vulnerability": "Insufficient validation of the NTLMv2 response length before processing it, leading to a potential buffer overflow.",
      "trigger_condition": "The NTLMv2 response length (nt_len) is not properly validated against the minimum required length (CIFS_ENCPWD_SIZE), allowing for the possibility of a buffer overflow if nt_len is less than CIFS_ENCPWD_SIZE.",
      "specific_code_behavior_causing_vulnerability": "The code fails to check that the NTLMv2 response length (nt_len) is at least CIFS_ENCPWD_SIZE before processing it, which can result in a buffer overflow if nt_len is insufficient.",
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check to ensure that the NTLMv2 response length (nt_len) is at least CIFS_ENCPWD_SIZE before proceeding with processing. This validation step helps prevent buffer overflows by ensuring that the NTLMv2 response length is sufficient for safe processing.",
      "id": 151,
      "code_after_change_normalized": "int FUN1(struct authenticate_message *VAR1,\nint VAR2, struct ksmbd_conn *VAR3,\nstruct ksmbd_session *VAR4)\n{\nchar *VAR5;\nunsigned int VAR6, VAR7;\nunsigned short VAR8, VAR9;\nint VAR10;\nif (VAR2 < sizeof(struct VAR11)) {\nFUN2(VAR12, \"STR\",\nVAR2);\nreturn -VAR13;\n}\nif (FUN3(VAR1->VAR14, \"STR\", 8)) {\nFUN2(VAR12, \"STR\",\nVAR1->VAR14);\nreturn -VAR13;\n}\nVAR6 = FUN4(VAR1->VAR15.VAR16);\nVAR8 = FUN5(VAR1->VAR15.VAR17);\nVAR7 = FUN4(VAR1->VAR18.VAR16);\nVAR9 = FUN5(VAR1->VAR18.VAR17);\nif (VAR2 < (VAR19)VAR7 + VAR9 || VAR2 < (VAR19)VAR6 + VAR8 ||\nVAR8 < VAR20)\nreturn -VAR13;\nVAR5 = FUN6((const char *)VAR1 + VAR7,\nVAR9, true, VAR3->VAR21);\nif (FUN7(VAR5))\nreturn FUN8(VAR5);\nFUN2(VAR12, \"STR\",\nVAR5);\nVAR10 = FUN9(VAR3, VAR4,\n(struct VAR22 *)((char *)VAR1 + VAR6),\nVAR8 - VAR20,\nVAR5, VAR3->VAR23.VAR24);\nFUN10(VAR5);\nif (VAR3->VAR23.VAR25 & VAR26) {\nstruct arc4_ctx *VAR27;\nunsigned int VAR28, VAR29;\nVAR28 = FUN4(VAR1->VAR30.VAR16);\nVAR29 = FUN5(VAR1->VAR30.VAR17);\nif (VAR2 < (VAR19)VAR28 + VAR29)\nreturn -VAR13;\nVAR27 = FUN11(sizeof(*VAR27), VAR31);\nif (!VAR27)\nreturn -VAR32;\nFUN12(VAR27, VAR4->VAR33,\nVAR34);\nFUN13(VAR27, VAR4->VAR33,\n(char *)VAR1 + VAR28, VAR29);\nFUN14(VAR27);\n}\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "int FUN1(struct authenticate_message *VAR1,\nint VAR2, struct ksmbd_conn *VAR3,\nstruct ksmbd_session *VAR4)\n{\nchar *VAR5;\nunsigned int VAR6, VAR7;\nunsigned short VAR8, VAR9;\nint VAR10;\nif (VAR2 < sizeof(struct VAR11)) {\nFUN2(VAR12, \"STR\",\nVAR2);\nreturn -VAR13;\n}\nif (FUN3(VAR1->VAR14, \"STR\", 8)) {\nFUN2(VAR12, \"STR\",\nVAR1->VAR14);\nreturn -VAR13;\n}\nVAR6 = FUN4(VAR1->VAR15.VAR16);\nVAR8 = FUN5(VAR1->VAR15.VAR17);\nVAR7 = FUN4(VAR1->VAR18.VAR16);\nVAR9 = FUN5(VAR1->VAR18.VAR17);\nif (VAR2 < (VAR19)VAR7 + VAR9 || VAR2 < (VAR19)VAR6 + VAR8)\nreturn -VAR13;\nVAR5 = FUN6((const char *)VAR1 + VAR7,\nVAR9, true, VAR3->VAR20);\nif (FUN7(VAR5))\nreturn FUN8(VAR5);\nFUN2(VAR12, \"STR\",\nVAR5);\nVAR10 = FUN9(VAR3, VAR4,\n(struct VAR21 *)((char *)VAR1 + VAR6),\nVAR8 - VAR22,\nVAR5, VAR3->VAR23.VAR24);\nFUN10(VAR5);\nif (VAR3->VAR23.VAR25 & VAR26) {\nstruct arc4_ctx *VAR27;\nunsigned int VAR28, VAR29;\nVAR28 = FUN4(VAR1->VAR30.VAR16);\nVAR29 = FUN5(VAR1->VAR30.VAR17);\nif (VAR2 < (VAR19)VAR28 + VAR29)\nreturn -VAR13;\nVAR27 = FUN11(sizeof(*VAR27), VAR31);\nif (!VAR27)\nreturn -VAR32;\nFUN12(VAR27, VAR4->VAR33,\nVAR34);\nFUN13(VAR27, VAR4->VAR33,\n(char *)VAR1 + VAR28, VAR29);\nFUN14(VAR27);\n}\nreturn VAR10;\n}\n",
      "code_after_change_raw": "int ksmbd_decode_ntlmssp_auth_blob(struct authenticate_message *authblob,\nint blob_len, struct ksmbd_conn *conn,\nstruct ksmbd_session *sess)\n{\nchar *domain_name;\nunsigned int nt_off, dn_off;\nunsigned short nt_len, dn_len;\nint ret;\nif (blob_len < sizeof(struct authenticate_message)) {\nksmbd_debug(AUTH, \"negotiate blob len %d too small\\n\",\nblob_len);\nreturn -EINVAL;\n}\nif (memcmp(authblob->Signature, \"NTLMSSP\", 8)) {\nksmbd_debug(AUTH, \"blob signature incorrect %s\\n\",\nauthblob->Signature);\nreturn -EINVAL;\n}\nnt_off = le32_to_cpu(authblob->NtChallengeResponse.BufferOffset);\nnt_len = le16_to_cpu(authblob->NtChallengeResponse.Length);\ndn_off = le32_to_cpu(authblob->DomainName.BufferOffset);\ndn_len = le16_to_cpu(authblob->DomainName.Length);\nif (blob_len < (u64)dn_off + dn_len || blob_len < (u64)nt_off + nt_len ||\nnt_len < CIFS_ENCPWD_SIZE)\nreturn -EINVAL;\ndomain_name = smb_strndup_from_utf16((const char *)authblob + dn_off,\ndn_len, true, conn->local_nls);\nif (IS_ERR(domain_name))\nreturn PTR_ERR(domain_name);\nksmbd_debug(AUTH, \"decode_ntlmssp_authenticate_blob dname%s\\n\",\ndomain_name);\nret = ksmbd_auth_ntlmv2(conn, sess,\n(struct ntlmv2_resp *)((char *)authblob + nt_off),\nnt_len - CIFS_ENCPWD_SIZE,\ndomain_name, conn->ntlmssp.cryptkey);\nkfree(domain_name);\nif (conn->ntlmssp.client_flags & NTLMSSP_NEGOTIATE_KEY_XCH) {\nstruct arc4_ctx *ctx_arc4;\nunsigned int sess_key_off, sess_key_len;\nsess_key_off = le32_to_cpu(authblob->SessionKey.BufferOffset);\nsess_key_len = le16_to_cpu(authblob->SessionKey.Length);\nif (blob_len < (u64)sess_key_off + sess_key_len)\nreturn -EINVAL;\nctx_arc4 = kmalloc(sizeof(*ctx_arc4), GFP_KERNEL);\nif (!ctx_arc4)\nreturn -ENOMEM;\ncifs_arc4_setkey(ctx_arc4, sess->sess_key,\nSMB2_NTLMV2_SESSKEY_SIZE);\ncifs_arc4_crypt(ctx_arc4, sess->sess_key,\n(char *)authblob + sess_key_off, sess_key_len);\nkfree_sensitive(ctx_arc4);\n}\nreturn ret;\n}\n",
      "code_before_change_raw": "int ksmbd_decode_ntlmssp_auth_blob(struct authenticate_message *authblob,\nint blob_len, struct ksmbd_conn *conn,\nstruct ksmbd_session *sess)\n{\nchar *domain_name;\nunsigned int nt_off, dn_off;\nunsigned short nt_len, dn_len;\nint ret;\nif (blob_len < sizeof(struct authenticate_message)) {\nksmbd_debug(AUTH, \"negotiate blob len %d too small\\n\",\nblob_len);\nreturn -EINVAL;\n}\nif (memcmp(authblob->Signature, \"NTLMSSP\", 8)) {\nksmbd_debug(AUTH, \"blob signature incorrect %s\\n\",\nauthblob->Signature);\nreturn -EINVAL;\n}\nnt_off = le32_to_cpu(authblob->NtChallengeResponse.BufferOffset);\nnt_len = le16_to_cpu(authblob->NtChallengeResponse.Length);\ndn_off = le32_to_cpu(authblob->DomainName.BufferOffset);\ndn_len = le16_to_cpu(authblob->DomainName.Length);\nif (blob_len < (u64)dn_off + dn_len || blob_len < (u64)nt_off + nt_len)\nreturn -EINVAL;\ndomain_name = smb_strndup_from_utf16((const char *)authblob + dn_off,\ndn_len, true, conn->local_nls);\nif (IS_ERR(domain_name))\nreturn PTR_ERR(domain_name);\nksmbd_debug(AUTH, \"decode_ntlmssp_authenticate_blob dname%s\\n\",\ndomain_name);\nret = ksmbd_auth_ntlmv2(conn, sess,\n(struct ntlmv2_resp *)((char *)authblob + nt_off),\nnt_len - CIFS_ENCPWD_SIZE,\ndomain_name, conn->ntlmssp.cryptkey);\nkfree(domain_name);\nif (conn->ntlmssp.client_flags & NTLMSSP_NEGOTIATE_KEY_XCH) {\nstruct arc4_ctx *ctx_arc4;\nunsigned int sess_key_off, sess_key_len;\nsess_key_off = le32_to_cpu(authblob->SessionKey.BufferOffset);\nsess_key_len = le16_to_cpu(authblob->SessionKey.Length);\nif (blob_len < (u64)sess_key_off + sess_key_len)\nreturn -EINVAL;\nctx_arc4 = kmalloc(sizeof(*ctx_arc4), GFP_KERNEL);\nif (!ctx_arc4)\nreturn -ENOMEM;\ncifs_arc4_setkey(ctx_arc4, sess->sess_key,\nSMB2_NTLMV2_SESSKEY_SIZE);\ncifs_arc4_crypt(ctx_arc4, sess->sess_key,\n(char *)authblob + sess_key_off, sess_key_len);\nkfree_sensitive(ctx_arc4);\n}\nreturn ret;\n}\n"
    }
  ],
  "CVE-2023-1073": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation and handling of linked list entries in the HID subsystem code.",
        "trigger_condition": "Insertion of a malicious USB device by a local user triggers the vulnerability, potentially leading to memory corruption.",
        "specific_code_behavior_causing_vulnerability": "The code uses the list_entry function without proper validation, which can result in memory corruption when accessing linked list entries. This behavior can be exploited by a local user inserting a malicious USB device to trigger the vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to replace the unsafe list_entry function with the safer list_first_entry_or_null function when accessing linked list entries. This change ensures that the list is handled correctly and prevents potential memory corruption issues that could be exploited by a local user. By making this modification, the code follows best practices for handling linked lists in the HID subsystem, enhancing security and stability.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-1073 in the code snippet. The vulnerability arises from the use of `list_entry` function without proper validation, which can lead to memory corruption when a malicious USB device is inserted. By replacing `list_entry` with `list_first_entry_or_null`, we ensure that the list is properly handled and prevent potential memory corruption issues that could be exploited by a local user to crash the system or escalate privileges. This modification enhances the security and stability of the code by following safer practices for handling linked lists in the Linux kernel's HID subsystem.",
      "GPT_purpose": "This function validates HID report values to ensure they are within expected ranges.",
      "GPT_function": "\n1. Validates HID report type and ID.\n2. Retrieves HID report based on type and ID.\n3. Checks for sufficient fields and values in the HID report.",
      "CVE_id": "CVE-2023-1073",
      "code_before_change": "struct hid_report *hid_validate_values(struct hid_device *hid,\n\t\t\t\t       enum hid_report_type type, unsigned int id,\n\t\t\t\t       unsigned int field_index,\n\t\t\t\t       unsigned int report_counts)\n{\n\tstruct hid_report *report;\n\n\tif (type > HID_FEATURE_REPORT) {\n\t\thid_err(hid, \"invalid HID report type %u\\n\", type);\n\t\treturn NULL;\n\t}\n\n\tif (id >= HID_MAX_IDS) {\n\t\thid_err(hid, \"invalid HID report id %u\\n\", id);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * Explicitly not using hid_get_report() here since it depends on\n\t * ->numbered being checked, which may not always be the case when\n\t * drivers go to access report values.\n\t */\n\tif (id == 0) {\n\t\t/*\n\t\t * Validating on id 0 means we should examine the first\n\t\t * report in the list.\n\t\t */\n\t\treport = list_entry(\n\t\t\t\thid->report_enum[type].report_list.next,\n\t\t\t\tstruct hid_report, list);\n\t} else {\n\t\treport = hid->report_enum[type].report_id_hash[id];\n\t}\n\tif (!report) {\n\t\thid_err(hid, \"missing %s %u\\n\", hid_report_names[type], id);\n\t\treturn NULL;\n\t}\n\tif (report->maxfield <= field_index) {\n\t\thid_err(hid, \"not enough fields in %s %u\\n\",\n\t\t\thid_report_names[type], id);\n\t\treturn NULL;\n\t}\n\tif (report->field[field_index]->report_count < report_counts) {\n\t\thid_err(hid, \"not enough values in %s %u field %u\\n\",\n\t\t\thid_report_names[type], id, field_index);\n\t\treturn NULL;\n\t}\n\treturn report;\n}",
      "code_after_change": "struct hid_report *hid_validate_values(struct hid_device *hid,\n\t\t\t\t       enum hid_report_type type, unsigned int id,\n\t\t\t\t       unsigned int field_index,\n\t\t\t\t       unsigned int report_counts)\n{\n\tstruct hid_report *report;\n\n\tif (type > HID_FEATURE_REPORT) {\n\t\thid_err(hid, \"invalid HID report type %u\\n\", type);\n\t\treturn NULL;\n\t}\n\n\tif (id >= HID_MAX_IDS) {\n\t\thid_err(hid, \"invalid HID report id %u\\n\", id);\n\t\treturn NULL;\n\t}\n\n\t/*\n\t * Explicitly not using hid_get_report() here since it depends on\n\t * ->numbered being checked, which may not always be the case when\n\t * drivers go to access report values.\n\t */\n\tif (id == 0) {\n\t\t/*\n\t\t * Validating on id 0 means we should examine the first\n\t\t * report in the list.\n\t\t */\n\t\treport = list_first_entry_or_null(\n\t\t\t\t&hid->report_enum[type].report_list,\n\t\t\t\tstruct hid_report, list);\n\t} else {\n\t\treport = hid->report_enum[type].report_id_hash[id];\n\t}\n\tif (!report) {\n\t\thid_err(hid, \"missing %s %u\\n\", hid_report_names[type], id);\n\t\treturn NULL;\n\t}\n\tif (report->maxfield <= field_index) {\n\t\thid_err(hid, \"not enough fields in %s %u\\n\",\n\t\t\thid_report_names[type], id);\n\t\treturn NULL;\n\t}\n\tif (report->field[field_index]->report_count < report_counts) {\n\t\thid_err(hid, \"not enough values in %s %u field %u\\n\",\n\t\t\thid_report_names[type], id, field_index);\n\t\treturn NULL;\n\t}\n\treturn report;\n}",
      "modified_lines": {
        "added": [
          "\t\treport = list_first_entry_or_null(",
          "\t\t\t\t&hid->report_enum[type].report_list,"
        ],
        "deleted": [
          "\t\treport = list_entry(",
          "\t\t\t\thid->report_enum[type].report_list.next,"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation and handling of linked list entries in the HID subsystem code.",
      "trigger_condition": "Insertion of a malicious USB device by a local user triggers the vulnerability, potentially leading to memory corruption.",
      "specific_code_behavior_causing_vulnerability": "The code uses the list_entry function without proper validation, which can result in memory corruption when accessing linked list entries. This behavior can be exploited by a local user inserting a malicious USB device to trigger the vulnerability.",
      "id": 152,
      "code_after_change_normalized": "struct hid_report *FUN1(struct hid_device *VAR1,\nenum hid_report_type VAR2, unsigned int VAR3,\nunsigned int VAR4,\nunsigned int VAR5)\n{\nstruct hid_report *VAR6;\nif (VAR2 > VAR7) {\nFUN2(VAR1, \"STR\", VAR2);\nreturn NULL;\n}\nif (VAR3 >= VAR8) {\nFUN2(VAR1, \"STR\", VAR3);\nreturn NULL;\n}\nif (VAR3 == 0) {\nVAR6 = FUN3(\n&VAR1->VAR9[VAR2].VAR10,\nstruct VAR11, VAR12);\n} else {\nVAR6 = VAR1->VAR9[VAR2].VAR13[VAR3];\n}\nif (!VAR6) {\nFUN2(VAR1, \"STR\", VAR14[VAR2], VAR3);\nreturn NULL;\n}\nif (VAR6->VAR15 <= VAR4) {\nFUN2(VAR1, \"STR\",\nVAR14[VAR2], VAR3);\nreturn NULL;\n}\nif (VAR6->VAR16[VAR4]->VAR17 < VAR5) {\nFUN2(VAR1, \"STR\",\nVAR14[VAR2], VAR3, VAR4);\nreturn NULL;\n}\nreturn VAR6;\n}\n",
      "code_before_change_normalized": "struct hid_report *FUN1(struct hid_device *VAR1,\nenum hid_report_type VAR2, unsigned int VAR3,\nunsigned int VAR4,\nunsigned int VAR5)\n{\nstruct hid_report *VAR6;\nif (VAR2 > VAR7) {\nFUN2(VAR1, \"STR\", VAR2);\nreturn NULL;\n}\nif (VAR3 >= VAR8) {\nFUN2(VAR1, \"STR\", VAR3);\nreturn NULL;\n}\nif (VAR3 == 0) {\nVAR6 = FUN3(\nVAR1->VAR9[VAR2].VAR10.VAR11,\nstruct VAR12, VAR13);\n} else {\nVAR6 = VAR1->VAR9[VAR2].VAR14[VAR3];\n}\nif (!VAR6) {\nFUN2(VAR1, \"STR\", VAR15[VAR2], VAR3);\nreturn NULL;\n}\nif (VAR6->VAR16 <= VAR4) {\nFUN2(VAR1, \"STR\",\nVAR15[VAR2], VAR3);\nreturn NULL;\n}\nif (VAR6->VAR17[VAR4]->VAR18 < VAR5) {\nFUN2(VAR1, \"STR\",\nVAR15[VAR2], VAR3, VAR4);\nreturn NULL;\n}\nreturn VAR6;\n}\n",
      "code_after_change_raw": "struct hid_report *hid_validate_values(struct hid_device *hid,\nenum hid_report_type type, unsigned int id,\nunsigned int field_index,\nunsigned int report_counts)\n{\nstruct hid_report *report;\nif (type > HID_FEATURE_REPORT) {\nhid_err(hid, \"invalid HID report type %u\\n\", type);\nreturn NULL;\n}\nif (id >= HID_MAX_IDS) {\nhid_err(hid, \"invalid HID report id %u\\n\", id);\nreturn NULL;\n}\nif (id == 0) {\nreport = list_first_entry_or_null(\n&hid->report_enum[type].report_list,\nstruct hid_report, list);\n} else {\nreport = hid->report_enum[type].report_id_hash[id];\n}\nif (!report) {\nhid_err(hid, \"missing %s %u\\n\", hid_report_names[type], id);\nreturn NULL;\n}\nif (report->maxfield <= field_index) {\nhid_err(hid, \"not enough fields in %s %u\\n\",\nhid_report_names[type], id);\nreturn NULL;\n}\nif (report->field[field_index]->report_count < report_counts) {\nhid_err(hid, \"not enough values in %s %u field %u\\n\",\nhid_report_names[type], id, field_index);\nreturn NULL;\n}\nreturn report;\n}\n",
      "code_before_change_raw": "struct hid_report *hid_validate_values(struct hid_device *hid,\nenum hid_report_type type, unsigned int id,\nunsigned int field_index,\nunsigned int report_counts)\n{\nstruct hid_report *report;\nif (type > HID_FEATURE_REPORT) {\nhid_err(hid, \"invalid HID report type %u\\n\", type);\nreturn NULL;\n}\nif (id >= HID_MAX_IDS) {\nhid_err(hid, \"invalid HID report id %u\\n\", id);\nreturn NULL;\n}\nif (id == 0) {\nreport = list_entry(\nhid->report_enum[type].report_list.next,\nstruct hid_report, list);\n} else {\nreport = hid->report_enum[type].report_id_hash[id];\n}\nif (!report) {\nhid_err(hid, \"missing %s %u\\n\", hid_report_names[type], id);\nreturn NULL;\n}\nif (report->maxfield <= field_index) {\nhid_err(hid, \"not enough fields in %s %u\\n\",\nhid_report_names[type], id);\nreturn NULL;\n}\nif (report->field[field_index]->report_count < report_counts) {\nhid_err(hid, \"not enough values in %s %u field %u\\n\",\nhid_report_names[type], id, field_index);\nreturn NULL;\n}\nreturn report;\n}\n"
    }
  ],
  "CVE-2023-2124": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper verification of buffer contents after skipping the replay of a buffer log item.",
        "trigger_condition": "The code does not verify the buffer contents after skipping the replay of a buffer log item, potentially leading to an out-of-bounds memory access vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code skips replaying a buffer log item without verifying the buffer contents, which can result in accessing memory out of bounds if the buffer is not properly validated."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a verification step for the buffer contents after skipping the replay of a buffer log item. This ensures that the buffer contents are validated before further processing, preventing potential out-of-bounds memory access. In this case, the solution involves adding code to verify the buffer contents if the buffer operations are available, thereby enhancing the security and stability of the system.",
      "GPT_analysis": "The above modification is necessary to address the out-of-bounds memory access vulnerability (CVE-2023-2124) in the code snippet. The vulnerability arises from a potential out-of-bounds memory access when verifying the buffer contents after skipping the replay of a buffer log item. \n\nBy adding the following lines to the code snippet:\n```c\nif (bp->b_ops) {\n    bp->b_ops->verify_read(bp);\n    error = bp->b_error;\n}\n```\nwe ensure that the buffer contents are verified before proceeding, mitigating the risk of accessing memory out of bounds and potentially crashing the system or escalating privileges. This modification helps to prevent unauthorized access to memory locations beyond the allocated buffer size, enhancing the security and stability of the system.",
      "GPT_purpose": "Recovering buffers during the second pass of XFS log recovery process.",
      "GPT_function": "\n1. Recovering buffers during the second pass of log recovery.\n2. Checking and handling cancellation of buffers.\n3. Reading and recovering buffers based on their flags.\n4. Validating and processing different types of buffers.\n5. Performing delayed writes on buffers and managing buffer cache.",
      "CVE_id": "CVE-2023-2124",
      "code_before_change": "STATIC int\nxlog_recover_buf_commit_pass2(\n\tstruct xlog\t\t\t*log,\n\tstruct list_head\t\t*buffer_list,\n\tstruct xlog_recover_item\t*item,\n\txfs_lsn_t\t\t\tcurrent_lsn)\n{\n\tstruct xfs_buf_log_format\t*buf_f = item->ri_buf[0].i_addr;\n\tstruct xfs_mount\t\t*mp = log->l_mp;\n\tstruct xfs_buf\t\t\t*bp;\n\tint\t\t\t\terror;\n\tuint\t\t\t\tbuf_flags;\n\txfs_lsn_t\t\t\tlsn;\n\n\t/*\n\t * In this pass we only want to recover all the buffers which have\n\t * not been cancelled and are not cancellation buffers themselves.\n\t */\n\tif (buf_f->blf_flags & XFS_BLF_CANCEL) {\n\t\tif (xlog_put_buffer_cancelled(log, buf_f->blf_blkno,\n\t\t\t\tbuf_f->blf_len))\n\t\t\tgoto cancelled;\n\t} else {\n\n\t\tif (xlog_is_buffer_cancelled(log, buf_f->blf_blkno,\n\t\t\t\tbuf_f->blf_len))\n\t\t\tgoto cancelled;\n\t}\n\n\ttrace_xfs_log_recover_buf_recover(log, buf_f);\n\n\tbuf_flags = 0;\n\tif (buf_f->blf_flags & XFS_BLF_INODE_BUF)\n\t\tbuf_flags |= XBF_UNMAPPED;\n\n\terror = xfs_buf_read(mp->m_ddev_targp, buf_f->blf_blkno, buf_f->blf_len,\n\t\t\t  buf_flags, &bp, NULL);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Recover the buffer only if we get an LSN from it and it's less than\n\t * the lsn of the transaction we are replaying.\n\t *\n\t * Note that we have to be extremely careful of readahead here.\n\t * Readahead does not attach verfiers to the buffers so if we don't\n\t * actually do any replay after readahead because of the LSN we found\n\t * in the buffer if more recent than that current transaction then we\n\t * need to attach the verifier directly. Failure to do so can lead to\n\t * future recovery actions (e.g. EFI and unlinked list recovery) can\n\t * operate on the buffers and they won't get the verifier attached. This\n\t * can lead to blocks on disk having the correct content but a stale\n\t * CRC.\n\t *\n\t * It is safe to assume these clean buffers are currently up to date.\n\t * If the buffer is dirtied by a later transaction being replayed, then\n\t * the verifier will be reset to match whatever recover turns that\n\t * buffer into.\n\t */\n\tlsn = xlog_recover_get_buf_lsn(mp, bp, buf_f);\n\tif (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) {\n\t\ttrace_xfs_log_recover_buf_skip(log, buf_f);\n\t\txlog_recover_validate_buf_type(mp, bp, buf_f, NULLCOMMITLSN);\n\t\tgoto out_release;\n\t}\n\n\tif (buf_f->blf_flags & XFS_BLF_INODE_BUF) {\n\t\terror = xlog_recover_do_inode_buffer(mp, item, bp, buf_f);\n\t\tif (error)\n\t\t\tgoto out_release;\n\t} else if (buf_f->blf_flags &\n\t\t  (XFS_BLF_UDQUOT_BUF|XFS_BLF_PDQUOT_BUF|XFS_BLF_GDQUOT_BUF)) {\n\t\tbool\tdirty;\n\n\t\tdirty = xlog_recover_do_dquot_buffer(mp, log, item, bp, buf_f);\n\t\tif (!dirty)\n\t\t\tgoto out_release;\n\t} else {\n\t\txlog_recover_do_reg_buffer(mp, item, bp, buf_f, current_lsn);\n\t}\n\n\t/*\n\t * Perform delayed write on the buffer.  Asynchronous writes will be\n\t * slower when taking into account all the buffers to be flushed.\n\t *\n\t * Also make sure that only inode buffers with good sizes stay in\n\t * the buffer cache.  The kernel moves inodes in buffers of 1 block\n\t * or inode_cluster_size bytes, whichever is bigger.  The inode\n\t * buffers in the log can be a different size if the log was generated\n\t * by an older kernel using unclustered inode buffers or a newer kernel\n\t * running with a different inode cluster size.  Regardless, if\n\t * the inode buffer size isn't max(blocksize, inode_cluster_size)\n\t * for *our* value of inode_cluster_size, then we need to keep\n\t * the buffer out of the buffer cache so that the buffer won't\n\t * overlap with future reads of those inodes.\n\t */\n\tif (XFS_DINODE_MAGIC ==\n\t    be16_to_cpu(*((__be16 *)xfs_buf_offset(bp, 0))) &&\n\t    (BBTOB(bp->b_length) != M_IGEO(log->l_mp)->inode_cluster_size)) {\n\t\txfs_buf_stale(bp);\n\t\terror = xfs_bwrite(bp);\n\t} else {\n\t\tASSERT(bp->b_mount == mp);\n\t\tbp->b_flags |= _XBF_LOGRECOVERY;\n\t\txfs_buf_delwri_queue(bp, buffer_list);\n\t}\n\nout_release:\n\txfs_buf_relse(bp);\n\treturn error;\ncancelled:\n\ttrace_xfs_log_recover_buf_cancel(log, buf_f);\n\treturn 0;\n}",
      "code_after_change": "STATIC int\nxlog_recover_buf_commit_pass2(\n\tstruct xlog\t\t\t*log,\n\tstruct list_head\t\t*buffer_list,\n\tstruct xlog_recover_item\t*item,\n\txfs_lsn_t\t\t\tcurrent_lsn)\n{\n\tstruct xfs_buf_log_format\t*buf_f = item->ri_buf[0].i_addr;\n\tstruct xfs_mount\t\t*mp = log->l_mp;\n\tstruct xfs_buf\t\t\t*bp;\n\tint\t\t\t\terror;\n\tuint\t\t\t\tbuf_flags;\n\txfs_lsn_t\t\t\tlsn;\n\n\t/*\n\t * In this pass we only want to recover all the buffers which have\n\t * not been cancelled and are not cancellation buffers themselves.\n\t */\n\tif (buf_f->blf_flags & XFS_BLF_CANCEL) {\n\t\tif (xlog_put_buffer_cancelled(log, buf_f->blf_blkno,\n\t\t\t\tbuf_f->blf_len))\n\t\t\tgoto cancelled;\n\t} else {\n\n\t\tif (xlog_is_buffer_cancelled(log, buf_f->blf_blkno,\n\t\t\t\tbuf_f->blf_len))\n\t\t\tgoto cancelled;\n\t}\n\n\ttrace_xfs_log_recover_buf_recover(log, buf_f);\n\n\tbuf_flags = 0;\n\tif (buf_f->blf_flags & XFS_BLF_INODE_BUF)\n\t\tbuf_flags |= XBF_UNMAPPED;\n\n\terror = xfs_buf_read(mp->m_ddev_targp, buf_f->blf_blkno, buf_f->blf_len,\n\t\t\t  buf_flags, &bp, NULL);\n\tif (error)\n\t\treturn error;\n\n\t/*\n\t * Recover the buffer only if we get an LSN from it and it's less than\n\t * the lsn of the transaction we are replaying.\n\t *\n\t * Note that we have to be extremely careful of readahead here.\n\t * Readahead does not attach verfiers to the buffers so if we don't\n\t * actually do any replay after readahead because of the LSN we found\n\t * in the buffer if more recent than that current transaction then we\n\t * need to attach the verifier directly. Failure to do so can lead to\n\t * future recovery actions (e.g. EFI and unlinked list recovery) can\n\t * operate on the buffers and they won't get the verifier attached. This\n\t * can lead to blocks on disk having the correct content but a stale\n\t * CRC.\n\t *\n\t * It is safe to assume these clean buffers are currently up to date.\n\t * If the buffer is dirtied by a later transaction being replayed, then\n\t * the verifier will be reset to match whatever recover turns that\n\t * buffer into.\n\t */\n\tlsn = xlog_recover_get_buf_lsn(mp, bp, buf_f);\n\tif (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) {\n\t\ttrace_xfs_log_recover_buf_skip(log, buf_f);\n\t\txlog_recover_validate_buf_type(mp, bp, buf_f, NULLCOMMITLSN);\n\n\t\t/*\n\t\t * We're skipping replay of this buffer log item due to the log\n\t\t * item LSN being behind the ondisk buffer.  Verify the buffer\n\t\t * contents since we aren't going to run the write verifier.\n\t\t */\n\t\tif (bp->b_ops) {\n\t\t\tbp->b_ops->verify_read(bp);\n\t\t\terror = bp->b_error;\n\t\t}\n\t\tgoto out_release;\n\t}\n\n\tif (buf_f->blf_flags & XFS_BLF_INODE_BUF) {\n\t\terror = xlog_recover_do_inode_buffer(mp, item, bp, buf_f);\n\t\tif (error)\n\t\t\tgoto out_release;\n\t} else if (buf_f->blf_flags &\n\t\t  (XFS_BLF_UDQUOT_BUF|XFS_BLF_PDQUOT_BUF|XFS_BLF_GDQUOT_BUF)) {\n\t\tbool\tdirty;\n\n\t\tdirty = xlog_recover_do_dquot_buffer(mp, log, item, bp, buf_f);\n\t\tif (!dirty)\n\t\t\tgoto out_release;\n\t} else {\n\t\txlog_recover_do_reg_buffer(mp, item, bp, buf_f, current_lsn);\n\t}\n\n\t/*\n\t * Perform delayed write on the buffer.  Asynchronous writes will be\n\t * slower when taking into account all the buffers to be flushed.\n\t *\n\t * Also make sure that only inode buffers with good sizes stay in\n\t * the buffer cache.  The kernel moves inodes in buffers of 1 block\n\t * or inode_cluster_size bytes, whichever is bigger.  The inode\n\t * buffers in the log can be a different size if the log was generated\n\t * by an older kernel using unclustered inode buffers or a newer kernel\n\t * running with a different inode cluster size.  Regardless, if\n\t * the inode buffer size isn't max(blocksize, inode_cluster_size)\n\t * for *our* value of inode_cluster_size, then we need to keep\n\t * the buffer out of the buffer cache so that the buffer won't\n\t * overlap with future reads of those inodes.\n\t */\n\tif (XFS_DINODE_MAGIC ==\n\t    be16_to_cpu(*((__be16 *)xfs_buf_offset(bp, 0))) &&\n\t    (BBTOB(bp->b_length) != M_IGEO(log->l_mp)->inode_cluster_size)) {\n\t\txfs_buf_stale(bp);\n\t\terror = xfs_bwrite(bp);\n\t} else {\n\t\tASSERT(bp->b_mount == mp);\n\t\tbp->b_flags |= _XBF_LOGRECOVERY;\n\t\txfs_buf_delwri_queue(bp, buffer_list);\n\t}\n\nout_release:\n\txfs_buf_relse(bp);\n\treturn error;\ncancelled:\n\ttrace_xfs_log_recover_buf_cancel(log, buf_f);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "",
          "\t\t/*",
          "\t\t * We're skipping replay of this buffer log item due to the log",
          "\t\t * item LSN being behind the ondisk buffer.  Verify the buffer",
          "\t\t * contents since we aren't going to run the write verifier.",
          "\t\t */",
          "\t\tif (bp->b_ops) {",
          "\t\t\tbp->b_ops->verify_read(bp);",
          "\t\t\terror = bp->b_error;",
          "\t\t}"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper verification of buffer contents after skipping the replay of a buffer log item.",
      "trigger_condition": "The code does not verify the buffer contents after skipping the replay of a buffer log item, potentially leading to an out-of-bounds memory access vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code skips replaying a buffer log item without verifying the buffer contents, which can result in accessing memory out of bounds if the buffer is not properly validated.",
      "id": 153,
      "code_after_change_normalized": "STATIC int\nFUN1(\nstruct xlog\t\t\t*VAR1,\nstruct list_head\t\t*VAR2,\nstruct xlog_recover_item\t*VAR3,\nxfs_lsn_t\t\t\tVAR4)\n{\nstruct xfs_buf_log_format\t*VAR5 = VAR3->VAR6[0].VAR7;\nstruct xfs_mount\t\t*VAR8 = VAR1->VAR9;\nstruct xfs_buf\t\t\t*VAR10;\nint\t\t\t\tVAR11;\nuint\t\t\t\tVAR12;\nxfs_lsn_t\t\t\tVAR13;\nif (VAR5->VAR14 & VAR15) {\nif (FUN2(VAR1, VAR5->VAR16,\nVAR5->VAR17))\ngoto VAR18;\n} else {\nif (FUN3(VAR1, VAR5->VAR16,\nVAR5->VAR17))\ngoto VAR18;\n}\nFUN4(VAR1, VAR5);\nVAR12 = 0;\nif (VAR5->VAR14 & VAR19)\nVAR12 |= VAR20;\nVAR11 = FUN5(VAR8->VAR21, VAR5->VAR16, VAR5->VAR17,\nVAR12, &VAR10, NULL);\nif (VAR11)\nreturn VAR11;\nVAR13 = FUN6(VAR8, VAR10, VAR5);\nif (VAR13 && VAR13 != -1 && FUN7(VAR13, VAR4) >= 0) {\nFUN8(VAR1, VAR5);\nFUN9(VAR8, VAR10, VAR5, VAR22);\nif (VAR10->VAR23) {\nVAR10->VAR23->FUN10(VAR10);\nVAR11 = VAR10->VAR24;\n}\ngoto VAR25;\n}\nif (VAR5->VAR14 & VAR19) {\nVAR11 = FUN11(VAR8, VAR3, VAR10, VAR5);\nif (VAR11)\ngoto VAR25;\n} else if (VAR5->VAR14 &\n(VAR26|VAR27|VAR28)) {\nbool\tVAR29;\nVAR29 = FUN12(VAR8, VAR1, VAR3, VAR10, VAR5);\nif (!VAR29)\ngoto VAR25;\n} else {\nFUN13(VAR8, VAR3, VAR10, VAR5, VAR4);\n}\nif (VAR30 ==\nFUN14(*((VAR31 *)FUN15(VAR10, 0))) &&\n(FUN16(VAR10->VAR32) != FUN17(VAR1->VAR9)->VAR33)) {\nFUN18(VAR10);\nVAR11 = FUN19(VAR10);\n} else {\nFUN20(VAR10->VAR34 == VAR8);\nVAR10->VAR35 |= VAR36;\nFUN21(VAR10, VAR2);\n}\nVAR25:\nFUN22(VAR10);\nreturn VAR11;\nVAR18:\nFUN23(VAR1, VAR5);\nreturn 0;\n}\n",
      "code_before_change_normalized": "STATIC int\nFUN1(\nstruct xlog\t\t\t*VAR1,\nstruct list_head\t\t*VAR2,\nstruct xlog_recover_item\t*VAR3,\nxfs_lsn_t\t\t\tVAR4)\n{\nstruct xfs_buf_log_format\t*VAR5 = VAR3->VAR6[0].VAR7;\nstruct xfs_mount\t\t*VAR8 = VAR1->VAR9;\nstruct xfs_buf\t\t\t*VAR10;\nint\t\t\t\tVAR11;\nuint\t\t\t\tVAR12;\nxfs_lsn_t\t\t\tVAR13;\nif (VAR5->VAR14 & VAR15) {\nif (FUN2(VAR1, VAR5->VAR16,\nVAR5->VAR17))\ngoto VAR18;\n} else {\nif (FUN3(VAR1, VAR5->VAR16,\nVAR5->VAR17))\ngoto VAR18;\n}\nFUN4(VAR1, VAR5);\nVAR12 = 0;\nif (VAR5->VAR14 & VAR19)\nVAR12 |= VAR20;\nVAR11 = FUN5(VAR8->VAR21, VAR5->VAR16, VAR5->VAR17,\nVAR12, &VAR10, NULL);\nif (VAR11)\nreturn VAR11;\nVAR13 = FUN6(VAR8, VAR10, VAR5);\nif (VAR13 && VAR13 != -1 && FUN7(VAR13, VAR4) >= 0) {\nFUN8(VAR1, VAR5);\nFUN9(VAR8, VAR10, VAR5, VAR22);\ngoto VAR23;\n}\nif (VAR5->VAR14 & VAR19) {\nVAR11 = FUN10(VAR8, VAR3, VAR10, VAR5);\nif (VAR11)\ngoto VAR23;\n} else if (VAR5->VAR14 &\n(VAR24|VAR25|VAR26)) {\nbool\tVAR27;\nVAR27 = FUN11(VAR8, VAR1, VAR3, VAR10, VAR5);\nif (!VAR27)\ngoto VAR23;\n} else {\nFUN12(VAR8, VAR3, VAR10, VAR5, VAR4);\n}\nif (VAR28 ==\nFUN13(*((VAR29 *)FUN14(VAR10, 0))) &&\n(FUN15(VAR10->VAR30) != FUN16(VAR1->VAR9)->VAR31)) {\nFUN17(VAR10);\nVAR11 = FUN18(VAR10);\n} else {\nFUN19(VAR10->VAR32 == VAR8);\nVAR10->VAR33 |= VAR34;\nFUN20(VAR10, VAR2);\n}\nVAR23:\nFUN21(VAR10);\nreturn VAR11;\nVAR18:\nFUN22(VAR1, VAR5);\nreturn 0;\n}\n",
      "code_after_change_raw": "STATIC int\nxlog_recover_buf_commit_pass2(\nstruct xlog\t\t\t*log,\nstruct list_head\t\t*buffer_list,\nstruct xlog_recover_item\t*item,\nxfs_lsn_t\t\t\tcurrent_lsn)\n{\nstruct xfs_buf_log_format\t*buf_f = item->ri_buf[0].i_addr;\nstruct xfs_mount\t\t*mp = log->l_mp;\nstruct xfs_buf\t\t\t*bp;\nint\t\t\t\terror;\nuint\t\t\t\tbuf_flags;\nxfs_lsn_t\t\t\tlsn;\nif (buf_f->blf_flags & XFS_BLF_CANCEL) {\nif (xlog_put_buffer_cancelled(log, buf_f->blf_blkno,\nbuf_f->blf_len))\ngoto cancelled;\n} else {\nif (xlog_is_buffer_cancelled(log, buf_f->blf_blkno,\nbuf_f->blf_len))\ngoto cancelled;\n}\ntrace_xfs_log_recover_buf_recover(log, buf_f);\nbuf_flags = 0;\nif (buf_f->blf_flags & XFS_BLF_INODE_BUF)\nbuf_flags |= XBF_UNMAPPED;\nerror = xfs_buf_read(mp->m_ddev_targp, buf_f->blf_blkno, buf_f->blf_len,\nbuf_flags, &bp, NULL);\nif (error)\nreturn error;\nlsn = xlog_recover_get_buf_lsn(mp, bp, buf_f);\nif (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) {\ntrace_xfs_log_recover_buf_skip(log, buf_f);\nxlog_recover_validate_buf_type(mp, bp, buf_f, NULLCOMMITLSN);\nif (bp->b_ops) {\nbp->b_ops->verify_read(bp);\nerror = bp->b_error;\n}\ngoto out_release;\n}\nif (buf_f->blf_flags & XFS_BLF_INODE_BUF) {\nerror = xlog_recover_do_inode_buffer(mp, item, bp, buf_f);\nif (error)\ngoto out_release;\n} else if (buf_f->blf_flags &\n(XFS_BLF_UDQUOT_BUF|XFS_BLF_PDQUOT_BUF|XFS_BLF_GDQUOT_BUF)) {\nbool\tdirty;\ndirty = xlog_recover_do_dquot_buffer(mp, log, item, bp, buf_f);\nif (!dirty)\ngoto out_release;\n} else {\nxlog_recover_do_reg_buffer(mp, item, bp, buf_f, current_lsn);\n}\nif (XFS_DINODE_MAGIC ==\nbe16_to_cpu(*((__be16 *)xfs_buf_offset(bp, 0))) &&\n(BBTOB(bp->b_length) != M_IGEO(log->l_mp)->inode_cluster_size)) {\nxfs_buf_stale(bp);\nerror = xfs_bwrite(bp);\n} else {\nASSERT(bp->b_mount == mp);\nbp->b_flags |= _XBF_LOGRECOVERY;\nxfs_buf_delwri_queue(bp, buffer_list);\n}\nout_release:\nxfs_buf_relse(bp);\nreturn error;\ncancelled:\ntrace_xfs_log_recover_buf_cancel(log, buf_f);\nreturn 0;\n}\n",
      "code_before_change_raw": "STATIC int\nxlog_recover_buf_commit_pass2(\nstruct xlog\t\t\t*log,\nstruct list_head\t\t*buffer_list,\nstruct xlog_recover_item\t*item,\nxfs_lsn_t\t\t\tcurrent_lsn)\n{\nstruct xfs_buf_log_format\t*buf_f = item->ri_buf[0].i_addr;\nstruct xfs_mount\t\t*mp = log->l_mp;\nstruct xfs_buf\t\t\t*bp;\nint\t\t\t\terror;\nuint\t\t\t\tbuf_flags;\nxfs_lsn_t\t\t\tlsn;\nif (buf_f->blf_flags & XFS_BLF_CANCEL) {\nif (xlog_put_buffer_cancelled(log, buf_f->blf_blkno,\nbuf_f->blf_len))\ngoto cancelled;\n} else {\nif (xlog_is_buffer_cancelled(log, buf_f->blf_blkno,\nbuf_f->blf_len))\ngoto cancelled;\n}\ntrace_xfs_log_recover_buf_recover(log, buf_f);\nbuf_flags = 0;\nif (buf_f->blf_flags & XFS_BLF_INODE_BUF)\nbuf_flags |= XBF_UNMAPPED;\nerror = xfs_buf_read(mp->m_ddev_targp, buf_f->blf_blkno, buf_f->blf_len,\nbuf_flags, &bp, NULL);\nif (error)\nreturn error;\nlsn = xlog_recover_get_buf_lsn(mp, bp, buf_f);\nif (lsn && lsn != -1 && XFS_LSN_CMP(lsn, current_lsn) >= 0) {\ntrace_xfs_log_recover_buf_skip(log, buf_f);\nxlog_recover_validate_buf_type(mp, bp, buf_f, NULLCOMMITLSN);\ngoto out_release;\n}\nif (buf_f->blf_flags & XFS_BLF_INODE_BUF) {\nerror = xlog_recover_do_inode_buffer(mp, item, bp, buf_f);\nif (error)\ngoto out_release;\n} else if (buf_f->blf_flags &\n(XFS_BLF_UDQUOT_BUF|XFS_BLF_PDQUOT_BUF|XFS_BLF_GDQUOT_BUF)) {\nbool\tdirty;\ndirty = xlog_recover_do_dquot_buffer(mp, log, item, bp, buf_f);\nif (!dirty)\ngoto out_release;\n} else {\nxlog_recover_do_reg_buffer(mp, item, bp, buf_f, current_lsn);\n}\nif (XFS_DINODE_MAGIC ==\nbe16_to_cpu(*((__be16 *)xfs_buf_offset(bp, 0))) &&\n(BBTOB(bp->b_length) != M_IGEO(log->l_mp)->inode_cluster_size)) {\nxfs_buf_stale(bp);\nerror = xfs_bwrite(bp);\n} else {\nASSERT(bp->b_mount == mp);\nbp->b_flags |= _XBF_LOGRECOVERY;\nxfs_buf_delwri_queue(bp, buffer_list);\n}\nout_release:\nxfs_buf_relse(bp);\nreturn error;\ncancelled:\ntrace_xfs_log_recover_buf_cancel(log, buf_f);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2023-21255": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper handling for asynchronous transactions in the buffer release process.",
        "trigger_condition": "Execution of buffer release function without ensuring the completion of asynchronous transactions, leading to a use-after-free vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check for the completion of asynchronous transactions associated with the buffer before releasing it, potentially resulting in a use-after-free scenario if the buffer is accessed after being released.",
        "solution": "To mitigate the vulnerability, it is necessary to ensure that all asynchronous transactions related to the buffer are completed before releasing it. This can be achieved by modifying the code to properly handle the completion of asynchronous transactions before proceeding with the buffer release process. In this case, the solution involves verifying the completion of asynchronous transactions and updating the buffer release logic accordingly."
      },
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-21255, which involves a possible memory corruption due to a use after free issue in the code snippet provided. By making the modification as suggested, the vulnerable code is replaced with a safer alternative that prevents the use after free scenario, thereby mitigating the risk of local escalation of privilege without requiring additional execution privileges. This modification ensures that the memory handling is done correctly and helps prevent potential security vulnerabilities in the code.",
      "GPT_purpose": "Freeing a binder buffer and handling related transactions in the binder driver.",
      "GPT_function": "\n1. Freeing a binder buffer within a process.\n2. Handling async transactions and target nodes.\n3. Releasing the transaction buffer and freeing allocated memory.",
      "CVE_id": "CVE-2023-21255",
      "code_before_change": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
      "code_after_change": "static void\nbinder_free_buf(struct binder_proc *proc,\n\t\tstruct binder_thread *thread,\n\t\tstruct binder_buffer *buffer, bool is_failure)\n{\n\tbinder_inner_proc_lock(proc);\n\tif (buffer->transaction) {\n\t\tbuffer->transaction->buffer = NULL;\n\t\tbuffer->transaction = NULL;\n\t}\n\tbinder_inner_proc_unlock(proc);\n\tif (buffer->async_transaction && buffer->target_node) {\n\t\tstruct binder_node *buf_node;\n\t\tstruct binder_work *w;\n\n\t\tbuf_node = buffer->target_node;\n\t\tbinder_node_inner_lock(buf_node);\n\t\tBUG_ON(!buf_node->has_async_transaction);\n\t\tBUG_ON(buf_node->proc != proc);\n\t\tw = binder_dequeue_work_head_ilocked(\n\t\t\t\t&buf_node->async_todo);\n\t\tif (!w) {\n\t\t\tbuf_node->has_async_transaction = false;\n\t\t} else {\n\t\t\tbinder_enqueue_work_ilocked(\n\t\t\t\t\tw, &proc->todo);\n\t\t\tbinder_wakeup_proc_ilocked(proc);\n\t\t}\n\t\tbinder_node_inner_unlock(buf_node);\n\t}\n\ttrace_binder_transaction_buffer_release(buffer);\n\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);\n\tbinder_alloc_free_buf(&proc->alloc, buffer);\n}",
      "modified_lines": {
        "added": [
          "\tbinder_release_entire_buffer(proc, thread, buffer, is_failure);"
        ],
        "deleted": [
          "\tbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper handling for asynchronous transactions in the buffer release process.",
      "trigger_condition": "Execution of buffer release function without ensuring the completion of asynchronous transactions, leading to a use-after-free vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check for the completion of asynchronous transactions associated with the buffer before releasing it, potentially resulting in a use-after-free scenario if the buffer is accessed after being released.",
      "solution": "To mitigate the vulnerability, it is necessary to ensure that all asynchronous transactions related to the buffer are completed before releasing it. This can be achieved by modifying the code to properly handle the completion of asynchronous transactions before proceeding with the buffer release process. In this case, the solution involves verifying the completion of asynchronous transactions and updating the buffer release logic accordingly.",
      "id": 154,
      "code_after_change_normalized": "static void\nFUN1(struct binder_proc *VAR1,\nstruct binder_thread *VAR2,\nstruct binder_buffer *VAR3, bool VAR4)\n{\nFUN2(VAR1);\nif (VAR3->VAR5) {\nVAR3->VAR5->VAR3 = NULL;\nVAR3->VAR5 = NULL;\n}\nFUN3(VAR1);\nif (VAR3->VAR6 && VAR3->VAR7) {\nstruct binder_node *VAR8;\nstruct binder_work *VAR9;\nVAR8 = VAR3->VAR7;\nFUN4(VAR8);\nFUN5(!VAR8->VAR10);\nFUN5(VAR8->VAR1 != VAR1);\nVAR9 = FUN6(\n&VAR8->VAR11);\nif (!VAR9) {\nVAR8->VAR10 = false;\n} else {\nFUN7(\nVAR9, &VAR1->VAR12);\nFUN8(VAR1);\n}\nFUN9(VAR8);\n}\nFUN10(VAR3);\nFUN11(VAR1, VAR2, VAR3, VAR4);\nFUN12(&VAR1->VAR13, VAR3);\n}\n",
      "code_before_change_normalized": "static void\nFUN1(struct binder_proc *VAR1,\nstruct binder_thread *VAR2,\nstruct binder_buffer *VAR3, bool VAR4)\n{\nFUN2(VAR1);\nif (VAR3->VAR5) {\nVAR3->VAR5->VAR3 = NULL;\nVAR3->VAR5 = NULL;\n}\nFUN3(VAR1);\nif (VAR3->VAR6 && VAR3->VAR7) {\nstruct binder_node *VAR8;\nstruct binder_work *VAR9;\nVAR8 = VAR3->VAR7;\nFUN4(VAR8);\nFUN5(!VAR8->VAR10);\nFUN5(VAR8->VAR1 != VAR1);\nVAR9 = FUN6(\n&VAR8->VAR11);\nif (!VAR9) {\nVAR8->VAR10 = false;\n} else {\nFUN7(\nVAR9, &VAR1->VAR12);\nFUN8(VAR1);\n}\nFUN9(VAR8);\n}\nFUN10(VAR3);\nFUN11(VAR1, VAR2, VAR3, 0, VAR4);\nFUN12(&VAR1->VAR13, VAR3);\n}\n",
      "code_after_change_raw": "static void\nbinder_free_buf(struct binder_proc *proc,\nstruct binder_thread *thread,\nstruct binder_buffer *buffer, bool is_failure)\n{\nbinder_inner_proc_lock(proc);\nif (buffer->transaction) {\nbuffer->transaction->buffer = NULL;\nbuffer->transaction = NULL;\n}\nbinder_inner_proc_unlock(proc);\nif (buffer->async_transaction && buffer->target_node) {\nstruct binder_node *buf_node;\nstruct binder_work *w;\nbuf_node = buffer->target_node;\nbinder_node_inner_lock(buf_node);\nBUG_ON(!buf_node->has_async_transaction);\nBUG_ON(buf_node->proc != proc);\nw = binder_dequeue_work_head_ilocked(\n&buf_node->async_todo);\nif (!w) {\nbuf_node->has_async_transaction = false;\n} else {\nbinder_enqueue_work_ilocked(\nw, &proc->todo);\nbinder_wakeup_proc_ilocked(proc);\n}\nbinder_node_inner_unlock(buf_node);\n}\ntrace_binder_transaction_buffer_release(buffer);\nbinder_release_entire_buffer(proc, thread, buffer, is_failure);\nbinder_alloc_free_buf(&proc->alloc, buffer);\n}\n",
      "code_before_change_raw": "static void\nbinder_free_buf(struct binder_proc *proc,\nstruct binder_thread *thread,\nstruct binder_buffer *buffer, bool is_failure)\n{\nbinder_inner_proc_lock(proc);\nif (buffer->transaction) {\nbuffer->transaction->buffer = NULL;\nbuffer->transaction = NULL;\n}\nbinder_inner_proc_unlock(proc);\nif (buffer->async_transaction && buffer->target_node) {\nstruct binder_node *buf_node;\nstruct binder_work *w;\nbuf_node = buffer->target_node;\nbinder_node_inner_lock(buf_node);\nBUG_ON(!buf_node->has_async_transaction);\nBUG_ON(buf_node->proc != proc);\nw = binder_dequeue_work_head_ilocked(\n&buf_node->async_todo);\nif (!w) {\nbuf_node->has_async_transaction = false;\n} else {\nbinder_enqueue_work_ilocked(\nw, &proc->todo);\nbinder_wakeup_proc_ilocked(proc);\n}\nbinder_node_inner_unlock(buf_node);\n}\ntrace_binder_transaction_buffer_release(buffer);\nbinder_transaction_buffer_release(proc, thread, buffer, 0, is_failure);\nbinder_alloc_free_buf(&proc->alloc, buffer);\n}\n"
    }
  ],
  "CVE-2023-2194": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation on the size of a variable used in a memcpy operation, allowing it to potentially exceed the bounds of the destination buffer.",
        "trigger_condition": "A local privileged user provides a value for the variable that is not within the expected range, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the size of the variable used in the memcpy operation is within a valid range, potentially allowing it to write beyond the end of the destination buffer."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the variable used in the memcpy operation does not exceed a certain maximum value (in this case, I2C_SMBUS_BLOCK_MAX). By validating the size of the variable before performing the memcpy operation, the code prevents potential out-of-bounds writes and enhances the security of the system.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-2194, which involves an out-of-bounds write vulnerability in the Linux kernel's SLIMpro I2C device driver. By adding the check `if (writelen > I2C_SMBUS_BLOCK_MAX) return -EINVAL;`, we ensure that the `writelen` variable is within a valid range (0-255) before proceeding with the `memcpy` operation. This check prevents potential out-of-bounds writes that could lead to system crashes or code execution by limiting the size of the copy operation to a safe range.",
      "GPT_purpose": "Write data to a SLIMpro I2C device using DMA for data transfer.",
      "GPT_function": "\n1. Copy data to dma_buffer using memcpy.\n2. Map dma_buffer to a physical address for DMA transfer.\n3. Encode message parameters and send the message using slimpro_i2c_send_msg.\n4. Unmap the dma_buffer after DMA transfer.",
      "CVE_id": "CVE-2023-2194",
      "code_before_change": "static int slimpro_i2c_blkwr(struct slimpro_i2c_dev *ctx, u32 chip,\n\t\t\t     u32 addr, u32 addrlen, u32 protocol, u32 writelen,\n\t\t\t     void *data)\n{\n\tdma_addr_t paddr;\n\tu32 msg[3];\n\tint rc;\n\n\tmemcpy(ctx->dma_buffer, data, writelen);\n\tpaddr = dma_map_single(ctx->dev, ctx->dma_buffer, writelen,\n\t\t\t       DMA_TO_DEVICE);\n\tif (dma_mapping_error(ctx->dev, paddr)) {\n\t\tdev_err(&ctx->adapter.dev, \"Error in mapping dma buffer %p\\n\",\n\t\t\tctx->dma_buffer);\n\t\treturn -ENOMEM;\n\t}\n\n\tmsg[0] = SLIMPRO_IIC_ENCODE_MSG(SLIMPRO_IIC_BUS, chip, SLIMPRO_IIC_WRITE,\n\t\t\t\t\tprotocol, addrlen, writelen);\n\tmsg[1] = SLIMPRO_IIC_ENCODE_FLAG_BUFADDR |\n\t\t SLIMPRO_IIC_ENCODE_UPPER_BUFADDR(paddr) |\n\t\t SLIMPRO_IIC_ENCODE_ADDR(addr);\n\tmsg[2] = (u32)paddr;\n\n\tif (ctx->mbox_client.tx_block)\n\t\treinit_completion(&ctx->rd_complete);\n\n\trc = slimpro_i2c_send_msg(ctx, msg, msg);\n\n\tdma_unmap_single(ctx->dev, paddr, writelen, DMA_TO_DEVICE);\n\treturn rc;\n}",
      "code_after_change": "static int slimpro_i2c_blkwr(struct slimpro_i2c_dev *ctx, u32 chip,\n\t\t\t     u32 addr, u32 addrlen, u32 protocol, u32 writelen,\n\t\t\t     void *data)\n{\n\tdma_addr_t paddr;\n\tu32 msg[3];\n\tint rc;\n\n\tif (writelen > I2C_SMBUS_BLOCK_MAX)\n\t\treturn -EINVAL;\n\n\tmemcpy(ctx->dma_buffer, data, writelen);\n\tpaddr = dma_map_single(ctx->dev, ctx->dma_buffer, writelen,\n\t\t\t       DMA_TO_DEVICE);\n\tif (dma_mapping_error(ctx->dev, paddr)) {\n\t\tdev_err(&ctx->adapter.dev, \"Error in mapping dma buffer %p\\n\",\n\t\t\tctx->dma_buffer);\n\t\treturn -ENOMEM;\n\t}\n\n\tmsg[0] = SLIMPRO_IIC_ENCODE_MSG(SLIMPRO_IIC_BUS, chip, SLIMPRO_IIC_WRITE,\n\t\t\t\t\tprotocol, addrlen, writelen);\n\tmsg[1] = SLIMPRO_IIC_ENCODE_FLAG_BUFADDR |\n\t\t SLIMPRO_IIC_ENCODE_UPPER_BUFADDR(paddr) |\n\t\t SLIMPRO_IIC_ENCODE_ADDR(addr);\n\tmsg[2] = (u32)paddr;\n\n\tif (ctx->mbox_client.tx_block)\n\t\treinit_completion(&ctx->rd_complete);\n\n\trc = slimpro_i2c_send_msg(ctx, msg, msg);\n\n\tdma_unmap_single(ctx->dev, paddr, writelen, DMA_TO_DEVICE);\n\treturn rc;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (writelen > I2C_SMBUS_BLOCK_MAX)",
          "\t\treturn -EINVAL;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of validation on the size of a variable used in a memcpy operation, allowing it to potentially exceed the bounds of the destination buffer.",
      "trigger_condition": "A local privileged user provides a value for the variable that is not within the expected range, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the size of the variable used in the memcpy operation is within a valid range, potentially allowing it to write beyond the end of the destination buffer.",
      "id": 155,
      "code_after_change_normalized": "static int FUN1(struct slimpro_i2c_dev *VAR1, u32 VAR2,\nu32 VAR3, u32 VAR4, u32 VAR5, u32 VAR6,\nvoid *VAR7)\n{\ndma_addr_t VAR8;\nu32 VAR9[3];\nint VAR10;\nif (VAR6 > VAR11)\nreturn -VAR12;\nFUN2(VAR1->VAR13, VAR7, VAR6);\nVAR8 = FUN3(VAR1->VAR14, VAR1->VAR13, VAR6,\nVAR15);\nif (FUN4(VAR1->VAR14, VAR8)) {\nFUN5(&VAR1->VAR16.VAR14, \"STR\",\nVAR1->VAR13);\nreturn -VAR17;\n}\nVAR9[0] = FUN6(VAR18, VAR2, VAR19,\nVAR5, VAR4, VAR6);\nVAR9[1] = VAR20 |\nFUN7(VAR8) |\nFUN8(VAR3);\nVAR9[2] = (VAR21)VAR8;\nif (VAR1->VAR22.VAR23)\nFUN9(&VAR1->VAR24);\nVAR10 = FUN10(VAR1, VAR9, VAR9);\nFUN11(VAR1->VAR14, VAR8, VAR6, VAR15);\nreturn VAR10;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct slimpro_i2c_dev *VAR1, u32 VAR2,\nu32 VAR3, u32 VAR4, u32 VAR5, u32 VAR6,\nvoid *VAR7)\n{\ndma_addr_t VAR8;\nu32 VAR9[3];\nint VAR10;\nFUN2(VAR1->VAR11, VAR7, VAR6);\nVAR8 = FUN3(VAR1->VAR12, VAR1->VAR11, VAR6,\nVAR13);\nif (FUN4(VAR1->VAR12, VAR8)) {\nFUN5(&VAR1->VAR14.VAR12, \"STR\",\nVAR1->VAR11);\nreturn -VAR15;\n}\nVAR9[0] = FUN6(VAR16, VAR2, VAR17,\nVAR5, VAR4, VAR6);\nVAR9[1] = VAR18 |\nFUN7(VAR8) |\nFUN8(VAR3);\nVAR9[2] = (VAR19)VAR8;\nif (VAR1->VAR20.VAR21)\nFUN9(&VAR1->VAR22);\nVAR10 = FUN10(VAR1, VAR9, VAR9);\nFUN11(VAR1->VAR12, VAR8, VAR6, VAR13);\nreturn VAR10;\n}\n",
      "code_after_change_raw": "static int slimpro_i2c_blkwr(struct slimpro_i2c_dev *ctx, u32 chip,\nu32 addr, u32 addrlen, u32 protocol, u32 writelen,\nvoid *data)\n{\ndma_addr_t paddr;\nu32 msg[3];\nint rc;\nif (writelen > I2C_SMBUS_BLOCK_MAX)\nreturn -EINVAL;\nmemcpy(ctx->dma_buffer, data, writelen);\npaddr = dma_map_single(ctx->dev, ctx->dma_buffer, writelen,\nDMA_TO_DEVICE);\nif (dma_mapping_error(ctx->dev, paddr)) {\ndev_err(&ctx->adapter.dev, \"Error in mapping dma buffer %p\\n\",\nctx->dma_buffer);\nreturn -ENOMEM;\n}\nmsg[0] = SLIMPRO_IIC_ENCODE_MSG(SLIMPRO_IIC_BUS, chip, SLIMPRO_IIC_WRITE,\nprotocol, addrlen, writelen);\nmsg[1] = SLIMPRO_IIC_ENCODE_FLAG_BUFADDR |\nSLIMPRO_IIC_ENCODE_UPPER_BUFADDR(paddr) |\nSLIMPRO_IIC_ENCODE_ADDR(addr);\nmsg[2] = (u32)paddr;\nif (ctx->mbox_client.tx_block)\nreinit_completion(&ctx->rd_complete);\nrc = slimpro_i2c_send_msg(ctx, msg, msg);\ndma_unmap_single(ctx->dev, paddr, writelen, DMA_TO_DEVICE);\nreturn rc;\n}\n",
      "code_before_change_raw": "static int slimpro_i2c_blkwr(struct slimpro_i2c_dev *ctx, u32 chip,\nu32 addr, u32 addrlen, u32 protocol, u32 writelen,\nvoid *data)\n{\ndma_addr_t paddr;\nu32 msg[3];\nint rc;\nmemcpy(ctx->dma_buffer, data, writelen);\npaddr = dma_map_single(ctx->dev, ctx->dma_buffer, writelen,\nDMA_TO_DEVICE);\nif (dma_mapping_error(ctx->dev, paddr)) {\ndev_err(&ctx->adapter.dev, \"Error in mapping dma buffer %p\\n\",\nctx->dma_buffer);\nreturn -ENOMEM;\n}\nmsg[0] = SLIMPRO_IIC_ENCODE_MSG(SLIMPRO_IIC_BUS, chip, SLIMPRO_IIC_WRITE,\nprotocol, addrlen, writelen);\nmsg[1] = SLIMPRO_IIC_ENCODE_FLAG_BUFADDR |\nSLIMPRO_IIC_ENCODE_UPPER_BUFADDR(paddr) |\nSLIMPRO_IIC_ENCODE_ADDR(addr);\nmsg[2] = (u32)paddr;\nif (ctx->mbox_client.tx_block)\nreinit_completion(&ctx->rd_complete);\nrc = slimpro_i2c_send_msg(ctx, msg, msg);\ndma_unmap_single(ctx->dev, paddr, writelen, DMA_TO_DEVICE);\nreturn rc;\n}\n"
    }
  ],
  "CVE-2023-2598": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for consecutive and same folio pages in the buffer registration code.",
        "trigger_condition": "When multiple pages are not properly validated to be consecutive and on the same folio, it can lead to out-of-bounds access beyond the buffer.",
        "specific_code_behavior_causing_vulnerability": "The code does not verify that the pages are consecutive and on the same folio, allowing potential out-of-bounds access to physical memory beyond the end of the buffer."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the pages being accessed are consecutive and on the same folio. This can be achieved by adding a condition to check for consecutive pages in the buffer registration code. By validating the continuity of pages, the code prevents out-of-bounds access and strengthens the security of the buffer handling process.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-2598, which allows out-of-bounds access to physical memory beyond the end of the buffer, leading to potential full local privilege escalation. \n\nThe specific change made in the code snippet is in the loop that checks if the pages are consecutive and on the same folio. By adding the condition `pages[i] != pages[i - 1] + 1`, the code ensures that the pages are indeed consecutive in memory, preventing out-of-bounds access.\n\nThis modification helps to strengthen the buffer registration code by ensuring that the pages being accessed are within the bounds of the buffer, thus mitigating the risk of unauthorized access to memory locations outside the intended range.",
      "GPT_purpose": "Registering a buffer for I/O operations in the io_uring subsystem of the Linux kernel.",
      "GPT_function": "\n1. Registering a buffer for I/O operations in the io_uring context.\n2. Handling buffer coalescing for huge pages.\n3. Allocating memory for the buffer registration.\n4. Accounting for pinned pages and mapping them to the buffer.\n5. Setting up buffer vectors based on the provided pages and their sizes.\n6. Handling cleanup and error cases appropriately.",
      "CVE_id": "CVE-2023-2598",
      "code_before_change": "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\n\t\t\t\t  struct io_mapped_ubuf **pimu,\n\t\t\t\t  struct page **last_hpage)\n{\n\tstruct io_mapped_ubuf *imu = NULL;\n\tstruct page **pages = NULL;\n\tunsigned long off;\n\tsize_t size;\n\tint ret, nr_pages, i;\n\tstruct folio *folio = NULL;\n\n\t*pimu = ctx->dummy_ubuf;\n\tif (!iov->iov_base)\n\t\treturn 0;\n\n\tret = -ENOMEM;\n\tpages = io_pin_pages((unsigned long) iov->iov_base, iov->iov_len,\n\t\t\t\t&nr_pages);\n\tif (IS_ERR(pages)) {\n\t\tret = PTR_ERR(pages);\n\t\tpages = NULL;\n\t\tgoto done;\n\t}\n\n\t/* If it's a huge page, try to coalesce them into a single bvec entry */\n\tif (nr_pages > 1) {\n\t\tfolio = page_folio(pages[0]);\n\t\tfor (i = 1; i < nr_pages; i++) {\n\t\t\tif (page_folio(pages[i]) != folio) {\n\t\t\t\tfolio = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (folio) {\n\t\t\t/*\n\t\t\t * The pages are bound to the folio, it doesn't\n\t\t\t * actually unpin them but drops all but one reference,\n\t\t\t * which is usually put down by io_buffer_unmap().\n\t\t\t * Note, needs a better helper.\n\t\t\t */\n\t\t\tunpin_user_pages(&pages[1], nr_pages - 1);\n\t\t\tnr_pages = 1;\n\t\t}\n\t}\n\n\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);\n\tif (!imu)\n\t\tgoto done;\n\n\tret = io_buffer_account_pin(ctx, pages, nr_pages, imu, last_hpage);\n\tif (ret) {\n\t\tunpin_user_pages(pages, nr_pages);\n\t\tgoto done;\n\t}\n\n\toff = (unsigned long) iov->iov_base & ~PAGE_MASK;\n\tsize = iov->iov_len;\n\t/* store original address for later verification */\n\timu->ubuf = (unsigned long) iov->iov_base;\n\timu->ubuf_end = imu->ubuf + iov->iov_len;\n\timu->nr_bvecs = nr_pages;\n\t*pimu = imu;\n\tret = 0;\n\n\tif (folio) {\n\t\tbvec_set_page(&imu->bvec[0], pages[0], size, off);\n\t\tgoto done;\n\t}\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t vec_len;\n\n\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\tbvec_set_page(&imu->bvec[i], pages[i], vec_len, off);\n\t\toff = 0;\n\t\tsize -= vec_len;\n\t}\ndone:\n\tif (ret)\n\t\tkvfree(imu);\n\tkvfree(pages);\n\treturn ret;\n}",
      "code_after_change": "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\n\t\t\t\t  struct io_mapped_ubuf **pimu,\n\t\t\t\t  struct page **last_hpage)\n{\n\tstruct io_mapped_ubuf *imu = NULL;\n\tstruct page **pages = NULL;\n\tunsigned long off;\n\tsize_t size;\n\tint ret, nr_pages, i;\n\tstruct folio *folio = NULL;\n\n\t*pimu = ctx->dummy_ubuf;\n\tif (!iov->iov_base)\n\t\treturn 0;\n\n\tret = -ENOMEM;\n\tpages = io_pin_pages((unsigned long) iov->iov_base, iov->iov_len,\n\t\t\t\t&nr_pages);\n\tif (IS_ERR(pages)) {\n\t\tret = PTR_ERR(pages);\n\t\tpages = NULL;\n\t\tgoto done;\n\t}\n\n\t/* If it's a huge page, try to coalesce them into a single bvec entry */\n\tif (nr_pages > 1) {\n\t\tfolio = page_folio(pages[0]);\n\t\tfor (i = 1; i < nr_pages; i++) {\n\t\t\t/*\n\t\t\t * Pages must be consecutive and on the same folio for\n\t\t\t * this to work\n\t\t\t */\n\t\t\tif (page_folio(pages[i]) != folio ||\n\t\t\t    pages[i] != pages[i - 1] + 1) {\n\t\t\t\tfolio = NULL;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\tif (folio) {\n\t\t\t/*\n\t\t\t * The pages are bound to the folio, it doesn't\n\t\t\t * actually unpin them but drops all but one reference,\n\t\t\t * which is usually put down by io_buffer_unmap().\n\t\t\t * Note, needs a better helper.\n\t\t\t */\n\t\t\tunpin_user_pages(&pages[1], nr_pages - 1);\n\t\t\tnr_pages = 1;\n\t\t}\n\t}\n\n\timu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);\n\tif (!imu)\n\t\tgoto done;\n\n\tret = io_buffer_account_pin(ctx, pages, nr_pages, imu, last_hpage);\n\tif (ret) {\n\t\tunpin_user_pages(pages, nr_pages);\n\t\tgoto done;\n\t}\n\n\toff = (unsigned long) iov->iov_base & ~PAGE_MASK;\n\tsize = iov->iov_len;\n\t/* store original address for later verification */\n\timu->ubuf = (unsigned long) iov->iov_base;\n\timu->ubuf_end = imu->ubuf + iov->iov_len;\n\timu->nr_bvecs = nr_pages;\n\t*pimu = imu;\n\tret = 0;\n\n\tif (folio) {\n\t\tbvec_set_page(&imu->bvec[0], pages[0], size, off);\n\t\tgoto done;\n\t}\n\tfor (i = 0; i < nr_pages; i++) {\n\t\tsize_t vec_len;\n\n\t\tvec_len = min_t(size_t, size, PAGE_SIZE - off);\n\t\tbvec_set_page(&imu->bvec[i], pages[i], vec_len, off);\n\t\toff = 0;\n\t\tsize -= vec_len;\n\t}\ndone:\n\tif (ret)\n\t\tkvfree(imu);\n\tkvfree(pages);\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "\t\t\t/*",
          "\t\t\t * Pages must be consecutive and on the same folio for",
          "\t\t\t * this to work",
          "\t\t\t */",
          "\t\t\tif (page_folio(pages[i]) != folio ||",
          "\t\t\t    pages[i] != pages[i - 1] + 1) {"
        ],
        "deleted": [
          "\t\t\tif (page_folio(pages[i]) != folio) {"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for consecutive and same folio pages in the buffer registration code.",
      "trigger_condition": "When multiple pages are not properly validated to be consecutive and on the same folio, it can lead to out-of-bounds access beyond the buffer.",
      "specific_code_behavior_causing_vulnerability": "The code does not verify that the pages are consecutive and on the same folio, allowing potential out-of-bounds access to physical memory beyond the end of the buffer.",
      "id": 156,
      "code_after_change_normalized": "static int FUN1(struct io_ring_ctx *VAR1, struct iovec *VAR2,\nstruct io_mapped_ubuf **VAR3,\nstruct page **VAR4)\n{\nstruct io_mapped_ubuf *VAR5 = NULL;\nstruct page **VAR6 = NULL;\nunsigned long VAR7;\nsize_t VAR8;\nint VAR9, VAR10, VAR11;\nstruct VAR12 *VAR12 = NULL;\n*VAR3 = VAR1->VAR13;\nif (!VAR2->VAR14)\nreturn 0;\nVAR9 = -VAR15;\nVAR6 = FUN2((unsigned long) VAR2->VAR14, VAR2->VAR16,\n&VAR10);\nif (FUN3(VAR6)) {\nVAR9 = FUN4(VAR6);\nVAR6 = NULL;\ngoto VAR17;\n}\nif (VAR10 > 1) {\nVAR12 = FUN5(VAR6[0]);\nfor (VAR11 = 1; VAR11 < VAR10; VAR11++) {\nif (FUN5(VAR6[VAR11]) != VAR12 ||\nVAR6[VAR11] != VAR6[VAR11 - 1] + 1) {\nVAR12 = NULL;\nbreak;\n}\n}\nif (VAR12) {\nFUN6(&VAR6[1], VAR10 - 1);\nVAR10 = 1;\n}\n}\nVAR5 = FUN7(FUN8(VAR5, VAR18, VAR10), VAR19);\nif (!VAR5)\ngoto VAR17;\nVAR9 = FUN9(VAR1, VAR6, VAR10, VAR5, VAR4);\nif (VAR9) {\nFUN6(VAR6, VAR10);\ngoto VAR17;\n}\nVAR7 = (unsigned long) VAR2->VAR14 & ~VAR20;\nVAR8 = VAR2->VAR16;\nVAR5->VAR21 = (unsigned long) VAR2->VAR14;\nVAR5->VAR22 = VAR5->VAR21 + VAR2->VAR16;\nVAR5->VAR23 = VAR10;\n*VAR3 = VAR5;\nVAR9 = 0;\nif (VAR12) {\nFUN10(&VAR5->VAR18[0], VAR6[0], VAR8, VAR7);\ngoto VAR17;\n}\nfor (VAR11 = 0; VAR11 < VAR10; VAR11++) {\nsize_t VAR24;\nVAR24 = FUN11(VAR25, VAR8, VAR26 - VAR7);\nFUN10(&VAR5->VAR18[VAR11], VAR6[VAR11], VAR24, VAR7);\nVAR7 = 0;\nVAR8 -= VAR24;\n}\nVAR17:\nif (VAR9)\nFUN12(VAR5);\nFUN12(VAR6);\nreturn VAR9;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct io_ring_ctx *VAR1, struct iovec *VAR2,\nstruct io_mapped_ubuf **VAR3,\nstruct page **VAR4)\n{\nstruct io_mapped_ubuf *VAR5 = NULL;\nstruct page **VAR6 = NULL;\nunsigned long VAR7;\nsize_t VAR8;\nint VAR9, VAR10, VAR11;\nstruct VAR12 *VAR12 = NULL;\n*VAR3 = VAR1->VAR13;\nif (!VAR2->VAR14)\nreturn 0;\nVAR9 = -VAR15;\nVAR6 = FUN2((unsigned long) VAR2->VAR14, VAR2->VAR16,\n&VAR10);\nif (FUN3(VAR6)) {\nVAR9 = FUN4(VAR6);\nVAR6 = NULL;\ngoto VAR17;\n}\nif (VAR10 > 1) {\nVAR12 = FUN5(VAR6[0]);\nfor (VAR11 = 1; VAR11 < VAR10; VAR11++) {\nif (FUN5(VAR6[VAR11]) != VAR12) {\nVAR12 = NULL;\nbreak;\n}\n}\nif (VAR12) {\nFUN6(&VAR6[1], VAR10 - 1);\nVAR10 = 1;\n}\n}\nVAR5 = FUN7(FUN8(VAR5, VAR18, VAR10), VAR19);\nif (!VAR5)\ngoto VAR17;\nVAR9 = FUN9(VAR1, VAR6, VAR10, VAR5, VAR4);\nif (VAR9) {\nFUN6(VAR6, VAR10);\ngoto VAR17;\n}\nVAR7 = (unsigned long) VAR2->VAR14 & ~VAR20;\nVAR8 = VAR2->VAR16;\nVAR5->VAR21 = (unsigned long) VAR2->VAR14;\nVAR5->VAR22 = VAR5->VAR21 + VAR2->VAR16;\nVAR5->VAR23 = VAR10;\n*VAR3 = VAR5;\nVAR9 = 0;\nif (VAR12) {\nFUN10(&VAR5->VAR18[0], VAR6[0], VAR8, VAR7);\ngoto VAR17;\n}\nfor (VAR11 = 0; VAR11 < VAR10; VAR11++) {\nsize_t VAR24;\nVAR24 = FUN11(VAR25, VAR8, VAR26 - VAR7);\nFUN10(&VAR5->VAR18[VAR11], VAR6[VAR11], VAR24, VAR7);\nVAR7 = 0;\nVAR8 -= VAR24;\n}\nVAR17:\nif (VAR9)\nFUN12(VAR5);\nFUN12(VAR6);\nreturn VAR9;\n}\n",
      "code_after_change_raw": "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\nstruct io_mapped_ubuf **pimu,\nstruct page **last_hpage)\n{\nstruct io_mapped_ubuf *imu = NULL;\nstruct page **pages = NULL;\nunsigned long off;\nsize_t size;\nint ret, nr_pages, i;\nstruct folio *folio = NULL;\n*pimu = ctx->dummy_ubuf;\nif (!iov->iov_base)\nreturn 0;\nret = -ENOMEM;\npages = io_pin_pages((unsigned long) iov->iov_base, iov->iov_len,\n&nr_pages);\nif (IS_ERR(pages)) {\nret = PTR_ERR(pages);\npages = NULL;\ngoto done;\n}\nif (nr_pages > 1) {\nfolio = page_folio(pages[0]);\nfor (i = 1; i < nr_pages; i++) {\nif (page_folio(pages[i]) != folio ||\npages[i] != pages[i - 1] + 1) {\nfolio = NULL;\nbreak;\n}\n}\nif (folio) {\nunpin_user_pages(&pages[1], nr_pages - 1);\nnr_pages = 1;\n}\n}\nimu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);\nif (!imu)\ngoto done;\nret = io_buffer_account_pin(ctx, pages, nr_pages, imu, last_hpage);\nif (ret) {\nunpin_user_pages(pages, nr_pages);\ngoto done;\n}\noff = (unsigned long) iov->iov_base & ~PAGE_MASK;\nsize = iov->iov_len;\nimu->ubuf = (unsigned long) iov->iov_base;\nimu->ubuf_end = imu->ubuf + iov->iov_len;\nimu->nr_bvecs = nr_pages;\n*pimu = imu;\nret = 0;\nif (folio) {\nbvec_set_page(&imu->bvec[0], pages[0], size, off);\ngoto done;\n}\nfor (i = 0; i < nr_pages; i++) {\nsize_t vec_len;\nvec_len = min_t(size_t, size, PAGE_SIZE - off);\nbvec_set_page(&imu->bvec[i], pages[i], vec_len, off);\noff = 0;\nsize -= vec_len;\n}\ndone:\nif (ret)\nkvfree(imu);\nkvfree(pages);\nreturn ret;\n}\n",
      "code_before_change_raw": "static int io_sqe_buffer_register(struct io_ring_ctx *ctx, struct iovec *iov,\nstruct io_mapped_ubuf **pimu,\nstruct page **last_hpage)\n{\nstruct io_mapped_ubuf *imu = NULL;\nstruct page **pages = NULL;\nunsigned long off;\nsize_t size;\nint ret, nr_pages, i;\nstruct folio *folio = NULL;\n*pimu = ctx->dummy_ubuf;\nif (!iov->iov_base)\nreturn 0;\nret = -ENOMEM;\npages = io_pin_pages((unsigned long) iov->iov_base, iov->iov_len,\n&nr_pages);\nif (IS_ERR(pages)) {\nret = PTR_ERR(pages);\npages = NULL;\ngoto done;\n}\nif (nr_pages > 1) {\nfolio = page_folio(pages[0]);\nfor (i = 1; i < nr_pages; i++) {\nif (page_folio(pages[i]) != folio) {\nfolio = NULL;\nbreak;\n}\n}\nif (folio) {\nunpin_user_pages(&pages[1], nr_pages - 1);\nnr_pages = 1;\n}\n}\nimu = kvmalloc(struct_size(imu, bvec, nr_pages), GFP_KERNEL);\nif (!imu)\ngoto done;\nret = io_buffer_account_pin(ctx, pages, nr_pages, imu, last_hpage);\nif (ret) {\nunpin_user_pages(pages, nr_pages);\ngoto done;\n}\noff = (unsigned long) iov->iov_base & ~PAGE_MASK;\nsize = iov->iov_len;\nimu->ubuf = (unsigned long) iov->iov_base;\nimu->ubuf_end = imu->ubuf + iov->iov_len;\nimu->nr_bvecs = nr_pages;\n*pimu = imu;\nret = 0;\nif (folio) {\nbvec_set_page(&imu->bvec[0], pages[0], size, off);\ngoto done;\n}\nfor (i = 0; i < nr_pages; i++) {\nsize_t vec_len;\nvec_len = min_t(size_t, size, PAGE_SIZE - off);\nbvec_set_page(&imu->bvec[i], pages[i], vec_len, off);\noff = 0;\nsize -= vec_len;\n}\ndone:\nif (ret)\nkvfree(imu);\nkvfree(pages);\nreturn ret;\n}\n"
    }
  ],
  "CVE-2023-28410": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper boundary checking for memory access operations within a buffer.",
        "trigger_condition": "An authenticated user attempts to perform memory access operations outside the bounds of a memory buffer.",
        "specific_code_behavior_causing_vulnerability": "The code does not validate whether the memory access operation stays within the bounds of the memory buffer, potentially leading to buffer overflows and enabling unauthorized access or modification of memory outside the allocated buffer."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a boundary check to ensure that memory access operations are within the valid range of the memory buffer. This involves checking if the range of memory addresses specified by 'addr' and 'len' overflows the size of the object. By implementing this boundary check, the code prevents buffer overflows and unauthorized memory access, enhancing the security of the code and preventing potential privilege escalation attacks.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-28410, which involves improper restriction of operations within the bounds of a memory buffer in the Intel i915 Graphics drivers. The vulnerability allows an authenticated user to potentially enable escalation of privilege via local access.\n\nThe specific change made in the code snippet is the addition of the `range_overflows_t` check to ensure that the memory access operation stays within the bounds of the memory buffer. By checking if the range of memory addresses specified by `addr` and `len` overflows the size of the object (`obj->base.size`), the code prevents potential buffer overflows and ensures that the memory access operation is within the valid range.\n\nThis modification helps to mitigate the vulnerability by adding a boundary check to prevent unauthorized access or modification of memory outside the allocated buffer, thereby enhancing the security of the code and preventing potential privilege escalation attacks.",
      "GPT_purpose": "To handle access to a virtual memory area associated with an Intel i915 Graphics driver object, allowing read and write operations while checking for object permissions and locking mechanisms.",
      "GPT_function": "\n1. Check if the object is read-only and disallow write access if it is.\n2. Calculate the address offset within the object.\n3. Lock the object for exclusive access.\n4. Map the object into the virtual address space.\n5. Perform a memory copy operation based on the write flag.\n6. Flush the mapped memory region if writing.\n7. Unmap the object from the virtual address space.\n8. Handle potential deadlock situations during object locking.",
      "CVE_id": "CVE-2023-28410",
      "code_before_change": "static int\nvm_access(struct vm_area_struct *area, unsigned long addr,\n\t  void *buf, int len, int write)\n{\n\tstruct i915_mmap_offset *mmo = area->vm_private_data;\n\tstruct drm_i915_gem_object *obj = mmo->obj;\n\tstruct i915_gem_ww_ctx ww;\n\tvoid *vaddr;\n\tint err = 0;\n\n\tif (i915_gem_object_is_readonly(obj) && write)\n\t\treturn -EACCES;\n\n\taddr -= area->vm_start;\n\tif (addr >= obj->base.size)\n\t\treturn -EINVAL;\n\n\ti915_gem_ww_ctx_init(&ww, true);\nretry:\n\terr = i915_gem_object_lock(obj, &ww);\n\tif (err)\n\t\tgoto out;\n\n\t/* As this is primarily for debugging, let's focus on simplicity */\n\tvaddr = i915_gem_object_pin_map(obj, I915_MAP_FORCE_WC);\n\tif (IS_ERR(vaddr)) {\n\t\terr = PTR_ERR(vaddr);\n\t\tgoto out;\n\t}\n\n\tif (write) {\n\t\tmemcpy(vaddr + addr, buf, len);\n\t\t__i915_gem_object_flush_map(obj, addr, len);\n\t} else {\n\t\tmemcpy(buf, vaddr + addr, len);\n\t}\n\n\ti915_gem_object_unpin_map(obj);\nout:\n\tif (err == -EDEADLK) {\n\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t}\n\ti915_gem_ww_ctx_fini(&ww);\n\n\tif (err)\n\t\treturn err;\n\n\treturn len;\n}",
      "code_after_change": "static int\nvm_access(struct vm_area_struct *area, unsigned long addr,\n\t  void *buf, int len, int write)\n{\n\tstruct i915_mmap_offset *mmo = area->vm_private_data;\n\tstruct drm_i915_gem_object *obj = mmo->obj;\n\tstruct i915_gem_ww_ctx ww;\n\tvoid *vaddr;\n\tint err = 0;\n\n\tif (i915_gem_object_is_readonly(obj) && write)\n\t\treturn -EACCES;\n\n\taddr -= area->vm_start;\n\tif (range_overflows_t(u64, addr, len, obj->base.size))\n\t\treturn -EINVAL;\n\n\ti915_gem_ww_ctx_init(&ww, true);\nretry:\n\terr = i915_gem_object_lock(obj, &ww);\n\tif (err)\n\t\tgoto out;\n\n\t/* As this is primarily for debugging, let's focus on simplicity */\n\tvaddr = i915_gem_object_pin_map(obj, I915_MAP_FORCE_WC);\n\tif (IS_ERR(vaddr)) {\n\t\terr = PTR_ERR(vaddr);\n\t\tgoto out;\n\t}\n\n\tif (write) {\n\t\tmemcpy(vaddr + addr, buf, len);\n\t\t__i915_gem_object_flush_map(obj, addr, len);\n\t} else {\n\t\tmemcpy(buf, vaddr + addr, len);\n\t}\n\n\ti915_gem_object_unpin_map(obj);\nout:\n\tif (err == -EDEADLK) {\n\t\terr = i915_gem_ww_ctx_backoff(&ww);\n\t\tif (!err)\n\t\t\tgoto retry;\n\t}\n\ti915_gem_ww_ctx_fini(&ww);\n\n\tif (err)\n\t\treturn err;\n\n\treturn len;\n}",
      "modified_lines": {
        "added": [
          "\tif (range_overflows_t(u64, addr, len, obj->base.size))"
        ],
        "deleted": [
          "\tif (addr >= obj->base.size)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper boundary checking for memory access operations within a buffer.",
      "trigger_condition": "An authenticated user attempts to perform memory access operations outside the bounds of a memory buffer.",
      "specific_code_behavior_causing_vulnerability": "The code does not validate whether the memory access operation stays within the bounds of the memory buffer, potentially leading to buffer overflows and enabling unauthorized access or modification of memory outside the allocated buffer.",
      "id": 157,
      "code_after_change_normalized": "static int\nFUN1(struct vm_area_struct *VAR1, unsigned long VAR2,\nvoid *VAR3, int VAR4, int VAR5)\n{\nstruct i915_mmap_offset *VAR6 = VAR1->VAR7;\nstruct drm_i915_gem_object *VAR8 = VAR6->VAR8;\nstruct i915_gem_ww_ctx VAR9;\nvoid *VAR10;\nint VAR11 = 0;\nif (FUN2(VAR8) && VAR5)\nreturn -VAR12;\nVAR2 -= VAR1->VAR13;\nif (FUN3(VAR14, VAR2, VAR4, VAR8->VAR15.VAR16))\nreturn -VAR17;\nFUN4(&VAR9, true);\nVAR18:\nVAR11 = FUN5(VAR8, &VAR9);\nif (VAR11)\ngoto VAR19;\nVAR10 = FUN6(VAR8, VAR20);\nif (FUN7(VAR10)) {\nVAR11 = FUN8(VAR10);\ngoto VAR19;\n}\nif (VAR5) {\nFUN9(VAR10 + VAR2, VAR3, VAR4);\nFUN10(VAR8, VAR2, VAR4);\n} else {\nFUN9(VAR3, VAR10 + VAR2, VAR4);\n}\nFUN11(VAR8);\nVAR19:\nif (VAR11 == -VAR21) {\nVAR11 = FUN12(&VAR9);\nif (!VAR11)\ngoto VAR18;\n}\nFUN13(&VAR9);\nif (VAR11)\nreturn VAR11;\nreturn VAR4;\n}\n",
      "code_before_change_normalized": "static int\nFUN1(struct vm_area_struct *VAR1, unsigned long VAR2,\nvoid *VAR3, int VAR4, int VAR5)\n{\nstruct i915_mmap_offset *VAR6 = VAR1->VAR7;\nstruct drm_i915_gem_object *VAR8 = VAR6->VAR8;\nstruct i915_gem_ww_ctx VAR9;\nvoid *VAR10;\nint VAR11 = 0;\nif (FUN2(VAR8) && VAR5)\nreturn -VAR12;\nVAR2 -= VAR1->VAR13;\nif (VAR2 >= VAR8->VAR14.VAR15)\nreturn -VAR16;\nFUN3(&VAR9, true);\nVAR17:\nVAR11 = FUN4(VAR8, &VAR9);\nif (VAR11)\ngoto VAR18;\nVAR10 = FUN5(VAR8, VAR19);\nif (FUN6(VAR10)) {\nVAR11 = FUN7(VAR10);\ngoto VAR18;\n}\nif (VAR5) {\nFUN8(VAR10 + VAR2, VAR3, VAR4);\nFUN9(VAR8, VAR2, VAR4);\n} else {\nFUN8(VAR3, VAR10 + VAR2, VAR4);\n}\nFUN10(VAR8);\nVAR18:\nif (VAR11 == -VAR20) {\nVAR11 = FUN11(&VAR9);\nif (!VAR11)\ngoto VAR17;\n}\nFUN12(&VAR9);\nif (VAR11)\nreturn VAR11;\nreturn VAR4;\n}\n",
      "code_after_change_raw": "static int\nvm_access(struct vm_area_struct *area, unsigned long addr,\nvoid *buf, int len, int write)\n{\nstruct i915_mmap_offset *mmo = area->vm_private_data;\nstruct drm_i915_gem_object *obj = mmo->obj;\nstruct i915_gem_ww_ctx ww;\nvoid *vaddr;\nint err = 0;\nif (i915_gem_object_is_readonly(obj) && write)\nreturn -EACCES;\naddr -= area->vm_start;\nif (range_overflows_t(u64, addr, len, obj->base.size))\nreturn -EINVAL;\ni915_gem_ww_ctx_init(&ww, true);\nretry:\nerr = i915_gem_object_lock(obj, &ww);\nif (err)\ngoto out;\nvaddr = i915_gem_object_pin_map(obj, I915_MAP_FORCE_WC);\nif (IS_ERR(vaddr)) {\nerr = PTR_ERR(vaddr);\ngoto out;\n}\nif (write) {\nmemcpy(vaddr + addr, buf, len);\n__i915_gem_object_flush_map(obj, addr, len);\n} else {\nmemcpy(buf, vaddr + addr, len);\n}\ni915_gem_object_unpin_map(obj);\nout:\nif (err == -EDEADLK) {\nerr = i915_gem_ww_ctx_backoff(&ww);\nif (!err)\ngoto retry;\n}\ni915_gem_ww_ctx_fini(&ww);\nif (err)\nreturn err;\nreturn len;\n}\n",
      "code_before_change_raw": "static int\nvm_access(struct vm_area_struct *area, unsigned long addr,\nvoid *buf, int len, int write)\n{\nstruct i915_mmap_offset *mmo = area->vm_private_data;\nstruct drm_i915_gem_object *obj = mmo->obj;\nstruct i915_gem_ww_ctx ww;\nvoid *vaddr;\nint err = 0;\nif (i915_gem_object_is_readonly(obj) && write)\nreturn -EACCES;\naddr -= area->vm_start;\nif (addr >= obj->base.size)\nreturn -EINVAL;\ni915_gem_ww_ctx_init(&ww, true);\nretry:\nerr = i915_gem_object_lock(obj, &ww);\nif (err)\ngoto out;\nvaddr = i915_gem_object_pin_map(obj, I915_MAP_FORCE_WC);\nif (IS_ERR(vaddr)) {\nerr = PTR_ERR(vaddr);\ngoto out;\n}\nif (write) {\nmemcpy(vaddr + addr, buf, len);\n__i915_gem_object_flush_map(obj, addr, len);\n} else {\nmemcpy(buf, vaddr + addr, len);\n}\ni915_gem_object_unpin_map(obj);\nout:\nif (err == -EDEADLK) {\nerr = i915_gem_ww_ctx_backoff(&ww);\nif (!err)\ngoto retry;\n}\ni915_gem_ww_ctx_fini(&ww);\nif (err)\nreturn err;\nreturn len;\n}\n"
    }
  ],
  "CVE-2023-3090": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Missing initialization of skb->cb in the ipvlan network driver.",
        "trigger_condition": "The vulnerability is triggered when the code attempts to write to the uninitialized control buffer (cb) associated with the socket buffer (skb), leading to a heap out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code snippet lacks proper initialization of skb->cb, allowing for potential out-of-bounds writes that can be exploited for local privilege escalation. This vulnerability is reachable if CONFIG_IPVLAN is enabled."
      },
      "solution": "To mitigate the vulnerability, it is necessary to initialize the control buffer (cb) associated with the socket buffer (skb) before any write operations are performed on it. In this case, the solution involves adding the line 'memset(IPCB(skb), 0, sizeof(*IPCB(skb)));' to properly initialize the control buffer and prevent out-of-bounds writes. This initialization step ensures that the control buffer is in a safe and consistent state, reducing the risk of security vulnerabilities like the one described in CVE-2023-3090.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2023-3090, which is a heap out-of-bounds write vulnerability in the Linux Kernel ipvlan network driver. The vulnerability occurs due to missing initialization of skb->cb in the ipvlan network driver.\n\nBy adding the line `memset(IPCB(skb), 0, sizeof(*IPCB(skb)));`, you are initializing the control buffer (cb) associated with the socket buffer (skb). This initialization ensures that the control buffer is properly set up and prevents potential out-of-bounds writes that could lead to security vulnerabilities like the one described in CVE-2023-3090.\n\nInitializing the control buffer is a good security practice to ensure that the buffer is in a safe and consistent state before any operations are performed on it. This modification helps to mitigate the vulnerability and improve the overall security of the ipvlan network driver.",
      "GPT_purpose": "Process outbound IPv4 packets in the ipvlan network driver.",
      "GPT_function": "\n1. Process outbound IPv4 packets in the ipvlan network driver.\n2. Set up routing information for the outgoing packet.\n3. Handle errors and update statistics for the network device.",
      "CVE_id": "CVE-2023-3090",
      "code_before_change": "static int ipvlan_process_v4_outbound(struct sk_buff *skb)\n{\n\tconst struct iphdr *ip4h = ip_hdr(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct net *net = dev_net(dev);\n\tstruct rtable *rt;\n\tint err, ret = NET_XMIT_DROP;\n\tstruct flowi4 fl4 = {\n\t\t.flowi4_oif = dev->ifindex,\n\t\t.flowi4_tos = RT_TOS(ip4h->tos),\n\t\t.flowi4_flags = FLOWI_FLAG_ANYSRC,\n\t\t.flowi4_mark = skb->mark,\n\t\t.daddr = ip4h->daddr,\n\t\t.saddr = ip4h->saddr,\n\t};\n\n\trt = ip_route_output_flow(net, &fl4, NULL);\n\tif (IS_ERR(rt))\n\t\tgoto err;\n\n\tif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\n\t\tip_rt_put(rt);\n\t\tgoto err;\n\t}\n\tskb_dst_set(skb, &rt->dst);\n\terr = ip_local_out(net, skb->sk, skb);\n\tif (unlikely(net_xmit_eval(err)))\n\t\tdev->stats.tx_errors++;\n\telse\n\t\tret = NET_XMIT_SUCCESS;\n\tgoto out;\nerr:\n\tdev->stats.tx_errors++;\n\tkfree_skb(skb);\nout:\n\treturn ret;\n}",
      "code_after_change": "static int ipvlan_process_v4_outbound(struct sk_buff *skb)\n{\n\tconst struct iphdr *ip4h = ip_hdr(skb);\n\tstruct net_device *dev = skb->dev;\n\tstruct net *net = dev_net(dev);\n\tstruct rtable *rt;\n\tint err, ret = NET_XMIT_DROP;\n\tstruct flowi4 fl4 = {\n\t\t.flowi4_oif = dev->ifindex,\n\t\t.flowi4_tos = RT_TOS(ip4h->tos),\n\t\t.flowi4_flags = FLOWI_FLAG_ANYSRC,\n\t\t.flowi4_mark = skb->mark,\n\t\t.daddr = ip4h->daddr,\n\t\t.saddr = ip4h->saddr,\n\t};\n\n\trt = ip_route_output_flow(net, &fl4, NULL);\n\tif (IS_ERR(rt))\n\t\tgoto err;\n\n\tif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\n\t\tip_rt_put(rt);\n\t\tgoto err;\n\t}\n\tskb_dst_set(skb, &rt->dst);\n\n\tmemset(IPCB(skb), 0, sizeof(*IPCB(skb)));\n\n\terr = ip_local_out(net, skb->sk, skb);\n\tif (unlikely(net_xmit_eval(err)))\n\t\tdev->stats.tx_errors++;\n\telse\n\t\tret = NET_XMIT_SUCCESS;\n\tgoto out;\nerr:\n\tdev->stats.tx_errors++;\n\tkfree_skb(skb);\nout:\n\treturn ret;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tmemset(IPCB(skb), 0, sizeof(*IPCB(skb)));",
          ""
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Missing initialization of skb->cb in the ipvlan network driver.",
      "trigger_condition": "The vulnerability is triggered when the code attempts to write to the uninitialized control buffer (cb) associated with the socket buffer (skb), leading to a heap out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code snippet lacks proper initialization of skb->cb, allowing for potential out-of-bounds writes that can be exploited for local privilege escalation. This vulnerability is reachable if CONFIG_IPVLAN is enabled.",
      "id": 158,
      "code_after_change_normalized": "static int FUN1(struct sk_buff *VAR1)\n{\nconst struct iphdr *VAR2 = FUN2(VAR1);\nstruct net_device *VAR3 = VAR1->VAR3;\nstruct VAR4 *VAR4 = FUN3(VAR3);\nstruct rtable *VAR5;\nint VAR6, VAR7 = VAR8;\nstruct flowi4 VAR9 = {\n.VAR10 = VAR3->VAR11,\n.VAR12 = FUN4(VAR2->VAR13),\n.VAR14 = VAR15,\n.VAR16 = VAR1->VAR17,\n.VAR18 = VAR2->VAR18,\n.VAR19 = VAR2->VAR19,\n};\nVAR5 = FUN5(VAR4, &VAR9, NULL);\nif (FUN6(VAR5))\ngoto VAR6;\nif (VAR5->VAR20 != VAR21 && VAR5->VAR20 != VAR22) {\nFUN7(VAR5);\ngoto VAR6;\n}\nFUN8(VAR1, &VAR5->VAR23);\nFUN9(FUN10(VAR1), 0, sizeof(*FUN10(VAR1)));\nVAR6 = FUN11(VAR4, VAR1->VAR24, VAR1);\nif (FUN12(FUN13(VAR6)))\nVAR3->VAR25.VAR26++;\nelse\nVAR7 = VAR27;\ngoto VAR28;\nVAR6:\nVAR3->VAR25.VAR26++;\nFUN14(VAR1);\nVAR28:\nreturn VAR7;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct sk_buff *VAR1)\n{\nconst struct iphdr *VAR2 = FUN2(VAR1);\nstruct net_device *VAR3 = VAR1->VAR3;\nstruct VAR4 *VAR4 = FUN3(VAR3);\nstruct rtable *VAR5;\nint VAR6, VAR7 = VAR8;\nstruct flowi4 VAR9 = {\n.VAR10 = VAR3->VAR11,\n.VAR12 = FUN4(VAR2->VAR13),\n.VAR14 = VAR15,\n.VAR16 = VAR1->VAR17,\n.VAR18 = VAR2->VAR18,\n.VAR19 = VAR2->VAR19,\n};\nVAR5 = FUN5(VAR4, &VAR9, NULL);\nif (FUN6(VAR5))\ngoto VAR6;\nif (VAR5->VAR20 != VAR21 && VAR5->VAR20 != VAR22) {\nFUN7(VAR5);\ngoto VAR6;\n}\nFUN8(VAR1, &VAR5->VAR23);\nVAR6 = FUN9(VAR4, VAR1->VAR24, VAR1);\nif (FUN10(FUN11(VAR6)))\nVAR3->VAR25.VAR26++;\nelse\nVAR7 = VAR27;\ngoto VAR28;\nVAR6:\nVAR3->VAR25.VAR26++;\nFUN12(VAR1);\nVAR28:\nreturn VAR7;\n}\n",
      "code_after_change_raw": "static int ipvlan_process_v4_outbound(struct sk_buff *skb)\n{\nconst struct iphdr *ip4h = ip_hdr(skb);\nstruct net_device *dev = skb->dev;\nstruct net *net = dev_net(dev);\nstruct rtable *rt;\nint err, ret = NET_XMIT_DROP;\nstruct flowi4 fl4 = {\n.flowi4_oif = dev->ifindex,\n.flowi4_tos = RT_TOS(ip4h->tos),\n.flowi4_flags = FLOWI_FLAG_ANYSRC,\n.flowi4_mark = skb->mark,\n.daddr = ip4h->daddr,\n.saddr = ip4h->saddr,\n};\nrt = ip_route_output_flow(net, &fl4, NULL);\nif (IS_ERR(rt))\ngoto err;\nif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\nip_rt_put(rt);\ngoto err;\n}\nskb_dst_set(skb, &rt->dst);\nmemset(IPCB(skb), 0, sizeof(*IPCB(skb)));\nerr = ip_local_out(net, skb->sk, skb);\nif (unlikely(net_xmit_eval(err)))\ndev->stats.tx_errors++;\nelse\nret = NET_XMIT_SUCCESS;\ngoto out;\nerr:\ndev->stats.tx_errors++;\nkfree_skb(skb);\nout:\nreturn ret;\n}\n",
      "code_before_change_raw": "static int ipvlan_process_v4_outbound(struct sk_buff *skb)\n{\nconst struct iphdr *ip4h = ip_hdr(skb);\nstruct net_device *dev = skb->dev;\nstruct net *net = dev_net(dev);\nstruct rtable *rt;\nint err, ret = NET_XMIT_DROP;\nstruct flowi4 fl4 = {\n.flowi4_oif = dev->ifindex,\n.flowi4_tos = RT_TOS(ip4h->tos),\n.flowi4_flags = FLOWI_FLAG_ANYSRC,\n.flowi4_mark = skb->mark,\n.daddr = ip4h->daddr,\n.saddr = ip4h->saddr,\n};\nrt = ip_route_output_flow(net, &fl4, NULL);\nif (IS_ERR(rt))\ngoto err;\nif (rt->rt_type != RTN_UNICAST && rt->rt_type != RTN_LOCAL) {\nip_rt_put(rt);\ngoto err;\n}\nskb_dst_set(skb, &rt->dst);\nerr = ip_local_out(net, skb->sk, skb);\nif (unlikely(net_xmit_eval(err)))\ndev->stats.tx_errors++;\nelse\nret = NET_XMIT_SUCCESS;\ngoto out;\nerr:\ndev->stats.tx_errors++;\nkfree_skb(skb);\nout:\nreturn ret;\n}\n"
    }
  ],
  "CVE-2023-31436": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of validation for the value of lmax after parsing the TCA_QFQ_LMAX attribute.",
        "trigger_condition": "The value of lmax exceeds the defined constant QFQ_MIN_LMAX or falls outside the valid range, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not check if the value of lmax is within the valid range after parsing the TCA_QFQ_LMAX attribute, allowing it to potentially exceed the defined constant QFQ_MIN_LMAX and cause an out-of-bounds write vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a validation check for the value of lmax after parsing the TCA_QFQ_LMAX attribute. This check ensures that lmax does not exceed the defined constant QFQ_MIN_LMAX or fall outside the valid range, preventing the out-of-bounds write vulnerability. In this case, the solution involves adding an if statement to validate the value of lmax and return an error if it exceeds the allowed range.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-31436, which allows an out-of-bounds write because the variable `lmax` can exceed the defined constant `QFQ_MIN_LMAX`. By adding the check `if (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT))` after parsing the `TCA_QFQ_LMAX` attribute, we ensure that the value of `lmax` is within the valid range. This check prevents the out-of-bounds write vulnerability by ensuring that `lmax` does not exceed the allowed maximum value or fall below the minimum value specified by `QFQ_MIN_LMAX`.",
      "GPT_purpose": "This function is responsible for changing or creating QFQ classes in the QFQ scheduler in the Linux kernel.",
      "GPT_function": "\n1. Modify QFQ class parameters based on input options.\n2. Check and validate weight and max length values.\n3. Create or modify QFQ class and associated aggregate.\n4. Handle rate estimator for the class.\n5. Manage aggregation of QFQ classes.\n6. Handle memory allocation and error handling.",
      "CVE_id": "CVE-2023-31436",
      "code_before_change": "static int qfq_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\n\t\t\t    struct nlattr **tca, unsigned long *arg,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)*arg;\n\tbool existing = false;\n\tstruct nlattr *tb[TCA_QFQ_MAX + 1];\n\tstruct qfq_aggregate *new_agg = NULL;\n\tu32 weight, lmax, inv_w;\n\tint err;\n\tint delta_w;\n\n\tif (tca[TCA_OPTIONS] == NULL) {\n\t\tpr_notice(\"qfq: no options\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, TCA_QFQ_MAX, tca[TCA_OPTIONS],\n\t\t\t\t\t  qfq_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_QFQ_WEIGHT]) {\n\t\tweight = nla_get_u32(tb[TCA_QFQ_WEIGHT]);\n\t\tif (!weight || weight > (1UL << QFQ_MAX_WSHIFT)) {\n\t\t\tpr_notice(\"qfq: invalid weight %u\\n\", weight);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tweight = 1;\n\n\tif (tb[TCA_QFQ_LMAX]) {\n\t\tlmax = nla_get_u32(tb[TCA_QFQ_LMAX]);\n\t\tif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {\n\t\t\tpr_notice(\"qfq: invalid max length %u\\n\", lmax);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tlmax = psched_mtu(qdisc_dev(sch));\n\n\tinv_w = ONE_FP / weight;\n\tweight = ONE_FP / inv_w;\n\n\tif (cl != NULL &&\n\t    lmax == cl->agg->lmax &&\n\t    weight == cl->agg->class_weight)\n\t\treturn 0; /* nothing to change */\n\n\tdelta_w = weight - (cl ? cl->agg->class_weight : 0);\n\n\tif (q->wsum + delta_w > QFQ_MAX_WSUM) {\n\t\tpr_notice(\"qfq: total weight out of range (%d + %u)\\n\",\n\t\t\t  delta_w, q->wsum);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cl != NULL) { /* modify existing class */\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\texisting = true;\n\t\tgoto set_change_agg;\n\t}\n\n\t/* create and init new class */\n\tcl = kzalloc(sizeof(struct qfq_class), GFP_KERNEL);\n\tif (cl == NULL)\n\t\treturn -ENOBUFS;\n\n\tgnet_stats_basic_sync_init(&cl->bstats);\n\tcl->common.classid = classid;\n\tcl->deficit = lmax;\n\n\tcl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t      classid, NULL);\n\tif (cl->qdisc == NULL)\n\t\tcl->qdisc = &noop_qdisc;\n\n\tif (tca[TCA_RATE]) {\n\t\terr = gen_new_estimator(&cl->bstats, NULL,\n\t\t\t\t\t&cl->rate_est,\n\t\t\t\t\tNULL,\n\t\t\t\t\ttrue,\n\t\t\t\t\ttca[TCA_RATE]);\n\t\tif (err)\n\t\t\tgoto destroy_class;\n\t}\n\n\tif (cl->qdisc != &noop_qdisc)\n\t\tqdisc_hash_add(cl->qdisc, true);\n\nset_change_agg:\n\tsch_tree_lock(sch);\n\tnew_agg = qfq_find_agg(q, lmax, weight);\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tsch_tree_unlock(sch);\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_KERNEL);\n\t\tif (new_agg == NULL) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgen_kill_estimator(&cl->rate_est);\n\t\t\tgoto destroy_class;\n\t\t}\n\t\tsch_tree_lock(sch);\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tif (existing)\n\t\tqfq_deact_rm_from_agg(q, cl);\n\telse\n\t\tqdisc_class_hash_insert(&q->clhash, &cl->common);\n\tqfq_add_to_agg(q, new_agg, cl);\n\tsch_tree_unlock(sch);\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n\ndestroy_class:\n\tqdisc_put(cl->qdisc);\n\tkfree(cl);\n\treturn err;\n}",
      "code_after_change": "static int qfq_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\n\t\t\t    struct nlattr **tca, unsigned long *arg,\n\t\t\t    struct netlink_ext_ack *extack)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_class *cl = (struct qfq_class *)*arg;\n\tbool existing = false;\n\tstruct nlattr *tb[TCA_QFQ_MAX + 1];\n\tstruct qfq_aggregate *new_agg = NULL;\n\tu32 weight, lmax, inv_w;\n\tint err;\n\tint delta_w;\n\n\tif (tca[TCA_OPTIONS] == NULL) {\n\t\tpr_notice(\"qfq: no options\\n\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb, TCA_QFQ_MAX, tca[TCA_OPTIONS],\n\t\t\t\t\t  qfq_policy, NULL);\n\tif (err < 0)\n\t\treturn err;\n\n\tif (tb[TCA_QFQ_WEIGHT]) {\n\t\tweight = nla_get_u32(tb[TCA_QFQ_WEIGHT]);\n\t\tif (!weight || weight > (1UL << QFQ_MAX_WSHIFT)) {\n\t\t\tpr_notice(\"qfq: invalid weight %u\\n\", weight);\n\t\t\treturn -EINVAL;\n\t\t}\n\t} else\n\t\tweight = 1;\n\n\tif (tb[TCA_QFQ_LMAX])\n\t\tlmax = nla_get_u32(tb[TCA_QFQ_LMAX]);\n\telse\n\t\tlmax = psched_mtu(qdisc_dev(sch));\n\n\tif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {\n\t\tpr_notice(\"qfq: invalid max length %u\\n\", lmax);\n\t\treturn -EINVAL;\n\t}\n\n\tinv_w = ONE_FP / weight;\n\tweight = ONE_FP / inv_w;\n\n\tif (cl != NULL &&\n\t    lmax == cl->agg->lmax &&\n\t    weight == cl->agg->class_weight)\n\t\treturn 0; /* nothing to change */\n\n\tdelta_w = weight - (cl ? cl->agg->class_weight : 0);\n\n\tif (q->wsum + delta_w > QFQ_MAX_WSUM) {\n\t\tpr_notice(\"qfq: total weight out of range (%d + %u)\\n\",\n\t\t\t  delta_w, q->wsum);\n\t\treturn -EINVAL;\n\t}\n\n\tif (cl != NULL) { /* modify existing class */\n\t\tif (tca[TCA_RATE]) {\n\t\t\terr = gen_replace_estimator(&cl->bstats, NULL,\n\t\t\t\t\t\t    &cl->rate_est,\n\t\t\t\t\t\t    NULL,\n\t\t\t\t\t\t    true,\n\t\t\t\t\t\t    tca[TCA_RATE]);\n\t\t\tif (err)\n\t\t\t\treturn err;\n\t\t}\n\t\texisting = true;\n\t\tgoto set_change_agg;\n\t}\n\n\t/* create and init new class */\n\tcl = kzalloc(sizeof(struct qfq_class), GFP_KERNEL);\n\tif (cl == NULL)\n\t\treturn -ENOBUFS;\n\n\tgnet_stats_basic_sync_init(&cl->bstats);\n\tcl->common.classid = classid;\n\tcl->deficit = lmax;\n\n\tcl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\n\t\t\t\t      classid, NULL);\n\tif (cl->qdisc == NULL)\n\t\tcl->qdisc = &noop_qdisc;\n\n\tif (tca[TCA_RATE]) {\n\t\terr = gen_new_estimator(&cl->bstats, NULL,\n\t\t\t\t\t&cl->rate_est,\n\t\t\t\t\tNULL,\n\t\t\t\t\ttrue,\n\t\t\t\t\ttca[TCA_RATE]);\n\t\tif (err)\n\t\t\tgoto destroy_class;\n\t}\n\n\tif (cl->qdisc != &noop_qdisc)\n\t\tqdisc_hash_add(cl->qdisc, true);\n\nset_change_agg:\n\tsch_tree_lock(sch);\n\tnew_agg = qfq_find_agg(q, lmax, weight);\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tsch_tree_unlock(sch);\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_KERNEL);\n\t\tif (new_agg == NULL) {\n\t\t\terr = -ENOBUFS;\n\t\t\tgen_kill_estimator(&cl->rate_est);\n\t\t\tgoto destroy_class;\n\t\t}\n\t\tsch_tree_lock(sch);\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tif (existing)\n\t\tqfq_deact_rm_from_agg(q, cl);\n\telse\n\t\tqdisc_class_hash_insert(&q->clhash, &cl->common);\n\tqfq_add_to_agg(q, new_agg, cl);\n\tsch_tree_unlock(sch);\n\tqdisc_class_hash_grow(sch, &q->clhash);\n\n\t*arg = (unsigned long)cl;\n\treturn 0;\n\ndestroy_class:\n\tqdisc_put(cl->qdisc);\n\tkfree(cl);\n\treturn err;\n}",
      "modified_lines": {
        "added": [
          "\tif (tb[TCA_QFQ_LMAX])",
          "\telse",
          "",
          "\tif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {",
          "\t\tpr_notice(\"qfq: invalid max length %u\\n\", lmax);",
          "\t\treturn -EINVAL;",
          "\t}"
        ],
        "deleted": [
          "\tif (tb[TCA_QFQ_LMAX]) {",
          "\t\tif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {",
          "\t\t\tpr_notice(\"qfq: invalid max length %u\\n\", lmax);",
          "\t\t\treturn -EINVAL;",
          "\t\t}",
          "\t} else"
        ]
      },
      "preconditions_for_vulnerability": "Lack of validation for the value of lmax after parsing the TCA_QFQ_LMAX attribute.",
      "trigger_condition": "The value of lmax exceeds the defined constant QFQ_MIN_LMAX or falls outside the valid range, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not check if the value of lmax is within the valid range after parsing the TCA_QFQ_LMAX attribute, allowing it to potentially exceed the defined constant QFQ_MIN_LMAX and cause an out-of-bounds write vulnerability.",
      "id": 159,
      "code_after_change_normalized": "static int FUN1(struct Qdisc *VAR1, u32 VAR2, u32 VAR3,\nstruct nlattr **VAR4, unsigned long *VAR5,\nstruct netlink_ext_ack *VAR6)\n{\nstruct qfq_sched *VAR7 = FUN2(VAR1);\nstruct VAR9 *VAR8 = (struct VAR9 *)*VAR5;\nbool VAR10 = false;\nstruct nlattr *VAR11[VAR12 + 1];\nstruct qfq_aggregate *VAR13 = NULL;\nu32 VAR14, VAR15, VAR16;\nint VAR17;\nint VAR18;\nif (VAR4[VAR19] == NULL) {\nFUN3(\"STR\");\nreturn -VAR20;\n}\nVAR17 = FUN4(VAR11, VAR12, VAR4[VAR19],\nVAR21, NULL);\nif (VAR17 < 0)\nreturn VAR17;\nif (VAR11[VAR22]) {\nVAR14 = FUN5(VAR11[VAR22]);\nif (!VAR14 || VAR14 > (1UL << VAR23)) {\nFUN3(\"STR\", VAR14);\nreturn -VAR20;\n}\n} else\nVAR14 = 1;\nif (VAR11[VAR24])\nVAR15 = FUN5(VAR11[VAR24]);\nelse\nVAR15 = FUN6(FUN7(VAR1));\nif (VAR15 < VAR25 || VAR15 > (1UL << VAR26)) {\nFUN3(\"STR\", VAR15);\nreturn -VAR20;\n}\nVAR16 = VAR27 / VAR14;\nVAR14 = VAR27 / VAR16;\nif (VAR8 != NULL &&\nVAR15 == VAR8->VAR28->VAR15 &&\nVAR14 == VAR8->VAR28->VAR29)\nreturn 0; \nVAR18 = VAR14 - (VAR8 ? VAR8->VAR28->VAR29 : 0);\nif (VAR7->VAR30 + VAR18 > VAR31) {\nFUN3(\"STR\",\nVAR18, VAR7->VAR30);\nreturn -VAR20;\n}\nif (VAR8 != NULL) { \nif (VAR4[VAR32]) {\nVAR17 = FUN8(&VAR8->VAR33, NULL,\n&VAR8->VAR34,\nNULL,\ntrue,\nVAR4[VAR32]);\nif (VAR17)\nreturn VAR17;\n}\nVAR10 = true;\ngoto VAR35;\n}\nVAR8 = FUN9(sizeof(struct VAR9), VAR36);\nif (VAR8 == NULL)\nreturn -VAR37;\nFUN10(&VAR8->VAR33);\nVAR8->VAR38.VAR2 = VAR2;\nVAR8->VAR39 = VAR15;\nVAR8->VAR40 = FUN11(VAR1->VAR41, &VAR42,\nVAR2, NULL);\nif (VAR8->VAR40 == NULL)\nVAR8->VAR40 = &VAR43;\nif (VAR4[VAR32]) {\nVAR17 = FUN12(&VAR8->VAR33, NULL,\n&VAR8->VAR34,\nNULL,\ntrue,\nVAR4[VAR32]);\nif (VAR17)\ngoto VAR44;\n}\nif (VAR8->VAR40 != &VAR43)\nFUN13(VAR8->VAR40, true);\nVAR35:\nFUN14(VAR1);\nVAR13 = FUN15(VAR7, VAR15, VAR14);\nif (VAR13 == NULL) { \nFUN16(VAR1);\nVAR13 = FUN9(sizeof(*VAR13), VAR36);\nif (VAR13 == NULL) {\nVAR17 = -VAR37;\nFUN17(&VAR8->VAR34);\ngoto VAR44;\n}\nFUN14(VAR1);\nFUN18(VAR7, VAR13, VAR15, VAR14);\n}\nif (VAR10)\nFUN19(VAR7, VAR8);\nelse\nFUN20(&VAR7->VAR45, &VAR8->VAR38);\nFUN21(VAR7, VAR13, VAR8);\nFUN16(VAR1);\nFUN22(VAR1, &VAR7->VAR45);\n*VAR5 = (unsigned long)VAR8;\nreturn 0;\nVAR44:\nFUN23(VAR8->VAR40);\nFUN24(VAR8);\nreturn VAR17;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct Qdisc *VAR1, u32 VAR2, u32 VAR3,\nstruct nlattr **VAR4, unsigned long *VAR5,\nstruct netlink_ext_ack *VAR6)\n{\nstruct qfq_sched *VAR7 = FUN2(VAR1);\nstruct VAR9 *VAR8 = (struct VAR9 *)*VAR5;\nbool VAR10 = false;\nstruct nlattr *VAR11[VAR12 + 1];\nstruct qfq_aggregate *VAR13 = NULL;\nu32 VAR14, VAR15, VAR16;\nint VAR17;\nint VAR18;\nif (VAR4[VAR19] == NULL) {\nFUN3(\"STR\");\nreturn -VAR20;\n}\nVAR17 = FUN4(VAR11, VAR12, VAR4[VAR19],\nVAR21, NULL);\nif (VAR17 < 0)\nreturn VAR17;\nif (VAR11[VAR22]) {\nVAR14 = FUN5(VAR11[VAR22]);\nif (!VAR14 || VAR14 > (1UL << VAR23)) {\nFUN3(\"STR\", VAR14);\nreturn -VAR20;\n}\n} else\nVAR14 = 1;\nif (VAR11[VAR24]) {\nVAR15 = FUN5(VAR11[VAR24]);\nif (VAR15 < VAR25 || VAR15 > (1UL << VAR26)) {\nFUN3(\"STR\", VAR15);\nreturn -VAR20;\n}\n} else\nVAR15 = FUN6(FUN7(VAR1));\nVAR16 = VAR27 / VAR14;\nVAR14 = VAR27 / VAR16;\nif (VAR8 != NULL &&\nVAR15 == VAR8->VAR28->VAR15 &&\nVAR14 == VAR8->VAR28->VAR29)\nreturn 0; \nVAR18 = VAR14 - (VAR8 ? VAR8->VAR28->VAR29 : 0);\nif (VAR7->VAR30 + VAR18 > VAR31) {\nFUN3(\"STR\",\nVAR18, VAR7->VAR30);\nreturn -VAR20;\n}\nif (VAR8 != NULL) { \nif (VAR4[VAR32]) {\nVAR17 = FUN8(&VAR8->VAR33, NULL,\n&VAR8->VAR34,\nNULL,\ntrue,\nVAR4[VAR32]);\nif (VAR17)\nreturn VAR17;\n}\nVAR10 = true;\ngoto VAR35;\n}\nVAR8 = FUN9(sizeof(struct VAR9), VAR36);\nif (VAR8 == NULL)\nreturn -VAR37;\nFUN10(&VAR8->VAR33);\nVAR8->VAR38.VAR2 = VAR2;\nVAR8->VAR39 = VAR15;\nVAR8->VAR40 = FUN11(VAR1->VAR41, &VAR42,\nVAR2, NULL);\nif (VAR8->VAR40 == NULL)\nVAR8->VAR40 = &VAR43;\nif (VAR4[VAR32]) {\nVAR17 = FUN12(&VAR8->VAR33, NULL,\n&VAR8->VAR34,\nNULL,\ntrue,\nVAR4[VAR32]);\nif (VAR17)\ngoto VAR44;\n}\nif (VAR8->VAR40 != &VAR43)\nFUN13(VAR8->VAR40, true);\nVAR35:\nFUN14(VAR1);\nVAR13 = FUN15(VAR7, VAR15, VAR14);\nif (VAR13 == NULL) { \nFUN16(VAR1);\nVAR13 = FUN9(sizeof(*VAR13), VAR36);\nif (VAR13 == NULL) {\nVAR17 = -VAR37;\nFUN17(&VAR8->VAR34);\ngoto VAR44;\n}\nFUN14(VAR1);\nFUN18(VAR7, VAR13, VAR15, VAR14);\n}\nif (VAR10)\nFUN19(VAR7, VAR8);\nelse\nFUN20(&VAR7->VAR45, &VAR8->VAR38);\nFUN21(VAR7, VAR13, VAR8);\nFUN16(VAR1);\nFUN22(VAR1, &VAR7->VAR45);\n*VAR5 = (unsigned long)VAR8;\nreturn 0;\nVAR44:\nFUN23(VAR8->VAR40);\nFUN24(VAR8);\nreturn VAR17;\n}\n",
      "code_after_change_raw": "static int qfq_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\nstruct nlattr **tca, unsigned long *arg,\nstruct netlink_ext_ack *extack)\n{\nstruct qfq_sched *q = qdisc_priv(sch);\nstruct qfq_class *cl = (struct qfq_class *)*arg;\nbool existing = false;\nstruct nlattr *tb[TCA_QFQ_MAX + 1];\nstruct qfq_aggregate *new_agg = NULL;\nu32 weight, lmax, inv_w;\nint err;\nint delta_w;\nif (tca[TCA_OPTIONS] == NULL) {\npr_notice(\"qfq: no options\\n\");\nreturn -EINVAL;\n}\nerr = nla_parse_nested_deprecated(tb, TCA_QFQ_MAX, tca[TCA_OPTIONS],\nqfq_policy, NULL);\nif (err < 0)\nreturn err;\nif (tb[TCA_QFQ_WEIGHT]) {\nweight = nla_get_u32(tb[TCA_QFQ_WEIGHT]);\nif (!weight || weight > (1UL << QFQ_MAX_WSHIFT)) {\npr_notice(\"qfq: invalid weight %u\\n\", weight);\nreturn -EINVAL;\n}\n} else\nweight = 1;\nif (tb[TCA_QFQ_LMAX])\nlmax = nla_get_u32(tb[TCA_QFQ_LMAX]);\nelse\nlmax = psched_mtu(qdisc_dev(sch));\nif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {\npr_notice(\"qfq: invalid max length %u\\n\", lmax);\nreturn -EINVAL;\n}\ninv_w = ONE_FP / weight;\nweight = ONE_FP / inv_w;\nif (cl != NULL &&\nlmax == cl->agg->lmax &&\nweight == cl->agg->class_weight)\nreturn 0; \ndelta_w = weight - (cl ? cl->agg->class_weight : 0);\nif (q->wsum + delta_w > QFQ_MAX_WSUM) {\npr_notice(\"qfq: total weight out of range (%d + %u)\\n\",\ndelta_w, q->wsum);\nreturn -EINVAL;\n}\nif (cl != NULL) { \nif (tca[TCA_RATE]) {\nerr = gen_replace_estimator(&cl->bstats, NULL,\n&cl->rate_est,\nNULL,\ntrue,\ntca[TCA_RATE]);\nif (err)\nreturn err;\n}\nexisting = true;\ngoto set_change_agg;\n}\ncl = kzalloc(sizeof(struct qfq_class), GFP_KERNEL);\nif (cl == NULL)\nreturn -ENOBUFS;\ngnet_stats_basic_sync_init(&cl->bstats);\ncl->common.classid = classid;\ncl->deficit = lmax;\ncl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\nclassid, NULL);\nif (cl->qdisc == NULL)\ncl->qdisc = &noop_qdisc;\nif (tca[TCA_RATE]) {\nerr = gen_new_estimator(&cl->bstats, NULL,\n&cl->rate_est,\nNULL,\ntrue,\ntca[TCA_RATE]);\nif (err)\ngoto destroy_class;\n}\nif (cl->qdisc != &noop_qdisc)\nqdisc_hash_add(cl->qdisc, true);\nset_change_agg:\nsch_tree_lock(sch);\nnew_agg = qfq_find_agg(q, lmax, weight);\nif (new_agg == NULL) { \nsch_tree_unlock(sch);\nnew_agg = kzalloc(sizeof(*new_agg), GFP_KERNEL);\nif (new_agg == NULL) {\nerr = -ENOBUFS;\ngen_kill_estimator(&cl->rate_est);\ngoto destroy_class;\n}\nsch_tree_lock(sch);\nqfq_init_agg(q, new_agg, lmax, weight);\n}\nif (existing)\nqfq_deact_rm_from_agg(q, cl);\nelse\nqdisc_class_hash_insert(&q->clhash, &cl->common);\nqfq_add_to_agg(q, new_agg, cl);\nsch_tree_unlock(sch);\nqdisc_class_hash_grow(sch, &q->clhash);\n*arg = (unsigned long)cl;\nreturn 0;\ndestroy_class:\nqdisc_put(cl->qdisc);\nkfree(cl);\nreturn err;\n}\n",
      "code_before_change_raw": "static int qfq_change_class(struct Qdisc *sch, u32 classid, u32 parentid,\nstruct nlattr **tca, unsigned long *arg,\nstruct netlink_ext_ack *extack)\n{\nstruct qfq_sched *q = qdisc_priv(sch);\nstruct qfq_class *cl = (struct qfq_class *)*arg;\nbool existing = false;\nstruct nlattr *tb[TCA_QFQ_MAX + 1];\nstruct qfq_aggregate *new_agg = NULL;\nu32 weight, lmax, inv_w;\nint err;\nint delta_w;\nif (tca[TCA_OPTIONS] == NULL) {\npr_notice(\"qfq: no options\\n\");\nreturn -EINVAL;\n}\nerr = nla_parse_nested_deprecated(tb, TCA_QFQ_MAX, tca[TCA_OPTIONS],\nqfq_policy, NULL);\nif (err < 0)\nreturn err;\nif (tb[TCA_QFQ_WEIGHT]) {\nweight = nla_get_u32(tb[TCA_QFQ_WEIGHT]);\nif (!weight || weight > (1UL << QFQ_MAX_WSHIFT)) {\npr_notice(\"qfq: invalid weight %u\\n\", weight);\nreturn -EINVAL;\n}\n} else\nweight = 1;\nif (tb[TCA_QFQ_LMAX]) {\nlmax = nla_get_u32(tb[TCA_QFQ_LMAX]);\nif (lmax < QFQ_MIN_LMAX || lmax > (1UL << QFQ_MTU_SHIFT)) {\npr_notice(\"qfq: invalid max length %u\\n\", lmax);\nreturn -EINVAL;\n}\n} else\nlmax = psched_mtu(qdisc_dev(sch));\ninv_w = ONE_FP / weight;\nweight = ONE_FP / inv_w;\nif (cl != NULL &&\nlmax == cl->agg->lmax &&\nweight == cl->agg->class_weight)\nreturn 0; \ndelta_w = weight - (cl ? cl->agg->class_weight : 0);\nif (q->wsum + delta_w > QFQ_MAX_WSUM) {\npr_notice(\"qfq: total weight out of range (%d + %u)\\n\",\ndelta_w, q->wsum);\nreturn -EINVAL;\n}\nif (cl != NULL) { \nif (tca[TCA_RATE]) {\nerr = gen_replace_estimator(&cl->bstats, NULL,\n&cl->rate_est,\nNULL,\ntrue,\ntca[TCA_RATE]);\nif (err)\nreturn err;\n}\nexisting = true;\ngoto set_change_agg;\n}\ncl = kzalloc(sizeof(struct qfq_class), GFP_KERNEL);\nif (cl == NULL)\nreturn -ENOBUFS;\ngnet_stats_basic_sync_init(&cl->bstats);\ncl->common.classid = classid;\ncl->deficit = lmax;\ncl->qdisc = qdisc_create_dflt(sch->dev_queue, &pfifo_qdisc_ops,\nclassid, NULL);\nif (cl->qdisc == NULL)\ncl->qdisc = &noop_qdisc;\nif (tca[TCA_RATE]) {\nerr = gen_new_estimator(&cl->bstats, NULL,\n&cl->rate_est,\nNULL,\ntrue,\ntca[TCA_RATE]);\nif (err)\ngoto destroy_class;\n}\nif (cl->qdisc != &noop_qdisc)\nqdisc_hash_add(cl->qdisc, true);\nset_change_agg:\nsch_tree_lock(sch);\nnew_agg = qfq_find_agg(q, lmax, weight);\nif (new_agg == NULL) { \nsch_tree_unlock(sch);\nnew_agg = kzalloc(sizeof(*new_agg), GFP_KERNEL);\nif (new_agg == NULL) {\nerr = -ENOBUFS;\ngen_kill_estimator(&cl->rate_est);\ngoto destroy_class;\n}\nsch_tree_lock(sch);\nqfq_init_agg(q, new_agg, lmax, weight);\n}\nif (existing)\nqfq_deact_rm_from_agg(q, cl);\nelse\nqdisc_class_hash_insert(&q->clhash, &cl->common);\nqfq_add_to_agg(q, new_agg, cl);\nsch_tree_unlock(sch);\nqdisc_class_hash_grow(sch, &q->clhash);\n*arg = (unsigned long)cl;\nreturn 0;\ndestroy_class:\nqdisc_put(cl->qdisc);\nkfree(cl);\nreturn err;\n}\n"
    }
  ],
  "CVE-2023-34319": [],
  "CVE-2023-35001": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of vm register contents in the nft_byteorder function when CAP_NET_ADMIN is present in any user or network namespace.",
        "trigger_condition": "The code performs out-of-bounds read/write operations due to incorrectly handling 32-bit data in switch cases for 2-byte operations.",
        "specific_code_behavior_causing_vulnerability": "The code uses u32 pointers to handle 16-bit data in switch cases for 2-byte operations, leading to out-of-bounds read/write vulnerabilities. This vulnerability arises when CAP_NET_ADMIN is present in any user or network namespace, causing improper handling of vm register contents."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the code correctly handles 16-bit data in switch cases for 2-byte operations. This involves changing the type of pointers to u16, using u16 pointers to handle 16-bit data, and updating assignments and operations within the switch cases to work with 16-bit data. By making these changes, the code now properly handles 16-bit data and prevents out-of-bounds read/write vulnerabilities.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2023-35001, which is an Out-Of-Bounds Read/Write vulnerability in the nft_byteorder function of the Linux Kernel nftables. The vulnerability occurs due to poorly handling vm register contents when CAP_NET_ADMIN is in any user or network namespace.\n\nThe key changes made in the modified code snippet are:\n1. Changing the type of pointers `s` and `d` from `u32` to `u16` to ensure proper handling of 16-bit data.\n2. Using `u16` pointers `s16` and `d16` to correctly handle 16-bit data in the switch cases for 2-byte operations.\n3. Updating the assignments and operations within the switch cases to work with 16-bit data (`u16`) instead of 32-bit data (`u32`).\n\nBy making these changes, the code is now correctly handling 16-bit data and ensuring that there are no out-of-bounds read or write operations, thus mitigating the vulnerability and improving the security of the nft_byteorder function in the Linux Kernel nftables.",
      "GPT_purpose": "Perform byte order conversion operations on data stored in nftables registers.",
      "GPT_function": "\n1. Evaluate byte order operations on network packet data.\n2. Convert byte order based on specified operation (NTOH or HTON).\n3. Handle different data sizes (8, 4, or 2 bytes) for byte order conversion.",
      "CVE_id": "CVE-2023-35001",
      "code_before_change": "void nft_byteorder_eval(const struct nft_expr *expr,\n\t\t\tstruct nft_regs *regs,\n\t\t\tconst struct nft_pktinfo *pkt)\n{\n\tconst struct nft_byteorder *priv = nft_expr_priv(expr);\n\tu32 *src = &regs->data[priv->sreg];\n\tu32 *dst = &regs->data[priv->dreg];\n\tunion { u32 u32; u16 u16; } *s, *d;\n\tunsigned int i;\n\n\ts = (void *)src;\n\td = (void *)dst;\n\n\tswitch (priv->size) {\n\tcase 8: {\n\t\tu64 src64;\n\n\t\tswitch (priv->op) {\n\t\tcase NFT_BYTEORDER_NTOH:\n\t\t\tfor (i = 0; i < priv->len / 8; i++) {\n\t\t\t\tsrc64 = nft_reg_load64(&src[i]);\n\t\t\t\tnft_reg_store64(&dst[i],\n\t\t\t\t\t\tbe64_to_cpu((__force __be64)src64));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_BYTEORDER_HTON:\n\t\t\tfor (i = 0; i < priv->len / 8; i++) {\n\t\t\t\tsrc64 = (__force __u64)\n\t\t\t\t\tcpu_to_be64(nft_reg_load64(&src[i]));\n\t\t\t\tnft_reg_store64(&dst[i], src64);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 4:\n\t\tswitch (priv->op) {\n\t\tcase NFT_BYTEORDER_NTOH:\n\t\t\tfor (i = 0; i < priv->len / 4; i++)\n\t\t\t\td[i].u32 = ntohl((__force __be32)s[i].u32);\n\t\t\tbreak;\n\t\tcase NFT_BYTEORDER_HTON:\n\t\t\tfor (i = 0; i < priv->len / 4; i++)\n\t\t\t\td[i].u32 = (__force __u32)htonl(s[i].u32);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\tswitch (priv->op) {\n\t\tcase NFT_BYTEORDER_NTOH:\n\t\t\tfor (i = 0; i < priv->len / 2; i++)\n\t\t\t\td[i].u16 = ntohs((__force __be16)s[i].u16);\n\t\t\tbreak;\n\t\tcase NFT_BYTEORDER_HTON:\n\t\t\tfor (i = 0; i < priv->len / 2; i++)\n\t\t\t\td[i].u16 = (__force __u16)htons(s[i].u16);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n}",
      "code_after_change": "void nft_byteorder_eval(const struct nft_expr *expr,\n\t\t\tstruct nft_regs *regs,\n\t\t\tconst struct nft_pktinfo *pkt)\n{\n\tconst struct nft_byteorder *priv = nft_expr_priv(expr);\n\tu32 *src = &regs->data[priv->sreg];\n\tu32 *dst = &regs->data[priv->dreg];\n\tu16 *s16, *d16;\n\tunsigned int i;\n\n\ts16 = (void *)src;\n\td16 = (void *)dst;\n\n\tswitch (priv->size) {\n\tcase 8: {\n\t\tu64 src64;\n\n\t\tswitch (priv->op) {\n\t\tcase NFT_BYTEORDER_NTOH:\n\t\t\tfor (i = 0; i < priv->len / 8; i++) {\n\t\t\t\tsrc64 = nft_reg_load64(&src[i]);\n\t\t\t\tnft_reg_store64(&dst[i],\n\t\t\t\t\t\tbe64_to_cpu((__force __be64)src64));\n\t\t\t}\n\t\t\tbreak;\n\t\tcase NFT_BYTEORDER_HTON:\n\t\t\tfor (i = 0; i < priv->len / 8; i++) {\n\t\t\t\tsrc64 = (__force __u64)\n\t\t\t\t\tcpu_to_be64(nft_reg_load64(&src[i]));\n\t\t\t\tnft_reg_store64(&dst[i], src64);\n\t\t\t}\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n\tcase 4:\n\t\tswitch (priv->op) {\n\t\tcase NFT_BYTEORDER_NTOH:\n\t\t\tfor (i = 0; i < priv->len / 4; i++)\n\t\t\t\tdst[i] = ntohl((__force __be32)src[i]);\n\t\t\tbreak;\n\t\tcase NFT_BYTEORDER_HTON:\n\t\t\tfor (i = 0; i < priv->len / 4; i++)\n\t\t\t\tdst[i] = (__force __u32)htonl(src[i]);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\tcase 2:\n\t\tswitch (priv->op) {\n\t\tcase NFT_BYTEORDER_NTOH:\n\t\t\tfor (i = 0; i < priv->len / 2; i++)\n\t\t\t\td16[i] = ntohs((__force __be16)s16[i]);\n\t\t\tbreak;\n\t\tcase NFT_BYTEORDER_HTON:\n\t\t\tfor (i = 0; i < priv->len / 2; i++)\n\t\t\t\td16[i] = (__force __u16)htons(s16[i]);\n\t\t\tbreak;\n\t\t}\n\t\tbreak;\n\t}\n}",
      "modified_lines": {
        "added": [
          "\tu16 *s16, *d16;",
          "\ts16 = (void *)src;",
          "\td16 = (void *)dst;",
          "\t\t\t\tdst[i] = ntohl((__force __be32)src[i]);",
          "\t\t\t\tdst[i] = (__force __u32)htonl(src[i]);",
          "\t\t\t\td16[i] = ntohs((__force __be16)s16[i]);",
          "\t\t\t\td16[i] = (__force __u16)htons(s16[i]);"
        ],
        "deleted": [
          "\tunion { u32 u32; u16 u16; } *s, *d;",
          "\ts = (void *)src;",
          "\td = (void *)dst;",
          "\t\t\t\td[i].u32 = ntohl((__force __be32)s[i].u32);",
          "\t\t\t\td[i].u32 = (__force __u32)htonl(s[i].u32);",
          "\t\t\t\td[i].u16 = ntohs((__force __be16)s[i].u16);",
          "\t\t\t\td[i].u16 = (__force __u16)htons(s[i].u16);"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of vm register contents in the nft_byteorder function when CAP_NET_ADMIN is present in any user or network namespace.",
      "trigger_condition": "The code performs out-of-bounds read/write operations due to incorrectly handling 32-bit data in switch cases for 2-byte operations.",
      "specific_code_behavior_causing_vulnerability": "The code uses u32 pointers to handle 16-bit data in switch cases for 2-byte operations, leading to out-of-bounds read/write vulnerabilities. This vulnerability arises when CAP_NET_ADMIN is present in any user or network namespace, causing improper handling of vm register contents.",
      "id": 160,
      "code_after_change_normalized": "void FUN1(const struct nft_expr *VAR1,\nstruct nft_regs *VAR2,\nconst struct nft_pktinfo *VAR3)\n{\nconst struct nft_byteorder *VAR4 = FUN2(VAR1);\nu32 *VAR5 = &VAR2->VAR6[VAR4->VAR7];\nu32 *VAR8 = &VAR2->VAR6[VAR4->VAR9];\nu16 *VAR10, *VAR11;\nunsigned int VAR12;\nVAR10 = (void *)VAR5;\nVAR11 = (void *)VAR8;\nswitch (VAR4->VAR13) {\ncase 8: {\nu64 VAR14;\nswitch (VAR4->VAR15) {\ncase VAR16:\nfor (VAR12 = 0; VAR12 < VAR4->VAR17 / 8; VAR12++) {\nVAR14 = FUN3(&VAR5[VAR12]);\nFUN4(&VAR8[VAR12],\nFUN5((__force VAR18)VAR14));\n}\nbreak;\ncase VAR19:\nfor (VAR12 = 0; VAR12 < VAR4->VAR17 / 8; VAR12++) {\nVAR14 = (__force VAR20)\nFUN6(FUN3(&VAR5[VAR12]));\nFUN4(&VAR8[VAR12], VAR14);\n}\nbreak;\n}\nbreak;\n}\ncase 4:\nswitch (VAR4->VAR15) {\ncase VAR16:\nfor (VAR12 = 0; VAR12 < VAR4->VAR17 / 4; VAR12++)\nVAR8[VAR12] = FUN7((__force VAR21)VAR5[VAR12]);\nbreak;\ncase VAR19:\nfor (VAR12 = 0; VAR12 < VAR4->VAR17 / 4; VAR12++)\nVAR8[VAR12] = (__force VAR22)FUN8(VAR5[VAR12]);\nbreak;\n}\nbreak;\ncase 2:\nswitch (VAR4->VAR15) {\ncase VAR16:\nfor (VAR12 = 0; VAR12 < VAR4->VAR17 / 2; VAR12++)\nVAR11[VAR12] = FUN9((__force VAR23)VAR10[VAR12]);\nbreak;\ncase VAR19:\nfor (VAR12 = 0; VAR12 < VAR4->VAR17 / 2; VAR12++)\nVAR11[VAR12] = (__force VAR24)FUN10(VAR10[VAR12]);\nbreak;\n}\nbreak;\n}\n}\n",
      "code_before_change_normalized": "void FUN1(const struct nft_expr *VAR1,\nstruct nft_regs *VAR2,\nconst struct nft_pktinfo *VAR3)\n{\nconst struct nft_byteorder *VAR4 = FUN2(VAR1);\nu32 *VAR5 = &VAR2->VAR6[VAR4->VAR7];\nu32 *VAR8 = &VAR2->VAR6[VAR4->VAR9];\nunion { u32 VAR10; u16 VAR11; } *VAR12, *VAR13;\nunsigned int VAR14;\nVAR12 = (void *)VAR5;\nVAR13 = (void *)VAR8;\nswitch (VAR4->VAR15) {\ncase 8: {\nu64 VAR16;\nswitch (VAR4->VAR17) {\ncase VAR18:\nfor (VAR14 = 0; VAR14 < VAR4->VAR19 / 8; VAR14++) {\nVAR16 = FUN3(&VAR5[VAR14]);\nFUN4(&VAR8[VAR14],\nFUN5((__force VAR20)VAR16));\n}\nbreak;\ncase VAR21:\nfor (VAR14 = 0; VAR14 < VAR4->VAR19 / 8; VAR14++) {\nVAR16 = (__force VAR22)\nFUN6(FUN3(&VAR5[VAR14]));\nFUN4(&VAR8[VAR14], VAR16);\n}\nbreak;\n}\nbreak;\n}\ncase 4:\nswitch (VAR4->VAR17) {\ncase VAR18:\nfor (VAR14 = 0; VAR14 < VAR4->VAR19 / 4; VAR14++)\nVAR13[VAR14].VAR10 = FUN7((__force VAR23)VAR12[VAR14].VAR10);\nbreak;\ncase VAR21:\nfor (VAR14 = 0; VAR14 < VAR4->VAR19 / 4; VAR14++)\nVAR13[VAR14].VAR10 = (__force VAR24)FUN8(VAR12[VAR14].VAR10);\nbreak;\n}\nbreak;\ncase 2:\nswitch (VAR4->VAR17) {\ncase VAR18:\nfor (VAR14 = 0; VAR14 < VAR4->VAR19 / 2; VAR14++)\nVAR13[VAR14].VAR11 = FUN9((__force VAR25)VAR12[VAR14].VAR11);\nbreak;\ncase VAR21:\nfor (VAR14 = 0; VAR14 < VAR4->VAR19 / 2; VAR14++)\nVAR13[VAR14].VAR11 = (__force VAR26)FUN10(VAR12[VAR14].VAR11);\nbreak;\n}\nbreak;\n}\n}\n",
      "code_after_change_raw": "void nft_byteorder_eval(const struct nft_expr *expr,\nstruct nft_regs *regs,\nconst struct nft_pktinfo *pkt)\n{\nconst struct nft_byteorder *priv = nft_expr_priv(expr);\nu32 *src = &regs->data[priv->sreg];\nu32 *dst = &regs->data[priv->dreg];\nu16 *s16, *d16;\nunsigned int i;\ns16 = (void *)src;\nd16 = (void *)dst;\nswitch (priv->size) {\ncase 8: {\nu64 src64;\nswitch (priv->op) {\ncase NFT_BYTEORDER_NTOH:\nfor (i = 0; i < priv->len / 8; i++) {\nsrc64 = nft_reg_load64(&src[i]);\nnft_reg_store64(&dst[i],\nbe64_to_cpu((__force __be64)src64));\n}\nbreak;\ncase NFT_BYTEORDER_HTON:\nfor (i = 0; i < priv->len / 8; i++) {\nsrc64 = (__force __u64)\ncpu_to_be64(nft_reg_load64(&src[i]));\nnft_reg_store64(&dst[i], src64);\n}\nbreak;\n}\nbreak;\n}\ncase 4:\nswitch (priv->op) {\ncase NFT_BYTEORDER_NTOH:\nfor (i = 0; i < priv->len / 4; i++)\ndst[i] = ntohl((__force __be32)src[i]);\nbreak;\ncase NFT_BYTEORDER_HTON:\nfor (i = 0; i < priv->len / 4; i++)\ndst[i] = (__force __u32)htonl(src[i]);\nbreak;\n}\nbreak;\ncase 2:\nswitch (priv->op) {\ncase NFT_BYTEORDER_NTOH:\nfor (i = 0; i < priv->len / 2; i++)\nd16[i] = ntohs((__force __be16)s16[i]);\nbreak;\ncase NFT_BYTEORDER_HTON:\nfor (i = 0; i < priv->len / 2; i++)\nd16[i] = (__force __u16)htons(s16[i]);\nbreak;\n}\nbreak;\n}\n}\n",
      "code_before_change_raw": "void nft_byteorder_eval(const struct nft_expr *expr,\nstruct nft_regs *regs,\nconst struct nft_pktinfo *pkt)\n{\nconst struct nft_byteorder *priv = nft_expr_priv(expr);\nu32 *src = &regs->data[priv->sreg];\nu32 *dst = &regs->data[priv->dreg];\nunion { u32 u32; u16 u16; } *s, *d;\nunsigned int i;\ns = (void *)src;\nd = (void *)dst;\nswitch (priv->size) {\ncase 8: {\nu64 src64;\nswitch (priv->op) {\ncase NFT_BYTEORDER_NTOH:\nfor (i = 0; i < priv->len / 8; i++) {\nsrc64 = nft_reg_load64(&src[i]);\nnft_reg_store64(&dst[i],\nbe64_to_cpu((__force __be64)src64));\n}\nbreak;\ncase NFT_BYTEORDER_HTON:\nfor (i = 0; i < priv->len / 8; i++) {\nsrc64 = (__force __u64)\ncpu_to_be64(nft_reg_load64(&src[i]));\nnft_reg_store64(&dst[i], src64);\n}\nbreak;\n}\nbreak;\n}\ncase 4:\nswitch (priv->op) {\ncase NFT_BYTEORDER_NTOH:\nfor (i = 0; i < priv->len / 4; i++)\nd[i].u32 = ntohl((__force __be32)s[i].u32);\nbreak;\ncase NFT_BYTEORDER_HTON:\nfor (i = 0; i < priv->len / 4; i++)\nd[i].u32 = (__force __u32)htonl(s[i].u32);\nbreak;\n}\nbreak;\ncase 2:\nswitch (priv->op) {\ncase NFT_BYTEORDER_NTOH:\nfor (i = 0; i < priv->len / 2; i++)\nd[i].u16 = ntohs((__force __be16)s[i].u16);\nbreak;\ncase NFT_BYTEORDER_HTON:\nfor (i = 0; i < priv->len / 2; i++)\nd[i].u16 = (__force __u16)htons(s[i].u16);\nbreak;\n}\nbreak;\n}\n}\n"
    }
  ],
  "CVE-2023-35788": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of boundary check for writing data to a buffer, specifically in the fl_set_geneve_opt function.",
        "trigger_condition": "Receiving TCA_FLOWER_KEY_ENC_OPTS_GENEVE packets with an option_len value that exceeds the size of the geneve_opt structure, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code writes data to the key->enc_opts.data buffer without verifying the boundaries, allowing potential out-of-bounds writes if the option_len is larger than the geneve_opt structure size."
      },
      "solution": "To mitigate the vulnerability, a boundary check should be added to ensure that the write operation stays within the bounds of the buffer. By checking if key->enc_opts.len is greater than FLOW_DIS_TUN_OPTS_MAX - 4 before writing data, the code prevents out-of-bounds writes and improves security.",
      "GPT_analysis": "The modification is necessary to prevent the out-of-bounds write vulnerability in the code snippet. The vulnerability occurs due to the unchecked boundary conditions when writing data to the `key->enc_opts.data` buffer. By adding the check `if (key->enc_opts.len > FLOW_DIS_TUN_OPTS_MAX - 4)`, the code ensures that the write operation stays within the bounds of the buffer, thus preventing potential memory corruption issues that could lead to denial of service or privilege escalation.\n\nIn summary, the modification adds a boundary check to prevent writing beyond the allocated buffer size, thereby addressing the vulnerability and improving the security of the code.",
      "GPT_purpose": "Set Geneve options for a flow key in the flower classifier.",
      "GPT_function": "\n1. Parse and set Geneve options for a flow key.\n2. Handle different cases for the Geneve options, including length and data validation.\n3. Check for missing required fields in the Geneve options.\n4. Write the Geneve options data into the flow key.\n5. Return the total size of the Geneve options.",
      "CVE_id": "CVE-2023-35788",
      "code_before_change": "static int fl_set_geneve_opt(const struct nlattr *nla, struct fl_flow_key *key,\n\t\t\t     int depth, int option_len,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct nlattr *tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1];\n\tstruct nlattr *class = NULL, *type = NULL, *data = NULL;\n\tstruct geneve_opt *opt;\n\tint err, data_len = 0;\n\n\tif (option_len > sizeof(struct geneve_opt))\n\t\tdata_len = option_len - sizeof(struct geneve_opt);\n\n\topt = (struct geneve_opt *)&key->enc_opts.data[key->enc_opts.len];\n\tmemset(opt, 0xff, option_len);\n\topt->length = data_len / 4;\n\topt->r1 = 0;\n\topt->r2 = 0;\n\topt->r3 = 0;\n\n\t/* If no mask has been prodived we assume an exact match. */\n\tif (!depth)\n\t\treturn sizeof(struct geneve_opt) + data_len;\n\n\tif (nla_type(nla) != TCA_FLOWER_KEY_ENC_OPTS_GENEVE) {\n\t\tNL_SET_ERR_MSG(extack, \"Non-geneve option type for mask\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb,\n\t\t\t\t\t  TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX,\n\t\t\t\t\t  nla, geneve_opt_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* We are not allowed to omit any of CLASS, TYPE or DATA\n\t * fields from the key.\n\t */\n\tif (!option_len &&\n\t    (!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS] ||\n\t     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE] ||\n\t     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA])) {\n\t\tNL_SET_ERR_MSG(extack, \"Missing tunnel key geneve option class, type or data\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Omitting any of CLASS, TYPE or DATA fields is allowed\n\t * for the mask.\n\t */\n\tif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]) {\n\t\tint new_len = key->enc_opts.len;\n\n\t\tdata = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA];\n\t\tdata_len = nla_len(data);\n\t\tif (data_len < 4) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is less than 4 bytes long\");\n\t\t\treturn -ERANGE;\n\t\t}\n\t\tif (data_len % 4) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is not a multiple of 4 bytes long\");\n\t\t\treturn -ERANGE;\n\t\t}\n\n\t\tnew_len += sizeof(struct geneve_opt) + data_len;\n\t\tBUILD_BUG_ON(FLOW_DIS_TUN_OPTS_MAX != IP_TUNNEL_OPTS_MAX);\n\t\tif (new_len > FLOW_DIS_TUN_OPTS_MAX) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Tunnel options exceeds max size\");\n\t\t\treturn -ERANGE;\n\t\t}\n\t\topt->length = data_len / 4;\n\t\tmemcpy(opt->opt_data, nla_data(data), data_len);\n\t}\n\n\tif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]) {\n\t\tclass = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS];\n\t\topt->opt_class = nla_get_be16(class);\n\t}\n\n\tif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]) {\n\t\ttype = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE];\n\t\topt->type = nla_get_u8(type);\n\t}\n\n\treturn sizeof(struct geneve_opt) + data_len;\n}",
      "code_after_change": "static int fl_set_geneve_opt(const struct nlattr *nla, struct fl_flow_key *key,\n\t\t\t     int depth, int option_len,\n\t\t\t     struct netlink_ext_ack *extack)\n{\n\tstruct nlattr *tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1];\n\tstruct nlattr *class = NULL, *type = NULL, *data = NULL;\n\tstruct geneve_opt *opt;\n\tint err, data_len = 0;\n\n\tif (option_len > sizeof(struct geneve_opt))\n\t\tdata_len = option_len - sizeof(struct geneve_opt);\n\n\tif (key->enc_opts.len > FLOW_DIS_TUN_OPTS_MAX - 4)\n\t\treturn -ERANGE;\n\n\topt = (struct geneve_opt *)&key->enc_opts.data[key->enc_opts.len];\n\tmemset(opt, 0xff, option_len);\n\topt->length = data_len / 4;\n\topt->r1 = 0;\n\topt->r2 = 0;\n\topt->r3 = 0;\n\n\t/* If no mask has been prodived we assume an exact match. */\n\tif (!depth)\n\t\treturn sizeof(struct geneve_opt) + data_len;\n\n\tif (nla_type(nla) != TCA_FLOWER_KEY_ENC_OPTS_GENEVE) {\n\t\tNL_SET_ERR_MSG(extack, \"Non-geneve option type for mask\");\n\t\treturn -EINVAL;\n\t}\n\n\terr = nla_parse_nested_deprecated(tb,\n\t\t\t\t\t  TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX,\n\t\t\t\t\t  nla, geneve_opt_policy, extack);\n\tif (err < 0)\n\t\treturn err;\n\n\t/* We are not allowed to omit any of CLASS, TYPE or DATA\n\t * fields from the key.\n\t */\n\tif (!option_len &&\n\t    (!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS] ||\n\t     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE] ||\n\t     !tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA])) {\n\t\tNL_SET_ERR_MSG(extack, \"Missing tunnel key geneve option class, type or data\");\n\t\treturn -EINVAL;\n\t}\n\n\t/* Omitting any of CLASS, TYPE or DATA fields is allowed\n\t * for the mask.\n\t */\n\tif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]) {\n\t\tint new_len = key->enc_opts.len;\n\n\t\tdata = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA];\n\t\tdata_len = nla_len(data);\n\t\tif (data_len < 4) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is less than 4 bytes long\");\n\t\t\treturn -ERANGE;\n\t\t}\n\t\tif (data_len % 4) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is not a multiple of 4 bytes long\");\n\t\t\treturn -ERANGE;\n\t\t}\n\n\t\tnew_len += sizeof(struct geneve_opt) + data_len;\n\t\tBUILD_BUG_ON(FLOW_DIS_TUN_OPTS_MAX != IP_TUNNEL_OPTS_MAX);\n\t\tif (new_len > FLOW_DIS_TUN_OPTS_MAX) {\n\t\t\tNL_SET_ERR_MSG(extack, \"Tunnel options exceeds max size\");\n\t\t\treturn -ERANGE;\n\t\t}\n\t\topt->length = data_len / 4;\n\t\tmemcpy(opt->opt_data, nla_data(data), data_len);\n\t}\n\n\tif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]) {\n\t\tclass = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS];\n\t\topt->opt_class = nla_get_be16(class);\n\t}\n\n\tif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]) {\n\t\ttype = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE];\n\t\topt->type = nla_get_u8(type);\n\t}\n\n\treturn sizeof(struct geneve_opt) + data_len;\n}",
      "modified_lines": {
        "added": [
          "",
          "\tif (key->enc_opts.len > FLOW_DIS_TUN_OPTS_MAX - 4)",
          "\t\treturn -ERANGE;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of boundary check for writing data to a buffer, specifically in the fl_set_geneve_opt function.",
      "trigger_condition": "Receiving TCA_FLOWER_KEY_ENC_OPTS_GENEVE packets with an option_len value that exceeds the size of the geneve_opt structure, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code writes data to the key->enc_opts.data buffer without verifying the boundaries, allowing potential out-of-bounds writes if the option_len is larger than the geneve_opt structure size.",
      "id": 161,
      "code_after_change_normalized": "static int FUN1(const struct nlattr *VAR1, struct fl_flow_key *VAR2,\nint VAR3, int VAR4,\nstruct netlink_ext_ack *VAR5)\n{\nstruct nlattr *VAR6[VAR7 + 1];\nstruct nlattr *class = NULL, *VAR8 = NULL, *VAR9 = NULL;\nstruct geneve_opt *VAR10;\nint VAR11, VAR12 = 0;\nif (VAR4 > sizeof(struct VAR13))\nVAR12 = VAR4 - sizeof(struct VAR13);\nif (VAR2->VAR14.VAR15 > VAR16 - 4)\nreturn -VAR17;\nVAR10 = (struct VAR13 *)&VAR2->VAR14.VAR9[VAR2->VAR14.VAR15];\nFUN2(VAR10, VAR18, VAR4);\nVAR10->VAR19 = VAR12 / 4;\nVAR10->VAR20 = 0;\nVAR10->VAR21 = 0;\nVAR10->VAR22 = 0;\nif (!VAR3)\nreturn sizeof(struct VAR13) + VAR12;\nif (FUN3(VAR1) != VAR23) {\nFUN4(VAR5, \"STR\");\nreturn -VAR24;\n}\nVAR11 = FUN5(VAR6,\nVAR7,\nVAR1, VAR25, VAR5);\nif (VAR11 < 0)\nreturn VAR11;\nif (!VAR4 &&\n(!VAR6[VAR26] ||\n!VAR6[VAR27] ||\n!VAR6[VAR28])) {\nFUN4(VAR5, \"STR\");\nreturn -VAR24;\n}\nif (VAR6[VAR28]) {\nint VAR29 = VAR2->VAR14.VAR15;\nVAR9 = VAR6[VAR28];\nVAR12 = FUN6(VAR9);\nif (VAR12 < 4) {\nFUN4(VAR5, \"STR\");\nreturn -VAR17;\n}\nif (VAR12 % 4) {\nFUN4(VAR5, \"STR\");\nreturn -VAR17;\n}\nVAR29 += sizeof(struct VAR13) + VAR12;\nFUN7(VAR16 != VAR30);\nif (VAR29 > VAR16) {\nFUN4(VAR5, \"STR\");\nreturn -VAR17;\n}\nVAR10->VAR19 = VAR12 / 4;\nFUN8(VAR10->VAR31, FUN9(VAR9), VAR12);\n}\nif (VAR6[VAR26]) {\nclass = VAR6[VAR26];\nVAR10->VAR32 = FUN10(class);\n}\nif (VAR6[VAR27]) {\nVAR8 = VAR6[VAR27];\nVAR10->VAR8 = FUN11(VAR8);\n}\nreturn sizeof(struct VAR13) + VAR12;\n}\n",
      "code_before_change_normalized": "static int FUN1(const struct nlattr *VAR1, struct fl_flow_key *VAR2,\nint VAR3, int VAR4,\nstruct netlink_ext_ack *VAR5)\n{\nstruct nlattr *VAR6[VAR7 + 1];\nstruct nlattr *class = NULL, *VAR8 = NULL, *VAR9 = NULL;\nstruct geneve_opt *VAR10;\nint VAR11, VAR12 = 0;\nif (VAR4 > sizeof(struct VAR13))\nVAR12 = VAR4 - sizeof(struct VAR13);\nVAR10 = (struct VAR13 *)&VAR2->VAR14.VAR9[VAR2->VAR14.VAR15];\nFUN2(VAR10, VAR16, VAR4);\nVAR10->VAR17 = VAR12 / 4;\nVAR10->VAR18 = 0;\nVAR10->VAR19 = 0;\nVAR10->VAR20 = 0;\nif (!VAR3)\nreturn sizeof(struct VAR13) + VAR12;\nif (FUN3(VAR1) != VAR21) {\nFUN4(VAR5, \"STR\");\nreturn -VAR22;\n}\nVAR11 = FUN5(VAR6,\nVAR7,\nVAR1, VAR23, VAR5);\nif (VAR11 < 0)\nreturn VAR11;\nif (!VAR4 &&\n(!VAR6[VAR24] ||\n!VAR6[VAR25] ||\n!VAR6[VAR26])) {\nFUN4(VAR5, \"STR\");\nreturn -VAR22;\n}\nif (VAR6[VAR26]) {\nint VAR27 = VAR2->VAR14.VAR15;\nVAR9 = VAR6[VAR26];\nVAR12 = FUN6(VAR9);\nif (VAR12 < 4) {\nFUN4(VAR5, \"STR\");\nreturn -VAR28;\n}\nif (VAR12 % 4) {\nFUN4(VAR5, \"STR\");\nreturn -VAR28;\n}\nVAR27 += sizeof(struct VAR13) + VAR12;\nFUN7(VAR29 != VAR30);\nif (VAR27 > VAR29) {\nFUN4(VAR5, \"STR\");\nreturn -VAR28;\n}\nVAR10->VAR17 = VAR12 / 4;\nFUN8(VAR10->VAR31, FUN9(VAR9), VAR12);\n}\nif (VAR6[VAR24]) {\nclass = VAR6[VAR24];\nVAR10->VAR32 = FUN10(class);\n}\nif (VAR6[VAR25]) {\nVAR8 = VAR6[VAR25];\nVAR10->VAR8 = FUN11(VAR8);\n}\nreturn sizeof(struct VAR13) + VAR12;\n}\n",
      "code_after_change_raw": "static int fl_set_geneve_opt(const struct nlattr *nla, struct fl_flow_key *key,\nint depth, int option_len,\nstruct netlink_ext_ack *extack)\n{\nstruct nlattr *tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1];\nstruct nlattr *class = NULL, *type = NULL, *data = NULL;\nstruct geneve_opt *opt;\nint err, data_len = 0;\nif (option_len > sizeof(struct geneve_opt))\ndata_len = option_len - sizeof(struct geneve_opt);\nif (key->enc_opts.len > FLOW_DIS_TUN_OPTS_MAX - 4)\nreturn -ERANGE;\nopt = (struct geneve_opt *)&key->enc_opts.data[key->enc_opts.len];\nmemset(opt, 0xff, option_len);\nopt->length = data_len / 4;\nopt->r1 = 0;\nopt->r2 = 0;\nopt->r3 = 0;\nif (!depth)\nreturn sizeof(struct geneve_opt) + data_len;\nif (nla_type(nla) != TCA_FLOWER_KEY_ENC_OPTS_GENEVE) {\nNL_SET_ERR_MSG(extack, \"Non-geneve option type for mask\");\nreturn -EINVAL;\n}\nerr = nla_parse_nested_deprecated(tb,\nTCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX,\nnla, geneve_opt_policy, extack);\nif (err < 0)\nreturn err;\nif (!option_len &&\n(!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS] ||\n!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE] ||\n!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA])) {\nNL_SET_ERR_MSG(extack, \"Missing tunnel key geneve option class, type or data\");\nreturn -EINVAL;\n}\nif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]) {\nint new_len = key->enc_opts.len;\ndata = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA];\ndata_len = nla_len(data);\nif (data_len < 4) {\nNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is less than 4 bytes long\");\nreturn -ERANGE;\n}\nif (data_len % 4) {\nNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is not a multiple of 4 bytes long\");\nreturn -ERANGE;\n}\nnew_len += sizeof(struct geneve_opt) + data_len;\nBUILD_BUG_ON(FLOW_DIS_TUN_OPTS_MAX != IP_TUNNEL_OPTS_MAX);\nif (new_len > FLOW_DIS_TUN_OPTS_MAX) {\nNL_SET_ERR_MSG(extack, \"Tunnel options exceeds max size\");\nreturn -ERANGE;\n}\nopt->length = data_len / 4;\nmemcpy(opt->opt_data, nla_data(data), data_len);\n}\nif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]) {\nclass = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS];\nopt->opt_class = nla_get_be16(class);\n}\nif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]) {\ntype = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE];\nopt->type = nla_get_u8(type);\n}\nreturn sizeof(struct geneve_opt) + data_len;\n}\n",
      "code_before_change_raw": "static int fl_set_geneve_opt(const struct nlattr *nla, struct fl_flow_key *key,\nint depth, int option_len,\nstruct netlink_ext_ack *extack)\n{\nstruct nlattr *tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX + 1];\nstruct nlattr *class = NULL, *type = NULL, *data = NULL;\nstruct geneve_opt *opt;\nint err, data_len = 0;\nif (option_len > sizeof(struct geneve_opt))\ndata_len = option_len - sizeof(struct geneve_opt);\nopt = (struct geneve_opt *)&key->enc_opts.data[key->enc_opts.len];\nmemset(opt, 0xff, option_len);\nopt->length = data_len / 4;\nopt->r1 = 0;\nopt->r2 = 0;\nopt->r3 = 0;\nif (!depth)\nreturn sizeof(struct geneve_opt) + data_len;\nif (nla_type(nla) != TCA_FLOWER_KEY_ENC_OPTS_GENEVE) {\nNL_SET_ERR_MSG(extack, \"Non-geneve option type for mask\");\nreturn -EINVAL;\n}\nerr = nla_parse_nested_deprecated(tb,\nTCA_FLOWER_KEY_ENC_OPT_GENEVE_MAX,\nnla, geneve_opt_policy, extack);\nif (err < 0)\nreturn err;\nif (!option_len &&\n(!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS] ||\n!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE] ||\n!tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA])) {\nNL_SET_ERR_MSG(extack, \"Missing tunnel key geneve option class, type or data\");\nreturn -EINVAL;\n}\nif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA]) {\nint new_len = key->enc_opts.len;\ndata = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_DATA];\ndata_len = nla_len(data);\nif (data_len < 4) {\nNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is less than 4 bytes long\");\nreturn -ERANGE;\n}\nif (data_len % 4) {\nNL_SET_ERR_MSG(extack, \"Tunnel key geneve option data is not a multiple of 4 bytes long\");\nreturn -ERANGE;\n}\nnew_len += sizeof(struct geneve_opt) + data_len;\nBUILD_BUG_ON(FLOW_DIS_TUN_OPTS_MAX != IP_TUNNEL_OPTS_MAX);\nif (new_len > FLOW_DIS_TUN_OPTS_MAX) {\nNL_SET_ERR_MSG(extack, \"Tunnel options exceeds max size\");\nreturn -ERANGE;\n}\nopt->length = data_len / 4;\nmemcpy(opt->opt_data, nla_data(data), data_len);\n}\nif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS]) {\nclass = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_CLASS];\nopt->opt_class = nla_get_be16(class);\n}\nif (tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE]) {\ntype = tb[TCA_FLOWER_KEY_ENC_OPT_GENEVE_TYPE];\nopt->type = nla_get_u8(type);\n}\nreturn sizeof(struct geneve_opt) + data_len;\n}\n"
    }
  ],
  "CVE-2023-3611": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of bounds check for the 'lmax' parameter in the qfq_change_agg() function.",
        "trigger_condition": "The 'lmax' parameter is updated based on packet sizes without proper validation, allowing it to exceed the maximum limit.",
        "specific_code_behavior_causing_vulnerability": "The code does not verify that the 'lmax' parameter falls within a valid range, leading to a potential out-of-bounds write vulnerability. This can be exploited to achieve local privilege escalation."
      },
      "solution": "To mitigate the vulnerability, a bounds check should be added to ensure that the 'lmax' parameter does not exceed the maximum limit (QFQ_MAX_LMAX). By validating the range of 'lmax', the code can prevent out-of-bounds writes and enhance security. The modified code includes this bounds check to address the vulnerability and prevent potential exploitation.",
      "GPT_analysis": "The modification is necessary to prevent the out-of-bounds write vulnerability in the qfq_change_agg() function. By adding the bounds check for lmax to ensure it does not exceed QFQ_MAX_LMAX, we can prevent the vulnerability from being exploited. This check ensures that the value of lmax falls within a valid range, preventing any potential out-of-bounds writes based on packet sizes without proper bounds checks. This modification helps to enhance the security of the code and prevent potential local privilege escalation attacks.",
      "GPT_purpose": "Modify the aggregation parameters of a QFQ class within a Qdisc structure.",
      "GPT_function": "\n1. Modify the weight and lmax parameters of a qfq_class.\n2. Find or create a new qfq_aggregate based on the given weight and lmax.\n3. Add the qfq_class to the qfq_aggregate.",
      "CVE_id": "CVE-2023-3611",
      "code_before_change": "static int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,\n\t\t\t   u32 lmax)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *new_agg = qfq_find_agg(q, lmax, weight);\n\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);\n\t\tif (new_agg == NULL)\n\t\t\treturn -ENOBUFS;\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tqfq_deact_rm_from_agg(q, cl);\n\tqfq_add_to_agg(q, new_agg, cl);\n\n\treturn 0;\n}",
      "code_after_change": "static int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,\n\t\t\t   u32 lmax)\n{\n\tstruct qfq_sched *q = qdisc_priv(sch);\n\tstruct qfq_aggregate *new_agg;\n\n\t/* 'lmax' can range from [QFQ_MIN_LMAX, pktlen + stab overhead] */\n\tif (lmax > QFQ_MAX_LMAX)\n\t\treturn -EINVAL;\n\n\tnew_agg = qfq_find_agg(q, lmax, weight);\n\tif (new_agg == NULL) { /* create new aggregate */\n\t\tnew_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);\n\t\tif (new_agg == NULL)\n\t\t\treturn -ENOBUFS;\n\t\tqfq_init_agg(q, new_agg, lmax, weight);\n\t}\n\tqfq_deact_rm_from_agg(q, cl);\n\tqfq_add_to_agg(q, new_agg, cl);\n\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tstruct qfq_aggregate *new_agg;",
          "\t/* 'lmax' can range from [QFQ_MIN_LMAX, pktlen + stab overhead] */",
          "\tif (lmax > QFQ_MAX_LMAX)",
          "\t\treturn -EINVAL;",
          "",
          "\tnew_agg = qfq_find_agg(q, lmax, weight);"
        ],
        "deleted": [
          "\tstruct qfq_aggregate *new_agg = qfq_find_agg(q, lmax, weight);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of bounds check for the 'lmax' parameter in the qfq_change_agg() function.",
      "trigger_condition": "The 'lmax' parameter is updated based on packet sizes without proper validation, allowing it to exceed the maximum limit.",
      "specific_code_behavior_causing_vulnerability": "The code does not verify that the 'lmax' parameter falls within a valid range, leading to a potential out-of-bounds write vulnerability. This can be exploited to achieve local privilege escalation.",
      "id": 162,
      "code_after_change_normalized": "static int FUN1(struct Qdisc *VAR1, struct qfq_class *VAR2, u32 VAR3,\nu32 VAR4)\n{\nstruct qfq_sched *VAR5 = FUN2(VAR1);\nstruct qfq_aggregate *VAR6;\nif (VAR4 > VAR7)\nreturn -VAR8;\nVAR6 = FUN3(VAR5, VAR4, VAR3);\nif (VAR6 == NULL) { \nVAR6 = FUN4(sizeof(*VAR6), VAR9);\nif (VAR6 == NULL)\nreturn -VAR10;\nFUN5(VAR5, VAR6, VAR4, VAR3);\n}\nFUN6(VAR5, VAR2);\nFUN7(VAR5, VAR6, VAR2);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct Qdisc *VAR1, struct qfq_class *VAR2, u32 VAR3,\nu32 VAR4)\n{\nstruct qfq_sched *VAR5 = FUN2(VAR1);\nstruct qfq_aggregate *VAR6 = FUN3(VAR5, VAR4, VAR3);\nif (VAR6 == NULL) { \nVAR6 = FUN4(sizeof(*VAR6), VAR7);\nif (VAR6 == NULL)\nreturn -VAR8;\nFUN5(VAR5, VAR6, VAR4, VAR3);\n}\nFUN6(VAR5, VAR2);\nFUN7(VAR5, VAR6, VAR2);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,\nu32 lmax)\n{\nstruct qfq_sched *q = qdisc_priv(sch);\nstruct qfq_aggregate *new_agg;\nif (lmax > QFQ_MAX_LMAX)\nreturn -EINVAL;\nnew_agg = qfq_find_agg(q, lmax, weight);\nif (new_agg == NULL) { \nnew_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);\nif (new_agg == NULL)\nreturn -ENOBUFS;\nqfq_init_agg(q, new_agg, lmax, weight);\n}\nqfq_deact_rm_from_agg(q, cl);\nqfq_add_to_agg(q, new_agg, cl);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int qfq_change_agg(struct Qdisc *sch, struct qfq_class *cl, u32 weight,\nu32 lmax)\n{\nstruct qfq_sched *q = qdisc_priv(sch);\nstruct qfq_aggregate *new_agg = qfq_find_agg(q, lmax, weight);\nif (new_agg == NULL) { \nnew_agg = kzalloc(sizeof(*new_agg), GFP_ATOMIC);\nif (new_agg == NULL)\nreturn -ENOBUFS;\nqfq_init_agg(q, new_agg, lmax, weight);\n}\nqfq_deact_rm_from_agg(q, cl);\nqfq_add_to_agg(q, new_agg, cl);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2023-3812": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper validation for packet length in the TUN/TAP device driver functionality.",
        "trigger_condition": "A local user generates a malicious packet with a length that exceeds the maximum allowed size, leading to an out-of-bounds memory access issue.",
        "specific_code_behavior_causing_vulnerability": "The code does not check the length of the packet being processed, allowing a user to create a packet that is too big and causing an out-of-bounds memory access flaw."
      },
      "solution": "To mitigate the vulnerability, it is necessary to add a check to ensure that the packet length does not exceed the maximum allowed size. This prevents the out-of-bounds memory access issue by restricting the packet size within safe limits. In this case, the solution involves adding a condition to validate the packet length against a predefined maximum size before processing the packet.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2023-3812, which involves an out-of-bounds memory access flaw in the Linux kernel's TUN/TAP device driver functionality. By adding the condition `len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN)` to the existing check `it->nr_segs > MAX_SKB_FRAGS + 1`, the code ensures that the length of the packet being processed does not exceed the maximum allowed size.\n\nThe condition `len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN)` restricts the packet length to a safe limit, preventing a user from generating a malicious packet that is too big and could lead to an out-of-bounds memory access issue. This check helps in preventing potential crashes or privilege escalation that could occur due to processing oversized packets.\n\nBy enforcing this additional length check, the modified code snippet mitigates the vulnerability by ensuring that the packet size is within acceptable bounds, thus enhancing the security and stability of the TUN/TAP device driver functionality in the Linux kernel.",
      "GPT_purpose": "Allocate and fill sk_buff fragments for a TUN/TAP device driver.",
      "GPT_function": "\n1. Allocate fragments for a TUN device.\n2. Check the number of segments in the input.\n3. Grow the size of the socket buffer (skb).\n4. Set the length and data length of the skb.\n5. Allocate and fill page descriptors for additional segments.\n6. Handle errors and free resources appropriately.",
      "CVE_id": "CVE-2023-3812",
      "code_before_change": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\n\t\t\t\t\t    size_t len,\n\t\t\t\t\t    const struct iov_iter *it)\n{\n\tstruct sk_buff *skb;\n\tsize_t linear;\n\tint err;\n\tint i;\n\n\tif (it->nr_segs > MAX_SKB_FRAGS + 1)\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tlocal_bh_disable();\n\tskb = napi_get_frags(&tfile->napi);\n\tlocal_bh_enable();\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlinear = iov_iter_single_seg_count(it);\n\terr = __skb_grow(skb, linear);\n\tif (err)\n\t\tgoto free;\n\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\tskb->truesize += skb->data_len;\n\n\tfor (i = 1; i < it->nr_segs; i++) {\n\t\tsize_t fragsz = it->iov[i].iov_len;\n\t\tstruct page *page;\n\t\tvoid *frag;\n\n\t\tif (fragsz == 0 || fragsz > PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t\tfrag = netdev_alloc_frag(fragsz);\n\t\tif (!frag) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t\tpage = virt_to_head_page(frag);\n\t\tskb_fill_page_desc(skb, i - 1, page,\n\t\t\t\t   frag - page_address(page), fragsz);\n\t}\n\n\treturn skb;\nfree:\n\t/* frees skb and all frags allocated with napi_alloc_frag() */\n\tnapi_free_frags(&tfile->napi);\n\treturn ERR_PTR(err);\n}",
      "code_after_change": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\n\t\t\t\t\t    size_t len,\n\t\t\t\t\t    const struct iov_iter *it)\n{\n\tstruct sk_buff *skb;\n\tsize_t linear;\n\tint err;\n\tint i;\n\n\tif (it->nr_segs > MAX_SKB_FRAGS + 1 ||\n\t    len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))\n\t\treturn ERR_PTR(-EMSGSIZE);\n\n\tlocal_bh_disable();\n\tskb = napi_get_frags(&tfile->napi);\n\tlocal_bh_enable();\n\tif (!skb)\n\t\treturn ERR_PTR(-ENOMEM);\n\n\tlinear = iov_iter_single_seg_count(it);\n\terr = __skb_grow(skb, linear);\n\tif (err)\n\t\tgoto free;\n\n\tskb->len = len;\n\tskb->data_len = len - linear;\n\tskb->truesize += skb->data_len;\n\n\tfor (i = 1; i < it->nr_segs; i++) {\n\t\tsize_t fragsz = it->iov[i].iov_len;\n\t\tstruct page *page;\n\t\tvoid *frag;\n\n\t\tif (fragsz == 0 || fragsz > PAGE_SIZE) {\n\t\t\terr = -EINVAL;\n\t\t\tgoto free;\n\t\t}\n\t\tfrag = netdev_alloc_frag(fragsz);\n\t\tif (!frag) {\n\t\t\terr = -ENOMEM;\n\t\t\tgoto free;\n\t\t}\n\t\tpage = virt_to_head_page(frag);\n\t\tskb_fill_page_desc(skb, i - 1, page,\n\t\t\t\t   frag - page_address(page), fragsz);\n\t}\n\n\treturn skb;\nfree:\n\t/* frees skb and all frags allocated with napi_alloc_frag() */\n\tnapi_free_frags(&tfile->napi);\n\treturn ERR_PTR(err);\n}",
      "modified_lines": {
        "added": [
          "\tif (it->nr_segs > MAX_SKB_FRAGS + 1 ||",
          "\t    len > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))"
        ],
        "deleted": [
          "\tif (it->nr_segs > MAX_SKB_FRAGS + 1)"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper validation for packet length in the TUN/TAP device driver functionality.",
      "trigger_condition": "A local user generates a malicious packet with a length that exceeds the maximum allowed size, leading to an out-of-bounds memory access issue.",
      "specific_code_behavior_causing_vulnerability": "The code does not check the length of the packet being processed, allowing a user to create a packet that is too big and causing an out-of-bounds memory access flaw.",
      "id": 163,
      "code_after_change_normalized": "static struct sk_buff *FUN1(struct tun_file *VAR1,\nsize_t VAR2,\nconst struct iov_iter *VAR3)\n{\nstruct sk_buff *VAR4;\nsize_t VAR5;\nint VAR6;\nint VAR7;\nif (VAR3->VAR8 > VAR9 + 1 ||\nVAR2 > (VAR10 - VAR11 - VAR12))\nreturn FUN2(-VAR13);\nFUN3();\nVAR4 = FUN4(&VAR1->VAR14);\nFUN5();\nif (!VAR4)\nreturn FUN2(-VAR15);\nVAR5 = FUN6(VAR3);\nVAR6 = FUN7(VAR4, VAR5);\nif (VAR6)\ngoto VAR16;\nVAR4->VAR2 = VAR2;\nVAR4->VAR17 = VAR2 - VAR5;\nVAR4->VAR18 += VAR4->VAR17;\nfor (VAR7 = 1; VAR7 < VAR3->VAR8; VAR7++) {\nsize_t VAR19 = VAR3->VAR20[VAR7].VAR21;\nstruct VAR22 *VAR22;\nvoid *VAR23;\nif (VAR19 == 0 || VAR19 > VAR24) {\nVAR6 = -VAR25;\ngoto VAR16;\n}\nVAR23 = FUN8(VAR19);\nif (!VAR23) {\nVAR6 = -VAR15;\ngoto VAR16;\n}\nVAR22 = FUN9(VAR23);\nFUN10(VAR4, VAR7 - 1, VAR22,\nVAR23 - FUN11(VAR22), VAR19);\n}\nreturn VAR4;\nVAR16:\nFUN12(&VAR1->VAR14);\nreturn FUN2(VAR6);\n}\n",
      "code_before_change_normalized": "static struct sk_buff *FUN1(struct tun_file *VAR1,\nsize_t VAR2,\nconst struct iov_iter *VAR3)\n{\nstruct sk_buff *VAR4;\nsize_t VAR5;\nint VAR6;\nint VAR7;\nif (VAR3->VAR8 > VAR9 + 1)\nreturn FUN2(-VAR10);\nFUN3();\nVAR4 = FUN4(&VAR1->VAR11);\nFUN5();\nif (!VAR4)\nreturn FUN2(-VAR12);\nVAR5 = FUN6(VAR3);\nVAR6 = FUN7(VAR4, VAR5);\nif (VAR6)\ngoto VAR13;\nVAR4->VAR2 = VAR2;\nVAR4->VAR14 = VAR2 - VAR5;\nVAR4->VAR15 += VAR4->VAR14;\nfor (VAR7 = 1; VAR7 < VAR3->VAR8; VAR7++) {\nsize_t VAR16 = VAR3->VAR17[VAR7].VAR18;\nstruct VAR19 *VAR19;\nvoid *VAR20;\nif (VAR16 == 0 || VAR16 > VAR21) {\nVAR6 = -VAR22;\ngoto VAR13;\n}\nVAR20 = FUN8(VAR16);\nif (!VAR20) {\nVAR6 = -VAR12;\ngoto VAR13;\n}\nVAR19 = FUN9(VAR20);\nFUN10(VAR4, VAR7 - 1, VAR19,\nVAR20 - FUN11(VAR19), VAR16);\n}\nreturn VAR4;\nVAR13:\nFUN12(&VAR1->VAR11);\nreturn FUN2(VAR6);\n}\n",
      "code_after_change_raw": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\nsize_t len,\nconst struct iov_iter *it)\n{\nstruct sk_buff *skb;\nsize_t linear;\nint err;\nint i;\nif (it->nr_segs > MAX_SKB_FRAGS + 1 ||\nlen > (ETH_MAX_MTU - NET_SKB_PAD - NET_IP_ALIGN))\nreturn ERR_PTR(-EMSGSIZE);\nlocal_bh_disable();\nskb = napi_get_frags(&tfile->napi);\nlocal_bh_enable();\nif (!skb)\nreturn ERR_PTR(-ENOMEM);\nlinear = iov_iter_single_seg_count(it);\nerr = __skb_grow(skb, linear);\nif (err)\ngoto free;\nskb->len = len;\nskb->data_len = len - linear;\nskb->truesize += skb->data_len;\nfor (i = 1; i < it->nr_segs; i++) {\nsize_t fragsz = it->iov[i].iov_len;\nstruct page *page;\nvoid *frag;\nif (fragsz == 0 || fragsz > PAGE_SIZE) {\nerr = -EINVAL;\ngoto free;\n}\nfrag = netdev_alloc_frag(fragsz);\nif (!frag) {\nerr = -ENOMEM;\ngoto free;\n}\npage = virt_to_head_page(frag);\nskb_fill_page_desc(skb, i - 1, page,\nfrag - page_address(page), fragsz);\n}\nreturn skb;\nfree:\nnapi_free_frags(&tfile->napi);\nreturn ERR_PTR(err);\n}\n",
      "code_before_change_raw": "static struct sk_buff *tun_napi_alloc_frags(struct tun_file *tfile,\nsize_t len,\nconst struct iov_iter *it)\n{\nstruct sk_buff *skb;\nsize_t linear;\nint err;\nint i;\nif (it->nr_segs > MAX_SKB_FRAGS + 1)\nreturn ERR_PTR(-EMSGSIZE);\nlocal_bh_disable();\nskb = napi_get_frags(&tfile->napi);\nlocal_bh_enable();\nif (!skb)\nreturn ERR_PTR(-ENOMEM);\nlinear = iov_iter_single_seg_count(it);\nerr = __skb_grow(skb, linear);\nif (err)\ngoto free;\nskb->len = len;\nskb->data_len = len - linear;\nskb->truesize += skb->data_len;\nfor (i = 1; i < it->nr_segs; i++) {\nsize_t fragsz = it->iov[i].iov_len;\nstruct page *page;\nvoid *frag;\nif (fragsz == 0 || fragsz > PAGE_SIZE) {\nerr = -EINVAL;\ngoto free;\n}\nfrag = netdev_alloc_frag(fragsz);\nif (!frag) {\nerr = -ENOMEM;\ngoto free;\n}\npage = virt_to_head_page(frag);\nskb_fill_page_desc(skb, i - 1, page,\nfrag - page_address(page), fragsz);\n}\nreturn skb;\nfree:\nnapi_free_frags(&tfile->napi);\nreturn ERR_PTR(err);\n}\n"
    }
  ],
  "CVE-2023-4273": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper bounds checking when copying file name characters into a stack variable.",
        "trigger_condition": "A local privileged attacker exploits the lack of bounds checking to overflow the kernel stack by copying excessive file name characters.",
        "specific_code_behavior_causing_vulnerability": "The code snippet copies file name characters into a stack variable without verifying the length of each file name part or the total length of the reconstructed file name. This allows an attacker to overflow the stack by providing excessively long file name parts.",
        "solution": "To mitigate the vulnerability, the code was modified to include proper bounds checking. Specifically, the changes added tracking of the total length of the reconstructed file name and checks to ensure that the length of each file name part does not exceed a certain limit. Additionally, the loop termination condition was updated to prevent the stack overflow by checking the total length against a maximum allowed length."
      },
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2023-4273, which involves a stack overflow issue in the exFAT driver of the Linux kernel. The vulnerability arises from the file name reconstruction function, specifically from copying file name characters into a stack variable without proper bounds checking.\n\nTo fix this vulnerability, the following changes were made in the code:\n1. Added an additional variable `uni_len` to keep track of the total length of the file name being reconstructed.\n2. Added a variable `len` to store the length of the file name extracted from each entry.\n3. Checked if the length of the extracted file name is equal to `EXFAT_FILE_NAME_LEN` and if the total length `uni_len` exceeds a maximum allowed length `MAX_NAME_LENGTH`. If either condition is met, the loop is terminated to prevent a stack overflow.\n4. Incremented the `uni_len` by the length of each extracted file name to ensure proper tracking of the total length.\n5. Updated the loop termination condition to include the check for the total length exceeding the maximum allowed length.\n\nBy implementing these changes, the modified code snippet ensures that the file name reconstruction process is performed safely without risking a stack overflow due to unbounded copying of file name characters into a stack variable.",
      "GPT_purpose": "Extract Unicode file names from an extended directory entry in an exFAT file system.",
      "GPT_function": "\n1. Retrieve file name entries from a directory index.\n2. Extract Unicode file names from the retrieved entries.\n3. Handle the end of name entry and increment the Unicode file name pointer.",
      "CVE_id": "CVE-2023-4273",
      "code_before_change": "static int exfat_get_uniname_from_ext_entry(struct super_block *sb,\n\t\tstruct exfat_chain *p_dir, int entry, unsigned short *uniname)\n{\n\tint i, err;\n\tstruct exfat_entry_set_cache es;\n\n\terr = exfat_get_dentry_set(&es, sb, p_dir, entry, ES_ALL_ENTRIES);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * First entry  : file entry\n\t * Second entry : stream-extension entry\n\t * Third entry  : first file-name entry\n\t * So, the index of first file-name dentry should start from 2.\n\t */\n\tfor (i = ES_IDX_FIRST_FILENAME; i < es.num_entries; i++) {\n\t\tstruct exfat_dentry *ep = exfat_get_dentry_cached(&es, i);\n\n\t\t/* end of name entry */\n\t\tif (exfat_get_entry_type(ep) != TYPE_EXTEND)\n\t\t\tbreak;\n\n\t\texfat_extract_uni_name(ep, uniname);\n\t\tuniname += EXFAT_FILE_NAME_LEN;\n\t}\n\n\texfat_put_dentry_set(&es, false);\n\treturn 0;\n}",
      "code_after_change": "static int exfat_get_uniname_from_ext_entry(struct super_block *sb,\n\t\tstruct exfat_chain *p_dir, int entry, unsigned short *uniname)\n{\n\tint i, err;\n\tstruct exfat_entry_set_cache es;\n\tunsigned int uni_len = 0, len;\n\n\terr = exfat_get_dentry_set(&es, sb, p_dir, entry, ES_ALL_ENTRIES);\n\tif (err)\n\t\treturn err;\n\n\t/*\n\t * First entry  : file entry\n\t * Second entry : stream-extension entry\n\t * Third entry  : first file-name entry\n\t * So, the index of first file-name dentry should start from 2.\n\t */\n\tfor (i = ES_IDX_FIRST_FILENAME; i < es.num_entries; i++) {\n\t\tstruct exfat_dentry *ep = exfat_get_dentry_cached(&es, i);\n\n\t\t/* end of name entry */\n\t\tif (exfat_get_entry_type(ep) != TYPE_EXTEND)\n\t\t\tbreak;\n\n\t\tlen = exfat_extract_uni_name(ep, uniname);\n\t\tuni_len += len;\n\t\tif (len != EXFAT_FILE_NAME_LEN || uni_len >= MAX_NAME_LENGTH)\n\t\t\tbreak;\n\t\tuniname += EXFAT_FILE_NAME_LEN;\n\t}\n\n\texfat_put_dentry_set(&es, false);\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tunsigned int uni_len = 0, len;",
          "\t\tlen = exfat_extract_uni_name(ep, uniname);",
          "\t\tuni_len += len;",
          "\t\tif (len != EXFAT_FILE_NAME_LEN || uni_len >= MAX_NAME_LENGTH)",
          "\t\t\tbreak;"
        ],
        "deleted": [
          "\t\texfat_extract_uni_name(ep, uniname);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper bounds checking when copying file name characters into a stack variable.",
      "trigger_condition": "A local privileged attacker exploits the lack of bounds checking to overflow the kernel stack by copying excessive file name characters.",
      "specific_code_behavior_causing_vulnerability": "The code snippet copies file name characters into a stack variable without verifying the length of each file name part or the total length of the reconstructed file name. This allows an attacker to overflow the stack by providing excessively long file name parts.",
      "solution": "To mitigate the vulnerability, the code was modified to include proper bounds checking. Specifically, the changes added tracking of the total length of the reconstructed file name and checks to ensure that the length of each file name part does not exceed a certain limit. Additionally, the loop termination condition was updated to prevent the stack overflow by checking the total length against a maximum allowed length.",
      "id": 164,
      "code_after_change_normalized": "static int FUN1(struct super_block *VAR1,\nstruct exfat_chain *VAR2, int VAR3, unsigned short *VAR4)\n{\nint VAR5, VAR6;\nstruct exfat_entry_set_cache VAR7;\nunsigned int VAR8 = 0, VAR9;\nVAR6 = FUN2(&VAR7, VAR1, VAR2, VAR3, VAR10);\nif (VAR6)\nreturn VAR6;\nfor (VAR5 = VAR11; VAR5 < VAR7.VAR12; VAR5++) {\nstruct exfat_dentry *VAR13 = FUN3(&VAR7, VAR5);\nif (FUN4(VAR13) != VAR14)\nbreak;\nVAR9 = FUN5(VAR13, VAR4);\nVAR8 += VAR9;\nif (VAR9 != VAR15 || VAR8 >= VAR16)\nbreak;\nVAR4 += VAR15;\n}\nFUN6(&VAR7, false);\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct super_block *VAR1,\nstruct exfat_chain *VAR2, int VAR3, unsigned short *VAR4)\n{\nint VAR5, VAR6;\nstruct exfat_entry_set_cache VAR7;\nVAR6 = FUN2(&VAR7, VAR1, VAR2, VAR3, VAR8);\nif (VAR6)\nreturn VAR6;\nfor (VAR5 = VAR9; VAR5 < VAR7.VAR10; VAR5++) {\nstruct exfat_dentry *VAR11 = FUN3(&VAR7, VAR5);\nif (FUN4(VAR11) != VAR12)\nbreak;\nFUN5(VAR11, VAR4);\nVAR4 += VAR13;\n}\nFUN6(&VAR7, false);\nreturn 0;\n}\n",
      "code_after_change_raw": "static int exfat_get_uniname_from_ext_entry(struct super_block *sb,\nstruct exfat_chain *p_dir, int entry, unsigned short *uniname)\n{\nint i, err;\nstruct exfat_entry_set_cache es;\nunsigned int uni_len = 0, len;\nerr = exfat_get_dentry_set(&es, sb, p_dir, entry, ES_ALL_ENTRIES);\nif (err)\nreturn err;\nfor (i = ES_IDX_FIRST_FILENAME; i < es.num_entries; i++) {\nstruct exfat_dentry *ep = exfat_get_dentry_cached(&es, i);\nif (exfat_get_entry_type(ep) != TYPE_EXTEND)\nbreak;\nlen = exfat_extract_uni_name(ep, uniname);\nuni_len += len;\nif (len != EXFAT_FILE_NAME_LEN || uni_len >= MAX_NAME_LENGTH)\nbreak;\nuniname += EXFAT_FILE_NAME_LEN;\n}\nexfat_put_dentry_set(&es, false);\nreturn 0;\n}\n",
      "code_before_change_raw": "static int exfat_get_uniname_from_ext_entry(struct super_block *sb,\nstruct exfat_chain *p_dir, int entry, unsigned short *uniname)\n{\nint i, err;\nstruct exfat_entry_set_cache es;\nerr = exfat_get_dentry_set(&es, sb, p_dir, entry, ES_ALL_ENTRIES);\nif (err)\nreturn err;\nfor (i = ES_IDX_FIRST_FILENAME; i < es.num_entries; i++) {\nstruct exfat_dentry *ep = exfat_get_dentry_cached(&es, i);\nif (exfat_get_entry_type(ep) != TYPE_EXTEND)\nbreak;\nexfat_extract_uni_name(ep, uniname);\nuniname += EXFAT_FILE_NAME_LEN;\n}\nexfat_put_dentry_set(&es, false);\nreturn 0;\n}\n"
    }
  ],
  "CVE-2023-45863": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper synchronization in handling concurrent access to shared resources.",
        "trigger_condition": "With root access, an attacker can trigger a race condition during the execution of the fill_kobj_path function, leading to an out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code snippet lacks proper synchronization mechanisms to prevent concurrent access to shared resources, allowing an attacker to exploit a race condition and trigger an out-of-bounds write in the fill_kobj_path function."
      },
      "solution": "To mitigate the vulnerability, a retry mechanism is added to the code snippet. This retry mechanism ensures that if the fill_kobj_path function fails, the path allocation and filling process is retried until successful. By adding this retry mechanism, the code enhances its robustness against race conditions and prevents the out-of-bounds write vulnerability.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability described in CVE-2023-45863. The vulnerability involves a race condition that can lead to an out-of-bounds write in the `fill_kobj_path` function. By adding the retry mechanism in the modified code, the function will attempt to fill the `path` again if the `fill_kobj_path` function fails. This retry mechanism helps prevent the out-of-bounds write by ensuring that the `path` is correctly filled before returning it to the caller. This approach enhances the robustness of the code and reduces the risk of exploitation due to the race condition.",
      "GPT_purpose": "Retrieve the path of a given kernel object.",
      "GPT_function": "\n1. kobject_get_path: Retrieves the path of a kobject.\n2. get_kobj_path_length: Calculates the length of the kobject's path.\n3. fill_kobj_path: Fills the kobject's path into a buffer.",
      "CVE_id": "CVE-2023-45863",
      "code_before_change": "char *kobject_get_path(const struct kobject *kobj, gfp_t gfp_mask)\n{\n\tchar *path;\n\tint len;\n\n\tlen = get_kobj_path_length(kobj);\n\tif (len == 0)\n\t\treturn NULL;\n\tpath = kzalloc(len, gfp_mask);\n\tif (!path)\n\t\treturn NULL;\n\tfill_kobj_path(kobj, path, len);\n\n\treturn path;\n}",
      "code_after_change": "char *kobject_get_path(const struct kobject *kobj, gfp_t gfp_mask)\n{\n\tchar *path;\n\tint len;\n\nretry:\n\tlen = get_kobj_path_length(kobj);\n\tif (len == 0)\n\t\treturn NULL;\n\tpath = kzalloc(len, gfp_mask);\n\tif (!path)\n\t\treturn NULL;\n\tif (fill_kobj_path(kobj, path, len)) {\n\t\tkfree(path);\n\t\tgoto retry;\n\t}\n\n\treturn path;\n}",
      "modified_lines": {
        "added": [
          "retry:",
          "\tif (fill_kobj_path(kobj, path, len)) {",
          "\t\tkfree(path);",
          "\t\tgoto retry;",
          "\t}"
        ],
        "deleted": [
          "\tfill_kobj_path(kobj, path, len);"
        ]
      },
      "preconditions_for_vulnerability": "Lack of proper synchronization in handling concurrent access to shared resources.",
      "trigger_condition": "With root access, an attacker can trigger a race condition during the execution of the fill_kobj_path function, leading to an out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code snippet lacks proper synchronization mechanisms to prevent concurrent access to shared resources, allowing an attacker to exploit a race condition and trigger an out-of-bounds write in the fill_kobj_path function.",
      "id": 165,
      "code_after_change_normalized": "char *FUN1(const struct kobject *VAR1, gfp_t VAR2)\n{\nchar *VAR3;\nint VAR4;\nVAR5:\nVAR4 = FUN2(VAR1);\nif (VAR4 == 0)\nreturn NULL;\nVAR3 = FUN3(VAR4, VAR2);\nif (!VAR3)\nreturn NULL;\nif (FUN4(VAR1, VAR3, VAR4)) {\nFUN5(VAR3);\ngoto VAR5;\n}\nreturn VAR3;\n}\n",
      "code_before_change_normalized": "char *FUN1(const struct kobject *VAR1, gfp_t VAR2)\n{\nchar *VAR3;\nint VAR4;\nVAR4 = FUN2(VAR1);\nif (VAR4 == 0)\nreturn NULL;\nVAR3 = FUN3(VAR4, VAR2);\nif (!VAR3)\nreturn NULL;\nFUN4(VAR1, VAR3, VAR4);\nreturn VAR3;\n}\n",
      "code_after_change_raw": "char *kobject_get_path(const struct kobject *kobj, gfp_t gfp_mask)\n{\nchar *path;\nint len;\nretry:\nlen = get_kobj_path_length(kobj);\nif (len == 0)\nreturn NULL;\npath = kzalloc(len, gfp_mask);\nif (!path)\nreturn NULL;\nif (fill_kobj_path(kobj, path, len)) {\nkfree(path);\ngoto retry;\n}\nreturn path;\n}\n",
      "code_before_change_raw": "char *kobject_get_path(const struct kobject *kobj, gfp_t gfp_mask)\n{\nchar *path;\nint len;\nlen = get_kobj_path_length(kobj);\nif (len == 0)\nreturn NULL;\npath = kzalloc(len, gfp_mask);\nif (!path)\nreturn NULL;\nfill_kobj_path(kobj, path, len);\nreturn path;\n}\n"
    }
  ],
  "CVE-2023-5717": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper synchronization and bounds checking in handling sibling events within a group.",
        "trigger_condition": "Calling perf_read_group() while an event's sibling_list is smaller than its child's sibling_list, leading to potential heap out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code does not properly update the group_generation counter after removing a sibling event from its group, potentially allowing memory corruption and out-of-bounds writes if perf_read_group() is called at a critical moment."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure proper synchronization and bounds checking when handling sibling events within a group. Specifically, after removing a sibling event from its group, the group_generation counter should be updated to maintain consistency and prevent out-of-bounds writes. The solution involves adding the line 'event->group_leader->group_generation++;' after decrementing 'event->group_leader->nr_siblings--'. This change helps prevent memory corruption and ensures that memory locations outside the allocated buffer are not accessed improperly.",
      "GPT_analysis": "The modification in the code snippet is necessary to address a heap out-of-bounds write vulnerability in the Linux kernel's perf component. The vulnerability occurs when perf_read_group() is called while an event's sibling_list is smaller than its child's sibling_list, potentially leading to memory corruption and local privilege escalation.\n\nBy adding the line `event->group_leader->group_generation++;` after decrementing `event->group_leader->nr_siblings--`, the modification ensures that the group_generation counter is updated appropriately. This change helps prevent potential out-of-bounds writes and ensures that memory locations outside the allocated buffer are not accessed improperly, thereby mitigating the vulnerability described in CVE-2023-5717.",
      "GPT_purpose": "Detaches a perf event from its group and handles sibling events.",
      "GPT_function": "\n1. Detaching a perf event from a group.\n2. Handling double detach scenarios.\n3. Removing a sibling event from its group.\n4. Upgrading sibling events to singleton events.\n5. Inheriting group flags from the previous leader.\n6. Adding events to groups and updating their state.\n7. Checking and updating event contexts.\n8. Calculating header sizes for events.",
      "CVE_id": "CVE-2023-5717",
      "code_before_change": "static void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event *sibling, *tmp;\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\n\tperf_put_aux_event(event);\n\n\t/*\n\t * If this is a sibling, remove it from its group.\n\t */\n\tif (leader != event) {\n\t\tlist_del_init(&event->sibling_list);\n\t\tevent->group_leader->nr_siblings--;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If this was a group event with sibling events then\n\t * upgrade the siblings to singleton events by adding them\n\t * to whatever list we are on.\n\t */\n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\n\n\t\tif (sibling->event_caps & PERF_EV_CAP_SIBLING)\n\t\t\tperf_remove_sibling_event(sibling);\n\n\t\tsibling->group_leader = sibling;\n\t\tlist_del_init(&sibling->sibling_list);\n\n\t\t/* Inherit group flags from the previous leader */\n\t\tsibling->group_caps = event->group_caps;\n\n\t\tif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\n\t\t\tadd_event_to_groups(sibling, event->ctx);\n\n\t\t\tif (sibling->state == PERF_EVENT_STATE_ACTIVE)\n\t\t\t\tlist_add_tail(&sibling->active_list, get_event_list(sibling));\n\t\t}\n\n\t\tWARN_ON_ONCE(sibling->ctx != event->ctx);\n\t}\n\nout:\n\tfor_each_sibling_event(tmp, leader)\n\t\tperf_event__header_size(tmp);\n\n\tperf_event__header_size(leader);\n}",
      "code_after_change": "static void perf_group_detach(struct perf_event *event)\n{\n\tstruct perf_event *leader = event->group_leader;\n\tstruct perf_event *sibling, *tmp;\n\tstruct perf_event_context *ctx = event->ctx;\n\n\tlockdep_assert_held(&ctx->lock);\n\n\t/*\n\t * We can have double detach due to exit/hot-unplug + close.\n\t */\n\tif (!(event->attach_state & PERF_ATTACH_GROUP))\n\t\treturn;\n\n\tevent->attach_state &= ~PERF_ATTACH_GROUP;\n\n\tperf_put_aux_event(event);\n\n\t/*\n\t * If this is a sibling, remove it from its group.\n\t */\n\tif (leader != event) {\n\t\tlist_del_init(&event->sibling_list);\n\t\tevent->group_leader->nr_siblings--;\n\t\tevent->group_leader->group_generation++;\n\t\tgoto out;\n\t}\n\n\t/*\n\t * If this was a group event with sibling events then\n\t * upgrade the siblings to singleton events by adding them\n\t * to whatever list we are on.\n\t */\n\tlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\n\n\t\tif (sibling->event_caps & PERF_EV_CAP_SIBLING)\n\t\t\tperf_remove_sibling_event(sibling);\n\n\t\tsibling->group_leader = sibling;\n\t\tlist_del_init(&sibling->sibling_list);\n\n\t\t/* Inherit group flags from the previous leader */\n\t\tsibling->group_caps = event->group_caps;\n\n\t\tif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\n\t\t\tadd_event_to_groups(sibling, event->ctx);\n\n\t\t\tif (sibling->state == PERF_EVENT_STATE_ACTIVE)\n\t\t\t\tlist_add_tail(&sibling->active_list, get_event_list(sibling));\n\t\t}\n\n\t\tWARN_ON_ONCE(sibling->ctx != event->ctx);\n\t}\n\nout:\n\tfor_each_sibling_event(tmp, leader)\n\t\tperf_event__header_size(tmp);\n\n\tperf_event__header_size(leader);\n}",
      "modified_lines": {
        "added": [
          "\t\tevent->group_leader->group_generation++;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper synchronization and bounds checking in handling sibling events within a group.",
      "trigger_condition": "Calling perf_read_group() while an event's sibling_list is smaller than its child's sibling_list, leading to potential heap out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code does not properly update the group_generation counter after removing a sibling event from its group, potentially allowing memory corruption and out-of-bounds writes if perf_read_group() is called at a critical moment.",
      "id": 166,
      "code_after_change_normalized": "static void FUN1(struct perf_event *VAR1)\n{\nstruct perf_event *VAR2 = VAR1->VAR3;\nstruct perf_event *VAR4, *VAR5;\nstruct perf_event_context *VAR6 = VAR1->VAR6;\nFUN2(&VAR6->VAR7);\nif (!(VAR1->VAR8 & VAR9))\nreturn;\nVAR1->VAR8 &= ~VAR9;\nFUN3(VAR1);\nif (VAR2 != VAR1) {\nFUN4(&VAR1->VAR10);\nVAR1->VAR3->VAR11--;\nVAR1->VAR3->VAR12++;\ngoto VAR13;\n}\nFUN5(VAR4, VAR5, &VAR1->VAR10, VAR10) {\nif (VAR4->VAR14 & VAR15)\nFUN6(VAR4);\nVAR4->VAR3 = VAR4;\nFUN4(&VAR4->VAR10);\nVAR4->VAR16 = VAR1->VAR16;\nif (VAR4->VAR8 & VAR17) {\nFUN7(VAR4, VAR1->VAR6);\nif (VAR4->VAR18 == VAR19)\nFUN8(&VAR4->VAR20, FUN9(VAR4));\n}\nFUN10(VAR4->VAR6 != VAR1->VAR6);\n}\nVAR13:\nFUN11(VAR5, VAR2)\nFUN12(VAR5);\nFUN12(VAR2);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct perf_event *VAR1)\n{\nstruct perf_event *VAR2 = VAR1->VAR3;\nstruct perf_event *VAR4, *VAR5;\nstruct perf_event_context *VAR6 = VAR1->VAR6;\nFUN2(&VAR6->VAR7);\nif (!(VAR1->VAR8 & VAR9))\nreturn;\nVAR1->VAR8 &= ~VAR9;\nFUN3(VAR1);\nif (VAR2 != VAR1) {\nFUN4(&VAR1->VAR10);\nVAR1->VAR3->VAR11--;\ngoto VAR12;\n}\nFUN5(VAR4, VAR5, &VAR1->VAR10, VAR10) {\nif (VAR4->VAR13 & VAR14)\nFUN6(VAR4);\nVAR4->VAR3 = VAR4;\nFUN4(&VAR4->VAR10);\nVAR4->VAR15 = VAR1->VAR15;\nif (VAR4->VAR8 & VAR16) {\nFUN7(VAR4, VAR1->VAR6);\nif (VAR4->VAR17 == VAR18)\nFUN8(&VAR4->VAR19, FUN9(VAR4));\n}\nFUN10(VAR4->VAR6 != VAR1->VAR6);\n}\nVAR12:\nFUN11(VAR5, VAR2)\nFUN12(VAR5);\nFUN12(VAR2);\n}\n",
      "code_after_change_raw": "static void perf_group_detach(struct perf_event *event)\n{\nstruct perf_event *leader = event->group_leader;\nstruct perf_event *sibling, *tmp;\nstruct perf_event_context *ctx = event->ctx;\nlockdep_assert_held(&ctx->lock);\nif (!(event->attach_state & PERF_ATTACH_GROUP))\nreturn;\nevent->attach_state &= ~PERF_ATTACH_GROUP;\nperf_put_aux_event(event);\nif (leader != event) {\nlist_del_init(&event->sibling_list);\nevent->group_leader->nr_siblings--;\nevent->group_leader->group_generation++;\ngoto out;\n}\nlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\nif (sibling->event_caps & PERF_EV_CAP_SIBLING)\nperf_remove_sibling_event(sibling);\nsibling->group_leader = sibling;\nlist_del_init(&sibling->sibling_list);\nsibling->group_caps = event->group_caps;\nif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\nadd_event_to_groups(sibling, event->ctx);\nif (sibling->state == PERF_EVENT_STATE_ACTIVE)\nlist_add_tail(&sibling->active_list, get_event_list(sibling));\n}\nWARN_ON_ONCE(sibling->ctx != event->ctx);\n}\nout:\nfor_each_sibling_event(tmp, leader)\nperf_event__header_size(tmp);\nperf_event__header_size(leader);\n}\n",
      "code_before_change_raw": "static void perf_group_detach(struct perf_event *event)\n{\nstruct perf_event *leader = event->group_leader;\nstruct perf_event *sibling, *tmp;\nstruct perf_event_context *ctx = event->ctx;\nlockdep_assert_held(&ctx->lock);\nif (!(event->attach_state & PERF_ATTACH_GROUP))\nreturn;\nevent->attach_state &= ~PERF_ATTACH_GROUP;\nperf_put_aux_event(event);\nif (leader != event) {\nlist_del_init(&event->sibling_list);\nevent->group_leader->nr_siblings--;\ngoto out;\n}\nlist_for_each_entry_safe(sibling, tmp, &event->sibling_list, sibling_list) {\nif (sibling->event_caps & PERF_EV_CAP_SIBLING)\nperf_remove_sibling_event(sibling);\nsibling->group_leader = sibling;\nlist_del_init(&sibling->sibling_list);\nsibling->group_caps = event->group_caps;\nif (sibling->attach_state & PERF_ATTACH_CONTEXT) {\nadd_event_to_groups(sibling, event->ctx);\nif (sibling->state == PERF_EVENT_STATE_ACTIVE)\nlist_add_tail(&sibling->active_list, get_event_list(sibling));\n}\nWARN_ON_ONCE(sibling->ctx != event->ctx);\n}\nout:\nfor_each_sibling_event(tmp, leader)\nperf_event__header_size(tmp);\nperf_event__header_size(leader);\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of sibling_list sizes in the perf_read_group() function.",
        "trigger_condition": "Calling perf_read_group() while an event's sibling_list is smaller than its child's sibling_list, leading to a heap out-of-bounds write vulnerability.",
        "specific_code_behavior_causing_vulnerability": "The code in the inherit_group() function does not properly handle the sizes of sibling_lists, allowing for potential out-of-bounds writes when perf_read_group() is called under specific conditions."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that memory operations within the function are within bounds. This involves setting leader->group_generation to parent_event->group_generation at the end of the inherit_group() function to prevent unintended memory access beyond the allocated buffer boundaries. By doing so, the code addresses the heap out-of-bounds write vulnerability and enhances the security of the memory operations.",
      "GPT_analysis": "The modification is necessary to address the heap out-of-bounds write vulnerability in the Linux kernel's perf component. By setting the `leader->group_generation` to `parent_event->group_generation` at the end of the `inherit_group` function, you ensure that the memory locations accessed within the allocated buffer are within bounds and prevent any potential out-of-bounds writes that could lead to local privilege escalation.\n\nThis modification helps to maintain the integrity and security of the memory operations within the function, ensuring that the sibling_list sizes are properly handled to prevent any unintended memory access beyond the allocated buffer boundaries.",
      "GPT_purpose": "Inheriting performance events from a parent task to a child task within a perf event context.",
      "GPT_function": "\n1. Inherit group function to handle event inheritance between parent and child tasks.\n2. Check for orphaned events during event inheritance.\n3. Iterate through sibling events to inherit them for the child task.",
      "CVE_id": "CVE-2023-5717",
      "code_before_change": "static int inherit_group(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *leader;\n\tstruct perf_event *sub;\n\tstruct perf_event *child_ctr;\n\n\tleader = inherit_event(parent_event, parent, parent_ctx,\n\t\t\t\t child, NULL, child_ctx);\n\tif (IS_ERR(leader))\n\t\treturn PTR_ERR(leader);\n\t/*\n\t * @leader can be NULL here because of is_orphaned_event(). In this\n\t * case inherit_event() will create individual events, similar to what\n\t * perf_group_detach() would do anyway.\n\t */\n\tfor_each_sibling_event(sub, parent_event) {\n\t\tchild_ctr = inherit_event(sub, parent, parent_ctx,\n\t\t\t\t\t    child, leader, child_ctx);\n\t\tif (IS_ERR(child_ctr))\n\t\t\treturn PTR_ERR(child_ctr);\n\n\t\tif (sub->aux_event == parent_event && child_ctr &&\n\t\t    !perf_get_aux_event(child_ctr, leader))\n\t\t\treturn -EINVAL;\n\t}\n\treturn 0;\n}",
      "code_after_change": "static int inherit_group(struct perf_event *parent_event,\n\t      struct task_struct *parent,\n\t      struct perf_event_context *parent_ctx,\n\t      struct task_struct *child,\n\t      struct perf_event_context *child_ctx)\n{\n\tstruct perf_event *leader;\n\tstruct perf_event *sub;\n\tstruct perf_event *child_ctr;\n\n\tleader = inherit_event(parent_event, parent, parent_ctx,\n\t\t\t\t child, NULL, child_ctx);\n\tif (IS_ERR(leader))\n\t\treturn PTR_ERR(leader);\n\t/*\n\t * @leader can be NULL here because of is_orphaned_event(). In this\n\t * case inherit_event() will create individual events, similar to what\n\t * perf_group_detach() would do anyway.\n\t */\n\tfor_each_sibling_event(sub, parent_event) {\n\t\tchild_ctr = inherit_event(sub, parent, parent_ctx,\n\t\t\t\t\t    child, leader, child_ctx);\n\t\tif (IS_ERR(child_ctr))\n\t\t\treturn PTR_ERR(child_ctr);\n\n\t\tif (sub->aux_event == parent_event && child_ctr &&\n\t\t    !perf_get_aux_event(child_ctr, leader))\n\t\t\treturn -EINVAL;\n\t}\n\tleader->group_generation = parent_event->group_generation;\n\treturn 0;\n}",
      "modified_lines": {
        "added": [
          "\tleader->group_generation = parent_event->group_generation;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Improper handling of sibling_list sizes in the perf_read_group() function.",
      "trigger_condition": "Calling perf_read_group() while an event's sibling_list is smaller than its child's sibling_list, leading to a heap out-of-bounds write vulnerability.",
      "specific_code_behavior_causing_vulnerability": "The code in the inherit_group() function does not properly handle the sizes of sibling_lists, allowing for potential out-of-bounds writes when perf_read_group() is called under specific conditions.",
      "id": 167,
      "code_after_change_normalized": "static int FUN1(struct perf_event *VAR1,\nstruct task_struct *VAR2,\nstruct perf_event_context *VAR3,\nstruct task_struct *VAR4,\nstruct perf_event_context *VAR5)\n{\nstruct perf_event *VAR6;\nstruct perf_event *VAR7;\nstruct perf_event *VAR8;\nVAR6 = FUN2(VAR1, VAR2, VAR3,\nVAR4, NULL, VAR5);\nif (FUN3(VAR6))\nreturn FUN4(VAR6);\nFUN5(VAR7, VAR1) {\nVAR8 = FUN2(VAR7, VAR2, VAR3,\nVAR4, VAR6, VAR5);\nif (FUN3(VAR8))\nreturn FUN4(VAR8);\nif (VAR7->VAR9 == VAR1 && VAR8 &&\n!FUN6(VAR8, VAR6))\nreturn -VAR10;\n}\nVAR6->VAR11 = VAR1->VAR11;\nreturn 0;\n}\n",
      "code_before_change_normalized": "static int FUN1(struct perf_event *VAR1,\nstruct task_struct *VAR2,\nstruct perf_event_context *VAR3,\nstruct task_struct *VAR4,\nstruct perf_event_context *VAR5)\n{\nstruct perf_event *VAR6;\nstruct perf_event *VAR7;\nstruct perf_event *VAR8;\nVAR6 = FUN2(VAR1, VAR2, VAR3,\nVAR4, NULL, VAR5);\nif (FUN3(VAR6))\nreturn FUN4(VAR6);\nFUN5(VAR7, VAR1) {\nVAR8 = FUN2(VAR7, VAR2, VAR3,\nVAR4, VAR6, VAR5);\nif (FUN3(VAR8))\nreturn FUN4(VAR8);\nif (VAR7->VAR9 == VAR1 && VAR8 &&\n!FUN6(VAR8, VAR6))\nreturn -VAR10;\n}\nreturn 0;\n}\n",
      "code_after_change_raw": "static int inherit_group(struct perf_event *parent_event,\nstruct task_struct *parent,\nstruct perf_event_context *parent_ctx,\nstruct task_struct *child,\nstruct perf_event_context *child_ctx)\n{\nstruct perf_event *leader;\nstruct perf_event *sub;\nstruct perf_event *child_ctr;\nleader = inherit_event(parent_event, parent, parent_ctx,\nchild, NULL, child_ctx);\nif (IS_ERR(leader))\nreturn PTR_ERR(leader);\nfor_each_sibling_event(sub, parent_event) {\nchild_ctr = inherit_event(sub, parent, parent_ctx,\nchild, leader, child_ctx);\nif (IS_ERR(child_ctr))\nreturn PTR_ERR(child_ctr);\nif (sub->aux_event == parent_event && child_ctr &&\n!perf_get_aux_event(child_ctr, leader))\nreturn -EINVAL;\n}\nleader->group_generation = parent_event->group_generation;\nreturn 0;\n}\n",
      "code_before_change_raw": "static int inherit_group(struct perf_event *parent_event,\nstruct task_struct *parent,\nstruct perf_event_context *parent_ctx,\nstruct task_struct *child,\nstruct perf_event_context *child_ctx)\n{\nstruct perf_event *leader;\nstruct perf_event *sub;\nstruct perf_event *child_ctr;\nleader = inherit_event(parent_event, parent, parent_ctx,\nchild, NULL, child_ctx);\nif (IS_ERR(leader))\nreturn PTR_ERR(leader);\nfor_each_sibling_event(sub, parent_event) {\nchild_ctr = inherit_event(sub, parent, parent_ctx,\nchild, leader, child_ctx);\nif (IS_ERR(child_ctr))\nreturn PTR_ERR(child_ctr);\nif (sub->aux_event == parent_event && child_ctr &&\n!perf_get_aux_event(child_ctr, leader))\nreturn -EINVAL;\n}\nreturn 0;\n}\n"
    },
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Lack of proper synchronization and boundary checking when manipulating sibling event lists in the perf component.",
        "trigger_condition": "Calling perf_read_group() while an event's sibling_list is smaller than its child's sibling_list, leading to potential heap out-of-bounds write.",
        "specific_code_behavior_causing_vulnerability": "The code fails to ensure proper alignment and bounds checking when adding events to the sibling list, allowing for potential memory corruption and local privilege escalation."
      },
      "solution": "To mitigate the vulnerability, it is necessary to increment the group_generation field when adding a new event to the sibling list. This ensures that memory operations stay within the allocated buffer and prevents out-of-bounds writes. By updating the group_generation field, the code aligns the sibling_list properly and reduces the risk of memory corruption. The solution involves modifying the code to include the increment of the group_generation field when adding events to the sibling list.",
      "GPT_analysis": "The modification is necessary to address the vulnerability CVE-2023-5717, which involves a heap out-of-bounds write in the Linux kernel's perf component. By incrementing the `group_generation` field in `group_leader`, the modification ensures that the sibling_list is properly aligned and prevents potential memory corruption that could lead to local privilege escalation.\n\nBy updating the `group_generation` field when adding a new event to the sibling list, the code ensures that memory operations stay within the allocated buffer and prevents any out-of-bounds writes that could be exploited by an attacker. This modification helps to mitigate the vulnerability and improve the security of the code.",
      "GPT_purpose": "Attaching a perf event to a group leader event in the Linux kernel's perf component.",
      "GPT_function": "\n1. Attaching a perf event to a group leader.\n2. Checking for double attachment due to group movement.\n3. Updating group leader's group capabilities based on event capabilities.\n4. Adding the event to the group leader's sibling list.\n5. Incrementing the number of siblings for the group leader.\n6. Updating header size for the group leader and its siblings.",
      "CVE_id": "CVE-2023-5717",
      "code_before_change": "static void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\n\tlockdep_assert_held(&event->ctx->lock);\n\n\t/*\n\t * We can have double attach due to group movement (move_group) in\n\t * perf_event_open().\n\t */\n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\n\tif (group_leader == event)\n\t\treturn;\n\n\tWARN_ON_ONCE(group_leader->ctx != event->ctx);\n\n\tgroup_leader->group_caps &= event->event_caps;\n\n\tlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\n\tperf_event__header_size(group_leader);\n\n\tfor_each_sibling_event(pos, group_leader)\n\t\tperf_event__header_size(pos);\n}",
      "code_after_change": "static void perf_group_attach(struct perf_event *event)\n{\n\tstruct perf_event *group_leader = event->group_leader, *pos;\n\n\tlockdep_assert_held(&event->ctx->lock);\n\n\t/*\n\t * We can have double attach due to group movement (move_group) in\n\t * perf_event_open().\n\t */\n\tif (event->attach_state & PERF_ATTACH_GROUP)\n\t\treturn;\n\n\tevent->attach_state |= PERF_ATTACH_GROUP;\n\n\tif (group_leader == event)\n\t\treturn;\n\n\tWARN_ON_ONCE(group_leader->ctx != event->ctx);\n\n\tgroup_leader->group_caps &= event->event_caps;\n\n\tlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\n\tgroup_leader->nr_siblings++;\n\tgroup_leader->group_generation++;\n\n\tperf_event__header_size(group_leader);\n\n\tfor_each_sibling_event(pos, group_leader)\n\t\tperf_event__header_size(pos);\n}",
      "modified_lines": {
        "added": [
          "\tgroup_leader->group_generation++;"
        ],
        "deleted": []
      },
      "preconditions_for_vulnerability": "Lack of proper synchronization and boundary checking when manipulating sibling event lists in the perf component.",
      "trigger_condition": "Calling perf_read_group() while an event's sibling_list is smaller than its child's sibling_list, leading to potential heap out-of-bounds write.",
      "specific_code_behavior_causing_vulnerability": "The code fails to ensure proper alignment and bounds checking when adding events to the sibling list, allowing for potential memory corruption and local privilege escalation.",
      "id": 168,
      "code_after_change_normalized": "static void FUN1(struct perf_event *VAR1)\n{\nstruct perf_event *VAR2 = VAR1->VAR2, *VAR3;\nFUN2(&VAR1->VAR4->VAR5);\nif (VAR1->VAR6 & VAR7)\nreturn;\nVAR1->VAR6 |= VAR7;\nif (VAR2 == VAR1)\nreturn;\nFUN3(VAR2->VAR4 != VAR1->VAR4);\nVAR2->VAR8 &= VAR1->VAR9;\nFUN4(&VAR1->VAR10, &VAR2->VAR10);\nVAR2->VAR11++;\nVAR2->VAR12++;\nFUN5(VAR2);\nFUN6(VAR3, VAR2)\nFUN5(VAR3);\n}\n",
      "code_before_change_normalized": "static void FUN1(struct perf_event *VAR1)\n{\nstruct perf_event *VAR2 = VAR1->VAR2, *VAR3;\nFUN2(&VAR1->VAR4->VAR5);\nif (VAR1->VAR6 & VAR7)\nreturn;\nVAR1->VAR6 |= VAR7;\nif (VAR2 == VAR1)\nreturn;\nFUN3(VAR2->VAR4 != VAR1->VAR4);\nVAR2->VAR8 &= VAR1->VAR9;\nFUN4(&VAR1->VAR10, &VAR2->VAR10);\nVAR2->VAR11++;\nFUN5(VAR2);\nFUN6(VAR3, VAR2)\nFUN5(VAR3);\n}\n",
      "code_after_change_raw": "static void perf_group_attach(struct perf_event *event)\n{\nstruct perf_event *group_leader = event->group_leader, *pos;\nlockdep_assert_held(&event->ctx->lock);\nif (event->attach_state & PERF_ATTACH_GROUP)\nreturn;\nevent->attach_state |= PERF_ATTACH_GROUP;\nif (group_leader == event)\nreturn;\nWARN_ON_ONCE(group_leader->ctx != event->ctx);\ngroup_leader->group_caps &= event->event_caps;\nlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\ngroup_leader->nr_siblings++;\ngroup_leader->group_generation++;\nperf_event__header_size(group_leader);\nfor_each_sibling_event(pos, group_leader)\nperf_event__header_size(pos);\n}\n",
      "code_before_change_raw": "static void perf_group_attach(struct perf_event *event)\n{\nstruct perf_event *group_leader = event->group_leader, *pos;\nlockdep_assert_held(&event->ctx->lock);\nif (event->attach_state & PERF_ATTACH_GROUP)\nreturn;\nevent->attach_state |= PERF_ATTACH_GROUP;\nif (group_leader == event)\nreturn;\nWARN_ON_ONCE(group_leader->ctx != event->ctx);\ngroup_leader->group_caps &= event->event_caps;\nlist_add_tail(&event->sibling_list, &group_leader->sibling_list);\ngroup_leader->nr_siblings++;\nperf_event__header_size(group_leader);\nfor_each_sibling_event(pos, group_leader)\nperf_event__header_size(pos);\n}\n"
    }
  ],
  "CVE-2023-6931": [
    {
      "vulnerability_behavior": {
        "preconditions_for_vulnerability": "Improper handling of the read_size field calculation in the perf_event structure, leading to a potential overflow.",
        "trigger_condition": "The read_size field of a perf_event structure overflows, causing a heap out-of-bounds write issue in the perf_read_group() function.",
        "specific_code_behavior_causing_vulnerability": "The code snippet does not properly validate the read_size field of perf_event structures and their siblings, allowing it to potentially exceed the 64k limit on records and leading to a heap out-of-bounds write vulnerability."
      },
      "solution": "To mitigate the vulnerability, it is necessary to ensure that the read_size calculation for perf_event structures and their siblings does not exceed the 64k limit on records. The solution involves validating the read_size against the limit and iterating over sibling events to check their read_size as well. By implementing these checks, the code prevents the overflow of the read_size field and mitigates the heap out-of-bounds write vulnerability.",
      "GPT_analysis": "The modification in the code snippet is necessary to address the vulnerability CVE-2023-6931, which involves a heap out-of-bounds write issue in the Linux kernel's Performance Events system component. The vulnerability occurs due to the potential overflow of the `read_size` field of a `perf_event` structure, leading to a heap out-of-bounds increment or write in the `perf_read_group()` function.\n\nThe corrected code snippet introduces changes to the `perf_event_validate_size()` function to prevent the overflow of the `read_size` field. By checking the `read_size` against the limit of 16*1024 (64k limit on records), the modified code ensures that the `read_size` calculation does not exceed the allowed bounds. Additionally, the modification includes iterating over sibling events to validate their `read_size` as well, ensuring that all related events are within the size limit.\n\nOverall, the changes made in the modified code snippet help mitigate the vulnerability by properly validating the `read_size` of `perf_event` structures and their siblings, thereby preventing the heap out-of-bounds write issue and enhancing the security of the system.",
      "GPT_purpose": "Validate the size of a perf_event to ensure it does not exceed a 64k limit on records.",
      "GPT_function": "\n1. perf_event_validate_size function validates the size of a perf_event.\n2. __perf_event_read_size function computes the read size for the event.\n3. __perf_event_header_size function computes the header size for the event.\n4. perf_event__id_header_size function computes the ID header size for the event.\n5. It checks if the sum of read size, header size, ID header size, and the size of struct perf_event_header exceeds the 64k limit.\n6. It returns true if the sum is within the limit and false otherwise.",
      "CVE_id": "CVE-2023-6931",
      "code_before_change": "static bool perf_event_validate_size(struct perf_event *event)\n{\n\t/*\n\t * The values computed here will be over-written when we actually\n\t * attach the event.\n\t */\n\t__perf_event_read_size(event, event->group_leader->nr_siblings + 1);\n\t__perf_event_header_size(event, event->attr.sample_type & ~PERF_SAMPLE_READ);\n\tperf_event__id_header_size(event);\n\n\t/*\n\t * Sum the lot; should not exceed the 64k limit we have on records.\n\t * Conservative limit to allow for callchains and other variable fields.\n\t */\n\tif (event->read_size + event->header_size +\n\t    event->id_header_size + sizeof(struct perf_event_header) >= 16*1024)\n\t\treturn false;\n\n\treturn true;\n}",
      "code_after_change": "static bool perf_event_validate_size(struct perf_event *event)\n{\n\tstruct perf_event *sibling, *group_leader = event->group_leader;\n\n\tif (__perf_event_read_size(event->attr.read_format,\n\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)\n\t\treturn false;\n\n\tif (__perf_event_read_size(group_leader->attr.read_format,\n\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)\n\t\treturn false;\n\n\tfor_each_sibling_event(sibling, group_leader) {\n\t\tif (__perf_event_read_size(sibling->attr.read_format,\n\t\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)\n\t\t\treturn false;\n\t}\n\n\treturn true;\n}",
      "modified_lines": {
        "added": [
          "\tstruct perf_event *sibling, *group_leader = event->group_leader;",
          "\tif (__perf_event_read_size(event->attr.read_format,",
          "\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)",
          "",
          "\tif (__perf_event_read_size(group_leader->attr.read_format,",
          "\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)",
          "\t\treturn false;",
          "",
          "\tfor_each_sibling_event(sibling, group_leader) {",
          "\t\tif (__perf_event_read_size(sibling->attr.read_format,",
          "\t\t\t\t\t   group_leader->nr_siblings + 1) > 16*1024)",
          "\t\t\treturn false;",
          "\t}"
        ],
        "deleted": [
          "\t/*",
          "\t * The values computed here will be over-written when we actually",
          "\t * attach the event.",
          "\t */",
          "\t__perf_event_read_size(event, event->group_leader->nr_siblings + 1);",
          "\t__perf_event_header_size(event, event->attr.sample_type & ~PERF_SAMPLE_READ);",
          "\tperf_event__id_header_size(event);",
          "\t/*",
          "\t * Sum the lot; should not exceed the 64k limit we have on records.",
          "\t * Conservative limit to allow for callchains and other variable fields.",
          "\t */",
          "\tif (event->read_size + event->header_size +",
          "\t    event->id_header_size + sizeof(struct perf_event_header) >= 16*1024)"
        ]
      },
      "preconditions_for_vulnerability": "Improper handling of the read_size field calculation in the perf_event structure, leading to a potential overflow.",
      "trigger_condition": "The read_size field of a perf_event structure overflows, causing a heap out-of-bounds write issue in the perf_read_group() function.",
      "specific_code_behavior_causing_vulnerability": "The code snippet does not properly validate the read_size field of perf_event structures and their siblings, allowing it to potentially exceed the 64k limit on records and leading to a heap out-of-bounds write vulnerability.",
      "id": 169,
      "code_after_change_normalized": "static bool FUN1(struct perf_event *VAR1)\n{\nstruct perf_event *VAR2, *VAR3 = VAR1->VAR3;\nif (FUN2(VAR1->VAR4.VAR5,\nVAR3->VAR6 + 1) > 16*1024)\nreturn false;\nif (FUN2(VAR3->VAR4.VAR5,\nVAR3->VAR6 + 1) > 16*1024)\nreturn false;\nFUN3(VAR2, VAR3) {\nif (FUN2(VAR2->VAR4.VAR5,\nVAR3->VAR6 + 1) > 16*1024)\nreturn false;\n}\nreturn true;\n}\n",
      "code_before_change_normalized": "static bool FUN1(struct perf_event *VAR1)\n{\nFUN2(VAR1, VAR1->VAR2->VAR3 + 1);\nFUN3(VAR1, VAR1->VAR4.VAR5 & ~VAR6);\nFUN4(VAR1);\nif (VAR1->VAR7 + VAR1->VAR8 +\nVAR1->VAR9 + sizeof(struct VAR10) >= 16*1024)\nreturn false;\nreturn true;\n}\n",
      "code_after_change_raw": "static bool perf_event_validate_size(struct perf_event *event)\n{\nstruct perf_event *sibling, *group_leader = event->group_leader;\nif (__perf_event_read_size(event->attr.read_format,\ngroup_leader->nr_siblings + 1) > 16*1024)\nreturn false;\nif (__perf_event_read_size(group_leader->attr.read_format,\ngroup_leader->nr_siblings + 1) > 16*1024)\nreturn false;\nfor_each_sibling_event(sibling, group_leader) {\nif (__perf_event_read_size(sibling->attr.read_format,\ngroup_leader->nr_siblings + 1) > 16*1024)\nreturn false;\n}\nreturn true;\n}\n",
      "code_before_change_raw": "static bool perf_event_validate_size(struct perf_event *event)\n{\n__perf_event_read_size(event, event->group_leader->nr_siblings + 1);\n__perf_event_header_size(event, event->attr.sample_type & ~PERF_SAMPLE_READ);\nperf_event__id_header_size(event);\nif (event->read_size + event->header_size +\nevent->id_header_size + sizeof(struct perf_event_header) >= 16*1024)\nreturn false;\nreturn true;\n}\n"
    }
  ]
}